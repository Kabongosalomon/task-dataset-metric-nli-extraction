<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Meta AI Research 2 Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Duval</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Meta AI Research 2 Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Seessel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Meta AI Research 2 Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Meta AI Research 2 Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Meta AI Research 2 Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Meta AI Research 2 Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Meta AI Research 2 Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Meta AI Research 2 Inria</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Self-supervised training on diverse, real, and unfiltered internet data leads to interesting properties emerging like geolocalization, fairness, multilingual hashtag embeddings, artistic and better semantic information. See supplemental material for license information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Discriminative self-supervised learning allows training models on any random group of internet images, and possibly recover salient information that helps differentiate between the images. Applied to ImageNet, this leads to objectcentric features that perform on par with supervised features on most object-centric downstream tasks. In this work, we question if using this ability, we can learn any salient and more representative information present in diverse unbounded set of images from across the globe. To do so, we train models on billions of random images without any data pre-processing or prior assumptions about what we want the model to learn. We scale our model size to dense 10 billion parameters to avoid underfitting on a large data size. We extensively study and validate our model performance on over 50 benchmarks including fairness, ro-bustness to distribution shift, geographical diversity, fine grained recognition, image copy detection and many image classification datasets. The resulting model, not only captures well semantic information, it also captures information about artistic style and learns salient information such as geolocations and multilingual word embeddings based on visual content only. More importantly, we discover that such model is more robust, more fair, less harmful and less biased than supervised models or models trained on objectcentric datasets such as ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the span of a few years, self-supervised learning has surpassed supervised methods as a way to pretrain neural networks <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b23">21,</ref><ref type="bibr" target="#b55">52,</ref><ref type="bibr" target="#b60">57]</ref>. At the core of this success lies discriminative approaches that learn by differentiate between images <ref type="bibr" target="#b40">[38,</ref><ref type="bibr" target="#b135">128]</ref> or clusters of images <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b18">16]</ref>. Despite little assumptions made by these methods on the underlying factors of variations in the data, they produces features that are general enough to be re-used as they are in a variety of supervised tasks. While this has been widely studied in the context of object-centric benchmarks, like Im-ageNet <ref type="bibr" target="#b110">[105]</ref> or COCO <ref type="bibr" target="#b84">[81]</ref>, we conjecture that this property is more general and could allow to recover any factor of variation in a given distribution of images. In other words, this property can be leveraged to "discover" properties in uncurated datasets of images.</p><p>These properties that a self-supervised model may discover depend on the factors of variation contained in the training data <ref type="bibr" target="#b16">[14]</ref>. For instance, learning features on objectcentric dataset will produce features that have object-centric properties <ref type="bibr" target="#b21">[19]</ref>, while training them in the wild may contain information that are related to people's general interests. While some of these signals may be related to metadata -e.g., hashtags, GPS coordinate -or semantic information about scenes or objects, other factors may be related to human-centric properties -e.g., fairness, artistic stylethat are harder to annotate automatically. In this work, we are interested in probing which of the properties emerge in visual features trained with no supervision on as many images from across the world as possible.</p><p>A difficulty with training models on images in the wild is the absence of control on the distribution of images, e.g., the data likely has concepts that are dis-proportionally represented compared to others. This means that an underparameterized network may underfit and only learn the most predominant concepts. For instance, studies <ref type="bibr" target="#b50">[47]</ref> show that even a 1 billion parameter model saturates after 32M images, and do not extract more information when trained on billion of images. Even without these difficulties, learning the diversity of concepts in images from billions of people around the world requires significantly larger models than what is deployed for training on ImageNet scale.</p><p>In this work, we question the limits of what can be learned on such data by further increasing the capacity of pretrained models to 10billion dense parameters. We address some of the engineering challenges and complexity of training at this scale and thoroughly evaluate the resulting model on in-domain problems as well as on out-ofdomain benchmarks. Unsurprisingly, the resulting network learn features that are superior to smaller models trained on the same data on standard benchmarks. More interestingly though, on in-domain benchmarks, we observe that some properties of the features captured by the larger model was far less present in smaller model. In particular, one of our key empirical findings is that self-supervised learning on random internet data leads to models that are more fair, less biased and less harmful. Second, we observe that our model is also able to leverage the diversity of concepts in the dataset to train more robust features, leading to better out-of-distribution generalization. We thoroughly study this finding on a variety of benchmarks to understand what may explain this property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised Training of Visual Features. Unsupervised feature learning has a long history in computer vision, and many approaches have been explored in this space. Initially, methods using a reconstruction loss have been explored with the use of autoencoders <ref type="bibr" target="#b107">[102,</ref><ref type="bibr" target="#b129">122]</ref>. More recently, a similar paradigm has been used in the context of masked-patch-prediction models <ref type="bibr" target="#b5">[4,</ref><ref type="bibr" target="#b59">56,</ref><ref type="bibr" target="#b139">132]</ref>, showing that scalable pre-training can be achieved. Alternatively, many creative pretext tasks have also been proposed, showing that good features can be trained that way <ref type="bibr" target="#b2">[1,</ref><ref type="bibr" target="#b36">34,</ref><ref type="bibr" target="#b68">65,</ref><ref type="bibr" target="#b73">70,</ref><ref type="bibr" target="#b80">77,</ref><ref type="bibr" target="#b88">85,</ref><ref type="bibr" target="#b92">89,</ref><ref type="bibr" target="#b93">90,</ref><ref type="bibr" target="#b101">96,</ref><ref type="bibr" target="#b102">97,</ref><ref type="bibr" target="#b131">124,</ref><ref type="bibr" target="#b132">125,</ref><ref type="bibr" target="#b150">143]</ref>. A popular trend was using instance discrimination <ref type="bibr" target="#b15">[13,</ref><ref type="bibr" target="#b23">21,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b39">37,</ref><ref type="bibr" target="#b56">53,</ref><ref type="bibr" target="#b60">57,</ref><ref type="bibr" target="#b135">128]</ref> as a training task. In this setup, each sample in the dataset is also it's own class. Several other interesting papers proposed to learn joint embeddings, "pulling together" different views of the same image <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b55">52,</ref><ref type="bibr" target="#b146">139]</ref>. Finally, a large body of work considered grouping instances and using clustering <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b30">28,</ref><ref type="bibr" target="#b49">46,</ref><ref type="bibr" target="#b67">64,</ref><ref type="bibr" target="#b83">80,</ref><ref type="bibr" target="#b137">130,</ref><ref type="bibr" target="#b142">135,</ref><ref type="bibr" target="#b153">146]</ref> or soft versions thereof <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b21">19]</ref> as training tasks. Many of those works have shown excellent performance on numerous downstream tasks, often showing that unsupervised features can surpass supervised ones. In this paper, we use the model proposed by Caron et al. <ref type="bibr" target="#b20">[18]</ref>, using soft assignments of images to prototypes.</p><p>Uncurated Data. Most works on unsupervised learning of features learn the models on supervised datasets like Im-ageNet <ref type="bibr" target="#b110">[105]</ref>. Some previous works have explored unsupervised training on images <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b36">34,</ref><ref type="bibr" target="#b53">50]</ref> and videos <ref type="bibr" target="#b90">[87]</ref> taken "in the wild". The conclusions of these works were mixed but these studies were conducted at a relatively small scale, both in model and data size. There are now evidences that self-supervised pretraining benefits greatly from large models <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b24">22,</ref><ref type="bibr" target="#b50">47,</ref><ref type="bibr" target="#b63">60]</ref>. Our work builds upon these findings to explore if we can learn good visual representations by training significantly larger models on random, uncurated and unlabeled images.</p><p>Scaling Architectures. Many works have shown the benefits of training large models on the quality of the resulting features <ref type="bibr" target="#b105">[100,</ref><ref type="bibr" target="#b121">114,</ref><ref type="bibr" target="#b138">131]</ref>. Training large models is especially important when pretraining on a large dataset, where a model with limited capacity will underfit <ref type="bibr" target="#b87">[84]</ref>. This becomes even more important when training with unsupervised learning. In that case, the network has to learn fea-female male other unknown Gender 1% 10% 25% 50% Percentage of Images <ref type="bibr">Figure 2</ref>. Geographical and Gender data distribution found in SEER Pretraining Data: we train our model on random group of a billion public Instagram images. We do not perform any data sampling or curation to achieve a certain data distribution. We instead discover that the random group of images naturally represent the geographic and demographic diversity of the world. (left): Geographical Data distribution of images found in the pre-training data. (right): Gender distribution found in the same images. Both distributions correspond to a random subset of 10M images from our 1 billion pre-training dataset. Percentages (%) denote fraction of 10M images from each country and gender found in the dataset.</p><p>tures that capture many aspects of the data, without being guided by a narrow output space defined by the manual annotation. To scale architecture size, all combinations of increasing width and depth have been explored in the self-supervised learning literature. <ref type="bibr">Kolesnikov et al. [73]</ref> demonstrated the importance of wider networks for learning high-quality visual features with self-supervision, Further, Chen et al. <ref type="bibr" target="#b24">[22]</ref> achieved impressive performance with deeper and wider configurations. The largest models trained for each algorithm vary a lot, with architecture that are both deeper and wider, such as ResNet-50-w5, ResNet-200-w2 or ResNet-152-w3. More generally, a large body of work is dedicated to building efficient models with large capacity <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b121">114,</ref><ref type="bibr" target="#b125">118,</ref><ref type="bibr" target="#b138">131]</ref>. Of particular interest, the RegNet model family <ref type="bibr" target="#b105">[100]</ref> achieves competitive performance on standard image benchmarks. while offering an efficient runtime and memory usage making them a good candidate for training at scale. In our work, we build up on the existing work and explore all 4 dimensions (depth, width, input resolution, compound) of scaling the RegNet architecture in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Large-Scale Benchmarking of Computer Vision Models.</head><p>Training high-quality visual representations that work well on a wide range of downstream tasks has been a core interest in the computer vision community. Recent advances in self-supervised learning <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b23">21,</ref><ref type="bibr" target="#b50">47,</ref><ref type="bibr" target="#b53">50,</ref><ref type="bibr" target="#b60">57]</ref> have shown that high quality visual features can be trained without labels. They surpass the performance of supervised learning on many computer vision tasks including object detection, image classification and low-shot learning.</p><p>The most widely used evaluation, initially proposed by Zhang et al. <ref type="bibr" target="#b149">[142]</ref>, consists in training linear classifiers on top of frozen features on ImageNet. While widely adopted, this evaluation has been criticized for being somewhat ar-tificial. A finer study has proposed by Sariyildiz et al., probing the performance of models when transferring to more distant concepts in ImageNet-22k <ref type="bibr" target="#b113">[107]</ref>. Many recent works, following Chen et al. <ref type="bibr" target="#b23">[21]</ref> demonstrate performance on other image classification datasets such as Oxford Flowers <ref type="bibr" target="#b96">[93]</ref>, Oxford Pets <ref type="bibr" target="#b100">[95]</ref>, MNIST <ref type="bibr" target="#b81">[78]</ref> or CI-FAR <ref type="bibr" target="#b77">[74]</ref>. These benchmarks are saturated with near perfect accuracy, and hence offer limited insight about the quality of a method.</p><p>Several works <ref type="bibr" target="#b75">[72,</ref><ref type="bibr" target="#b104">99,</ref><ref type="bibr" target="#b148">141]</ref> proposed a collection of more than 30 datasets to measure the generalization of weakly / fully-supervised models <ref type="bibr" target="#b38">[36,</ref><ref type="bibr" target="#b87">84,</ref><ref type="bibr" target="#b141">134]</ref>. Our work builds up on these studies and aims at validating the generalization of our self-supervised trained model on a large set of evaluation tasks. To this end we use more than 50 computer vision tasks that allow to capture the model's performance on various applications of computer vision. We argue that measuring model generalization on out-of-domain tasks is important as models can be used "off-the-self" for applications that are hard to anticipate.</p><p>Fairness of computer vision models. Several concerns have surfaced around the societal impact of computer vision models <ref type="bibr" target="#b33">[31]</ref>, to name a few: mis-classification of people's membership in social groups (e.g., gender) <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b71">68]</ref>, computer vision systems that reinforce harmful stereotypes <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b115">109]</ref> and the gender biases towards darker-skinned people <ref type="bibr" target="#b17">[15]</ref>. Further, studies <ref type="bibr" target="#b143">[136]</ref> show that training on Im-ageNet might lead to potential biases and harms in models, that are then transferred to the downstream tasks that model is applied on. Dulhanty and Wong <ref type="bibr" target="#b42">[40]</ref> studied the demographics on ImageNet, showing that males aged 15 to 29 make up the largest subgroup. Stock and Cisse <ref type="bibr" target="#b120">[113]</ref> have shown that models trained on ImageNet exhibit misclassifications consistent with racial stereotypes. De Vreis  et al. <ref type="bibr" target="#b32">[30]</ref> showed that the ImageNet trained models lack geographical fairness/diversity and work poorly on images from non-Western countries. Recently, effort has been made by Yang et al. <ref type="bibr" target="#b143">[136]</ref> to reduce these biases by removing 2,702 synsets (out of 2,800 total) from the person subtree used in ImageNet. Motivated by the importance of building socially responsible models, we follow recent works <ref type="bibr" target="#b54">[51]</ref> to systematically study the fairness, harms and biases of our models trained using self-supervised learning on random group of internet images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-supervised objective</head><p>We train our model using SwAV <ref type="bibr" target="#b20">[18]</ref>, and provide a short description of this algorithm here. Given two data augmentations of an image, that we refer to as s and t, we compute their codes q s and q t . SwAV trains a network by learning to predict the codes from the other view by minimizing the following loss function:</p><formula xml:id="formula_0">(z t , q s ) + (z s , q t ),<label>(1)</label></formula><p>where z s and z t are the outputs of the network for augmentations s and t. The codes are typically predicted using a linear model C, and the loss then takes the following form:</p><formula xml:id="formula_1">(z, q) = ? k q (k) log exp 1 ? z c k k exp 1 ? z c k ,<label>(2)</label></formula><p>where the c k are prototypes. We obtain the codes by matching the features against prototypes using the Sinkhorn algorithm. We defer the reader to <ref type="bibr" target="#b20">[18]</ref> for more details. The  . Impact of activation checkpointing. Memory profile of the 10B model on 8 GPUs and 8 images as input before and after inserting dynamic activation checkpointing. The peak memory usage is reduced from 26GB to 16GB. The computation time increases by 15% for same number of images but allows to increase batch size, hence increasing computational efficiency.</p><p>objective function can be minimized with stochastic gradient descent methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pre-training Data</head><p>In this work, we are interested in training high-quality visual representations on a large collection of random, unfiltered, unlabeled internet images. To this end, we train our models on a subset of randomly selected 1 billion public and non-EU (to conform to GDPR) Instagram (IG) images. We do not apply any other pre-filtering and also do not curate the data distribution. Our dataset is unfiltered but we monitor the resulting geographical and gender distribution on a subset of randomly selected 10M images in <ref type="figure">Fig. 2</ref>. As shown on the left panel, we find 192 different countries represented in our pre-training data. Similarly, we observe that our data represents images from various genders, as shown on the right panel. We also quantitatively measure the fairness of our model in Sec. 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scaling the model architecture</head><p>Scaling Axes. Self-supervised learning requires no annotations/labels for training models which means we can train large models from trillions of images at internet scale. Following previous works <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b50">47]</ref> which demonstrated the possibility to train high-quality visual features from billions of internet images using self-supervised learning, we consider 3 axes of scaling: 1) data size, 2) model size, and 3) data and model size.</p><p>In our work, we are interested in scaling along second axis i.e. model size first. The reasoning behind this choice is two-folds: a) training on large data requires large enough model in order to take advantage of the data scale and discover properties present in the dataset, and b) model size appears to be a strong lever for low-shot learning <ref type="bibr" target="#b50">[47]</ref> and we are interested in pushing these limits further.</p><p>Choosing and Scaling Model Architecture. Towards our goal of scaling model size and pushing the limits further in self-supervised learning, we target training a 10B parameters dense model, which, to the best of our knowledge, is the largest dense 1 computer vision model (contrary to the model in <ref type="bibr" target="#b109">[104]</ref> which is a "sparse" model). Following studies <ref type="bibr" target="#b50">[47]</ref>, we explore RegNet [100] (a ConvNet) architecture which has demonstrated promising model size scaling without any signs of saturation in performance. Further, since the largest model defined in RegNet family is a 1.5B parameters model, we explore several strategies to increasing the architecture size to 10B parameters.</p><p>To increase the model size, we explore four dimensions: width, depth, resolution and compound scaling for the RegNet model family. Additionally, we explore a variant RegNet-Z <ref type="bibr" target="#b37">[35]</ref> of this model family. Appendix <ref type="table" target="#tab_4">Table 14</ref> summarizes the variants. We trained each variant on 100M images using the same experimental setup and for each variant training, we evaluated model performance on the downstream task of linear classification on ImageNet-1K. Our observations are as follows: (i) the less wider but deeper models didn't change model performance on downstream task compared to the base model. However, such models lead to faster training time, (ii) high input resolution models increase model runtime without increasing model parameters and yielded only modest increases in accuracy, (iii) wider and deeper model with more FLOPs (than base model) improved performance on downstream task, and (iv) RegNet-Z model architecture are more intensive and not efficient for scaling parameters.</p><p>Following these findings, we decided to keep the resolution fixed and increase the width (and/or depth) of the base model to scale to 10 billion parameters model. We note that for better training speed, we ultimately kept the depth same and increased the width. Our full model details are described in Appendix C.</p><p>Fully Sharded Data Parallel. We train our models using PyTorch on NVIDIA A100 GPUs and our biggest model with 10B parameters requires 40GB of GPU memory (with additional 40GB required for optimizer state during pretraining). On a single V100 32G GPU or more recent 40GB A100, such a model can not fit and hence the DDP (Distributed Data Parallel) training can not be used. We instead resort to model sharding and use the Fully Sharded Data Parallel FSDP <ref type="bibr" target="#b106">[101,</ref><ref type="bibr" target="#b140">133]</ref> training which shards the model such that each layer of the model is sharded across different data parallel workers (GPUs). The computation of minibatch is still local to each GPU worker. FSDP decomposes the all-reduce operations in DDP into separate reduce-scatter and all-gather operations. During the reduce-scatter phase, the gradients are <ref type="bibr" target="#b2">1</ref> where every input is processed by every parameter, as defined in <ref type="bibr" target="#b109">[104]</ref> summed in equal blocks among ranks on each GPU based on their rank index. During the all-gather phase, the sharded portion of aggregated gradients available on each GPU are made available to all GPUs. During the forward pass, the parameters of the layer to be computed are temporarily assembled before they are re-sharded. For training efficiency, the communication and computation are overlapped: un-sharding the next layer parameters (via all-gather) while computing the current layer. We illustrate the communication and compute optimizations in <ref type="figure" target="#fig_1">Fig. 3</ref>. For our model training, we leverage the FSDP implementation from Fairscale 2 and adapt it for our model. Activation Checkpointing Automation. In our model trainings, we use Activation Checkpointing <ref type="bibr" target="#b25">[23]</ref> which is the technique of trading compute for memory. It works by discarding all model activations during the forward pass except the layers that have been configured to be "checkpointed". During the backward pass (backpropagation), the forward pass on a part of the model (between two checkpointing layers) is re-computed. While this technique can help increase the batch size (leading to more compute to be overlapped with communication which leads to more efficient training), one downside is that manual configuration / tuning of which layers should be "checkpointed" is needed. This can be time-consuming and often hard to find the optimal checkpointing state. Further, for the models that are hard to fit in memory, it can become very difficult to perform manual tuning.</p><p>To address this, we implemented a Dynamic Programming algorithm 3 to find the best checkpoint positions for a given model rather than manual tuning. The algorithm is as follows: (Step 1) we first collect the amount of activation's memory allocation produced at each layer using automatic tooling, in an array m, (Step 2) we optimally split with dynamic programming this array in consecutive sub-arrays delimited by K points p i where 0 ? i ? K and p i ? p i+1 such that:</p><formula xml:id="formula_2">argmin pi max pi j=pi?1 m j (3) (</formula><p>Step 3) the points p i such that 0 &lt; i &lt; K are our activation checkpoints points, minimizing the maximum amount of cumulative activation memory for K ? 2 activation checkpoints, and (Step 4) we iterate this algorithm increasing K until we manage to fit our desired batch size on the GPU memory.</p><p>In practice, when applied to our 10billion parameter model, the algorithm selected 4 activation checkpoints locations. We further adapted the checkpoint positions for any further trade-offs not accounted in the algorithm. The impact on memory reduction is shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>Optimizing training speed. To optimize the training speed of the model, we use several optimizations. We use mixed-precision for training and perform the forward pass computations in FP16. Since computation happens in FP16, for the un-sharding of parameters via all-gather operation (which performs communication of parameters over the network), we exchange FP16 weights instead of FP32. This speeds-up the training by communicating model parameters faster. We note that for certain special layers such as SyncBatchNorm, we still use FP32 as otherwise the training becomes unstable. Further, we use LARC optimizer <ref type="bibr" target="#b144">[137]</ref> from NVIDIA Apex library 4 for large batch size training. Since the model parameters are sharded, we adapted the LARC implementation to compute the distributed norms of parameters but without all-gather of model weights. We share more details on this in Appendix D. Additionally, we add the activation checkpointing in the order FSDP(checkpointing(model layer)) instead of the other way around. This is because activation checkpointing re-computes the forward pass on part of the model during back-propagation and doing a forward pass on FSDP wrapped layer requires "un-sharding" of layer which involves communication of weights across all GPUs. Hence, FSDP(checkpointing(model layer)) ensures that we do not trigger excessive "un-sharding" / communication cost across GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pretraining the SEER model</head><p>We use open source VISSL library <ref type="bibr" target="#b52">[49]</ref> for our model training and implement FSDP and activation checkpointing integration for RegNet-Y model architecture. We generate a wider RegNetY-10B parameters architecture with the configuration: w 0 = 1744, w a = 620.83, w m = 2.52, depth = (2, 17, 7, 1), group width = 1010. We use a 3-layer multi-layer perceptron (MLP) projection head of dimensions 20280 ? 8192, 8192 ? 8192 and 8192 ? 256. We do not use BatchNorm layers in the head. We use SyncBatchNorm in the model trunk and synchronize BatchNorm stats globally across all GPU workers. Following <ref type="bibr" target="#b50">[47]</ref>, we use SwAV algorithm with same data augmentations and 6 crops per image of resolutions 2 ? 160 + 4?96 5 . For the SwAV objective, we use 16, 000 prototypes, temperature ? set to 0.1, sinkhorn regularization parameter (epsilon) to 0.03 and perform 10 iterations of sinkhorn algo-rithm. We train our model with stochastic gradient descent (SGD) momentum of 0.9 using a large batch size of 7, 936 different images distributed over 496 NVIDIA A100 GPUs results in 16 different images per GPU. We use a weight decay of 1e?5, LARS optimizer <ref type="bibr" target="#b144">[137]</ref>, activation checkpointing <ref type="bibr" target="#b25">[23]</ref> and FSDP for training the model. We use learning rate warmup <ref type="bibr" target="#b51">[48]</ref> and linearly ramp up learning rate from 0.15 to 9.3 for the first 5, 500 iterations. After warmup, we use cosine learning rate schedule and decay the learning rate to final value 0.0093. We train on 1 billion images in total leading to 126K training iterations. We share details about other smaller variants of SEER model in Appendix <ref type="table" target="#tab_7">Table 16</ref>.</p><p>Reliable model training and evaluations. To pre-train the large dense 10Billion parameters dense model, pretraining reliability is crucial. Further, whereas we pretrain the model on 496 GPUs using FSDP model sharding, we want to use and evaluate the model on many downstream tasks but using much fewer GPUs (e.g. 8 GPUs). We implemented an efficient model state dictionary checkpointing technique that helps us achieve reliable pre-training on 496 GPUs and scalable model evaluations on 8 GPUs. We discuss more details on this in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We extensively validate the performance of our model on over 50 benchmarks tasks. In Sec. 4.1, we evaluate and compare the performance of our model on 4 different fairness benchmarks including 3 fairness indicators. In Sec. 4.2, we further study the performance on many downstream tasks in computer vision including out-of-domain robustness in Sec. <ref type="bibr" target="#b5">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Fairness</head><p>The ubiquitous use of computer vision models in many applications has also raised questions about their societal implications. This necessitates the need to properly measure and quantify what harms and biases a model has with respect to societal groups of various membership types (e.g. age, gender, race, skintone etc.). SEER models demonstrate strong performance on a broad range of publicly available computer vision benchmark tasks. As models improve in performance on such tasks, the likelihood of using a model "off-the-shelf" for downstream applications increases and the nature and context of such applications is hard to anticipate. Motivated by this, we probe the fairness of SEER models.</p><p>We follow the protocols et al. <ref type="bibr" target="#b54">[51]</ref> to probe the performance of our larger SEER models on three different fair- We observe that our model obtains the best precision and it increases with model size. We note that our motivation behind these fairness probes is not to validate the use of any given model. As noted in <ref type="bibr" target="#b54">[51]</ref>, for a given model, the choice of what fairness probes to measure depends on the application and use context. This choice must be thoroughly assessed by the stakeholder so as to answer why those probes are chosen, what kind of assumptions are embedded in this choice, and what specific questions do the system designers aim to answer <ref type="bibr" target="#b70">[67,</ref><ref type="bibr">75]</ref>. Therefore, we ask practitioners and developers to not treat these results as a validation of use of a model.</p><formula xml:id="formula_3">P@1 difference Model Data Arch. gender skintone (male -female) (ligher -darker) Supervised INet-1K RG-128Gf 24% 8% Self-supervised pretraining on ImageNet SwAV INet-1K RG-128Gf 31% 11% Pretrained on random internet images SEER (ours) IG-1B RG-128Gf 9% 7% SEER (ours) IG-1B RG-10B 2% 3%</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Indicator1: Same Attribute Retrieval</head><p>We directly apply the benchmark protocol (including data preparation) as proposed in <ref type="bibr" target="#b54">[51]</ref>. In this experiment, we perform similarity search, which requires a set of Queries and a Database. For Queries, we use the mini test split of Casual Conversations <ref type="bibr" target="#b58">[55]</ref> which has 2, 982 videos (two videos per participant with one dark and one bright lighting video when possible). The dataset provides selfidentified age (from 18 to 85) and gender ('male', 'female', 'other' and 'n/a') labels along with annotated Fitzpatrick skintone <ref type="bibr" target="#b45">[43]</ref>. For each video, when possible, we use the middle frame and use the face crops from each image. As Database, we use the UTK-Faces <ref type="bibr" target="#b151">[144]</ref> dataset which has 24, 108 face images annotated with apparent age and gender labels. Following Buolamwini et al. <ref type="bibr" target="#b17">[15]</ref>, we group the Fitzpatrick scale into two types: Lighter (Type I to Type III) and Darker (Type IV to Type VI). As a result, we obtain four gender-skintone subgroups [female, male] ? [lighter, darker] and four age subgroups 18 ? 30, 30 ? 45, 45 ? 70, 70+. We extract features on Casual Conversations and UTK-Faces and for each query, retrieve the closest image in the Database based on cosine similarity metric. We perform similarity search for the gender attribute and measure P@1 for different sub-groups: gender, skintone and age groups.</p><p>This first indicator allows to measure the disparity in the learned representations of people by directly using the raw model embeddings. If a model has higher P@1 for "male" than for "female", this indicator tells how much the model falsely recognizes a true female population as male i.e. does mis-gendering. We show the results of this indicator in <ref type="table">Table 1</ref> and further measure the disparity between different genders and skintones in <ref type="table" target="#tab_2">Table 2</ref>. We make sevaral observations. First, models pretrained on ImageNet-1K have a higher disparity. Second, SEER models have the lowest  disparity between different genders and skintones. Finally, we observe that for SEER models, as the model size increases, the disparity decreases. That means the model embeddings seem to recognize different genders and skintones more fairly. We hypothesize that this is because SEER is pretrained on a very diverse dataset (see <ref type="figure">Fig. 2</ref>) and the size of the model allows to better extract the salient information present in the image leading to better visual features. The baseline models are trained on ImageNet-1K whose dispar-ity has been empirically confirmed in previous work <ref type="bibr" target="#b143">[136]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Indicator2: Label Association</head><p>We use the Casual Conversations dataset as described in Sec. 4.1.1 which has 2, 982 images of faces of people. For the unsupervised models, since they do not predict labels by design, we first adapt the model by finetuning it on a subset of ImageNet-22K <ref type="bibr" target="#b54">[51]</ref>. For fair comparison, we apply the same finetuning steps to all models. Afterwards, for each image in the Casual Conversations dataset, we perform model inference and record top-5 label predictions along with the confidence scores. For each image, we study the type of label predicted where the labels are grouped in various association types as described in <ref type="table" target="#tab_4">Table 4</ref>. This indicator allows to measure the harmful predictions of a model, in particular when mis-labeling images of people. These harms can be bigger if the type of predicted labels are derogatory or reinforce harmful stereotypes <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b115">109]</ref>. As proposed in the benchmark <ref type="bibr" target="#b54">[51]</ref>, we study the predictions with a confidence threshold of 0.1 <ref type="bibr" target="#b120">[113]</ref>. This is in contrast to reporting the top-5 predicted labels, irrespective of confidence. We compare our models  with two baselines and report the results of this study in <ref type="table">Table 3</ref>.</p><p>On one hand we see that the supervised model trained on ImageNet makes the most Non-Human predictions for all gender, skintone and age-groups. Within this, the models predict Non-Human labels most often for "female" and age group "45-70". Moreover, the supervised ImageNet model also makes the most Crime predictions for all gender, skintone and age-groups. The disparity is greatest for "maledarker". On the other hand, SEER models make the most Human predictions. For a given face crop image, this model will more likely predict one of the [face, people] labels for all gender, skintone and age groups. we note that the Human label prediction is least for "male" skintone with a disparity of 30% between "male" and "female". Also, we observe that as the SEER model size increases, the association of the Human labels increases significantly (+10% from RG-128Gf to RG-10B across genders and skintones). We hypothesize that since SEER is trained on the humancentric Instagram data (while ImageNet is object centric), it has learned better and fairer representations of people. Further, since the Instagram data represents content from "female" more (see <ref type="figure">Figure 2</ref>), the dataset makes more humancentric predictions for female.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Indicator3: Geographical Fairness</head><p>We use the DollarStreet dataset <ref type="bibr" target="#b32">[30]</ref> and benchmark protocol <ref type="bibr" target="#b54">[51]</ref> for evaluating the disparity in object recognition accuracy in different parts of the world. The dataset is composed of 16, 073 images from 289 households of varying income levels, representing 94 concepts across 54 countries over 4 regions of the world. The data distribution per country and per region is shown in Appendix <ref type="figure" target="#fig_6">Fig. 17</ref>. As in the previous Indicator2, since self-supervised models do not have capability to predict labels, we finetune the models on a subset of ImageNet-22K. We use the manual mapping of DollarStreet classes to ImageNet-22K classes proposed in previous work <ref type="bibr" target="#b54">[51]</ref>. Using this mapping, we retain from ImageNet-22K a subset of 127K images spanning 108 concepts. For fair comparison, we finetune all the models on this data. Once the models are adapted, we run inference on the 16K images from DollarStreet and record the model predictions. We measure performance by computing the top-5 accuracy. For analysing fairness, we follow <ref type="bibr" target="#b54">[51]</ref> and aggregate the model predictions by household and split per income level and per region. We report this analysis in <ref type="table" target="#tab_6">Table 5</ref>.</p><p>This indicator measures if a model is capable of recognizing concepts across different income households across different regions of the world. From the analysis in <ref type="table" target="#tab_6">Table 5</ref> we observe that the improvement of SEER models over the supervised baseline is smallest for high income households and the American / European regions. At the same time, the relative improvement in accuracy is significant for the other groups (+23% for low-income households and +21% for the African region). As the model size increases to 10B parameters, the trend holds. As for the previous experiments, we hypothesize that the performance of SEER follows this pattern because of the diversity of our pre-training data. As shown in <ref type="figure">Fig. 2</ref>, the pre-training data distribution is geographically diverse compared to datasets such as ImageNet, which mostly contain data from Western countries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Hate Speech Detection: HatefulMemes</head><p>For this experiment, we use the HatefulMemes Challenge Dataset <ref type="bibr" target="#b72">[69]</ref>. This is a multi-modal dataset consisting of 10, 000 images with associated text annotated with types of hate speech. The hate speech categories are: inciting violence, dehumanizing, inferiority, contempt, mocking, slurs, exclusion and no hate-speech. Those are further split into different protected categories (race, religion, gender, disability, nationality and 'pc empty' for no protected category). The train split contains 8, 500 memes and the dev set contains 500 memes. The distribution of different protected categories and types of hate-speech is shown in Appendix <ref type="figure" target="#fig_5">Figure 16</ref>. We use our model as an image encoder and extract the visual features for all images in the HatefulMemes dataset. For all models, we extract the features before the final pooling layer in order to preserve the spatial information. We use BERT-Base <ref type="bibr" target="#b34">[32]</ref> as the text encoder. We concatenate the image features with the BERT text features and train an MLP head on top. We use the AdamW optimizer <ref type="bibr" target="#b86">[83]</ref> with epsilon 1e ? 8, a learning rate of 8e ? 5. We use a linear learning rate warmup for 2, 000 iterations followed by step decays value by 1e ? 5 every 500 iterations. We train 6 for a total of 22, 000 iterations with a batch size of 64. We report the best ROC AUC metric on the dev set during training.</p><p>For each model, we run the evaluation with three seeds <ref type="figure" target="#fig_1">(100, 200 and 300)</ref> and report the average ROC AUC on the dev set in <ref type="table" target="#tab_7">Table 6</ref>. We observe that our SEER models outperform supervised ImageNet trained models by more than 2 pts. Interestingly, the same self-supervised learning algorithm applied on ImageNet (SwAV) does not yield good performance. We further note that as the model size increases to 10B parameters, the performance increases. We hypothesize that, similar to the fairness indicators in Sec. 4.1, the diversity and the human-centric nature of the pre-training data leads to better hate-speech detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transfer Learning on computer vision tasks</head><p>In previous Sec. 4.1, we extensively analysed the SEER models for societal implications models can have by, for instance, mislabeling photos of people with harmful labels (derogatory, stereotypes), disparity in learned representation of people's social membership (e.g. mis-gendering), hate speech detection and fairness in object recognition capability for various income households across the globe and we observed promising results for our models across the board.</p><p>In this section, we analyze the quality of visual representations learned by model on a broad range of computer vision tasks as there's no general agreement on what qualifies for universal or "ideal" visual representation <ref type="bibr" target="#b85">[82]</ref>. To this end, we benchmark the robustness of models to distribution shift in Sec. 4.2.2, fine-grained recognition performance on challenging datasets such as iNaturalist18 <ref type="bibr" target="#b127">[120]</ref> in Sec. <ref type="bibr" target="#b5">4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Baselines</head><p>For all benchmarks in this section, we compare performance of our models with supervised learning and state-of-the-art self-supervised learning approaches on Im-ageNet. For each self-supervised learning approach, we chose the largest publicly available pre-trained model checkpoint and ran evaluations with them. Concretely, the models we compare with are ConvNets including SimCLRv2-RN152w3+SK <ref type="bibr" target="#b24">[22]</ref> (795M params), BYOL-RN200w2 <ref type="bibr" target="#b55">[52]</ref> (250M params), SwAV-RN50w5 (585M params) and SwAV-RG128Gf <ref type="bibr" target="#b20">[18]</ref> (693M params) and more recent Vision Transformers <ref type="bibr" target="#b38">[36]</ref> including MoCov3 ViT-B/16 <ref type="bibr" target="#b27">[25]</ref> (85M params) DINO ViT-B/16 <ref type="bibr" target="#b21">[19]</ref>. For SEER models, we trained several model sizes from 40M parameters to 10B parameters as described in Appendix <ref type="table" target="#tab_7">Table 16</ref>.  . Influence of SEER model scale on out-of-domain data generalization and robustness to distribution shift. Across all the datasets, we observe that as the model size increases, the performance increases significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Out-of-domain Generalization and Robustness</head><p>For most "off-the-shelf" models in computer vision, it is hard to anticipate the exact application of models and impossible to train a model on precisely the data distribution that the model will be applied to. Inevitably, the model will encounter out-of-domain data on which the model performance can vary widely. For instance, even though deep learning models have surpassed human performance on Im-ageNet dataset <ref type="bibr" target="#b61">[58]</ref>, recent works <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b35">33]</ref> have demonstrated that these models still make simple mistakes and have lower accuracy (than ImageNet and human) on new benchmarks <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b108">103]</ref>. Therefore, understanding the out-ofdomain generalization of models is important. Motivated by this, we probe the performance of our models on out-ofdomain datasets. To measure the generalization capabilities of our model, we report the performance of the finetuned model on several alternative test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets.</head><p>A recent comprehensive study <ref type="bibr" target="#b91">[88]</ref> analyzed the out-of-domain generalization and robustness of Ima-geNet models on several datasets (which have distribution shifts) and found that across all datasets, the accuracy of models dropped well below the expectation set by the ImageNet validation set. A few datasets tested are: ImageNet-Adversarial <ref type="bibr" target="#b65">[62]</ref> (contains natural adversarial images), ImageNet-R <ref type="bibr" target="#b64">[61]</ref> (renditions), ImageNet-Sketch <ref type="bibr" target="#b130">[123]</ref> (sketches), ImageNet-Real <ref type="bibr" target="#b13">[11]</ref> (corrected labels in original dataset), ImageNet-V2 <ref type="bibr" target="#b108">[103]</ref> (new test set for ImageNet benchmark), ObjectNet <ref type="bibr" target="#b6">[5]</ref>. Each of these datasets have the subset or same labels as the original ImageNet-1K and we use these dataset for our models benchmarking.</p><p>Evaluation Protocol. We use our pre-trained SEER model trunk for initialization and attach a linear classifier head on top. We full-finetune the model weights on Ima-geNet task for 15 epochs using SGD momentum 0.9, weight decay 1e ? 4, learning rate of 0.04 for batch size 256 and finetune on 128 NVIDIA GPUs by scaling learning rate following Goyal et al. <ref type="bibr" target="#b51">[48]</ref>. We use step learning rate schedule with gamma of 0.1 and decay at steps <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b14">12]</ref>. After finetuning the model, we evaluate the finetuned model on all 5 datasets by performing inference only and report the top-1 accuracy of several models (including our baseline models) on all datasets including the ImageNet validation set in <ref type="table" target="#tab_9">Table 7</ref>.</p><p>Results. <ref type="table" target="#tab_9">Table 7</ref> shows performance of our SEER model and comparison to its smaller versions and the baseline <ref type="figure">Figure 8</ref>. Qualitative analysis of out-of-domain performance and robustness as detailed in Sec. 4.2.2. For the same architecture (RG-128Gf), we show few example images of improvements between our model (SEER) and ImageNet trained supervised and self-supervised (SwAV) model. We note that on out-of-domain data, self-supervised models generalize better than supervised models. Further, SEER model significantly outperforms self-supervised pre-training on ImageNet. models in Sec. 4.2.1. We observe several interesting trends from this comparison. (i) self-supervised pretraining objective are more robust and generalize better to out-of-domain data distribution compared to the supervised training objective. (ii) our SEER model, trained on Instagram data achieves better generalization than the self-supervised mod-els trained on ImageNet. (iii) as the size of SEER models increases, the out-of-domain generalization improves significantly as evident in <ref type="figure" target="#fig_6">Figure 7</ref>.</p><p>We further investigate our third observation for SEER models by evaluating the whole family of SEER models (outlined in Appendix <ref type="table" target="#tab_7">Table 16</ref>) we trained. The trend of influence of model scale on performance is demonstrated in <ref type="figure" target="#fig_6">Figure 7</ref>. We note that the influence of scale on generalization holds true for all datasets and we observe a a log-linear scaling trend in performance improvement with model as, for all the test sets considered. Further, on some datasets such as adversarial ImageNet-A dataset, performance nearly doubles from 19.6% to 52.7%. The gains are most shy on ImageNet-ReaL (only +2.7%) dataset which essentially is same as the ImageNet validation set but with the relabeling to improve on mistakes in the original annotation process. We qualitatively investigate the performance improvement on all these datasets and compare qualitative results for ImageNet trained models vs SEER in <ref type="figure">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disentangling Factors of Influence using dSprites</head><p>We investigate what are the factors that contribute to the better performance of SEER models on out-of-domain generalization. We hypothesize that the location and orientation of objects are two common factors of variation in the out-of-domain datasets. For this, we evaluate SEER models on dSprites <ref type="bibr" target="#b89">[86]</ref> dataset which contains simple black/white shapes rendered in 2D and offers two tasks: location and orientation prediction. This dataset has 664K images and is an image classification task with 16 different locations and orientations each.</p><p>We use linear probe to evaluate SEER and baseline models on dSprites dataset. We initialize models with respective model weight and attach an MLP classifier head on top. While keeping the model trunk fixed, we train the linear classifier head for 28 epochs using SGD momentum 0.9, weight decay 0.0005, learning rate of 0.01 for a batchsize of 256 and step learning rate schedule with gamma factor 0.1 with decay at steps <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b26">24]</ref>. We share the results in <ref type="table">Table 8</ref> and observe that the our SEER models achieve equal or slightly better performance than baseline models on both the tasks. We thus reason that the pretraining data domain for our model instead contributes to better out-ofdomain performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Fine-Grained Recognition</head><p>We next evaluate the performance of our models on a challenging fine-grained image classification task of recognizing various animal special in the iNaturalist18 <ref type="bibr" target="#b66">[63]</ref> dataset. We further evaluate how well the models generalize on a wildlife monitoring (and preservation as a result) task iWildCam-WILDS <ref type="bibr" target="#b74">[71]</ref>. This dataset has real-world geographic shift as the images are taken by camera traps all over the world and across different camera traps, there is drastic variation in illumination, color, camera angle, background, vegetation, and relative animal frequencies, which Datasets. iNaturalist18 dataset is composed of 437, 513 images of super-classes Mammalia, Aves, and Reptilia in train set representing a total of 8, 142 fine-grained species. iWildCam-WILDS, adapted from iWildCam 2020 competition dataset <ref type="bibr" target="#b9">[8]</ref>, contains 129, 809 images of 182 species (including "no animal") in the train set where the images are taken by 243 camera traps deployed all over the world. The in-distribution test set comprises 8, 154 images taken by the same 243 camera traps. The goal is to identify the animal species, if any, within each photo and due to the data challenges such as camouflage, blur, occlusion, motion, perspective etc, the task is quite challenging.</p><p>Evaluation Protocol. We evaluate the SEER and baseline models on these datasets using two protocols: linear and full-finetuning.</p><p>Following recent works <ref type="bibr" target="#b38">[36]</ref>, we perform finetuning at input image resolution 384. Further, for iNaturalist18 fullfinetuning, we initialize the model weights with SEER models and attach a linear 8142 dimensional MLP head. We finetune the full model using SGD momentum of 0.9 for 48 epochs. We use learning rate of 0.015 for a batchsize of 256 images, weight decay of 1e ? 4 and use global SyncBatchNorm synchronizing the statistics across all   <ref type="table">Table 9</ref>. Fine-grained recognition image classification performance of models measured via linear probe and full-finetuning on iNatu-ralist18 and iWildCam-WILDS datasets as described in Sec. 4.2.3. For iWildCam-WILDS dataset, we report performance on testID split. We observe that for both linear and full-finetuning probes, our model achieves the best performance. Further, as the size of our model increases, the performance consistently increases. Qualitative analysis of performance is presented on iNaturalist18 in <ref type="figure" target="#fig_8">Figure 9</ref> and on iWildCam-WILDS in <ref type="figure" target="#fig_12">Figure 10</ref>.</p><p>GPU workers. We use cosine learning rate schedule decaying learning rate at every iteration to the final value of 5e ? 7. We do not regularize BatchNorm and neither the bias in the model layers.</p><p>For iWildCam-WILDS full-finetuning, we initialize the model weights from the SEER model full-finetuned on iNaturalist18 (following the guideline <ref type="bibr" target="#b9">[8]</ref>) and attach a linear 182 dimensional MLP head and full-finetune the model. We use SGD momentum of 0.9 to finetune for 60 epochs, weight decay of 1e ? 4, step learning rate schedule with learning rate of 0.001 decayed at epochs <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b42">40]</ref> by gamma 0.1. We use global SyncBatchNorm to synchronize statistics across all GPU workers and also regularize BatchNorm and bias in the model layers.</p><p>For linear evaluation on iNaturalist18, we initialize models with respective model weights and attach an MLP classifier head on top. While keeping the model trunk fixed, we train the linear classifier head for 28 epochs using SGD momentum 0.9, weight decay 0.0, learning rate of 0.015 for a batchsize of 256 and step learning rate schedule with gamma factor 0.1 with decay at steps <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b26">24]</ref>.</p><p>For linear probe on iWildCam-WILDS, similar to fullfinetuning, we initialize the model from the SEER model weights full-finetuned on iNaturalist18 and follow the same linear probe strategy as for iNaturalist18 in above paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Arch We observe state-of-the-art performance using SEER models with the performance increasing with model size. We show qualitative results in <ref type="figure" target="#fig_1">Figure 13</ref>.</p><p>Results. We report the performance of (several variants) SEER models and baseline models from 4.2.1 in Table <ref type="bibr" target="#b10">9</ref>. We observe that (i) SEER models consistently achieve better visual representation for both linear and full- finetuning protocols on both iNaturalist18 and iWildCam-WILDS datasets, (ii) further, as the size of SEER models increases, the performance increases, (iii) on iNatural-ist18 dataset which has many challenges (such as occlusion, camouflage, blur, motion), SEER outperforms other baseline models and current state-of-the-art model on leaderboard <ref type="bibr" target="#b9">8</ref> indicating the visual quality of SEER features is more robust to these challenges, and (iv) finally, we note that the SEER models achieve significantly better accuracy +3.8% on iNaturalist18 for finetuning protocol. We hypothesize that since SEER models are trained on random Instagram data which is human-centric images and iNatu-ralist18 contains fine-grained images of animals, mammal, aves species, full-finetuning helps to adapt the model better. We show qualitative result analysis in iNaturalist18 and iWildCam-WILDS in <ref type="figure" target="#fig_8">Figure 9</ref> and <ref type="figure" target="#fig_12">Figure 10</ref> respectively. <ref type="bibr" target="#b9">8</ref> https : / / wilds . stanford . edu / leaderboard / #iwildcam</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Image Copy detection</head><p>We evaluate the performance of our models on Image Copy Detection <ref type="bibr" target="#b41">[39]</ref> task which tests the robustness of models to adversarial attacks. This task has important practical applications in computer vision for various real-world problems such as content integrity, misinformation and user safety. This task involves identifying the source of an altered image within a large collection of unrelated images. The images are altered / manipulated by applying several data distortions such as blur, insertions, print and scan, etc making this a challenging task.</p><p>Dataset. We use Copydays dataset "strong" subset which has 157 images in the Database and 3, 212 images as Queries. We augment the data with 10K random distractor images from YFCC100M <ref type="bibr" target="#b122">[115]</ref> following previous works <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b21">19]</ref> and denote this setting as CD10K. Image retrieval benefits from PCA whitening and thus we use an additional 20K images from YFCC100M following <ref type="figure" target="#fig_12">Figure 10</ref>. Qualitative analysis of fine-grained performance on iWildCam-WILDS as described in <ref type="table">Table 9</ref>. We show a few example images from iWildCam-WILDS where SEER demonstrates better performance than pre-training on ImageNet-1K. Each row represent a different category of animals, feline predators, small mammals, zebras, gazelles and birds. SEER is better at identifying animals species across those categories across various challenges such as camouflage, blur, occlusion, motion, and unusual perspectives. <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b21">19]</ref> to train PCA whitening.</p><p>Evaluation Protocol. We extract the features of our models on all images in Database, Queries, 10K distractors and 20K whitening set. Following <ref type="bibr" target="#b123">[116]</ref>, the features are pooled with regionalized pooling layer (R-MAC) with spatial level 3 9 which by design also L2 normalizes the features. We train the PCA whitening on 20K images and apply this whitening to the Database and Queries features. We then perform copy detection using cosine similarity between the database and query features and <ref type="bibr" target="#b10">9</ref> We experimented with R-MAC and GeM both and found R-MAC to work best for SEER models evaluate the performance using mean average precision (mAP) metric.</p><p>Results. We report the performance of our SEER models and baseline models in <ref type="table" target="#tab_12">Table 10</ref>. We observe that (i) selfsupervised models achieve competitive performance on this task which corroborates the finding in previous work <ref type="bibr" target="#b21">[19]</ref>, (ii) We further observe that as model size increases, copy detection performance improves for the same features size, (iii) we observe 90.6% mAP with best SEER model which is an improvement of +5.1% over previous best results. We show some qualitative analysis and comparison in Appendix <ref type="figure" target="#fig_1">Figure 13</ref> and additional implementation details in Appendix G.  <ref type="table">Table 11</ref>. Representation learning using linear probe on standard image classification datasets as described in Sec. 4.2.5. We compare performance of the several models on downstream classification tasks. We observe that, despite training our model on random internet images, our model achieves competitive or better results than ImageNet based supervised and self-supervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Representation learning using Linear-probe</head><p>One of the objectives of our work is to learn task-agnostic high-quality visual features from random internet image in the wild. To this end, we evaluate the quality of visual representations learned by our models during pretraining on a variety of datasets in computer vision <ref type="bibr" target="#b148">[141]</ref>. There are two widely used protocols for evaluating the visual features quality: linear-probe and full-finetuning. While it has been proven that fine-tuning exceeds the performance of linear classifiers <ref type="bibr" target="#b148">[141]</ref>, for our benchmarking, we choose linear-probe protocol. We make this choice because full-finetuning adapts visual features to each dataset and can compensate for and potentially mask the failures to learn general and robust representations during the pretraining. However, linear classifiers can highlight these failures which provides a better measure of the features quality.</p><p>Datasets. We follow the previous work <ref type="bibr" target="#b148">[141]</ref> to select 25 tasks (summarized in Appendix <ref type="table">Table 13</ref>) that can be grouped into few categories based on the task domain.</p><p>(i) standard datasets such as ImageNet-1K, Places205 and VOC07 which haven been widely used for testing features quality in many previous works <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b20">18,</ref><ref type="bibr" target="#b53">50,</ref><ref type="bibr" target="#b135">128,</ref><ref type="bibr" target="#b141">134]</ref>; (ii) medical and satellite images such as in RESISC45 <ref type="bibr" target="#b28">[26]</ref>, Eu-roSAT <ref type="bibr" target="#b62">[59]</ref>, PatchCamelyon <ref type="bibr" target="#b128">[121]</ref>; (iii) structured datasets containing synthetic images and we select these datasets as even the best ImagNet representations fail to capture the aspects in these datasets such as counting and depth prediction tasks on CLEVR <ref type="bibr" target="#b69">[66]</ref> which contains simple 3D shapes with two tasks, camera-elevation prediction on SmallNorb <ref type="bibr" target="#b82">[79]</ref> which has images of artificial objects viewed under varying conditions, location and orientation prediction tasks on dSprites <ref type="bibr" target="#b89">[86]</ref> which contain 2D rendered black/white shapes; (iv) activity recognition in videos by taking the middle frame on datasets Kinetics-700 <ref type="bibr" target="#b22">[20]</ref> and UCF-101 <ref type="bibr" target="#b118">[111]</ref> and scene recognition on SUN397 <ref type="bibr" target="#b136">[129]</ref>; (v) self-driving related tasks such as german traffic sign recognition in GTSRB <ref type="bibr" target="#b119">[112]</ref>, measuring the distance of nearest vehicle in KITTI-Distance <ref type="bibr" target="#b48">[45]</ref>; (vi) textures on datasets such as DTD <ref type="bibr" target="#b29">[27]</ref>; (vii) natural datasets such as STL-10 <ref type="bibr" target="#b31">[29]</ref>, Oxford-IIT Pets <ref type="bibr" target="#b100">[95]</ref>, Oxford Flowers102 <ref type="bibr" target="#b98">[94]</ref>, Caltech-101 <ref type="bibr" target="#b44">[42]</ref> and finally (viii) optical character recognition (OCR) tasks such as on SVHN <ref type="bibr" target="#b95">[92]</ref> which involves street number transcription on the distribution of Google Street View photos.</p><p>Evaluation Protocol. We train linear classifiers by learning a multinomial logistic regression on the visual features. We initialize models with respective model weight and attach a linear classifier head initialized from scratch <ref type="bibr" target="#b11">10</ref>  SEER vs SOTA SUP <ref type="figure">Figure 11</ref>. Linear probe performance comparison of our SEER model on 25 linear benchmark tasks as described in Sec. 4.2.5. On the left, for the same architecture RG-128Gf, we show the delta in accuracy between SEER model (trained on Instagram) and supervised model trained on ImageNet. In the middle, we show the performance delta between best SEER model (trained on Instagram) and best self-supervised model (any approach, architecture, scale) trained on ImageNet. On the right, we show the delta but with the best supervised model trained on ImageNet.</p><p>of 256 and step learning rate schedule with gamma factor 0.1 with decay at steps <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b26">24]</ref>. For majority of tasks above, we use the same settings and note any differences in Appendix F.</p><p>Results. We report the linear probe numbers for all our models in Appendix <ref type="table" target="#tab_9">Table 17</ref> and in <ref type="table">Table 11</ref>, we report the performance of models on three standard tasks commonly used in computer vision. Further, in <ref type="figure">Figure 11</ref>, we summarize the difference in visual features quality of our best SEER model (RegNet-10B) compared to the best Im-ageNet based supervised and self-supervised performance on respective tasks. We observe that (i) For the same model size (RG-128Gf), our model trained on random images in the wild outperforms self-supervised models trained on Im-ageNet on 17 out of 25 tasks, (ii) our best model (RG-10B) also surpassed the best state-of-the-art self-supervised models (any size, approach data, and architecture) on 17 out of 25 tasks and achieves competitive (within 1% accuracy) on 5 out of 8 tasks, (iii) we further note that our best model also surpasses the best supervised (fully supervised or weaklysupervised) models (any size, architecture) on 14 out of 25 tasks and achieves competitive accuracy on the remaining.</p><p>(iv) on tasks in datasets such as medical imaging, satellite images, structured images, OCR, activity recognition in videos, our model consistently outperforms ImageNet models. On the other datasets such as Oxford Pets, Cars etc which are highly object centric, training on object-centric datasets gives better results yet our model achieves competitive performance despite training on random images in wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Salient Properties</head><p>Motivated by the use of discriminative self-supervised approach for training on random group of internet images, we also evaluate if the model learns some salient properties present in the images and differentiates between images. Towards this, in Sec. 5.1 we probe our model for the ability to predicting the GPS coordinates from images taken from all over the world. Further, we also probe the model embeddings space for the ability to embed together similar concepts with variations all over the world (for example, "wedding" concept varies culturally across the globe). For this, we qualitatively study the embeddings of hashtags (all languages, regions) in the model space in Sec. 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Geo Localization</head><p>In this task, we are interested in auditing if the model has learned some salient property allowing it to predict the gps coordinates of a given input image. We do so by coping with the problem of geolocalization i.e. predicting the GPS coordinates of images taken from all over the world. Such images exhibit a wide range of variations, i.e. picturing different objects, using different camera settings, taken Metric is the fraction of images localized within the given radius using the GCD distance. +HSC = using hierarchical classification +HSSC = with hierarchical and scene set classification.</p><p>at different daytime or seasons. Moreover, the images provide very few visual clues about the respective GPS location. Unlike previous works <ref type="bibr" target="#b114">[108,</ref><ref type="bibr" target="#b133">126,</ref><ref type="bibr" target="#b134">127,</ref><ref type="bibr" target="#b145">138]</ref>, we neither make prior assumptions on the task nor simplify the problem by restricting the task to images from famous landmarks and cities, natural areas like deserts or mountains. We want to test if the model works at a global scale without any assumptions on the data or task. We follow Muller et al. <ref type="bibr" target="#b94">[91]</ref> and treat this problem as a classification problem, sub-dividing the Earth into geographical cells. There are three types of partitionings: coarse, middle and fine, with varying number of cells. We visually illustrate the difference between those partitionings in Appendix <ref type="figure">Figure 18</ref>. In our experiment, we use the fine partitioning which divides the globe into 12, 893 cells. We finetune our model on a subset of YFCC100M <ref type="bibr" target="#b122">[115]</ref> introduced for the MediaEval Placing Task MP-16 <ref type="bibr" target="#b79">[76]</ref>. This subset includes 4, 219, 225 geo-tagged images from Flickr 11 . The dataset contains ambiguous photos of indoor environments, food, and humans for which the location is difficult to predict. During finetuning, we validate <ref type="bibr" target="#b13">11</ref> Available at:</p><p>https : / / multimedia -commons . s3website-us-west-2.amazonaws.com the performance on a validation set composed of 22, 855 geo-tagged images. We finetune for 15 epochs using SGD wth a momentum of 0.9, a weight decay of 1e ? 4, and a learning rate of 0.05. We decay the learning rate by 0.01 at epochs <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b14">12]</ref>. Finally, we evaluate the finetuned model on the im2gps <ref type="bibr" target="#b57">[54]</ref> test set, containing 237 geo-tagged images. We perform inference and record the top predicted cell for each image in the test set. The predicted cells are mapped back to a geographical latitude and longitude and the great circle distance (GCD) is computed by comparing to the ground truth latitude/longitude. Following Hays et al. <ref type="bibr" target="#b57">[54]</ref>, we report accuracy as the percentage of test images that are predicted within a certain distance to the ground-truth location. The results are presented in <ref type="table" target="#tab_2">Table 12</ref>. SEER models achieve the state-of-theart geolocalization results for all different distance thresholds. Moreover, as the size of models increases, geolocalization accuracy improves. We show qualitative results for this evaluation in <ref type="figure" target="#fig_3">Figure 14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Multilingual Hashtag Embeddings</head><p>In this experiment, we want to leverage our image encoder to get some qualitative understanding of our data and test if the model has learned some interesting salient properties. Also, since our network is self-supervised, it can be effectively used for that purpose. Indeed, our features show good representation properties without heavy finetuning. The image encoder can directly be used as a proxy metric for the metadata associated with images. Since we work on Instagram data, we propose to study whether our model allows to properly represent hashtags associated with the images.</p><p>In order to construct hashtag embeddings, we took a random subsample of 6, 000, 480 images with their associated hashtags. We sorted hashtags by their frequency, and kept the 30, 000 most frequent ones. The most frequent one appeared 220, 345 times and least frequent one 183 times. For each hashtag, we retrieved the set of images it is associated to, and computed their 256-dimensional features / model embeddings using the SEER RG-128Gf model. We represent the hashtag by simply taking the average of those features.</p><p>Given the hashtag features, we reduce the dimension to 50 using PCA. We represent the hashtags in a 2D plane by computing a t-sne map <ref type="bibr" target="#b126">[119]</ref>. We use the scikit-learn <ref type="bibr" target="#b103">[98]</ref> implementation, with a perplexity of 40.0, a learning rate of 100.0 and running for 5000 iterations.</p><p>Given that we have geo-diverse data i.e. our pretraining data represents images from all over the world as shown in <ref type="figure">Figure 2</ref>, we represent this diversity by color-coding the hashtag features to represent the countries. For each hashtag, we associate it with a country by taking a majority vote across images associated with that tag. Because of the predominance of US-based data (see <ref type="figure">Figure 2</ref>), this vote-based method leads to 14, 962 hashtags associated with the United States. Nonetheless, more than half of the data is associated with other countries and we obtain a wide coverage, with hashtags from 91 countries being represented. We represent the hashtag embeddings in <ref type="figure" target="#fig_9">Figure 12</ref>. For readability, we present the actual text associated with the features for 250 US-based tags.</p><p>We observe that our model indeed embeds together the hashtags in different languages but corresponding to same concept from all over the world. For instance, the concept wedding has hashtags: "shaadi" (Indian wedding), "nikah", "bridesmaid" etc all embedded in close proximity and likewise many other multilingual clusters appear. Further, the clusters are fine-grained for example: within the concept "wedding" sub-clusters appear like one for the wedding photoshoot, wedding dress, wedding design/styles etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we have demonstrated the potential of using self-supervised training on random internet images to train models that are more fair and less harmful (less harmful predictions, improved and less disparate learned attribute representations and larger improvement in object recognition on images from low/medium income households and non-Western countries). We train a 10B parameters dense model and observe that fairness indicator results improve as model size increases. We also observe better robustness to distribution shift, SOTA image copy detection and new metadata information captured by model such as gps prediction and multilingual word embeddings. The model also captures semantic information better and outperforms SOTA models (supervised and self-supervised) trained on ImageNet on 20 out 25 image classification tasks in computer vision while achieving competitive performance on the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>SEER Supervised <ref type="figure" target="#fig_1">Figure 13</ref>. Qualitative results of Copy Detection results on "strong" subset of Copydays as described in Sec. 4.2.4. The objective is to find the original image based on a copy of that image. Shown are queries in which the SEER model finds the correct image, while the supervised model fails to do so. We define correct as the original image being ranked first among all 10,157 database and distractor images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Licence Information for Photos in the Paper</head><p>The photos in <ref type="figure">Figure 1</ref> in section "Geographical diversity" (in order from left to right, top to bottom) are from dollar street, and have the following licences: The photos in <ref type="figure" target="#fig_5">Figure 6</ref> (in order left to right, top to bottom) are from dollar street, and have the following licences:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Datasets</head><p>We evaluate performance of SEER models on several tasks in computer vision. The list of tasks and the data distribution is presented in <ref type="table">Table 13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SEER model architecture and training Hyperparams</head><p>In <ref type="table" target="#tab_4">Table 14</ref>, we share in detail all the model architecture variants we explore to scale the SEER model size to 10billion parameters. In <ref type="table" target="#tab_7">Table 16</ref>, we summarize all the different sizes of SEER model and the configurations. For the 10Billion parameters SEER model, we describe the pretraining hyperparams in <ref type="table" target="#tab_6">Table 15</ref>. <ref type="table" target="#tab_4">Table 14</ref>. Model size scaling dimensions and variants explored for scaling architecture to 10B parameters. We chose a RegNetY-128gf model with 700M params as a base model and generated 6 variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Adapting LARC implementation for FSDP training</head><p>LARC <ref type="bibr" target="#b144">[137]</ref> scales the learning rate of each layer l based on the norm of the parameters w l , the norm of the gradient of the parameters ?w l , the weight decay ? and a trust coefficient ?, following the formula:</p><formula xml:id="formula_4">? l = ? ||w l || ||?w l || + ||w l || * ?<label>(4)</label></formula><p>When training with FSDP, parameters and their gradients are sharded across GPUs. To avoid adding additional parameter consolidation across GPUs to compute the norms, we adapt the implementation of LARC to compute a distributed norm without exchanging the weights, by decomposing the computation of the norm into a sum of squares and a square root.</p><p>The sum of square can be computed on each shard separately and then all-reduced before taking the square root of the resulting sum. We also batch the all-reduce of all For the 10 billion model, trained with FSDP, saving one checkpoint containing the model and optimizer state would require to consolidate the model and optimizer states, sharded across multiple GPUs during training, on a single GPU. This is impractical for memory reasons (it would account for 80GB of memory for FP32 weights) as well as communication reasons (consolidating mean communicating weights across GPUs).</p><p>Instead, for our 10 billion model, each GPU saves its own shard of the model weights and optimizer state, along with some metadata allowing to re-consolidate or re-shard the checkpoints offline. In addition, rank 0 GPU will save additional metadata allowing to locate the checkpoint shards.</p><p>During training, and after an interruption, we use the checkpoint shards to reload the model. Since training is resumed on the same number of GPU as the number number of shards, each GPU can simply load the shard corresponding to its rank and restart from there. This design allows to naturally exploit parallelism during model checkpointing and model reload. In practice, it makes state checkpointing <ref type="figure" target="#fig_4">Figure 15</ref>. The two types of state checkpoints we use for our 10 billion model. For training, sharded checkpoints (weights sharded by GPU, vertical strips in the diagram) are used for efficiency. For evaluation, sliced checkpoints (each slice containing a layer, horizontal strips in the diagram) are used to be agnostic to the number of GPU. A script transforms our vertically split checkpoints to horizontally split checkpoints between training and evaluation.</p><p>very fast for our 10 billion model.</p><p>For evaluations such as linear evaluation, it is impractical to use as many GPUs as were used during training, so we cannot rely on the same mechanism to load our model. Instead, we transform the sharded checkpoints into evaluation checkpoints that are structured in such a way that they can be loaded by a model sharded on a arbitrary number of GPUs.</p><p>To do so, we transform our checkpoint shards into a sliced checkpoint, where each slice of the checkpoint consists of the full weights of a given layer of the model. Additional metadata is saved in order to keep track of all the slices. This design, which avoids the memory issues of saving and loading a single consolidated checkpoint, is illustrated in <ref type="figure" target="#fig_4">Figure-15</ref>.</p><p>To load such a sliced checkpoint for evaluation, we build on the usual mechanism of FSDP, which consolidates layers one by one before each forward, only instead of calling forward, we initialize the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Representation Learning: Additional Results</head><p>In this section, we provide additional results and details for Sec. 4.2.5. We also provide results on low-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Linear Probe: complete Results</head><p>We present detailed results for our model and baseline models 4.2.1 on image classification datasets via linear probe in <ref type="table" target="#tab_9">Table 17</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Low-Shot Transfer</head><p>We also evaluate the performance of our 10Billion parameters SEER model in the low-shot setting, i.e. with a fraction of data (1% and 10% ImageNet) on the downstream task similar to previous studies <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b50">47]</ref>. The results are in <ref type="table">Table 18</ref>. We observe the performance consistently improves on these both tasks as the model size increases. Especially on the 1% fraction, the improvement is the the most +2 pts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Copy Detection Nuances</head><p>For the CopyDays benchmark, we follow a typical retrieval pipeline. This consists of extracting features from the train, database, and query datasets, post-processing these features, training PCA on the train set, applying PCA on the query and database set, and finally ranking the database for each query. From this ranking we calculate and report the mean average precision (mAP).</p><p>This pipeline has many dials to turn. We ran ex-  <ref type="table">Table 18</ref>. Low-shot learning on ImageNet and Places205 We compare our approach with semi-supervised approaches and selfsupervised pretraining on low-shot learning. Our model is finetuned on either 1% or 10% of ImageNet, and does not access the rest of ImageNet images. As opposed to our method, the other methods use all the images from ImageNet during pretraining or finetuning.</p><p>periments with the res4 and res5 layers, different postprocessing methods including, Regional Maximum Activations of Convolutions (R-MAC) <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b123">116]</ref>, Generalized-Mean (GeM) pooling <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b21">19]</ref>, and Average and Max Pooling, which are special cases of GeM pooling. We also swept over different image sizes, PCA dimensions, and R-MAC spatial levels. Our best results typically used the res4 layer with R-MAC spatial level 3, while the best PCA dimension and image size depended on the model. When using R-MAC, we trained the PCA on the full max pool matrix of the different crops. When applying PCA on the database and query datasets, we apply PCA on the same full max pool matrix of the different crops before summing the crops and normalizing the output as in the R-MAC algorithm. All of the code used is available in the open source VISSL library <ref type="bibr" target="#b52">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Data distributions</head><p>For the fairness evaluations on DollarStreet and Hateful Memes challenge, we show the data distribution in <ref type="figure" target="#fig_6">Figure 17</ref> and <ref type="figure" target="#fig_5">Figure 16</ref> respectively. Further, for studying the geolocalization salient property, we visually demonstrate the differences between various types of cell partitionings in <ref type="figure">Figure 18</ref>. Hateful memes: types of attacks  <ref type="figure">Figure 18</ref>. The im2gps evaluation requires finetuning a classification model on either coarse, medium, or fine partitionings of the world. The model outputs a probability distribution over these partitions, we predict the partition with the greatest probability, and choose the mean latitude and longitude of the predicted partition for our final prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Overview of FSDP data-parallel training. Each model layer is sharded across all data-parallel workers. Dotted lines represent synchronisation points between GPUs and arrows represent dependencies. We overlap computation and communication to improve the efficiency of FSDP training by, for example, scheduling all-gathers and forwards on different CUDA streams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>Figure 4. Impact of activation checkpointing. Memory profile of the 10B model on 8 GPUs and 8 images as input before and after inserting dynamic activation checkpointing. The peak memory usage is reduced from 26GB to 16GB. The computation time increases by 15% for same number of images but allows to increase batch size, hence increasing computational efficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Geographical fairness object recognition top-5 accuracy per country of SEER RG-10B model on Dollar Street Dataset. This dataset comprises of 54 countries and we show accuracy per country.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative analysis of Geographical fairness on DollarStreet dataset as described in Sec. 4.1.3. For a fixed architecture, here RG-128Gf, we show a few example images of improvement where our SEER model outperforms self-supervised and supervised pretraining on ImageNet-1K. The examples are from various households of varying income levels and regions of the world. See supplemental material for license information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7</head><label>7</label><figDesc>Figure 7. Influence of SEER model scale on out-of-domain data generalization and robustness to distribution shift. Across all the datasets, we observe that as the model size increases, the performance increases significantly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>iNaturalist18</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Qualitative analysis of fine-grained recognition performance as described in Sec. 4.2.3. We show few example images from iNaturalist18 where SEER model demonstrates better performance than pre-training on ImageNet-1K. Each row represents a different category of animals, small mammals, birds, larger mammals and sea mammals. SEER is better at identifying a wide variety of animal species across different view points, lightning conditions, obstructions and zooms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>T-SNE map of Hashtag representations. We color-code hashtags from different countries. For easy readability, the hashtag text is provided for a subset of 250 tags. We see clear patterns emerging, for examples the tags about food, sports, animals being grouped together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 .</head><label>14</label><figDesc>Visualization of diverse examples from the im2gps test sets. We chart RegNet-256Gf SEER predictions and targets, and display how many kilometers the SEER model predictions are off from the target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>?</head><label></label><figDesc>"Kitchens" South Korea -Photo: Luc Forsyth (CC BY 4.0) ? "Armchairs" Romania -Photo: Catalin Georgescu (CC BY 4.0) ? "Everyday Shoes" Bulgaria -Photo: Boryana Katsarova (CC BY 4.0) ? "Spices" India -Photo: AJ Sharma (CC BY 4.0) ? "Necklaces" Pakistan -Photo: Hisham Najam (CC BY 4.0) ? "Bathrooms" Kenya -Photo: Chris Dade (CC BY 4.0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>r a c e r e l i g i o n s e x d i s a b i l i t y n a t i o n a l i t y e m p t y 10 100</head><label>10</label><figDesc>i t i n g v i o l e n c e d e h u m a n i z i n g i n f e r i o r i t y c o n t e m p t m o c k i n g s l u r s e x c l u s i o n e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 .Figure 17 .</head><label>1617</label><figDesc>Data distribution of types of hate-speech in dev set of HatefulMemes dataset. Data Distribution of Dollar Street dataset which features 94 concepts across 54 countries and 4 regions of the world. M or occo Test I m age ( 31.7917? N, 7.0926? W) M edium F ine Act ual Predict ed I m 2gps W or ld Par t it ioning Ex am ple C oar se</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.2.2, fine-grained image recognition in Sec. 4.2.3, image copy detection in Sec. 4.2.4 and finally test the feature representation quality via linear probe on over 25 computer vision datasets in Sec. 4.2.5.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>Disparity in Gender retrieval performance between subgroups for different gender and skintone corresponding to the retrieval performance in Table 1. Lower number is better and in- dicates lower disparity or in other words, the model works equally well for male / female genders and lighter/darker skintone. Our biggest model achieves lowest disparity and overall higher preci- sion.ness indicators: (i) disparities in learned representations of people's membership in social groups Sec. 4.1.1, (ii) harmful mislabeling of images of people in Sec. 4.1.2, (iii) geographical disparity in object recognition in Sec. 4.1.3. Further, we also test on multimodal (image and text) hate speech detection for different types of hate-speech in Sec. 4.1.4.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>We then, for each gender and skintone perform inference of transferred models on the Casual Conversations Dataset and measure percentage of images associated with different labels at confidence threshold 0.1 following<ref type="bibr" target="#b54">[51]</ref>. We observe that our model makes the least Harmful predictions and most Human predictions on images of people.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Gender Skintone</cell><cell></cell><cell></cell><cell cols="2">Age Groups</cell></row><row><cell>Model</cell><cell>Data</cell><cell>Arch.</cell><cell>Assoc.</cell><cell>female darker</cell><cell>female lighter</cell><cell>male darker</cell><cell>male lighter</cell><cell cols="4">18-30 30-45 45-70 70+</cell></row><row><cell>Supervised</cell><cell cols="3">INet-1K RG-128Gf Non-Human</cell><cell>2.3</cell><cell>6.0</cell><cell>2.0</cell><cell>1.8</cell><cell>2.1</cell><cell>2.4</cell><cell>5.4</cell><cell>4.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Crime</cell><cell>1.2</cell><cell>0.2</cell><cell>0.7</cell><cell>0.4</cell><cell>0.6</cell><cell>0.9</cell><cell>0.1</cell><cell>3.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Human</cell><cell>37.4</cell><cell>18.5</cell><cell>29.5</cell><cell>17.5</cell><cell>26.9</cell><cell>25.7</cell><cell cols="2">22.8 21.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Possibly-Human</cell><cell>24.3</cell><cell>41.4</cell><cell>50.1</cell><cell>54.0</cell><cell>43.9</cell><cell>43.7</cell><cell cols="2">39.7 22.7</cell></row><row><cell cols="4">Self-Supervised pretraining on ImageNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SwAV</cell><cell cols="3">INet-1K RG-128Gf Non-Human</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.1</cell><cell>0.1</cell><cell>0.2</cell><cell>0.2</cell><cell>0.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Crime</cell><cell>0.1</cell><cell>0.1</cell><cell>0.3</cell><cell>0.1</cell><cell>0.1</cell><cell>0.3</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Human</cell><cell>58.7</cell><cell>58.2</cell><cell>32.2</cell><cell>43.1</cell><cell>46.6</cell><cell>44.7</cell><cell cols="2">57.9 46.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Possibly-Human</cell><cell>66.9</cell><cell>66.4</cell><cell>82.5</cell><cell>70.4</cell><cell>70.8</cell><cell>73.4</cell><cell cols="2">69.1 53.2</cell></row><row><cell cols="3">Pretrained on random internet images</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SEER (ours) IG-1B</cell><cell cols="2">RG-128Gf Non-Human</cell><cell>0.1</cell><cell>0.6</cell><cell>0.7</cell><cell>0.7</cell><cell>0.8</cell><cell>0.1</cell><cell>0.5</cell><cell>3.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Crime</cell><cell>0.1</cell><cell>0.1</cell><cell>0.2</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.2</cell><cell>0.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Human</cell><cell>78.7</cell><cell>73.3</cell><cell>40.0</cell><cell>43.3</cell><cell>58.4</cell><cell>57.4</cell><cell cols="2">66.1 67.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Possibly-Human</cell><cell>23.8</cell><cell>21.8</cell><cell>56.4</cell><cell>40.6</cell><cell>38.7</cell><cell>38.6</cell><cell cols="2">24.8 6.45</cell></row><row><cell cols="2">SEER (ours) IG-1B</cell><cell>RG-10B</cell><cell>Non-Human</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0</cell><cell>0.1</cell><cell>0</cell><cell>0.1</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Crime</cell><cell>0</cell><cell>0</cell><cell>0.2</cell><cell>0.1</cell><cell>0</cell><cell>0.1</cell><cell>0</cell><cell>1.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Human</cell><cell>93.0</cell><cell>87.3</cell><cell>57.2</cell><cell>59.8</cell><cell>73.3</cell><cell>72.7</cell><cell cols="2">82.4 79.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Possibly-Human</cell><cell>20.2</cell><cell>27.9</cell><cell>72.6</cell><cell>65.1</cell><cell>44.9</cell><cell>48.3</cell><cell cols="2">39.5 22.6</cell></row><row><cell>Association</cell><cell>Type</cell><cell cols="2">Labels in the ImageNet taxonomy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">swine, slug, snake, monkey, lemur,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">chimpanzee, baboon, animal, bonobo,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Non-Human</cell><cell>Harmful</cell><cell cols="2">mandrill, rat, dog, capuchin, gorilla,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">mountain gorilla, ape,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">great ape, orangutan.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Crime</cell><cell>Harmful</cell><cell>prison</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Human</cell><cell cols="3">Non-Harmful face, people</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Possibly-Human Non-Harmful makeup, khimar, beard</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Table 3. Label Association Fairness Indicator2 results of several models on the Casual Conversations Dataset as described in Sec. 4.1.2. This indicator helps measure magnitude of Harmful (Non-Human, Crime) label predictions for images of people. Lower [Non-Human, Crime] is better and Higher [Human] is better. Since self-supervised models don't predict labels, all models need to be adapted to image classification task. We full-finetune all models on same subset of ImageNet-22K dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Label association types for ImageNet taxonomy for computing Harmful and Non-Harmful label associations (Sec. 4.1.2).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Geographical Fairness Indicator3 results and diversity analysis of object recognition accuracy for different income households and regions of the world as described in Sec. 4.1.3. This indicator allows measuring how good the model is at detecting objects all over the world and in various households of varying income brackets. Higher number is better. We observe that our model achieves better object recognition accuracy for all income brackets and regions of the world. Moreover, the object recognition accuracy improves the most for low-and medium-income brackets and for non-America/non-Europe regions of the world.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Hate Speech Detection Performance of several models on the HatefulMemes dev set as described in Sec. 4.1.4. For each model, we run evaluation using three different seeds and report the average performance over all seeds. We observe that our model achieves the best performance in hate speech detection and the performance improves as the model size increases.</figDesc><table><row><cell>,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>.2.3 including the application of model in wildlife conservation efforts, image retrieval (copy detection) in Sec. 4.2.4, and representation learning via linear-probe to test image classification performance on more than 25 standard object and scene datasets including ImageNet-1K<ref type="bibr" target="#b111">[106]</ref> (object centric), Places205<ref type="bibr" target="#b152">[145]</ref> (scene centric) and PASCAL VOC07<ref type="bibr" target="#b43">[41]</ref> (multi-label) in Sec. 4.2.5. We also compare our model performance with state-of-the-art supervised and self-supervised learning on ImageNet-1K on computer vision datasets capturing variety of applications such as OCR, activity recognition in videos, scene recognition, medical and satellite images, structured datasets (to test localization) in and show full results in Appendix F.</figDesc><table><row><cell>SEER: street view</cell><cell>SEER: street view</cell><cell>SEER: street view</cell><cell>SEER: street view</cell><cell>SEER: street view</cell><cell>SEER: street view</cell><cell>SEER: street view</cell><cell>SEER: street view</cell></row><row><cell>SwAV: homes</cell><cell>SwAV: homes</cell><cell>SwAV: fruit trees</cell><cell>SwAV: homes</cell><cell>SwAV: homes</cell><cell>SwAV: homes</cell><cell>SwAV: homes</cell><cell>SwAV: fruit trees</cell></row><row><cell>Sup: parking lots</cell><cell>Sup: homes</cell><cell>Sup: vegetable plots</cell><cell>Sup: trash</cell><cell>Sup: fruit trees</cell><cell>Sup: homes</cell><cell>Sup: roofs</cell><cell>Sup: fruit trees</cell></row><row><cell>SEER: spices</cell><cell>SEER: spices</cell><cell>SEER: spices</cell><cell>SEER: spices</cell><cell>SEER: spices</cell><cell>SEER: spices</cell><cell>SEER: spices</cell><cell>SEER: spices</cell></row><row><cell>SwAV: grains</cell><cell>SwAV: medication</cell><cell>SwAV: makeup</cell><cell>SwAV: medication</cell><cell>SwAV: social drinks</cell><cell>SwAV: medication</cell><cell>SwAV: cooking pots</cell><cell>SwAV: grains</cell></row><row><cell>Sup: cleaning equip</cell><cell>Sup: medication</cell><cell>Sup: makeup</cell><cell>Sup: medication</cell><cell>Sup: medication</cell><cell>Sup: makeup</cell><cell>Sup: cooking pots</cell><cell>Sup: bowls</cell></row><row><cell>SEER: shoes</cell><cell>SEER: shoes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SwAV: cooking eq.</cell><cell>SwAV: storage room</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sup: cooking eq.</cell><cell>Sup: power outlet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SEER: armchair</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SwAV: wheelbarrow</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Sup: oven</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>Out-of-domain performance of all models on various dataset with distribution shift as described in Sec. 4.2.2. The models are finetuned on ImageNet and resulting models are evaluated (inference only) on the target datasets. The best numbers for each dataset are in bold. Our model outperforms supervised and self-supervised models trained on ImageNet.</figDesc><table><row><cell>Model</cell><cell>Arch.</cell><cell cols="2">Pretrain Param</cell><cell cols="7">INet val INet-A INet-R INet-Sketch INet-ReaL INet-v2 ObjectNet</cell></row><row><cell>Supervised</cell><cell>RG-128Gf</cell><cell cols="2">INet-1K 693M</cell><cell>82.1</cell><cell>21.6</cell><cell>41.0</cell><cell>27.7</cell><cell>87.0</cell><cell>71.3</cell><cell>44.1</cell></row><row><cell cols="3">Self-supervised pretraining on full ImageNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DINO</cell><cell>ViT-B/16</cell><cell cols="2">INet-1K 85M</cell><cell>81.4</cell><cell>21.4</cell><cell>46.1</cell><cell>33.3</cell><cell>86.4</cell><cell>70.1</cell><cell>39.4</cell></row><row><cell cols="4">SimCLR-v2 RN152w3+SK INet-1K 794M</cell><cell>83.5</cell><cell>35.2</cell><cell>46.7</cell><cell>34.7</cell><cell>87.7</cell><cell>73.0</cell><cell>48.0</cell></row><row><cell>BYOL</cell><cell>RN200w2</cell><cell cols="2">INet-1K 250M</cell><cell>83.5</cell><cell>43.0</cell><cell>47.1</cell><cell>35.5</cell><cell>88.1</cell><cell>73.1</cell><cell>50.7</cell></row><row><cell>SwAV</cell><cell>RN50w5</cell><cell cols="2">INet-1K 585M</cell><cell>81.8</cell><cell>26.5</cell><cell>39.6</cell><cell>26.9</cell><cell>86.8</cell><cell>70.0</cell><cell>43.9</cell></row><row><cell>SwAV</cell><cell>RG-128Gf</cell><cell cols="2">INet-1K 693M</cell><cell>82.9</cell><cell>28.0</cell><cell>42.8</cell><cell>32.0</cell><cell>87.4</cell><cell>71.8</cell><cell>44.7</cell></row><row><cell>SwAV</cell><cell>RG-128Gf</cell><cell cols="2">INet-22k 693M</cell><cell>83.9</cell><cell>37.8</cell><cell>47.8</cell><cell>37.9</cell><cell>88.7</cell><cell>73.3</cell><cell>50.0</cell></row><row><cell cols="3">Pretrained on random internet images</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SEER (ours) RG-128Gf</cell><cell>IG-1B</cell><cell>693M</cell><cell>84.5</cell><cell>43.6</cell><cell>51.0</cell><cell>40.2</cell><cell>89.3</cell><cell>74.7</cell><cell>54.3</cell></row><row><cell cols="2">SEER (ours) RG-10B</cell><cell>IG-1B</cell><cell>10B</cell><cell>85.8</cell><cell>52.7</cell><cell>56.1</cell><cell>45.6</cell><cell>89.8</cell><cell>76.2</cell><cell>60.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>Image Copy Detection performance (mAP) on the "strong" subset of the Copydays dataset as described in Sec. 4.2.4.</figDesc><table><row><cell></cell><cell>.</cell><cell>dims</cell><cell>size</cell><cell>mAP</cell></row><row><cell cols="3">Supervised pretraining on ImageNet</cell><cell></cell><cell></cell></row><row><cell cols="5">Multigrain [10] ResNet-50 2048 long 800 82.5</cell></row><row><cell cols="2">Supervised [19] ViT-B16</cell><cell>1536</cell><cell>224 2</cell><cell>76.4</cell></row><row><cell cols="3">Self-supervised pretraining on ImageNet</cell><cell></cell><cell></cell></row><row><cell>DINO [19]</cell><cell>ViT-B/16</cell><cell>1536</cell><cell>224 2</cell><cell>81.7</cell></row><row><cell>DINO [19]</cell><cell>ViT-B/8</cell><cell>1536</cell><cell>320 2</cell><cell>85.5</cell></row><row><cell>SwAV</cell><cell cols="4">ResNet-50 1024 long 224 76.2</cell></row><row><cell>SwAV</cell><cell cols="4">RG-128Gf 2904 long 224 83.0</cell></row><row><cell cols="3">Pretrained on random internet images</cell><cell></cell><cell></cell></row><row><cell>SEER (ours)</cell><cell cols="4">RG-128Gf 2904 long 224 86.5</cell></row><row><cell>SEER (ours)</cell><cell cols="4">RG-256Gf 4096 long 224 87.8</cell></row><row><cell>SEER (ours)</cell><cell>RG-10B</cell><cell cols="3">4096 long 384 88.8</cell></row><row><cell>SEER (ours)</cell><cell>RG-10B</cell><cell cols="3">9500 long 384 90.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 .</head><label>12</label><figDesc>Geo Localization results of several models on Im2GPS test set (top) containing 237 images and Im2GPS3k test set (bottom).</figDesc><table><row><cell></cell><cell cols="2">Accuracy within Distance (km)</cell></row><row><cell>Street</cell><cell>City</cell><cell>Region Country Continent</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 15 .</head><label>15</label><figDesc>Hyperaprams of SEER 10B params model pretraining. the layers together. As a result, enabling LARC only incurs the overhead of one single all-reduce on a tensor of size 2L where L is the number of layers of our model.E. Model State Dictionary checkpointingAs typically done during training, we save our model state checkpoints allowing us to restart the training upon interruption as well as evaluate the model at intermediate training stages. For small models, this is typically done by saving the model weights and optimizer state in one file, in addition to information about the current training step and learning rate scheduler data.</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Batch size</cell><cell>7936</cell></row><row><cell>Crops</cell><cell>2x160+4x96</cell></row><row><cell>Head</cell><cell>[28280, 8192, 8192, 256] (no BatchNorm)</cell></row><row><cell>Training epochs</cell><cell>1</cell></row><row><cell>Training Images</cell><cell>1 Billion</cell></row><row><cell cols="2">loss sinkhorn iterations 10</cell></row><row><cell>loss epsilon</cell><cell>0.03</cell></row><row><cell>loss temperature</cell><cell>0.1</cell></row><row><cell>Weight decay</cell><cell>1e-5</cell></row><row><cell>Warm-up iterations</cell><cell>5500</cell></row><row><cell>SGD momentum</cell><cell>0.9</cell></row><row><cell>SyncBatchNorm</cell><cell>yes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 16 .Table 17 .</head><label>1617</label><figDesc>Configuration of all SEER models with number of parameters varying from 40 million to 10 billion.<ref type="bibr" target="#b79">76</ref>.19 88.67 96.6 73.88 93.42 85.6 87.43 64.35 95.32 53.23 84.22 76.04 53.62 82.7 71.14 42.0 92.53 92.75 71.03 73.6 99.28 79.13 42.31 RG-16gf 84.03 76.13 78.90 89.04 96.46 74.20 93.77 85.28 88.46 66.63 95.19 55.18 83.35 78.05 55.31 83.81 72.86 41.94 92.85 93.84 72.78 73.7 99.29 79.81 44.2 RG-32gf 84.49 75.49 80.32 89.83 97.08 77.18 93.39 84.66 87.81 70.04 95.51 53.76 85.02 78.0 58.21 84.24 74.57 44.31 94.03 93.95 72.74 74.67 99.45 81.53 46.36 RG-64gf 87.24 73.41 82.53 90.81 97.1 77.55 93.88 83.77 88.97 74.71 97.03 54.77 84.36 78.11 61.52 85.5 75.73 45.44 94.78 94.65 74.57 74.2 99.32 82.03 47.86 RG-128gf 87.98 72.67 84.57 91.10 97.52 78.62 94.34 84.14 89.61 77.43 95.45 55.15 87.15 79.71 64.51 86.83 77.39 47.96 95.24 95.27 76.28 74.33 99.3 81.95 49.05 RG-256gf 89.12 72.69 87.67 91.11 97.6 80.69 94.8 81.42 89.16 77.25 95.61 56.56 87.73 80.66 66.58 87.8 78.95 48.77 96.99 96.08 78.12 75.40 99.44 88.03 51.9 RG-10B 89.28 74.98 90.3 91.0 97.5 81.1 95.61 86.4 90.71 81.90 96.26 58.03 -85.3 68.03 90.0 81.53 54.82 97.3 96.3 80.0 78.34 99.42 82.4 51.13 SwAV RN50 80.7 71.26 77.88 89.27 96.72 68.72 92.61 80.79 91.74 73.30 92.02 41.87 85.79 88.89 62.65 86.38 74.23 45.44 96.46 93.12 73.53 70.32 99.24 78.49 39.31 RG-128gf 83.64 69.85 82.22 89.18 97.26 76.22 94.32 74.05 91.11 75.92 95.68 48.87 85.00 89.05 68.79 89.85 79.48 45.95 97.48 95.5 76.11 75.67 99.24 80.81 41.46 RN50-w5 81.87 69.67 82.05 88.57 97.48 75.90 94.73 74.42 91.7 82.28 91.74 46.83 85.11 81.17 70.18 88.79 78.28 45.98 97.41 94.72 -77.67 99.25 79.26 42.32 DINO RN50 81.79 70.18 78.68 89.47 96.88 70.16 93.48 79.41 91.23 68.59 89.35 41.64 84.12 89.98 60.64 85.42 75.21 47.66 96.49 94.2 73.39 71.73 99.26 78.70 39.69 DeiT-S/16 52.63 53.12 80.95 89.60 96.98 73.78 93.05 70.37 83.73 29.48 23.72 39.44 85.47 92.21 59.83 87.17 78.69 46.83 97.45 94.52 72.86 68.32 96.94 79.36 40.02 DeiT-S/8 55.08 53.35 83.67 90.61 96.52 73.67 92.37 72.03 86.53 29.56 23.29 36.69 86.32 93.95 67.73 89.19 81.21 53.36 98.1 95.48 74.67 68.18 97.61 83.25 42.43 DeiT-B/16 54.06 53.25 82.96 90.19 97.3 74.79 93.97 70.53 87.27 31.06 25.22 40.76 85.50 93.93 71.51 89.01 82.2 52.1 98.3 96.72 74.15 69.65 97.6 82.69 42.5 DeiT-B/8 55.15 53.29 85.96 90.18 97.4 76.54 93.67 71.97 87.57 32.58 24.45 40.21 86.87 92.67 75.17 91.15 82.9 57.79 98.73 97.59 75.6 71.79 97.87 82.8 44.67 70.4 35.9 85.0 63.1 43.5 85.6 85.4 52.6 60.2 97.6 64.2 40.84.4 48.3 93.4 76.3 51.1 95.7 90.7 60.2 75.4 98.4 72.7 47.8 v3-ViT-B/16 57.03 56.03 81.09 90.44 96.90 73.09 93.35 73.76 84.98 30.77 25.51 44.63 86.74 91.94 63.01 90.67 82.52 44.04 97.89 94.44 73.60 70.25 97.69 79.41 41.17 88.3 61.6 93.6 79.1 62.3 96.4 94.3 63.7 71.4 98.7 77.3 49.3 RN200w2 78.74 68.42 77.03 90.80 96.34 74.68 92.53 75.95 87.17 78.56 94.64 38.74 84.25 91.91 66.04 91.62 81.19 45.59 97.73 92.34 72.98 73.53 98.97 80.37 40.56 90.0 57.1 94.8 79.9 52.0 97.6 92.7 65.2 70.6 97.2 78.8 52.4 RN152w3+SK 56.57 48.76 75.0 87.63 94.32 70.32 89.77 55.37 80.07 50.12 64.58 41.25 81.53 88.15 60.75 68.86 59.23 41.22 93.45 91.87 67.31 70.66 95.74 70.8 38.83 Imagenet Supervised RN50 65.06 74.13 72.96 88.52 94.98 68.09 88.56 89.75 92.29 71.83 90.61 39.58 83.42 92.21 64.22 87.49 75.43 44.04 95.42 93.40 67.55 68.38 98.95 73.26 35.13 RG-64gf 70.74 73.59 78.4 91.66 95.5 73.94 90.45 79.46 88.03 74.39 96.75 38.10 83.17 93.36 71.35 89.08 79.76 47.21 97.48 94.71 73.32 74.2 98.91 78.36 39.93 RG-128gf 73.65 73.35 78.52 90.54 95.82 73.03 91.07 79.11 88.40 76.79 95.06 39.73 83.6 93.06 70.24 89.81 80.96 45.89 97.28 94.95 74.16 71.32 99.03 76.64 40.15 DeiT-B/16 53.37 54.25 82.13 90.29 96.6 72.08 92.48 69.43 83.28 33.33 24.28 34.11 84.73 93.41 67.28 90.83 81.6 43.62 98.14 93.56 73.78 69.05 97.99 77.56 42.10 o ViT-B/16 INet-22k 54.76 53.84 89.9 93.04 96.64 78.40 93.53 74.95 86.38 33.19 24.51 33.8 86.03 93.98 75.68 93.65 88.04 52.52 99.3 99.68 80.22 70.99 98.21 85.79 49.17 94.9 76.8 95.0 80.7 71.5 99.2 96.3 69.6 70.9 98.6 82.4 57.7 88.2 78.3 88.7 70.3 49.1 96.6 96.1 73.3 70.2 98.3 81.6 57.2 95.1 90.9 98.0 87.5 69.4 99.7 99.2 81.8 64.7 99.2 91.5 72.0 95.1 91.5 97.9 87.4 71.6 99.7 99.2 82.2 69.2 99.2 92.0 73.0 Linear probe results for all models on 25 different datasets. All models are evaluated by us unless otherwise indicated in which case, results are from Table 10 of [99].</figDesc><table><row><cell>.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/facebookresearch/fairscale 3 We implemented it in open sourced library https://github. com/facebookresearch/vissl.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/NVIDIA/apex<ref type="bibr" target="#b6">5</ref> We use lower resolution 160 instead of 224 for the bigger crop for better training speed. Our experiments (for smaller model sizes) yielded marginal difference in performance on downstream task between the two crop sizes</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We use open source library https : / / github . com / facebookresearch/mmf.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We note that iWildCam-WILDS dataset enables us to test the practical application of computer vision research in wildlife preservation effort where the models are used to recognize animal species (if any) in the camera trap.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">We follow<ref type="bibr" target="#b53">[50]</ref> which uses a BatchNorm followed by linear layer. We found that this setting leads to robust hyperparameter choice and sweeping hyperparams such as learning rate, weight decay only leads to marginal (+/-0.1) change in performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno>brush SEER: necklace SwAV: necklace Sup: phone SEER: necklace SwAV: wall clock Sup: computer SEER: armchair SwAV: bed Sup: bed SEER: armchair SwAV: shower Sup: shower SEER: armchair SwAV: armchair Sup: floor SEER: armchair SwAV: furniture Sup: bed SEER: armchair</idno>
		<title level="m">SwAV: plates of food Sup: furniture SEER: armchair SwAV: cleaning eq. Sup: tools SEER: armchair SwAV: bathroom Sup: bathroom SEER: stoves SwAV: cooking pots Sup: oven SEER: stoves SwAV: oven Sup: oven SEER: stoves SwAV: cooking pots Sup: cooking pots SEER: stoves SwAV: cooking pots Sup: oven SEER: stoves SwAV: cooking pots Sup: oven SEER: stoves SwAV: cooking eq. Sup: kitchen SEER: stoves SwAV: bathroom Sup: bathroom SEER: stoves SwAV: homes Sup: homes SEER: garden cart SwAV: lawn mower Sup: safety pin SEER: acorn SwAV: hotdog Sup: parachute INet-Sketch INet-R SEER: lion SwAV: cliff, drop-off Sup: custard apple INet-A SEER: honeycomb SwAV: wok Sup: dutch oven INet-Real SEER: basketball SwAV: rugby ball Sup: running shoe INet-V2</title>
		<imprint/>
	</monogr>
	<note>Sup: instrument SEER: necklace SwAV: furniture Sup: jacket SEER: necklace SwAV: necklace Sup: hair brush SEER: necklace SwAV: instrument Sup: power outlet SEER: necklace SwAV: furniture Sup: shower SEER: necklace SwAV: hair brush Sup: hair</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<idno>drop-off SEER: theater curtain SwAV: laptop Sup: web-site SEER: crosswords SwAV: dial phone Sup: ruler SEER: meat cleaver SwAV: bath towel Sup: windsor tie SEER: police wagon SwAV: sports car Sup: race car SEER: hen SwAV: bucket Sup: shield SEER: badger SwAV: tray Sup: alp SEER: viaduc SwAV: spider web Sup: spider web SEER: daisy SwAV: hen of the woods Sup: coral reef SEER: carbonara SwAV: strainer Sup: corn SEER: scandal SwAV: plunger Sup: plunger</idno>
		<title level="m">table lamp Sup: table lamp SEER: sunglasses SwAV: monitor Sup: polaroid camera SEER: doormat SwAV: bath towel Sup: suit of clothes SEER: mushroom SwAV: nail Sup: nail SEER: ambulance SwAV: carton Sup: plunger SEER: paintbrush SwAV: screwdriver Sup</title>
		<imprint/>
	</monogr>
	<note>SwAV: cliff, drop-off Sup: cliff</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Alcorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shinn</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">Markus</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Vicreg: Variance-invariance-covariance regularization for self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bardes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04906</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">To &quot;see&quot; is to stereotype: Image tagging algorithms, gender recognition, and the accuracy-fairness trade-off</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Barlas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyriakos</forename><surname>Kyriakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Guest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Styliani</forename><surname>Kleanthous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahna</forename><surname>Otterbacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Hum.-Comput. Interact</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">CSCW3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The iwildcam 2020 competition dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elijah</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvi</forename><surname>Gjoka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<title level="m">Jonathon Shlens, and Barret Zoph. Revisiting resnets: Improved training and scaling strategies</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">MultiGrain: a unified image embedding for classes and instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05509</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Xiaohua Zhai, and A?ron van den Oord</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolesnikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
	</analytic>
	<monogr>
		<title level="m">Are we done with imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exposing and correcting the gender bias in image captioning datasets and models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Grounding inductive biases in natural images: invariance stems from variations in data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05121</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gender shades: Intersectional accuracy disparities in commercial gender classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In FACCT</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Chloe Hillier, and Andrew Zisserman. A short note on the kinetics-700 human action dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno>PMLR. 18</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>Geoffrey Gordon, David Dunson, and Miroslav Dud?k</editor>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="11" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Does object recognition work for everyone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Tutorial on fairness, accountability, transparency and ethics in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<idno>2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A study and comparison of human and deep learning recognition performance under visual distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Karam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fast and accurate model scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Discriminative unsupervised feature learning with convolutional neural networks. CoRR, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6909</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Evaluation of gist descriptors for web-scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsimrat</forename><surname>Sandhawalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Amsaleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Image and Video Retrieval, CIVR &apos;09</title>
		<meeting>the ACM International Conference on Image and Video Retrieval, CIVR &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Auditing imagenet: Towards a model-driven framework for annotating demographic attributes of large-scale image datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dulhanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Soleil et peau</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Sun and skin</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Journal de M?decine Esth?tique</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="33" to="34" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
	<note>in French</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Milking cowmask for semi-supervised image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12022</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Online bagof-visual-words generation for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11552</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Selfsupervised pretraining of visual features in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01988</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
	</analytic>
	<monogr>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Duval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/vissl,2021.6" />
		<imprint>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scaling and benchmarking self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Levent Sagun, and Nicolas Usunier. Fairness indicators for systematic assessments of visual feature extractors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><forename type="middle">Romero</forename><surname>Soriano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07603</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Im2gps: estimating geographic information from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Towards measuring fairness in ai: the casual conversations dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Dolhansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqueline</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian Canton</forename><surname>Ferrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biometrics, Behavior, and Identity Science</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Natural adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning by neighbourhood discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Self-supervised feature learning by learning to spot artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Don&apos;t ask if artificial intelligence is good or fair, ask how it shifts power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratyusha</forename><surname>Kalluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">583</biblScope>
			<biblScope unit="issue">7815</biblScope>
			<biblScope unit="page" from="169" to="169" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The misgendering machines: Trans/hci implications of automatic gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Os</forename><surname>Keyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Hum.-Comput. Interact</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2003" />
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Firooz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Donggeun Yoo, and In So Kweon. Learning image representations by completing damaged jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Wilds: A benchmark of in-the-wild distribution shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Michael</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">Lanas</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irena</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Stavness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><forename type="middle">S</forename><surname>Earnshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Kulynych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Madras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smitha</forename><surname>Milli</surname></persName>
		</author>
		<title level="m">ioluwa Deborah Raji, Angela Zhou, and Richard Zemel. Participatory approaches to machine learning. International Conference on Machine Learning Workshop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">The benchmarking initiative for multimedia evaluation: Mediaeval 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">A sober look at the unsupervised learning of disentangled representations and their evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>R?tsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Cross pixel optical flow similarity for self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05636</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement testing sprites dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<ptr target="https://github.com/deepmind/dsprites-dataset/" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Accuracy on the line: On the strong correlation between out-of-distribution and in-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Geolocation estimation of photos using a hierarchical model and scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Muller-Budack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kader</forename><surname>Pustu-Iren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Ewerth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
				<title level="m">Sixth Indian Conference on Computer Vision, Graphics Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
				<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Zero: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02054</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Jie</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Andr? Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<pubPlace>Alexander C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Concept generalization in visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Bulent Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9629" to="9639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">City-scale location recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Schwemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carly</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">D</forename><surname>Bello-Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Oklobdzija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martijn</forename><surname>Schoonvelde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">W</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Diagnosing gender bias in image recognition systems. Socius, 6:2378023120967171</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lockhart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">The german traffic sign recognition benchmark: A multi-class classification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="498" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Particular object retrieval with integral max-pooling of cnn activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877,2020.15</idno>
		<imprint/>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08237,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Fixefficientnet. preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">Rotation equivariant cnns for digital pathology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bastiaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Linmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Transitive invariance for self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Planetphoto geolocation with convolutional neural networks. Lecture Notes in Computer Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Wide-area image geolocalization with aerial reference imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Simmim: A simple framework for masked image modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title level="m" type="main">Automatic crossreplica sharding of weight update in data-parallel training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Clusterfit: Improving generalization of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klint</forename><surname>Qinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FACCT</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Accurate image localization based on google maps street view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Kostas Daniilidis, Petros Maragos, and Nikos Paragios</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision -ECCV 2010</note>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Deny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03230</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">Scaling vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><forename type="middle">Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Splitbrain autoencoders: Unsupervised learning by crosschannel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Age progression/regression by conditional adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger, editors</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">UK -Photo: Jeny Garcia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;street</forename><surname>View</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">Bolivia -Photo: Zoriah Miller</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;street</forename><surname>View</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">Burundi -Photo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;street</forename><surname>View</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Johan Eriksson (CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b157">
	<monogr>
		<title level="m" type="main">Brazil -Photo: Leony Carvalho</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;street</forename><surname>View</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b158">
	<monogr>
		<title level="m" type="main">India -Photo: Zoriah Miller</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;street</forename><surname>View</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">Haiti -Photo: Zoriah Miller</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;street</forename><surname>View</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<title level="m" type="main">India -Photo: Zoriah Miller</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;street</forename><surname>View</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">Philippines -Photo: Luc Forsyth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;street</forename><surname>View</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Spices</surname></persName>
		</author>
		<title level="m">India -Photo: AJ Sharma</title>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">Nigeria -Photo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Spices</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Adeola Olagunju (CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<title level="m" type="main">Netherlands -Photo: Global Exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Spices</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>? &amp;quot;spices</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uk -Photo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Chris Dade (CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>? &amp;quot;spices</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Usa -Photo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Elizabeth Barentine (CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<title level="m" type="main">India -Photo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Spices</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Zoriah Miller (CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Spices</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">India -Photo: Kunal Apastamb</title>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b169">
	<monogr>
		<title level="m" type="main">India -Photo: Vanshika Sharma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Spices</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b170">
	<monogr>
				<title level="m">Everyday Shoes&quot; Jordan -Photo: Zoriah Miller</title>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
				<title level="m">Everyday Shoes&quot; India -Photo: Abhineet Malhotra</title>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<title level="m" type="main">Everyday Shoes&quot; China -Photo</title>
		<imprint>
			<publisher>Jonathan Taylor</publisher>
		</imprint>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<title level="m" type="main">Everyday Shoes&quot; Brazil -Photo: Moises Morero</title>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
				<title level="m">Everyday Shoes&quot; Bulgaria -Photo: Boryana Katsarova</title>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b175">
	<monogr>
		<title level="m" type="main">UK -Photo: Chris Dade</title>
		<imprint/>
	</monogr>
	<note>Everyday Shoes. CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title level="m" type="main">Everyday Shoes&quot; China -Photo</title>
		<imprint>
			<publisher>Jonathan Taylor</publisher>
		</imprint>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
				<title level="m">Everyday Shoes&quot; Kenya -Photo: Johan Selin</title>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b178">
	<monogr>
		<title level="m" type="main">Pakistan -Photo: Hisham Najam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Necklaces</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>? &amp;quot;necklaces</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Usa -Photo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Elizabeth Barentine (CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Necklaces</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">India -Photo: Akshay Jain</title>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b181">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>? &amp;quot;necklaces</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Usa -Photo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Isaiah Williams (CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b182">
	<monogr>
		<title level="m" type="main">Netherlands -Photo: Global Exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Necklaces</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b183">
	<monogr>
		<title level="m" type="main">Necklaces&quot; Serbia -Photo: Darko Rajkovic</title>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Necklaces</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">India -Photo: Kunal Apastamb</title>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Necklaces</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Romania -Photo</title>
		<imprint/>
	</monogr>
	<note>Catalin Georgescu (CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b186">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>? &amp;quot;armchairs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Usa -Photo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Sarah Diamond (CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b187">
	<monogr>
		<title level="m" type="main">Cote d&apos;Ivoire -Photo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>? &amp;quot;armchairs</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Zoriah Miller (CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>? &amp;quot;armchairs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Romania -Photo</title>
		<imprint/>
	</monogr>
	<note>Catalin Georgescu (CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<title level="m" type="main">Vietnam -Photo: Victrixia Montes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>? &amp;quot;armchairs</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b190">
	<monogr>
		<title level="m" type="main">Kyrgyzstan -Photo: Svetlana Lebedeva</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>? &amp;quot;armchairs</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<title level="m" type="main">Armchairs&quot; Colombia -Photo</title>
		<imprint/>
	</monogr>
	<note>Zoriah Miller (CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b192">
	<monogr>
		<title level="m" type="main">Nigeria -Photo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>? &amp;quot;armchairs</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Johan Eriksson (CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b193">
	<monogr>
		<title level="m" type="main">India -Photo: Preksha Panchamia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Stoves</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main">Palestine -Photo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Stoves</surname></persName>
		</author>
		<imprint>
			<publisher>Eman Jomaa</publisher>
		</imprint>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Stoves</surname></persName>
		</author>
		<title level="m">Latvia -Photo: Konstatins Sigulis</title>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Stoves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">India -Photo: Akshay Jain</title>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b197">
	<monogr>
		<title level="m" type="main">Nepal -Photo: Luc Forsyth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Stoves</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b198">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Stoves</surname></persName>
		</author>
		<editor>Jonathan Taylor</editor>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b199">
	<monogr>
		<title level="m" type="main">Nepal -Photo: Luc Forsyth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Stoves</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CC BY 4.0)</note>
</biblStruct>

<biblStruct xml:id="b200">
	<monogr>
		<title level="m" type="main">Nepal -Photo: Luc Forsyth (CC BY 4.0) OCR SVHN 73257 model: RegNetY-128gf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? &amp;quot;</forename><surname>Stoves</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">224</biblScope>
		</imprint>
	</monogr>
	<note>2, 7, 17, 1] [264, 264, 264, 264. 528, 1056, 2904, 7392</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
