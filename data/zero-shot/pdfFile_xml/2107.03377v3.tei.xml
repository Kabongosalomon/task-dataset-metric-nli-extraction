<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long Short-Term Transformer for Online Action Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingze</forename><surname>Xu</surname></persName>
							<email>xumingze@amazon.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Amazon</settlement>
									<country>AWS AI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Amazon</settlement>
									<country>AWS AI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Amazon</settlement>
									<country>AWS AI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Amazon</settlement>
									<country>AWS AI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xia</surname></persName>
							<email>wxia@amazon.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Amazon</settlement>
									<country>AWS AI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Amazon</settlement>
									<country>AWS AI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
							<email>soattos@amazon.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Amazon</settlement>
									<country>AWS AI</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Long Short-Term Transformer for Online Action Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Long Short-term TRansformer (LSTR), a temporal modeling algorithm for online action detection, which employs a long-and short-term memory mechanism to model prolonged sequence data. It consists of an LSTR encoder that dynamically leverages coarse-scale historical information from an extended temporal window (e.g., 2048 frames spanning of up to 8 minutes), together with an LSTR decoder that focuses on a short time window (e.g., 32 frames spanning 8 seconds) to model the fine-scale characteristics of the data. Compared to prior work, LSTR provides an effective and efficient method to model long videos with fewer heuristics, which is validated by extensive empirical analysis. LSTR achieves state-of-the-art performance on three standard online action detection benchmarks, THUMOS'14, TVSeries, and HACS Segment. Code has been made available at: https://xumingze0308.github.io/projects/lstr.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given an incoming stream of video frames, online action detection <ref type="bibr" target="#b11">[14]</ref> is concerned with the task of classifying what is happening at each frame without seeing the future. Unlike offline methods that assume the entire video is available, online methods process the data causally, up to the current time. In this paper, we present an online temporal modeling algorithm capable of capturing temporal relations on prolonged sequences up to 8 minutes long, while retaining fine granularity of the event in the representation. This is achieved by modeling activities at different temporal scales, so as to capture a variety of events ranging from bursts to slow trends.</p><p>Specifically, we propose a method, named Long Short-term TRansformer (LSTR), to jointly model long-and short-term temporal dependencies. LSTR has two main advantages over prior work. 1) It stores the history directly thus avoiding the pitfalls of recurrent models <ref type="bibr" target="#b15">[18,</ref><ref type="bibr" target="#b47">50,</ref><ref type="bibr" target="#b25">28,</ref><ref type="bibr" target="#b7">10]</ref>. Backpropagation through time, BPTT, is not needed as the model can directly attend to any useful frames from memory. 2) It separates long-and short-term memories, which allows modeling short-term context while extracting useful correlations from the long-term history. This allows us to compress the long-term history without losing important fine-scale information.</p><p>As shown in <ref type="figure">Fig. 1</ref>, we explicitly divide the entire history into the long-and short-term memories and build our model with an encoder-decoder architecture. Specifically, the LSTR encoder compresses and abstracts the long-term memory into a latent representation of fixed length, and the LSTR decoder uses a short window of transient frames to perform self-attention and cross-attention operations on the extracted token embeddings from the LSTR encoder. In the LSTR encoder, an extended temporal support becomes beneficial in dealing with untrimmed, streaming videos by devising twostage memory compression, which is shown to be computationally efficient in both training and inference. Our overall long short-term Transformer architecture gives rise to an effective and efficient representation for modeling prolonged sequence data. <ref type="figure">Figure 1</ref>: Overview of Long Short-term TRansformer (LSTR). Given a live streaming video, LSTR sequentially identifies the actions happening in each incoming frame by using an encoderdecoder architecture, without future context. The dashed brown arrows indicate the data flow of the long-and short-term memories following the first-in-first-out (FIFO) logic. (Best viewed in color.)</p><p>We validate LSTR on standard benchmark datasets (THUMOS'14 <ref type="bibr" target="#b27">[30]</ref>, TVSeries <ref type="bibr" target="#b11">[14]</ref>, and HACS Segment <ref type="bibr" target="#b73">[76]</ref>). These have distinct characteristics such as video length spanning from a few seconds to tens of minutes. Experimental results establish LSTR as the state-of-the-art for online action detection. Ablation studies further showcase LSTR's abilities in modeling long video sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Online Action Detection. Temporal action localization aims to detect the onset and termination of action instances after observing the entire video <ref type="bibr" target="#b49">[52,</ref><ref type="bibr" target="#b68">71,</ref><ref type="bibr" target="#b20">23,</ref><ref type="bibr" target="#b50">53,</ref><ref type="bibr" target="#b75">78,</ref><ref type="bibr" target="#b2">5,</ref><ref type="bibr" target="#b34">37,</ref><ref type="bibr" target="#b36">39,</ref><ref type="bibr" target="#b35">38]</ref>. Embodied perception, however, requires causal processing <ref type="bibr" target="#b70">[73]</ref>, where we can only process the data up to the present. Online action detection focuses on this setting <ref type="bibr" target="#b11">[14]</ref>. RED <ref type="bibr" target="#b19">[22]</ref> uses a reinforcement loss to encourage recognizing actions as early as possible. TRN <ref type="bibr" target="#b69">[72]</ref> models greater temporal context by simultaneously performing online action detection and anticipation. IDN <ref type="bibr" target="#b16">[19]</ref> learns discriminative features and accumulates only relevant information for the present. LAP-Net <ref type="bibr" target="#b43">[46]</ref> proposes an adaptive sampling strategy to obtain optimal features. PKD <ref type="bibr" target="#b74">[77]</ref> transfers knowledge from offline to online models using curriculum learning. As with early action detection <ref type="bibr" target="#b24">[27,</ref><ref type="bibr" target="#b37">40]</ref>, Shou et al. <ref type="bibr" target="#b51">[54]</ref> focus on online detection of action start (ODAS). StartNet <ref type="bibr" target="#b21">[24]</ref> decomposes ODAS into two stages and learns with policy gradient. WOAD <ref type="bibr" target="#b22">[25]</ref> uses weakly-supervised learning with video-level labels.</p><p>Temporal/Sequence Modeling. Causal time series analysis has traditionally assumed the existence of a latent "state" variable that captures all information in past data, and is updated using only the current datum <ref type="bibr" target="#b46">[49,</ref><ref type="bibr" target="#b25">28,</ref><ref type="bibr" target="#b7">10]</ref>. While the Separation Principle ensures that such a state exists for linear-Gaussian time series, in general it is not possible to summarize all past history of complex data in a finite-dimensional sufficient statistic. Therefore, we directly model the history, in accordance with other work on video understanding <ref type="bibr" target="#b66">[69,</ref><ref type="bibr" target="#b42">45,</ref><ref type="bibr" target="#b65">68]</ref>. Earlier work on action recognition usually relies on heuristic sub-sampling (typically 3 to 7 video frames) for more feasible training <ref type="bibr" target="#b71">[74,</ref><ref type="bibr" target="#b59">62,</ref><ref type="bibr" target="#b18">21,</ref><ref type="bibr" target="#b38">41,</ref><ref type="bibr" target="#b54">57]</ref>. 3D ConvNets <ref type="bibr" target="#b56">[59,</ref><ref type="bibr" target="#b5">8,</ref><ref type="bibr" target="#b57">60]</ref> are used to perform spatio-temporal feature modeling on more frames, but they fail to capture temporal correlations beyond their receptive field. Recently, Wu et al. <ref type="bibr" target="#b66">[69]</ref> propose long-term feature banks to capture objects and scene features, but discarding their temporal order which is clearly informative. Most of work above does not explicitly separate the long-and short-term context modeling, but instead integrates all observed features with simple mechanisms such as pooling or concatenation. We are motivated by work in Cognitive Science <ref type="bibr" target="#b41">[44,</ref><ref type="bibr" target="#b9">12,</ref><ref type="bibr" target="#b6">9,</ref><ref type="bibr" target="#b32">35]</ref> that has shed light on the design principles for modeling long-term dependencies with attention mechanism <ref type="bibr" target="#b63">[66,</ref><ref type="bibr" target="#b10">13,</ref><ref type="bibr" target="#b45">48,</ref><ref type="bibr" target="#b72">75]</ref>.</p><p>Transformers for Action Understanding. Transformers have achieved breakthrough success in NLP <ref type="bibr" target="#b44">[47,</ref><ref type="bibr" target="#b12">15]</ref> and are adopted in computer vision for image recognition <ref type="bibr" target="#b14">[17,</ref><ref type="bibr" target="#b55">58]</ref> and object detection <ref type="bibr" target="#b4">[7]</ref>. Recent papers exploit Transformers for temporal modeling tasks in videos, such as action recognition <ref type="bibr" target="#b40">[43,</ref><ref type="bibr" target="#b48">51,</ref><ref type="bibr" target="#b33">36,</ref><ref type="bibr" target="#b1">4,</ref><ref type="bibr" target="#b0">3]</ref> and temporal action localization <ref type="bibr" target="#b39">[42,</ref><ref type="bibr" target="#b53">56]</ref>, and achieve promising results. However, computational and memory demands result in most work being limited to short video clips, with few exceptions <ref type="bibr" target="#b10">[13,</ref><ref type="bibr" target="#b3">6]</ref> that focuses on designing Transformers to model long-range context. The mechanism for aggregating long-and short-term information is relatively unexplored <ref type="bibr" target="#b29">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Long Short-Term Transformer</head><p>Given a live streaming video, our goal is to identify the actions performed in each video frame using only past and current observations. Future information is not accessible during inference. Formally, a streaming video at time t is represented by a batch of ? past frames I t = {I t?? , ? ? ? , I t }, <ref type="figure">Figure 2</ref>: Visualization of Long Short-Term Transformer (LSTR), which is formulated in an encoder-decoder manner. Specifically, the LSTR encoder compresses the long-term memory of size m L to n 1 encoded latent features, and the LSTR decoder references related context information from the encoded memory with the short-term memory of size m S for action recognition of the present. The LSTR encoder and decoder are built with Transformer decoder units <ref type="bibr" target="#b58">[61]</ref>, which take the input tokens (dark green arrows) and output tokens (dark blue arrows) as inputs. During inference, LSTR processes every incoming frame in an online manner, absent future context. (Best viewed in color.) which reads "I up to time t." The online action detection system receives I t as input, and classifies the action category? t belonging to one of (K + 1) classes,? t ? {0, 1, ? ? ? , K}, ideally using the posterior probability P (? t = k|I t ), where k = 0 denotes the probability that no event is occurring at frame t. We design our method by assuming that there is a pretrained feature extractor <ref type="bibr" target="#b59">[62]</ref> that processes each video frame I t into a feature vector f t ? R C of C dimensions 1 . These vectors form a (? ? C)-dimensional temporal sequence that serves as the input of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Our method is based on the intuition that frames observed recently provide precise information about the ongoing action instance, while frames over an extended period offer contextual references for actions that are potentially happening right now. We propose Long Short-term TRansformer (LSTR) in an explicit encoder-decoder manner, as shown in <ref type="figure">Fig. 2</ref>. In particular, the feature vectors of m L frames in the distant past are stored in a long-term memory, and a short-term memory stores the features of m S recent frames. The LSTR encoder compresses and abstracts features in long-term memory to an encoded latent representation of n 1 vectors. The LSTR decoder queries the encoded long-term memory with the short-term memory for decoding, leading to the action prediction? t . This design follows the line of thought in combining long-and short-term information for action understanding <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b59">62,</ref><ref type="bibr" target="#b66">69]</ref>, but addresses several key challenges to efficiently achieve this goal, exploiting the flexibility of Transformers <ref type="bibr" target="#b58">[61]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Long-and Short-Term Memories</head><p>We store the streaming input of feature vectors into two consecutive memories. The first is the short-term memory which stores only a small number of frames that are recently observed. We implement it with a first-in-first-out (FIFO) queue of m S slots. At time T , it stores the feature vectors as M S = {f T , ? ? ? , f T ?m S +1 }. When a frame becomes "older" than m S time steps, it graduates from M S and enters into the long-term memory, which is implemented with another FIFO queue of m L slots. The long term memory stores M L = {f T ?m S , ? ? ? , f t?m S ?m L +1 }. The long-term memory serves as the input memory to the LSTR encoder and the short-term memory serves as the queries for the LSTR decoder. In practice, the long-term memory stores a much longer time span than the short-term memory (m S m L ). A typical choice is m L = 2048, which represents 512 seconds worth of video contents with 4 frames per second (FPS) sampling rate, and m S = 32 representing 8 seconds. We add a sinusoidal positional encoding s <ref type="bibr" target="#b58">[61]</ref> to each frame feature in the memories relative to current time T (i.e., the frame at T ? ? receives a positional embedding of s ? ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">LSTR Encoder</head><p>The LSTR encoder aims at encoding the long-term memory of m L feature vectors into a latent representation that LSTR can use for decoding useful temporal context. This task requires a large capacity in capturing the relations and temporal context across a span of hundreds or even thousands of frames. Prior work on modeling long-term context for action understanding relies on heuristic temporal sub-sampling <ref type="bibr" target="#b66">[69,</ref><ref type="bibr" target="#b59">62]</ref> or recurrent networks <ref type="bibr" target="#b13">[16]</ref> to make training feasible, at the cost of losing specific information of each time step. Attention-based architectures, such as Transformer <ref type="bibr" target="#b58">[61]</ref>, have recently been shown promising for similar tasks that require long-range temporal modeling <ref type="bibr" target="#b48">[51]</ref>. A straightforward choice for LSTR encoder would be to use a Transformer encoder based on self-attention. However, its time complexity, O(m 2 L C), grows quadratically with the memory sequence length m L . This limits our ability to model long-term memory with sufficient length to cover long videos. Though recent work <ref type="bibr" target="#b61">[64]</ref> has explored self-attention with linear complexity, repeatedly referencing information from the long-term memory with multi-layer Transformers is still computationally heavy. In LSTR, we propose to use the two-stage memory compression mechanism based on Transformer decoder units <ref type="bibr" target="#b58">[61]</ref> to achieve more effective memory encoding.</p><p>The Transformer decoder unit <ref type="bibr" target="#b58">[61]</ref> takes two sets of inputs. The first set includes a fixed number of n learnable output tokens ? ? R n?C , where C is the embedding dimension. The second set includes another m input tokens ? ? R m?C , where m can be a rather large number. It first applies one layer of multi-head self-attention on ?. The outputs ? are then used as queries in an "QKV cross-attention" operation and the input embeddings ? serve as key and value. The two steps can be written as</p><formula xml:id="formula_0">? = SelfAttn(?) = Softmax( ? ? ? T ? C )? and CrossAttn(?(? ), ?) = Softmax( ?(? ) ? ? T ? C )?,</formula><p>where ? : R n?C ? R n?C denotes the intermediate layers between the two attention operations. One appealing property of this design is that it transforms the m ? C dimensional input tokens into output tokens of n ? C dimensions in O(n 2 C + nmC) time complexity. When n m, the time complexity becomes linear to m, making it an ideal candidate for compressing long-term memory. This property is also utilized in <ref type="bibr" target="#b29">[32]</ref> to efficiently process large volume inputs, such as image pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-Stage Memory Compression.</head><p>Stacking multiple Transformer decoder units on the long-term memory, as in <ref type="bibr" target="#b58">[61]</ref>, can form a memory encoder with linear complexity with respect to the memory size m L . However, running the encoder at each time step can still be time consuming. We can further reduce the time complexity with a two-stage memory compression design. The first stage has one Transformer decoder unit with n 0 output tokens. Its input tokens are the entire long-term memory of size m L . The outputs of the first stage are used as the input tokens to the second stage, which has enc stacked Transformer decoder units and n 1 output tokens. Then, the long-term memory of size m L ? C is compressed into a latent representation of size n 1 ? C, which can then be efficiently queried in the LSTR decoder later. This two-stage memory compression design is illustrated in <ref type="figure">Fig. 2</ref>.</p><p>Compared to an (1 + enc )-layer Transformer encoder with O(m 2 L (1 + enc )C) time complexity or stacked Transformer decoder units with n output-tokens having O((n 2 + nm L )(1 + enc )C) time complexity, the proposed LSTR encoder has complexity of O(n 2 0 C + n 0 m L C + (n 2 1 + n 1 n 0 ) enc C). Because both n 0 and n 1 are much smaller than m L , and enc is usually larger than 1, using two-stage memory compression could be more efficient. In Sec. 3.6, we will show that, during online inference, it further enables us to reduce the runtime of the Transformer decoder unit of the first stage. In Sec. 4.5, we empirically found this design also leads to better performance for online action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">LSTR Decoder</head><p>The short-term memory contains informative features for classifying actions on the latest time step. The LSTR decoder uses the short-term memory as queries to retrieve useful information from the encoded long-term memory produced by the LSTR encoder. The LSTR decoder is formed by stacking dec layers of Transform decoder units. It takes the outputs of the LSTR encoder as input tokens and the m S feature vectors in the short-term memory as output tokens. It outputs m S probability vectors {p T , ? ? ? , p T ?m S +1 } ? [0, 1] K+1 , each p t representing the predicted probability distribution of K action categories and one "background" class at time t. During inference, we only take the probability vector p T from the output token corresponding to the current time T for the classification result.</p><p>However, having the additional outputs on the older frames allows the model to leverage more supervision signals during training. The details will be described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training LSTR</head><p>LSTR can be trained without temporal unrolling and Backpropagation Through Time (BPTT) as in LSTM <ref type="bibr" target="#b25">[28]</ref>, which is a common property of Transformers <ref type="bibr" target="#b58">[61]</ref>. We construct each training sample by randomly sampling an ending time T and filling the long-and short-term memories by tracing back in time for m S + m L frames. We use the empirical cross entropy loss between the predicted probability distribution p T at time T and the ground truth action label y T ? {0, 1, ? ? ? , K} as</p><formula xml:id="formula_1">L(yT , pT ; T ) = ? K k=0 ?(k ? yT ) log p k T ,<label>(1)</label></formula><p>where p k T is the k-th element of the probability vector p T , predicted on the latest frame at T . Additionally, we add a directional attention mask <ref type="bibr" target="#b58">[61]</ref> to the short-term memory so that any frame in the short-term memory can only depend on its previous frames. In this way, we can make predictions on all frames in the short-term memory as if they are the latest ones. Thus we can provide supervision on every frame in the short-term memory, and the complete loss function L is then</p><formula xml:id="formula_2">LT = T t=T ?ms+1 L(yt, pt; T ),<label>(2)</label></formula><p>where p t denotes the prediction from the output token corresponding to time t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Online Inference with LSTR</head><p>During online inference, the video frame features are streamed to the model as time passes. Running LSTR's long-term memory encoder from scratch for each frame results in a time complexity of O(n 2 0 C + n 0 m L C) and O((n 2 1 + n 1 n 0 ) enc C) for the first and second memory compression stages, respectively. However, at each time step, there is only one new video frame to be updated. We show it is possible to achieve even more efficient online inference by storing the intermediate results for the Transformer decoder unit of the first stage. First, the queries of the first Transformer decoder unit are fixed. So their self-attention outputs can be pre-computed and used throughout the inference. Second, the cross-attention operation in the first stage can be written as</p><formula xml:id="formula_3">CrossAttn(qi, {fT ?? + s? }) = m S +m L ?1 ? =m S exp((fT ?? + s? ) ? qi/ ? C) m S +m L ?1 ? =m S exp((fT ?? + s? ) ? qi/ ? C) ? (fT ?? + s? ),<label>(3)</label></formula><p>where the index ? = T ? t is the relative position of a frame t in the long-term memory to the latest time T . This calculation depends on the un-normalized attention weight matrix A ? R m L ?n0 , with elements a ? i = (f T ?? + s ? ) ? q i . A can be decomposed into the sum of two matrices A f and A s . We have their elements as a f ? i = f T ?? ? q i and a s ? i = s ? ? q i . The queries after the first self-attention operation, Q = [q 1 , . . . , q n0 ], and the position embedding s ? are fixed during inference. Thus the matrix A s can be pre-computed and used for every incoming frame. We additionally maintain a FIFO queue of vectors a t = Q f t of size m L . A f at any time step T can be obtained by stacking all vectors currently in this queue. Updating this queue at each time step requires O(n 0 C) time complexity for the matrix-vector product. Now we can obtain the matrix A with only n 0 ? m L additions by adding A s and A f together, instead of n 0 ? m L ? C multiplications and additions using Eq. (3). This means the amortized time complexity for computing the attention weights can be reduced to O(n 0 (m L + C)). Although the time complexity of the cross-attention operation is still O(n 0 m L C) due to the inevitable operation of weighted sum, since C is usually larger than 1024 <ref type="bibr" target="#b23">[26]</ref>, this is still a considerable reduction of runtime. LSTR's walltime efficiency is discussed in Sec. 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our model on three publicly-available datasets: THUMOS'14 <ref type="bibr" target="#b27">[30]</ref>, TVSeries <ref type="bibr" target="#b11">[14]</ref> and HACS Segment <ref type="bibr" target="#b73">[76]</ref>. THUMOS'14 includes over 20 hours of sports video annotated with 20 actions. We follow prior work <ref type="bibr" target="#b69">[72,</ref><ref type="bibr" target="#b16">19]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Settings</head><p>Feature Encoding. We follow the experimental settings of state-of-the-art methods <ref type="bibr" target="#b69">[72,</ref><ref type="bibr" target="#b16">19]</ref>. We extract video frames at 24 FPS and set the video chunk size to 6. Decisions are made at the chunk level, and thus accuracy is evaluated at every 0.25 second. For feature encoding, we adopt the TSN <ref type="bibr" target="#b59">[62]</ref> models implemented in an open-source toolbox <ref type="bibr" target="#b8">[11]</ref>. Specifically, the features are extracted by one visual model with the ResNet-50 <ref type="bibr" target="#b23">[26]</ref> architecture from the central frame of each chunk and one motion model with the BN-Inception <ref type="bibr" target="#b28">[31]</ref> architecture from the stacked optical flow fields between 6 consecutive frames <ref type="bibr" target="#b59">[62]</ref>. The visual and motion features are concatenated along the channel dimension as the final feature f. We experiment with feature extractors pretrained on two datasets, ActivityNet and Kinetics. Implementation Details. We implemented our proposed model in PyTorch <ref type="bibr">[1]</ref>, and performed all experiments on a system with 8 Nvidia V100 graphics cards. For all Transformer units, we set their number of heads as 16 and hidden units as 1024 dimensions. To learn model weights, we used the Adam <ref type="bibr" target="#b31">[34]</ref> optimizer with weight decay 5 ? 10 ?5 . The learning rate was linearly increased from zero to 5 ? 10 ?5 in the first 2/5 of training iterations and then reduced to zero following a cosine function. Our models were optimized with batch size of 16, and the training was terminated after 25 epochs.</p><p>Evaluation Protocols. We follow prior work and use per-frame mean average precision (mAP) to evaluate the performance of online action detection. We also use per-frame calibrated average precision (cAP) <ref type="bibr" target="#b11">[14]</ref> that was proposed for TVSeries to correct the imbalance between positive and negative samples, cAP = k cP rec(k) * I(k)/P , where cP rec = T P/(T P + F P/w), I(k) is 1 if frame k is a true positive, P is the number of true positives, and w is the negative and positive ratio.  We compare LSTR against other state-of-the-art methods <ref type="bibr" target="#b69">[72,</ref><ref type="bibr" target="#b16">19,</ref><ref type="bibr" target="#b22">25]</ref> on THUMOS'14, TVSeries, and HACS Segment. Specifically, on THUMOS'14 and TVSeries, we implement LSTR with the long-and short-term memories of 512 and 8 seconds, respectively. On HACS Segment, we reduce the long-term memory to 256 seconds, considering that its videos are strictly shorter than 4 minutes. For LSTR, we implement the two-stage memory compression using Transformer decoder units. We set the token numbers to n 0 = 16 and n 1 = 32 and the Transformer layers to enc = 2 and dec = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the State-of-the-art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Online Action Detection</head><p>THUMOS'14. We compare LSTR with recent work on THUMOS'14, including methods that use 3D ConvNets <ref type="bibr" target="#b50">[53]</ref> and RNNs <ref type="bibr" target="#b67">[70,</ref><ref type="bibr" target="#b16">19,</ref><ref type="bibr" target="#b22">25]</ref>, reinforcement learning <ref type="bibr" target="#b19">[22]</ref>, and curriculum learning <ref type="bibr" target="#b74">[77]</ref>. <ref type="table" target="#tab_0">Table 1a</ref> and 1b shows that LSTR significantly outperforms the the state-of-the-art methods <ref type="bibr" target="#b66">[69,</ref><ref type="bibr" target="#b22">25]</ref> by 3.7% and 2.4% in terms of mAP using ActivityNet and Kinetics pretrained features, respectively.</p><p>TVSeries. <ref type="table" target="#tab_0">Table 1a</ref> and 1b show the online action detection results that LSTR outperforms the state-of-the-art methods <ref type="bibr" target="#b17">[20,</ref><ref type="bibr" target="#b74">77]</ref> by 2.8% and 2.7% in terms of cAP using ActivityNet and Kinetics pretrained features, respectively. Following prior work <ref type="bibr" target="#b11">[14]</ref>, we also investigate LSTR's performance at different action stages by evaluating each decile (ten-percent interval) of the video frames separately. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Action Anticipation</head><p>We extend the idea of LSTR to action anticipation for up to 2 seconds (i.e., 8 steps in 4 FPS) into the future. Specifically, we concatenate another 8 learnable output tokens (with positional embedding) after the short-term memory in the LSTR decoder to produce the prediction results accordingly. <ref type="table" target="#tab_0">Table 1c</ref> shows that LSTR significantly outperforms the state-of-the-art methods <ref type="bibr" target="#b69">[72,</ref><ref type="bibr" target="#b43">46]</ref> by 7.5% mAP on THUMOS and 2.1% cAP on TVSeries, using ActivityNet pretrained features. We experiment for design choices of long-and short-term memories. Unless noted otherwise, we use THUMOS'14, which contains various video lengths, and Kinetics pretrained features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Design Choices of Long-and Short-Term Memories</head><p>Lengths of long-and short-term memories. We first analyze the effect of different lengths of long-term m L and short-term m S memory. In particular, we test m S ? {4, 8, 16} seconds with m L starting from 0 second (no long-term memory). Note that we choose the max length (1024 seconds for THUMOS'14 and 256 seconds for HACS Segment) to cover lengths of 98% videos, and do not have proper datasets to test longer m L . <ref type="figure" target="#fig_1">Fig. 3</ref> shows that LSTR is beneficial from larger m L in most cases.</p><p>In addition, when m L is short (? 16 in our cases), using larger m S obtains better results and when m L is sufficient (? 32 in our cases), increasing m S does not always guarantee better performance. Can we downsample long-term memory? We implement LSTR with m S as 8 seconds and m L as 512 seconds, and test the effect of downsampling long-term memory. <ref type="table" target="#tab_3">Table 3</ref> shows the results that downsampling with strides smaller than 4 does not cause performance drop, but more aggressive strides dramatically decrease the detection accuracy. Note that, when extracting frame features in 4 FPS, both LSTR encoder (n 0 = 16) and downsampling with stride 128 compress the long-term memory to 16 features, but LSTR achieves much better performance (69.5% vs. 65.9% in mAP). This demonstrates the effectiveness of our "adaptive compression" compared to heuristics downsampling.</p><p>Can we compensate reduced memory length with RNN? We note that LSTR's performance notably decreases when it can only access very limited memory (e.g., m L + m S ? 16 seconds). Here we test if RNN can compensate LSTR's reduced memory or even fully replace the LSTR encoder. We implement LSTR using m S = 8 seconds with an extra Gated Recurrent Unit (GRU) <ref type="bibr" target="#b7">[10]</ref> (its architecture is visualized in the Supplementary Material) to capture all history outside the long-and short-term memories. The dashed line in <ref type="figure" target="#fig_1">Fig. 3</ref> shows the results. Plugging-in RNNs indeed improves the performance when m L is small, but when m L is large (? 64 seconds), it does not improve the accuracy anymore. Note that RNNs are not used in any other experiments in this paper. We continue to explore the design trade-offs of LSTR. Unless noted otherwise, we use short-term memory of 8 seconds, long-term memory of 512 seconds, and Kinetics pretrained features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Design Choices of LSTR</head><p>Number of layers and tokens. First, we assess using different numbers of token embeddings (i.e., n 0 and n 1 ) in LSTR encoder. <ref type="figure" target="#fig_3">Fig 4 (left)</ref> shows that LSTR is quite robust to different choices (the best and worst performance gap is only about 1.5%), but using n 0 = 16 and n 1 = 32 gets highest accuracy. Second, we experiment for the effect of using different numbers of Transformer decoder units (i.e., enc and dec ). As shown in <ref type="figure" target="#fig_3">Fig 4 (right)</ref>, LSTR does not need a large model to get the best performance, and in practice, using more layers can cause overfitting.  Can we unify the temporal modeling using only self-attention models? We test if long-term M L and short-term M S memory can be learned as a whole using self-attention models. Specifically, we concatenate M L and M S and feed them into a standard Transformer Encoder <ref type="bibr" target="#b58">[61]</ref> with a similar model size to LSTR. <ref type="table" target="#tab_4">Table 4</ref> (row 8 vs. row 1) shows that LSTR achieves better performance especially when m L is large (e.g., 69.5% vs. 66.5% when m L = 512 and 66.6% vs. 65.7% when m L = 8). This shows the advantage of LSTR for temporal modeling on long-and short-term context.</p><p>Can we remove the LSTR encoder? We explore this by directly feeding M L into the LSTR decoder, and using M S as tokens to reference useful information. <ref type="table" target="#tab_4">Table 4</ref> shows that LSTR outperforms this baseline (row 8 vs. row 2), especially when m L is large. We also compare it with self-attention models (row 1) and observe that although neither of them can effectively model prolonged memory, this baseline outperforms the Transformer Encoder. This also demonstrates the effectiveness of our idea of using short-term memory to query related context from long-range context.</p><p>Can the LSTR encoder learn effectively using self-attention? To evaluate the "bottleneck" design with cross-attention in LSTR encoder, we try modeling M L using standard Transformer Encoder units <ref type="bibr" target="#b58">[61]</ref>. Note that this still captures M L and M S with the similar workflow of LSTR, but does not compress and encode M L using learnable tokens. <ref type="table" target="#tab_4">Table 4</ref> (row 8 vs. row 3) shows that LSTR outperforms this baseline with all m L settings. In addition, the performance of this baseline decreases when m L gets larger, which suggests the superior ability of LSTR for modeling long-range patterns.</p><p>How to design the memory compression for the LSTR encoder? First, we use a projection layer, consisting of a learnable matrix of size n 0 ? m L followed by MLP layers, to compress the long-term memory along the temporal dimension. <ref type="table" target="#tab_4">Table 4</ref> shows that using this simple projection layer (row 4) slightly outperforms the model without long-term memory (row 1), but is worse than attention-based compression methods. Second, we evaluate the one-stage design with enc + 1 Transformer decoder units. <ref type="table" target="#tab_4">Table 4</ref> (row 8 vs. row 5) shows that two-stage compression is stably better than one-stage, and their performance gap gets larger when using larger m L (0.5% when m L = 8 and 0.8% when m L = 512). Third, we compare cross-attention and self-attention for two-stage compression by replacing the second Transformer decoder with Transformer encoder. LSTR stably outperforms this baseline (row 6) by about 0.5% in mAP. However, its performance is still better than models with one-stage compression of about 1.3% (row 6 vs. row 3) and 0.3% (row 6 vs. row 5) on average.</p><p>Can we remove the LSTR decoder? We remove the LSTR decoder to evaluate its contribution. Specifically, we feed the entire memory to LSTR encoder and attach a multi-layer (MLP) classifier on its output tokens embeddings. Similar to the above experiments, we increase the model size to ensure a fair comparison. <ref type="table" target="#tab_4">Table 4</ref> shows that LSTR outperforms this baseline (row 7) by about 4% on large m L (e.g., 512 and 1024) and about 2.5% on relative small m L (e.g., 8 and 16). Cross-attention vs. heuristics for integrating long-and short-term memories. We explore integrating the long-and short-term memories by using average pooling and concatenation. Specifically, the encoded long-term features of size n 1 ? C is converted to a vector of C elements by channel-wise averaging, and the short-term memory of size m S ? C is encoded by dec Transformer encoder units. Then, each slot of the short-term features is either averaged or concatenated with the long-term feature vector for action classification. Note that these models still benefit from LSTR's effectiveness for long-term modeling. <ref type="table" target="#tab_5">Table 5</ref> shows that using average pooling and concatenation obtain comparable results, but LSTR with cross-attention stably outperforms these baselines. We report LSTR's runtime in frames per second (FPS) on a system with a single V100 GPU, and use the videos from THUMOS'14 dataset. The results are shown in <ref type="table" target="#tab_6">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Runtime</head><p>We start by comparing the runtime between LSTR's different design choices without considering the pre-processing (e.g., feature extraction). First, LSTR runs at 91.6 FPS using our two-stage memory compression (row 4), whereas using the one-stage design runs at a slower 59.5 FPS (row 3). Our two-stage design is more efficient because it does not need to reference information from the long-term memory multiple times, and can be further accelerated during online inference (see Sec. <ref type="bibr">3.6)</ref>. Second, we test the LSTR encoder using self-attention mechanisms (row 2). This design does not compress the long-term memory, thus increasing the computational cost of both the LSTR encoder and decoder, leading to a slower speed of 50.2 FPS. Third, we test the standard Transformer Encoder <ref type="bibr" target="#b58">[61]</ref> (row 1), whose runtime speed, 43.2 FPS, is about 2? slower than LSTR.</p><p>We also compare LSTR with state-of-the-art recurrent models. As we are not aware of any prior work that reports their runtime, we test TRN <ref type="bibr" target="#b69">[72]</ref> using their official open-source code <ref type="bibr">[2]</ref>. The result shows that TRN runs at 123.3 FPS, which is faster than LSTR. This is because recurrent models abstract the entire history as a compact representation but LSTR needs to process much more information. On the other hand, LSTR achieves much higher performance, outperforming TRN by about 7.5% in mAP on THUMOS'14 and about 4.5% in cAP on TVSeries.</p><p>For end-to-end online inference, we follow the state-of-the-art methods <ref type="bibr" target="#b69">[72,</ref><ref type="bibr" target="#b16">19,</ref><ref type="bibr" target="#b22">25]</ref> and build LSTR on two-stream features <ref type="bibr" target="#b59">[62]</ref>. LSTR together with pre-processing techniques run at 4.6 FPS. <ref type="table" target="#tab_6">Table 6</ref> shows that the speed bottleneck is the motion feature extraction -it accounts for about 90% of the total runtime including the optical flow computation with DenseFlow <ref type="bibr" target="#b60">[63]</ref>. One can improve the efficiency largely by using real-time optical flow extractors (e.g., PWC-Net <ref type="bibr" target="#b52">[55]</ref>) or using only visual features extracted by a light-weight backbone (e.g., MobileNet <ref type="bibr" target="#b26">[29]</ref> and FBNet <ref type="bibr" target="#b64">[67]</ref>).  In <ref type="table" target="#tab_7">Table 7</ref>, we list the action classes from THUMOS'14 where LSTR gets the highest (color green) and the lowest (color red) per-frame APs. In <ref type="figure" target="#fig_4">Fig. 5</ref>, we illustrate four sample frames with incorrect predictions. More visualizations are included in the Supplementary Material. We observe that LSTR sees a decrease in detection accuracy when the action incurs only tiny motion or the subject is very far away from the camera, but excels at recognizing actions with long temporal span and multiple stages, such as "PoleVault" and "Long Jump". This suggests we may explore extending the temporal modeling capability of LSTR to both spatial and temporal domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Error Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present LSTR which captures both long-and short-term correlations in past observations of a time series by compressing long-term memory into encoded latent features and referencing related temporal context from them with short-term memory. This demonstrates the importance of separately modeling long-and short-term information and then integrating them for online inference tasks. Experiments on multiple datasets and ablation studies validate the effectiveness and efficiency of the LSTR design in dealing with prolonged video sequences. However, we note that LSTR is operating only on the temporal dimension. An end-to-end video understanding system requires simultaneous spatial and temporal modeling for optimal results. Therefore extending the idea of LSTR to spatio-temporal modeling remains an open yet challenging problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments and Disclosure of Funding</head><p>We thank the anonymous reviewers for their helpful suggestions. This work was funded by Amazon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Appendix</head><p>7.1 Qualitative Results <ref type="figure">Fig. 6a</ref> shows some qualitative results. We can see that, in most cases, LSTR can quickly recognize the actions and make relatively consistent predictions for each action instance. Two typical failure cases are shown in <ref type="figure">Fig. 6b</ref>. The top sample contains the "Billiards"action that incurs only tiny motion. As discussed in Sec. 4.7, LSTR's detection accuracy is observed to decrease on this kind of actions. The bottom sample is challenging -the "Open door" action occurs behind the female reporter and is barely visible. Red circle indicates where the action is happening in each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Background Background  To better understand the design choice in Sec. 4.4, we show its overall structure in <ref type="figure">Fig. 7</ref>. Specifically, in addition to the long-and short-term memories, we use an extra GRU to capture all the history "outside" the long-and short-term memories as a compact representation, g ? R 1?C . We then concatenate the outputs of the LSTR encoder and the GRU as more comprehensive temporal features of size (n 1 + 1) ? C, and feed them into the LSTR decoder as input tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Action Anticipation Cont'd LSTR Decoder</head><p>Long-Term Memory Short-Term Memory T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S to re</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Store</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concat</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTR Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long Short-term TRansformer (LSTR)</head><p>! Token Embeddings Anticipation time T+1 to T+ ! <ref type="figure">Figure 8</ref>: Overview of the architecture of using LSTR for action anticipation.</p><p>In Sec. 4.3.2, we extended the idea of LSTR to action anticipation for up to 2 seconds into the future, and compared it with state-of-the-art methods in <ref type="table" target="#tab_0">Table 1c</ref>. Here we provide more details about its structure. As shown in <ref type="figure">Fig. 8</ref>, we concatenate m F token embeddings after the short-term memory as m S + m F output tokens into the LSTR decoder. These m F "anticipation tokens" are added with the positional embedding and directional attention mask together with the short-term memory. Then the outputs of the m F tokens make predictions for the next m F steps accordingly. As LSTR's performance is evaluated in 4 FPS (see Sec. 4.2), m F is set to 8 for action anticipation of 2 seconds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and train on the validation set (200 untrimmed videos) and evaluate on the test set (213 untrimmed videos). TVSeries contains 27 episodes of 6 popular TV series, totaling 16 hours of video. The dataset is annotated with 30 realistic, everyday actions (e.g., open door). HACS Segment is a large-scale dataset of web videos. It contains 35,300 untrimmed videos over 200 human action classes for training and 5,530 untrimmed videos for validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Effect of using different lengths of long-and short-term memories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Left: Results of different number of token embeddings for our two-stage memory compression. Right: Results of different number of Transformer decoder units for enc and dec .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Failure cases on THUMOS'14. Action classes from left to right are "BaseballPitch", "FrisbeeCatch", "Billiards", and "CricketShot". Red circle indicates where the action is happening.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Qualitative results and failure cases of LSTR. The curves indicate the predicted scores of the ground truth and "background" classes. (Best viewed in color.) 7.2 Can we compensate reduced memory length with RNN? Cont'd Overview of the architecture of using LSTR with an extra GRU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Online action detection and anticipation results on THUMOS'14 and TVSeries in terms of mAP and cAP, respectively. For online action detection, LSTR outperforms the state-of-the-art methods on THUMOS'14 by 3.7% and 2.4% in mAP and on TVSeries by 2.8% and 2.7% in cAP, using ActivityNet and Kinetics pretrained features, respectively. LSTR also achieves promising results for action anticipation. *Results are reproduced by us using their papers' default settings.</figDesc><table><row><cell cols="3">(a) Results of online action detec-</cell><cell cols="3">(b) Results of online action detec-</cell><cell cols="3">(c) Results of action anticipation</cell></row><row><cell cols="3">tion using ActivityNet features</cell><cell cols="3">tion using Kinetics features</cell><cell cols="3">using ActivityNet features</cell></row><row><cell></cell><cell cols="2">THUMOS'14 TVSeries</cell><cell></cell><cell cols="2">THUMOS'14 TVSeries</cell><cell></cell><cell cols="2">THUMOS'14 TVSeries</cell></row><row><cell></cell><cell cols="2">mAP (%) mcAP (%)</cell><cell></cell><cell cols="2">mAP (%) mcAP (%)</cell><cell></cell><cell cols="2">mAP (%) mcAP (%)</cell></row><row><cell>CDC [53]</cell><cell>44.4</cell><cell>-</cell><cell>FATS [33]</cell><cell>59.0</cell><cell>84.6</cell><cell>EFC [22]</cell><cell>34.4</cell><cell>72.5</cell></row><row><cell>RED [22]</cell><cell>45.3</cell><cell>79.2</cell><cell>IDN [19]</cell><cell>60.3</cell><cell>86.1</cell><cell>ED [22]</cell><cell>36.6</cell><cell>74.5</cell></row><row><cell>TRN [72]</cell><cell>47.2</cell><cell>83.7</cell><cell>TRN [72]</cell><cell>62.1</cell><cell>86.2</cell><cell>RED [22]</cell><cell>37.5</cell><cell>75.1</cell></row><row><cell>FATS [33]</cell><cell>51.6</cell><cell>81.7</cell><cell>PKD [77]</cell><cell>64.5</cell><cell>86.4</cell><cell>TRN [72]</cell><cell>38.9</cell><cell>75.7</cell></row><row><cell>IDN [19]</cell><cell>50.0</cell><cell>84.7</cell><cell>WOAD [25]</cell><cell>67.1</cell><cell>-</cell><cell>TTM [65]</cell><cell>40.9</cell><cell>77.9</cell></row><row><cell>LAP [46]</cell><cell>53.3</cell><cell>85.3</cell><cell>LFB* [69]</cell><cell>64.8</cell><cell>85.8</cell><cell>LAP [46]</cell><cell>42.6</cell><cell>78.7</cell></row><row><cell>TFN [20]</cell><cell>55.7</cell><cell>85.0</cell><cell>LSTR (ours)</cell><cell>69.5</cell><cell>89.1</cell><cell>LSTR (ours)</cell><cell>50.1</cell><cell>80.8</cell></row><row><cell>LFB* [69]</cell><cell>61.6</cell><cell>84.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSTR (ours)</cell><cell>65.3</cell><cell>88.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Online action detection results when only portions of videos are considered in cAP (%) on TVSeries (e.g., 80%-90% means only frames of this range of action instances were evaluated). LSTR outperforms existing methods at every time stage, especially on boundary locations.</figDesc><table><row><cell></cell><cell>Features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Portion of Video</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="10">0-10% 10-20% 20-30% 30-40% 40-50% 50-60% 60-70% 70-80% 80-90% 90-100%</cell></row><row><cell>TRN [72]</cell><cell></cell><cell>78.8</cell><cell>79.6</cell><cell>80.4</cell><cell>81.0</cell><cell>81.6</cell><cell>81.9</cell><cell>82.3</cell><cell>82.7</cell><cell>82.9</cell><cell>83.3</cell></row><row><cell>IDN [19] TFN [20]</cell><cell>ActivityNet</cell><cell>80.6 83.1</cell><cell>81.1 84.4</cell><cell>81.9 85.4</cell><cell>82.3 85.8</cell><cell>82.6 87.1</cell><cell>82.8 88.4</cell><cell>82.6 87.6</cell><cell>82.9 87.0</cell><cell>83.0 86.7</cell><cell>83.9 85.6</cell></row><row><cell>LSTR (ours)</cell><cell></cell><cell>83.6</cell><cell>85.0</cell><cell>86.3</cell><cell>87.0</cell><cell>87.8</cell><cell>88.5</cell><cell>88.6</cell><cell>88.9</cell><cell>89.0</cell><cell>88.9</cell></row><row><cell>IDN [19]</cell><cell></cell><cell>81.7</cell><cell>81.9</cell><cell>83.1</cell><cell>82.9</cell><cell>83.2</cell><cell>83.2</cell><cell>83.2</cell><cell>83.0</cell><cell>83.3</cell><cell>86.6</cell></row><row><cell>PKD [77]</cell><cell>Kinetics</cell><cell>82.1</cell><cell>83.5</cell><cell>86.1</cell><cell>87.2</cell><cell>88.3</cell><cell>88.4</cell><cell>89.0</cell><cell>88.7</cell><cell>88.9</cell><cell>87.7</cell></row><row><cell>LSTR (ours)</cell><cell></cell><cell>84.4</cell><cell>85.6</cell><cell>87.2</cell><cell>87.8</cell><cell>88.8</cell><cell>89.4</cell><cell>89.6</cell><cell>89.9</cell><cell>90.0</cell><cell>90.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>shows that LSTR outperforms existing methods at every stage of action instances.</figDesc><table /><note>HACS Segment. LSTR achieves 82.6% on HACS Segment in term of mAP using Kinetics pretrained features. Note that HACS Segment is a new large-scale dataset with only a few previous results. LSTR outperforms existing methods RNN [28] (77.6%) by 5.0% and TRN [72] (78.9%) by 3.7%.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of LSTR using downsampled long-term memory on THUMOS'14 in mAP (%). In particular, we use long-term memory of 512 seconds and short-term memory as 8 seconds.</figDesc><table><row><cell>Temporal Stride</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>128</cell></row><row><cell>LSTR</cell><cell cols="8">69.5 69.5 69.5 69.2 68.7 67.3 66.6 65.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results of different designs of the LSTR encoder and decoder. The length of short-term memory is set to 8 seconds. "TR" denotes Transformer. The last row is our proposed LSTR design.</figDesc><table><row><cell>LSTR Encoder</cell><cell>LSTR Decoder</cell><cell></cell><cell cols="2">Length of Long-Term Memory mL (secs)</cell></row><row><cell></cell><cell></cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64 128 256 512 1024</cell></row><row><cell>N/A</cell><cell>TR Encoder</cell><cell cols="3">65.7 66.8 67.1 67.2 67.3 66.8 66.5 66.2</cell></row><row><cell>N/A</cell><cell>TR Decoder</cell><cell cols="3">66.5 67.3 67.7 68.1 68.3 67.9 67.0 66.5</cell></row><row><cell>TR Encoder</cell><cell>TR Decoder</cell><cell cols="3">65.9 66.4 66.7 67.4 67.5 67.2 67.0 66.6</cell></row><row><cell>Projection Layer</cell><cell>TR Decoder</cell><cell cols="3">66.2 67.1 67.4 67.7 67.5 67.2 66.9 66.8</cell></row><row><cell>TR Decoder</cell><cell>TR Decoder</cell><cell cols="3">66.1 67.1 67.4 68.0 68.5 68.6 68.7 68.7</cell></row><row><cell>TR Decoder + TR Encoder</cell><cell>TR Decoder</cell><cell cols="3">66.2 67.3 67.6 68.4 68.6 68.8 68.9 69.0</cell></row><row><cell>TR Decoder + TR Decoder</cell><cell>N/A</cell><cell cols="3">64.0 64.7 65.9 66.1 66.5 66.2 65.4 65.2</cell></row><row><cell>TR Decoder + TR Decoder</cell><cell>TR Decoder</cell><cell cols="3">66.6 67.8 68.2 68.8 69.2 69.4 69.5 69.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results of LSTR using different memory integration methods in mAP (%). Our proposed integration method using cross-attention stably outperforms the heuristic methods.</figDesc><table><row><cell>Memory Integration Methods</cell><cell></cell><cell cols="3">Length of Long-Term Memory mL (secs)</cell></row><row><cell></cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64 128 256 512 1024</cell></row><row><cell>Average Pooling</cell><cell cols="4">66.1 67.0 67.3 67.5 68.2 68.4 68.6 68.6</cell></row><row><cell>Concatenation</cell><cell cols="4">65.9 67.2 67.5 67.7 68.4 68.5 68.7 68.6</cell></row><row><cell>Cross-Attention (ours)</cell><cell cols="4">66.6 67.8 68.2 68.8 69.2 69.4 69.5 69.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Runtime of LSTR with different design choices. The last row is our proposed LSTR design.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Frames Per Second (FPS)</cell><cell></cell></row><row><cell>LSTR Encoder</cell><cell>LSTR Decoder</cell><cell>OptFlow Computation</cell><cell>RGB Feature Extraction</cell><cell>OptFlow Feature Extraction</cell><cell>LSTR</cell></row><row><cell>N/A</cell><cell>TR Encoder</cell><cell></cell><cell></cell><cell></cell><cell>43.2</cell></row><row><cell>TR Encoder TR Decoder</cell><cell>TR Decoder TR Decoder</cell><cell>8.1</cell><cell>70.5</cell><cell>14.6</cell><cell>50.2 59.5</cell></row><row><cell cols="2">TR Decoder + TR Decoder TR Decoder</cell><cell></cell><cell></cell><cell></cell><cell>91.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Action classes with highest and lowest performance on THUMOS'14.</figDesc><table><row><cell cols="9">Action Classes HammerThrow PoleVault LongJump Diving BaseballPitch FrisbeeCatch Billiards CricketShot</cell></row><row><cell>AP (%)</cell><cell>92.8</cell><cell>89.7</cell><cell>86.9</cell><cell>86.7</cell><cell>55.4</cell><cell>49.4</cell><cell>39.8</cell><cell>38.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Qualitative results on THUMOS'14 (top two samples) and TVSeries (bottom two samples).</figDesc><table><row><cell>Background</cell><cell>Diving</cell><cell>Background</cell></row><row><cell></cell><cell>Ground Truth</cell><cell></cell></row><row><cell>Background</cell><cell>BasketballDunk</cell><cell>Background</cell></row><row><cell></cell><cell>Ground Truth</cell><cell></cell></row><row><cell></cell><cell>Drink</cell><cell></cell></row><row><cell></cell><cell>Pour</cell><cell></cell></row><row><cell></cell><cell>Ground Truth</cell><cell></cell></row><row><cell>Background (a) Background</cell><cell>Ground Truth Open door</cell><cell>Background Background</cell></row><row><cell></cell><cell>Ground Truth</cell><cell></cell></row></table><note>Billiards (b) Failure cases on THUMOS'14 (top sample) and TVSeries (bottom sample).</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In practice, some feature extractors<ref type="bibr" target="#b56">[59]</ref> take consecutive frames to produce one feature vector. Nonetheless, it is still temporally "centered" on a single frame. Thus we use the single frame notation here for simplicity.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SST: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mikhail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burtsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Kuratov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigory V</forename><surname>Peganov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sapunov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11527</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the Kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual working memory as visual attention sustained internally over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Openmmlab&apos;s next generation video understanding toolbox and benchmark</title>
		<ptr target="https://github.com/open-mmlab/mmaction2" />
	</analytic>
	<monogr>
		<title level="m">MMAction2 Contributors</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Attention and memory: An integrated framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Cowan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cees Snoek, and Tinne Tuytelaars. Online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Roeland De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to discriminate information for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjun</forename><surname>Eun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyoung</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal filtering networks for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjun</forename><surname>Eun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyoung</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RED: Reinforced encoder-decoder networks for action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">TURN TAP: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Startnet: Online detection of action start in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingze</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">WOAD: Weakly supervised online action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Max-margin early event detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The THUMOS challenge on action recognition for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perceiver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03206</idno>
		<title level="m">General perception with iterative attention</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporally smooth online action detection using cycle-consistent future anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young Hwi</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonghyeon</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Working memory as internal attention: Toward an integrative account of internal and external selection processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Kiyonaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Egner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychonomic bulletin &amp; review</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Marsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11746</idno>
		<title level="m">Vidtr: Video transformer without convolutions</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BSN: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">BMN: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning activity progression in lstms for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learnable pooling with context gating for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06905</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Activity graph transformer for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megha</forename><surname>Nawhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08540</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dotan</forename><surname>Asselmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00719</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Working memory and attention-a conceptual analysis and review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Oberauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">LAP-Net: Adaptive features sampling via learning action progression for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanqing</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alois</forename><surname>Knoll</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.07915</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
		<title level="m">Bidirectional recurrent neural networks. ITSS</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13915</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">CDC: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Online action detection in untrimmed, streaming videos-modeling and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Vetro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">PWC-Net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Relaxed transformer decoders for direct action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01894</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Non-local netvlad encoding for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<title level="m">Alexandre Sablayrolles, and Herv? J?gou. Training data-efficient image transformers &amp; distillation through attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/denseflow" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">TTPP: Temporal transformer with progressive prediction for efficient action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhou</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">FBNet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Towards long-form video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Chao-Yuan Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">End-to-end learning of driving models from large-scale video datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">R-C3D: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Temporal recurrent networks for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingze</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crandall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Unsupervised traffic accident detection in first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingze</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ella</forename><forename type="middle">M</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Atkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Hacs: Human action clips and segments dataset for recognition and temporal localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Privileged knowledge distillation for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peisen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09158</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
