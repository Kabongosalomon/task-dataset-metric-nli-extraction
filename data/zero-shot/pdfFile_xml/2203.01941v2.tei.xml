<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Autoregressive Image Generation using Residual Quantization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyup</forename><surname>Lee</surname></persName>
							<email>doyup.lee@postech.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kakao</forename><surname>Postech</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
							<email>chiheon.kim@kakaobrain.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kakao</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
							<email>shkim@kakaobrain.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kakao</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>POSTECH</roleName><forename type="first">Minsu</forename><surname>Cho</surname></persName>
							<email>mscho@postech.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wook-Shin</forename><surname>Han</surname></persName>
							<email>wshan@postech.ac.kr</email>
						</author>
						<title level="a" type="main">Autoregressive Image Generation using Residual Quantization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For autoregressive (AR) modeling of high-resolution images, vector quantization (VQ) represents an image as a sequence of discrete codes. A short sequence length is important for an AR model to reduce its computational costs to consider long-range interactions of codes. However, we postulate that previous VQ cannot shorten the code sequence and generate high-fidelity images together in terms of the rate-distortion trade-off. In this study, we propose the two-stage framework, which consists of Residual-Quantized VAE (RQ-VAE) and RQ-Transformer, to effectively generate high-resolution images. Given a fixed codebook size, RQ-VAE can precisely approximate a feature map of an image and represent the image as a stacked map of discrete codes. Then, RQ-Transformer learns to predict the quantized feature vector at the next position by predicting the next stack of codes. Thanks to the precise approximation of RQ-VAE, we can represent a 256?256 image as 8?8 resolution of the feature map, and RQ-Transformer can efficiently reduce the computational costs. Consequently, our framework outperforms the existing AR models on various benchmarks of unconditional and conditional image generation. Our approach also has a significantly faster sampling speed than previous AR models to generate high-quality images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vector quantization (VQ) becomes a fundamental technique for autoregerssive (AR) models to generate highresolution images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref>. Specifically, an image is represented as a sequence of discrete codes, after the feature map of the image is quantized by VQ and rearranged by an ordering such as raster scan <ref type="bibr" target="#b33">[34]</ref>. After the quantization, AR model is trained to sequentially predict the codes in the * Equal contribution ? Corresponding author <ref type="figure" target="#fig_4">Figure 1</ref>. Examples of our conditional generation for 256?256 images. The images in the first row are generated from the classes of ImageNet. The images in the second row are generated from text conditions ("A cheeseburger in front of a mountain range covered with snow." and "a cherry blossom tree on the blue ocean"). The text conditions are unseen during the training.</p><p>sequence. That is, AR models can generate high-resolution images without predicting whole pixels in an image.</p><p>We postulate that reducing the sequence length of codes is important for AR modeling of images. A short sequence of codes can significantly reduce the computational costs of an AR model, since an AR uses the codes in previous positions to predict the next code. However, previous studies have a limitation to reducing the sequence length of images in terms of the rate-distortion trade-off <ref type="bibr" target="#b41">[42]</ref>. Namely, VQ-VAE <ref type="bibr" target="#b44">[45]</ref> requires an exponentially increasing size of codebook to reduce the resolution of the quantized feature map, while conserving the quality of reconstructed images. However, a huge codebook leads to the increase of model parameters and the codebook collapse problem <ref type="bibr" target="#b9">[10]</ref>, which makes the training of VQ-VAE unstable.</p><p>In this study, we propose a Residual-Quantized VAE (RQ-VAE), which uses a residual quantization (RQ) to precisely approximate the feature map and reduce its spatial resolution. Instead of increasing the codebook size, RQ uses a fixed size of codebook to recursively quantize the feature map in a coarse-to-fine manner. After D iterations of RQ, the feature map is represented as a stacked map of D discrete codes. Since RQ can compose as many vectors as the codebook size to the power of D, RQ-VAE can precisely approximate a feature map, while conserving the information of the encoded image without a huge codebook.</p><p>Thanks to the precise approximation, RQ-VAE can further reduce the spatial resolution of the quantized feature map than previous studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref>. For example, our RQ-VAE can use 8?8 resolution of feature maps for AR modeling of 256?256 images.</p><p>In addition, We propose RQ-Transformer to predict the codes extracted by RQ-VAE. For the input of RQ-Transformer, the quantized feature map in RQ-VAE is converted into a sequence of feature vectors. Then, RQ-Transformer predicts the next D codes to estimate the feature vector at the next position. Thanks to the reduced resolution of feature maps by RQ-VAE, RQ-Transformer can significantly reduce the computational costs and easily learn the long-range interactions of inputs. We also propose two training techniques for RQ-Transformer, soft labeling and stochastic sampling for the codes of RQ-VAE. They further improve the performance of RQ-Transformer by resolving the exposure bias <ref type="bibr" target="#b37">[38]</ref> in the training of AR models. Consequently, as shown in <ref type="figure" target="#fig_4">Figure 1</ref>, our model can generate high-quality images.</p><p>Our main contributions are summarized as follows. 1) We propose RQ-VAE, which represents an image as a stacked map of discrete codes, while producing high-fidelity reconstructed images. 2) We propose RQ-Transformer to effectively predict the codes of RQ-VAE and its training techniques to resolve the exposure bias. 3) We show that our approach outperforms previous AR models and significantly improves the quality of generated images, computational costs, and sampling speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>AR Modeling for Image Synthesis AR models have shown promising results of image generation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b44">45]</ref> as well as text <ref type="bibr" target="#b3">[4]</ref> and audio <ref type="bibr" target="#b9">[10]</ref> generation. AR modeling of raw pixels is possible <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40]</ref>, but it is infeasible for high-resolution images due to the slow speed and low quality of generated images. Thus, previous stud-ies incorporate VQ-VAE <ref type="bibr" target="#b13">[14]</ref>, which uses VQ to represent an image as discrete codes, and uses an AR model to predict the codes of VQ-VAE. VQ-GAN <ref type="bibr" target="#b13">[14]</ref>, improves the perceptual quality of reconstructed images using adversarial <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref> and perceptual loss <ref type="bibr" target="#b27">[28]</ref>. However, when the resolution of the feature map is further reduced, VQ-GAN cannot precisely approximate the feature map of an image due to the limited size of codebook.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VQs in Other Applications</head><p>Composite quantizations have been used in other applications to represent a vector as a composition of codes for the precise approximation under a given codebook size <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>. For the nearest neighbor search, product quantization (PQ) <ref type="bibr" target="#b16">[17]</ref> approximates a vector as the sum of linearly independent vectors in the codebook. As a generalized version of PQ, additive quantization (AQ) <ref type="bibr" target="#b0">[1]</ref> uses the dependent vectors in the codebook, but finding the codes is an NP-hard task <ref type="bibr" target="#b7">[8]</ref>. Residual quantization (RQ, also known as stacked quantization) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref> iteratively quantizes a vector and its residuals and represents the vector as a stack of codes, which has been used for neural network compression <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. For AR modeling of images, our RQ-VAE adopts RQ to discretize the feature map of an image. However, different from previous studies, RQ-VAE uses a single shared codebook for all quantization steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>We propose the two-stage framework with RQ-VAE and RQ-Transformer for AR modeling of images (see <ref type="figure">Figure 2</ref>). RQ-VAE uses a codebook to represent an image as a stacked map of D discrete codes. Then, our RQ-Transformer autoregressively predicts the next D codes at the next spatial position. We also introduce how our RQ-Transformer resolves the exposure bias <ref type="bibr" target="#b37">[38]</ref> in the training of AR models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Stage 1: Residual-Quantized VAE</head><p>In this section, we first introduce the formulation of VQ and VQVAE. Then, we propose RQ-VAE, which can precisely approximate a feature map without increasing the codebook size, and explain how RQ-VAE represents an image as a stacked map of discrete codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Formulation of VQ and VQ-VAE</head><p>Let a codebook C be a finite set {(k, e(k))} k? <ref type="bibr">[K]</ref> , which consists of the pairs of a code k and its code embedding e(k) ? R nz , where K is the codebook size and n z is the dimensionality of code embeddings. Given a vector z ? R nz , Q(z; C) denotes VQ of z, which is the code whose embedding is nearest to z, that is,</p><formula xml:id="formula_0">Q(z; C) = arg min k?[K]</formula><p>z ? e(k) <ref type="bibr" target="#b1">2</ref> 2 .  <ref type="figure">Figure 2</ref>. An overview of our two-stage image generation framework composed of RQ-VAE and RQ-Transformer. In stage 1, RQ-VAE uses the residual quantizer to represent an image as a stack of D = 4 codes. After the stacked map of codes is reshaped, RQ-Transformer predicts the D codes at the next position. More details are available in Section 3.</p><p>After VQ-VAE encodes an image into a discrete code map, VQ-VAE reconstructs the original image from the encoded code map. Let E and G be an encoder and a decoder of VQ-VAE. Given an image X ? R Ho?Wo?3 , VQ-VAE extracts the feature map</p><formula xml:id="formula_2">Z = E(X) ? R H?W ?nz , where (H, W ) = (H o /f, W o /f ) is the spatial resolution of Z,</formula><p>and f is a downsampling factor. By applying the VQ to each feature vector at each position, VQ-VAE quantizes Z and returns its code map M ? [K] H?W and its quantized feature map? ? R H?W ?nz as</p><formula xml:id="formula_3">M hw = Q(Z hw ; C),? hw = e(M hw ),<label>(2)</label></formula><p>where Z hw ? R nz is a feature vector at (h, w), and M hw is its code. Finally, the input is reconstructed asX = G(?). We remark that reducing the spatial resolution of?, (H, W ), is important for AR modeling, since the computational cost of an AR model increases with HW . However, since VQ-VAE conducts a lossy compression of images, there is a trade-off between reducing (H, W ) and conserving the information of X. Specifically, VQ-VAE with the codebook size K uses HW log 2 K bits to represent an image as the codes. Note that the best achievable reconstruction error depends on the number of bits in terms of the rate-distortion theory <ref type="bibr" target="#b41">[42]</ref>. Thus, to further reduce (H, W ) to (H/2, W/2) but preserve the reconstruction quality, VQ-VAE requires the codebook of size K 4 . However, VQ-VAE with a large codebook is inefficient due to the codebook collapse problem <ref type="bibr" target="#b9">[10]</ref> with unstable training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Residual Quantization</head><p>Instead of increasing the codebook size, we adopt a residual quantization (RQ) to discretize a vector z. Given a quantization depth D, RQ represents z as an ordered D codes</p><formula xml:id="formula_4">RQ(z; C, D) = (k 1 , ? ? ? , k D ) ? [K] D ,<label>(3)</label></formula><p>where C is the codebook of size |C| = K, and k d is the code of z at depth d. Starting with 0-th residual r 0 = z, RQ recursively computes k d , which is the code of the residual r d?1 , and the next residual r d as</p><formula xml:id="formula_5">k d = Q(r d?1 ; C), r d = r d?1 ? e(k d ),<label>(4)</label></formula><p>for d = 1, ? ? ? , D. In addition, we define? (d) = d i=1 e(k i ) as the partial sum of up to d code embeddings, and? :=? (D) is the quantized vector of z.</p><p>The recursive quantization of RQ approximates the vector z in a coarse-to-fine manner. Note that? <ref type="bibr" target="#b0">(1)</ref> is the closest code embedding e(k 1 ) in the codebook to z. Then, the remaining codes are subsequently chosen to reduce the quantization error at each depth. Hence, the partial sum up to d, z (d) , provides a finer approximation as d increases.</p><p>Although we can separately construct a codebook for each depth d, a single shared codebook C is used for every quantization depth. The shared codebook has two advantages for RQ to approximate a vector z. First, using separate codebooks requires an extensive hyperparameter search to determine the codebook size at each depth, but the shared codebook only requires to determine the total codebook size K. Second, the shared codebook makes all code embeddings available for every quantization depth. Thus, a code can be used at every depth to maximize its utility.</p><p>We remark that RQ can more precisely approximate a vector than VQ when their codebook sizes are the same. While VQ partitions the entire vector space R nz into K clusters, RQ with depth D partitions the vector space into K D clusters at most. That is, RQ with D has the same partition capacity as VQ with K D codes. Thus, we can increase D for RQ to replace VQ with an exponentially growing codebook.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">RQ-VAE</head><p>In <ref type="figure">Figure 2</ref>, we propose RQ-VAE to precisely quantize a feature map of an image. RQ-VAE is also comprised of the encoder-decoder architecture of VQ-VAE, but the VQ module is replaced with the RQ module above. Specifically, RQ-VAE with depth D represents a feature map Z as a stacked map of codes M ? [K] H?W ?D and extract? Z (d) ? R H?W ?nz , which is the quantized feature map at depth d for d ? [D] such that</p><formula xml:id="formula_6">M hw = RQ(E(X) hw ; C, D), Z (d) hw = d d =1 e(M hwd ).<label>(5)</label></formula><p>For brevity, the quantized feature map? (D) at depth D is also denoted by?. Finally, the decoder G reconstructs the input image from? asX = G(?).</p><p>Our RQ-VAE can make AR models to effectively generate high-resolution images with low computational costs. For a fixed downsampling factor f , RQ-VAE can produce more realistic reconstructions than VQ-VAE, since RQ-VAE can precisely approximate a feature map using a given codebook size. Note that the fidelity of reconstructed images is critical for the maximum quality of generated images. In addition, the precise approximation by RQ-VAE allows more increase of f and decrease of (H, W ) than VQ-VAE, while preserving the reconstruction quality. Consequently, RQ-VAE enables an AR model to reduce its computational costs, increase the speed of image generation, and learn the long-range interactions of codes better.</p><p>Training of RQ-VAE To train the encoder E and the decoder G of RQ-VAE, we use the gradient descent with respect to the loss L = L recon + ?L commit with a multiplicative factor ? &gt; 0, The reconstruction loss L recon and the commitment loss L commit are defined as</p><formula xml:id="formula_7">L recon = X ?X 2 2 ,<label>(6)</label></formula><formula xml:id="formula_8">L commit = D d=1 Z ? sg ? (d) 2 2 ,<label>(7)</label></formula><p>where sg[?] is the stop-gradient operator, and the straightthrough estimator <ref type="bibr" target="#b44">[45]</ref> is used for the backpropagation through the RQ module. Note that L commit is the sum of quantization errors from every d, not a single term Z ? sg[?] 2 2 . It aims to make? (d) sequentially decrease the quantization error of Z as d increases. Thus, RQ-VAE approximates the feature map in a coarse-to-fine manner and keeps the training stable. The codebook C is updated by the exponential moving average of the clustered features <ref type="bibr" target="#b44">[45]</ref>.</p><p>Adversarial Training of RQ-VAE RQ-VAE is also trained with adversarial learning to improve the perceptual quality of reconstructed images. The patch-based adversarial loss <ref type="bibr" target="#b21">[22]</ref> and the perceptual loss <ref type="bibr" target="#b22">[23]</ref> are used together as described in the previous study <ref type="bibr" target="#b13">[14]</ref>. We include the details in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stage 2: RQ-Transformer</head><p>In this section, we propose RQ-Transformer in <ref type="figure">Figure 2</ref> to autoregressively predict a code stack of RQ-VAE. After we formulate the AR modeling of codes extracted by RQ-VAE, we introduce how our RQ-Transformer efficiently learns the stacked map of discrete codes. We also propose the training techniques for RQ-Transformer to prevent the exposure bias <ref type="bibr" target="#b37">[38]</ref> in the training of AR models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">AR Modeling for Codes with Depth D</head><p>After RQ-VAE extracts a code map M ? [K] H?W ?D , the raster scan order <ref type="bibr" target="#b33">[34]</ref> rearranges the spatial indices of M to a 2D array of codes</p><formula xml:id="formula_9">S ? [K] T ?D where T = HW . That is, S t , which is a t-th row of S, contains D codes as S t = (S t1 , ? ? ? , S tD ) ? [K] D for t ? [T ].<label>(8)</label></formula><p>Regarding S as discrete latent variables of an image, AR models learn p(S) which is autoregressively factorized as</p><formula xml:id="formula_10">p(S) = T t=1 D d=1 p(S td |S &lt;t,d , S t,&lt;d ).<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">RQ-Transformer Architecture</head><p>A na?ve approach can unfold S into a sequence of length T D using the raster-scan order and feed it to the conventional transformer <ref type="bibr" target="#b45">[46]</ref>. However, it neither leverages the reduced length of T by RQ-VAE and nor reduces the computational costs. Thus, we propose RQ-Transformer to efficiently learn the codes extracted by RQ-VAE with depth D. As shown in <ref type="figure">Figure 2</ref>, RQ-Transformer consists of spatial transformer and depth transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Transformer</head><p>The spatial transformer is a stack of masked self-attention blocks to extract a context vector that summarizes the information in previous positions. For the input of the spatial transformer, we reuse the learned codebook of RQ-VAE with depth D. Specifically, we define the input u t of the spatial transformer as</p><formula xml:id="formula_11">u t = PE T (t) + D d=1 e(S t?1,d ) for t &gt; 1,<label>(10)</label></formula><p>where PE T (t) is a positional embedding for spatial position t in the raster-scan order. Note that the second term is equal to the quantized feature vector of an image in Eq. 5. For the input at the first position, we define u 1 as a learnable embedding, which is regarded as the start of a sequence. After the sequence (u t ) T t=1 is processed by the spatial transformer, a context vector h t encodes all information of S &lt;t as h t = SpatialTransformer(u 1 , ? ? ? , u t ).</p><p>Depth Transformer Given the context vector h t , the depth transformer autoregressively predicts D codes (S t1 , ? ? ? , S tD ) at position t. At position t and depth d, the input v td of the depth transformer is defined as the sum of the code embeddings of up to depth d ? 1 such that</p><formula xml:id="formula_13">v td = PE D (d) + d?1 d =1 e(S td ) for d &gt; 1,<label>(12)</label></formula><p>where PE D (d) is a positional embedding for depth d and shared at every position t. We do not use P E T (t) in v td , since the positional information is already encoded in u t . For d = 1, we use v t1 = PE D (1)+h t . Note that the second term in Eq. 12 corresponds to a quantized feature vector Z</p><formula xml:id="formula_14">(d?1) hw at depth d ? 1 in Eq. 5.</formula><p>Thus, the depth transformer predicts the next code for a finer estimation of? t based on the previous estimations up to d ? 1. Finally, the depth transformer predicts the conditional distribution p td (k) = p(S td = k|S &lt;t,d , S t,&lt;d ) as</p><formula xml:id="formula_15">p td = DepthTransformer(v t1 , ? ? ? , v td ).<label>(13)</label></formula><p>RQ-Transformer is trained to minimize L AR , which is the negative log-likelihood (NLL) loss:</p><formula xml:id="formula_16">L AR = E S E t,d [? log p(S td |S &lt;t,d , S t,&lt;d )] .<label>(14)</label></formula><p>Computational Complexity Our RQ-Transformer can efficiently learn and predict the T ? D code maps of RQ-VAE, since RQ-Transformer has lower computational complexity than the na?ve approach, which uses the unfolded 1D sequence of T D codes. When computing T D length of sequences, a transformer with N layers has O(N T 2 D 2 ) of computational complexity <ref type="bibr" target="#b45">[46]</ref>. On the other hand, let us consider a RQ-Transformer with total N layers, where the number of layers in the spatial transformer and depth transformer is N spatial and N depth , respectively. Then, the spatial transformer requires O(N spatial T 2 ) and the depth transformer requires O(N depth T D 2 ), since the maximum sequence lengths for the spatial transformer and depth transformer are T and D, respectively. Hence, the computational complexity of RQ-Transformer is O(N spatial T 2 + N depth T D 2 ), which is much less than O(N T 2 D 2 ). In Section 4.3, we show that our RQ-Transformer has a faster speed of image generation than previous AR models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Soft Labeling and Stochastic Sampling</head><p>The exposure bias <ref type="bibr" target="#b37">[38]</ref> is known to deteriorate the performance of an AR model due to the error accumulation from the discrepancy of predictions in training and inference.</p><p>During an inference of RQ-Transformer, the prediction errors can also accumulate along with the depth D, since finer estimation of the feature vector becomes harder as d increases. Thus, we propose soft labeling and stochastic sampling of codes from RQ-VAE to resolve the exposure bias. Scheduled sampling <ref type="bibr" target="#b1">[2]</ref> is a way to reduce the discrepancy. However, it is unsuitable for a large-scale AR model, since multiple inferences are required at each training step and increase the training cost. Instead, we leverage the geometric relationship of code embeddings in RQ-VAE. We define a cate-</p><formula xml:id="formula_17">gorical distribution on [K] conditioned by a vector z ? R nz as Q ? (k|z), where ? &gt; 0 is a temperature Q ? (k |z) ? e ? z?e(k) 2 2 /? for k ? [K].<label>(15)</label></formula><p>As ? approaches zero, Q ? is sharpened and converges to the</p><formula xml:id="formula_18">one-hot distribution Q 0 (k |z) = 1[k = Q(z; C)].</formula><p>Soft Labeling of Target Codes Based on the distance between code embeddings, soft labeling is used to improve the training of RQ-Transformer by explicit supervision on the geometric relationship between the codes in RQ-VAE. For a position t and a depth d, let Z t be a feature vector of an image and r t,d?1 be a residual vector at depth d ? 1 in Eq. 4. Then, the NLL loss in Eq. 14 uses the one-hot label Q 0 (?|r t,d?1 ) as the supervision of S td . Instead of the onehot labels, we use the softened distribution Q ? (?|r t,d?1 ) as the supervision.</p><p>Stochastic Sampling for Codes of RQ-VAE Along with the soft labeling above, we propose stochastic sampling of the code map from RQ-VAE to reduce the discrepancy in training and inference. Instead of the deterministic code selection of RQ in Eq. 4, we select the code S td by sampling from Q ? (?|r t,d?1 ). Note that our stochastic sampling is equivalent to the original code selection of SQ in the limit of ? ? 0. The stochastic sampling provides different compositions of codes S for a given feature map of an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we empirically validate our model for high-quality image generation. We evaluate our model on unconditional image generation benchmarks in Section 4.1 and conditional image generation in Section 4.2. The computational efficiency of RQ-Transformer is shown in Section 4.3. We also conduct an ablation study to understand the effectiveness of RQ-VAE in Section 4.4.  For a fair comparison, we adopt the model architecture of VQ-GAN <ref type="bibr" target="#b13">[14]</ref>. However, since RQ-VAEs convert 256?256?3 RGB images into 8?8?4 codes, we add an encoder and decode block to RQ-VAE and further decreases the resolution of the feature map by half. All RQ-Transformers have N spatial = 24 and N depth = 4 except for the model of 1.4B parameters that has N spatial = 42 and N depth = 6. We include all details of implementation in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Unconditional Image Generation</head><p>The quality of unconditional image generation is evaluated on the LSUN-{cat, bedroom, church} <ref type="bibr" target="#b47">[48]</ref> and FFHQ <ref type="bibr" target="#b24">[25]</ref> datasets. The codebook size K is 2048 for FFHQ and 16384 for LSUN. For the FFHQ dataset, RQ-VAE is trained from scratch for 100 epochs. We also use early stopping for RQ-Transformer when the validation loss is minimized, since the small size of FFHQ leads to overfitting of AR models. For the LSUN datasets, we use a pretrained RQ-VAE on ImageNet and finetune the model for one epoch on each dataset. Considering the dataset size, we use RQ-Transformer of 612M parameters for LSUN-{cat, bedroom} and 370M parameters for LSUN-church and FFHQ. For the evaluation measure, we use Frechet Inception Distance (FID) <ref type="bibr" target="#b19">[20]</ref> between 50K generated samples and all training samples. Following the previous studies <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, we also use top-k and top-p sampling to report the best performance. <ref type="table" target="#tab_1">Table 1</ref> shows that our model outperforms the other AR models on unconditional image generation. For small-scale datasets such as LSUN-church and FFHQ, our model outperforms DCT <ref type="bibr" target="#b32">[33]</ref> and VQ-GAN <ref type="bibr" target="#b13">[14]</ref> with marginal improvements. However, for a larger scale of datasets such as LSUN-{cat, bedroom}, our model significantly outperforms other AR models and diffusion-based models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref>. We conjecture that the performance improvement comes from the shorter sequence length by RQ-VAE, since SQ-Transformer can easily learn the long-range interactions between codes in the short length of the sequence. In the first two rows of <ref type="figure" target="#fig_0">Figure 3</ref>, we show that RQ-Transformer can unconditionally generate high-quality images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Conditional Image Generation</head><p>We use ImageNet <ref type="bibr" target="#b8">[9]</ref> and CC-3M <ref type="bibr" target="#b42">[43]</ref> for a class-and text-conditioned image generation, respectively. We train RQ-VAE with K=16,384 on ImageNet training data for 10 epochs and reuse the trained RQ-VAE for CC-3M. For Im-ageNet, we also use RQ-VAE trained for 50 epochs to examine the effect of improved reconstruction quality on image generation quality of RQ-Transformer in <ref type="table">Table 2</ref>. For conditioning, we append the embeddings of class and text conditions to the start of input for spatial transformer. The texts of CC-3M are represented as a sequence of at most 32 tokens using a byte pair encoding <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b46">47]</ref>. <ref type="table">Table 2</ref> shows that our model significantly outperforms previous models on ImageNet. Our RQ-Transformer of 480M parameters is competitive with the previous AR mod- els including VQ-VAE2 <ref type="bibr" target="#b38">[39]</ref>, DCT <ref type="bibr" target="#b32">[33]</ref>, and VQ-GAN <ref type="bibr" target="#b13">[14]</ref> without rejection sampling, although our model has 3? less parameters than VQ-GAN. In addition, RQ-Transformer of 821M parameters outperforms the previous AR models without rejection sampling. Our stochastic sampling is also effective for performance improvement, while RQ-Transformer without it still outperforms other AR models. RQ-Transformer of 1.4B parameters achieves 11.56 of FID score without rejection sampling. When we increase the training epoch of RQ-VAE from 10 into 50 and improve the reconstruction quality, RQ-Transformer of 1.4B parameters further improves the performance and achieves 8.71 of FID. Moreover, when we further increase the number of parameters to 3.8B, RQ-Transformer achieves 7.55 of FID score without rejection sampling and is competitive with BigGAN <ref type="bibr" target="#b2">[3]</ref>. When ResNet-101 <ref type="bibr" target="#b17">[18]</ref> is used for rejection sampling with 5% and 12.5% of acceptance rates for 1.4B and 3.8B parameters, respectively, our model outperforms ADM <ref type="bibr" target="#b10">[11]</ref> and achieves the state-of-the-art score of FID. <ref type="figure" target="#fig_0">Figure 3</ref> also shows that our model can generate highquality images.</p><p>RQ-Transformer can also generate high-quality images based on various text conditions of CC-3M. RQ-Transformer shows significantly higher performance than VQ-GAN with a similar number of parameters. In addition, although RQ-Transformer has 23% of parameters, our model significantly outperforms ImageBART <ref type="bibr" target="#b12">[13]</ref> on both FID and CLIP score <ref type="bibr" target="#b35">[36]</ref> (with ViT-B/32 <ref type="bibr" target="#b11">[12]</ref>). The results imply that RQ-Transformer can easily learn the relationship between a text and an image when the reduced sequence length is used for the image. <ref type="figure" target="#fig_0">Figure 3</ref> shows that RQ-Transformer trained on CC-3M can generate high-quality images using various text conditions. In addition, the text conditions in <ref type="figure" target="#fig_4">Figure 1</ref> are novel compositions of visual concepts, which are unseen in training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Computational Efficiency of RQ-Transformer</head><p>In <ref type="figure" target="#fig_1">Figure 4</ref>, we evaluate the sampling speed of RQ-Transformer and make a comparison with VQ-GAN. Both the models have 1.4B parameters. The shape of the input code map for VQ-GAN and RQ-Transformer are set to be 16?16?1 and 8?8?4, respectively. We use a single NVIDIA A100 GPU for each model to generate 5000 samples with 100, 200, and 500 of batch size. The reported speeds in <ref type="figure" target="#fig_1">Figure 4</ref>  Moreover, thanks to the memory saving from the short sequence length of RQ-VAE, RQ-Transformer can increase the batch size up to 500, which is not allowed for VQ-GAN. Thus, RQ-Transformer can further accelerate the sampling speed, which is 0.02 seconds per image, and be 7.3? faster than VQ-GAN with batch size 200. Thus, RQ-Transformer is more computationally efficient than previous AR models, while achieving state-of-the-art results on high-resolution image generation benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study on RQ-VAE</head><p>We conduct an ablation study to understand the effect of RQ with respect to the codebook size (K) and the shape of the code map (H ? W ? D). We measure the rFID, which is FID between original images and reconstructed images, on ImageNet validation data. <ref type="table" target="#tab_3">Table 4</ref> shows that increasing the quantization depth D is more effective to improve the reconstruction quality than increasing the codebook size K.</p><p>Here, we remark that RQ-VAE with D = 1 is equivalent to VQ-GAN. For a fixed codebook size K=16,384, the rFID significantly deteriorates as the spatial resolution H ? W is reduced from 16?16 to 8?8. Even when the codebook size is increased to K=131,072, the rFID cannot recover the rFID with 16?16 feature maps, since the restoration of rFID requires the codebook of size K=16,384 4 in terms of the rate-distortion trade-off. Contrastively, note that the rFIDs are significantly improved when we increase the quantization depth D with a codebook of fixed size K=16,384. Thus, our RQ-VAE can further reduce the spatial resolution than VQ-GAN, while conserving the reconstruction quality. Although RQ-VAE with D &gt; 4 can further improve the reconstruction quality, we use RQ-VAE with 8?8?4 code map for AR modeling of images, considering the computational costs of RQ-Transformer. In addition, the longer training of RQ-VAE can further improve the reconstruction quality, but we train RQ-VAE for 10 epochs as the default due to its increased training time. <ref type="figure">Figure 5</ref> and 6 substantiate our claim that RQ-VAE conducts the coarse-to-fine estimation of feature maps. For example, <ref type="figure">Figure 5</ref> shows the reconstructed images G(? (d) ) of a quantized feature map at depth d in Eq. 4. When we only use the codes at d = 1, the reconstructed image is blurry and only contains coarse information of the original image. However, as d increases and the information of remaining codes is sequentially added, the reconstructed image includes more clear and fine-grained details of the image. We visualize the distribution of the code usage at each depth d over the norm of code embeddings in <ref type="figure" target="#fig_3">Figure 6</ref>. Since RQ conducts the coarse-to-fine approximation of a feature map, a smaller norm of code embeddings are used as d increases. Moreover, the overlaps between the code usage distributions show that many codes are shared in different levels of depth d. Thus, the shared codebook of RQ-VAE can maximize the utility of its codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Discrete representation of visual images is important for an AR model to generate high-resolution images. In this work, we have proposed RQ-VAE and RQ-Transformer for high-quality image generation. Under a fixed codebook size, RQ-VAE can precisely approximate a feature map of an image to represent the image as a short sequence of codes. Thus, RQ-Transformer effectively learns to predict the codes to generate high-quality images with low computational costs. Consequently, our approach outperforms the previous AR models on various image generation benchmarks such as LSUNs, FFHQ, ImageNet, and CC-3M.</p><p>Our study has three main limitations. First, our model does not outperform StyleGAN2 <ref type="bibr" target="#b25">[26]</ref> on unconditional image generation, especially with a small-scale dataset such as FFHQ, due to overfitting of AR models. Thus, regularizing AR models is worth exploration for high-resolution image generation on a small dataset. Second, our study does not enlarge the model and training data for text-toimage generation. As a previous study <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37]</ref> shows that a huge transformer can effectively learn the zero-shot text-toimage generation, increasing the number of parameters is an interesting future work. Third, AR models can only cap- <ref type="figure">Figure 5</ref>. The examples of coarse-to-fine approximation by RQ-VAE. The first example is the original image, and the others are reconstructed from? <ref type="bibr">(d)</ref> . As d increases, the reconstructed images become clear and include fine-grained details of the original image. ture unidirectional contexts to generate images compared to other generative models. Thus, modeling of bidirectional contexts can further improve the quality of image generation and enable AR models to be used for image manipulation such as image inpainting and outpainting <ref type="bibr" target="#b12">[13]</ref>.</p><p>Although our study significantly reduces the computational costs for AR modeling of images, training of largescale AR models is still expensive, consumes high amounts of electrical energy, and can leave a huge carbon footprint, as the scale of model and training dataset becomes large. Thus, efficient training of large-scale AR models is still worth exploration to avoid environmental pollution. <ref type="table">Table 5</ref>. The hyperparameters for implementing RQ-Transformer. We follow the same notation in the main paper. ne represents the dimensionality of features in RQ-Transformer, and # heads represents the number of heads in self-attentions of RQ-Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Nspatial </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Architecture of RQ-VAE</head><p>For the architecture of RQ-VAE, we follow the architecture of VQ-GAN <ref type="bibr" target="#b13">[14]</ref> for a fair comparison. However, we add two residual blocks with 512 channels each followed by a down-/up-sampling block to extract feature maps of resolution 8?8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Architecture of RQ-Transformer</head><p>The RQ-Transformer, which consists of the spatial transformer and the depth transformer, adopts a stack of self-attention blocks <ref type="bibr" target="#b45">[46]</ref> for each compartment. In <ref type="table">Table 5</ref>, we include the detailed information of hyperparameters to implement our RQ-Transformers. All RQ-Transformers in <ref type="table">Table 5</ref> uses RQ-VAE with 8?8?4 shape of codes. For CC-3M, the length of text conditions is 32, and the last token in text conditions predicts the code at the first position of images. Thus, the total sequence length (T ) of RQ-Transformer is 95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Training Details</head><p>For ImageNet, RQ-VAE is trained for 10 epochs with batch size 128. We use the Adam optimizer <ref type="bibr" target="#b26">[27]</ref> with ? 1 = 0.5 and ? 2 = 0.9, and learning rate is set 0.00004. The learning rate is linearly warmed up during the first 0.5 epoch. We do not use learning rate decay, weight decaying, nor dropout. For the adversarial and perceptual loss, we follow the experimental setting of VQ-GAN <ref type="bibr" target="#b13">[14]</ref>. In particular, the weight for the adversarial loss is set 0.75 and the weight for the perceptual loss is set 1.0. To increase the codebook usage of RQ-VAE, we use random restart of unused codes proposed in JukeBox <ref type="bibr" target="#b9">[10]</ref>. For LSUN-{cat, bedroom, church}, we use the pretrained RQ-VAE on ImageNet and finetune it for one epoch with 0.000004 of learning rate. For FFHQ, we train RQ-VAE for 150 epochs of training data with 0.00004 of learning rate and five epochs of warm-up. For CC-3M, we use the pretrained RQ-VAE on ImageNet without finetuning.</p><p>All RQ-Transformers are trained using the AdamW optimizer <ref type="bibr" target="#b30">[31]</ref> with ? 1 = 0.9 and ? 2 = 0.95. We use the cosine learning rate schedule with 0.0005 of the initial learning rate. The RQ-Transformer is trained for 90, 200, 300 epochs for LSUN-bedroom, -cat, and -church respectively. The weight decay is set 0.0001, and the batch size is 16 for FFHQ and 2048 for other datasets. In all experiments, the dropout rate of each self-attention block is set 0.1 except 0.3 for 3.8B parameters of RQ-Transformer. We use eight NVIDIA A100 GPUs to train RQ-Transformer of 1.4B parameters, and four GPUs to train RQ-Transformers of other sizes. The training time is &lt;9 days for LSUN-cat, LSUN-bedroom, &lt;4.5 days for ImageNet, and CC-3M, and &lt;1 day for LSUN-church and FFHQ. We use the early stopping at 39 epoch for the FFHQ dataset, considering the overfitting of the RQ-Transformer due to the small scale of the dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Results of Generated Images by RQ-Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Nearest Neighbor Search of Generated Images for FFHQ</head><p>For the training of FFHQ, we use early stopping for RQ-Transformer when the validation loss is minimized, since RQ-Transformer can memorize all training samples due to the small scale of FFHQ. Despite the use of early stopping, we further examine whether our model memorizes the training samples or generates new images. To visualize the nearest neighbors in the training images of FFHQ to generated images, we use a KD-tree <ref type="bibr" target="#b4">[5]</ref>, which is constructed by the VGG-16 features <ref type="bibr" target="#b43">[44]</ref> of training images. <ref type="figure" target="#fig_4">Figure 11</ref> shows that our model does not memorize the training data, but generates new face images for unconditional sample generation of FFHQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Ablation Study on Soft Labeling and Stochastic Sampling</head><p>For 821M parameters of RQ-Transformer trained on ImageNet, RQ-Transformer achieves 14.06 of FID score when neither stochastic sampling nor soft labeling is used. When stochastic sampling is applied to the training of RQ-Transformer, 13.24 of FID score is achieved. When only soft labeling is used without stochastic sampling, RQ-Transformer achieves 14.87 of FID score, and the performance worsens. However, when both stochastic sampling and soft labeling are used together, RQ-Transformer achieves 13.11 of FID score, which is improved performance than baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Additional Examples of Class-Conditioned Image Generation for ImageNet</head><p>We visualize the additional examples of class-conditional image generation by RQ-Transformer trained on ImageNet. <ref type="figure" target="#fig_4">Figure 12</ref>, 13, and 14 show the generated samples by RQ-Transformer with 1.4B parameters conditioned on a few selected classes. Those images are sampled with top-k 512 and top-p 0.95.In addition, <ref type="figure" target="#fig_3">Figure 16</ref> shows the generated samples using the rejection sampling with ResNet-101 is applied with various acceptance rates. The images in <ref type="figure" target="#fig_3">Figure 16</ref> are sampled with fixed p = 1.0 for top-p sampling and different acceptance rates of the rejection sampling and top-k values. The (top-k, acceptance rate)s are (512, 0,5), (1024, 0.25), and (2048, 0.05), and their corresponding FID scores are 7.08, 5.62, and 4.45. <ref type="figure" target="#fig_4">Figure 15</ref> shows the generated samples of RQ-Transformer with 3.8B parameters using rejection sampling with (4098, 0.125).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Additional Examples of Text-Conditioned Image Generation for CC-3M</head><p>We visualize the additional examples of text-conditioned image generation by RQ-Transformer trained on CC-3M. <ref type="figure" target="#fig_4">Figure  17</ref> shows the generated samples conditioned by various texts, which are unseen during training. Specifically, we manually choose four pairs of sentences, which share visual content with different contexts and styles, to validate the compositional generalization of our model. All images are sampled with top-k 1024 and top-p 0.9. Additionally, <ref type="figure" target="#fig_4">Figure 18</ref> shows the samples conditioned on randomly chosen texts from the validation set of CC-3M. For the images in <ref type="figure" target="#fig_4">Figure 18</ref>, we use the re-ranking with CLIP similarity score <ref type="bibr" target="#b35">[36]</ref> as in <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b12">[13]</ref> and select the image with the highest CLIP score among 16 generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6. The Effects of Top-k &amp; Top-p Sampling on FID Scores</head><p>In this section, we show the FID scores of the RQ-Transformer trained on ImageNet according to the choice of k and p for top-k and top-p sampling, respectively. <ref type="figure" target="#fig_4">Figure 19</ref> and 20 shows the FID scores of 821M and 1400M parameters of RQ-Transformer according to different ks and ps. Although we report the global minimum FID score, the minimum FID score at each k is not significantly deviating from the global minimum. For instance, the minimum FID attained by RQ-Transformer with 1.4B parameters is 11.58 while the minimum for each k is at most 11.87. When the rejection sampling of generated images is used to select high-quality images, <ref type="figure" target="#fig_4">Figure 21</ref> shows that higher top-k values are effective as the acceptance rate decreases, since various and high-quality samples can be generated with higher top-k values. Finally, for the CC-3M dataset, <ref type="figure" target="#fig_17">Figure 22</ref> shows the FID scores and CLIP similarity scores according to different top-k and top-p values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Results of Reconstruction Images by RQ-VAE C.1. Coarse-to-Fine Approximation of Feature Maps by RQ-VAE</head><p>In this section, we further explain that RQ-VAE with depth D conducts the coarse-to-fine approximation of a feature map. <ref type="table">Table 6</ref> shows the reconstruction error L recon , the commitment loss L commit , and the perceptual loss <ref type="bibr" target="#b22">[23]</ref>, when RQ-VAE uses the partial sum? (d) of up to d code embeddings for the quantized feature map of an image. All three losses, which are the reconstruction and perceptual loss of a reconstructed image, and the commitment loss L commit of the feature map, monotonically decrease as d increases. The results imply that RQ-VAE can precisely approximate the feature map of an <ref type="table">Table 6</ref>. Results of coarse-to-fine approximation by the RQ-VAE with 8?8?4 shape of M. Reconstruction loss Lrecon, commitment loss Lcommit, perceptual loss, and reconstruction FID (rFID) are measured on ImageNet validation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X</head><p>Lrecon Lcommit Perceptual loss rFID G(? <ref type="bibr" target="#b0">(1)</ref> ) 0.018 0.12 0.12 100.86 G(? <ref type="bibr" target="#b1">(2)</ref> ) 0.014 0.10 0.090 22.74 G(? (3) ) 0.012 0.091 0.075 7.66 G(? <ref type="bibr" target="#b3">(4)</ref> ) 0.010 0.082 0.068 <ref type="bibr">4.73</ref> image, when RQ-VAE iteratively quantizes the feature map and its residuals. <ref type="figure" target="#fig_0">Figure 23</ref> also shows that the reconstructed images contain more fine-grained information of the original images as d increases. Thus, the experimental results validate that our RQ-VAE conducts the coarse-to-fine approximation, and RQ-Transformer can learn to generate the feature vector at the next position in a coarse-to-fine manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. The Effects of Adversarial and Perceptual Losses on Training of RQ-VAE</head><p>In <ref type="figure" target="#fig_1">Figure 24</ref>, we visualize the reconstructed images by RQ-VAEs, which are trained without and with adversarial and perceptual losses. When the adversarial and perceptual losses are not used (the second and third columns), the reconstructed images are blurry, since the codebook is insufficient to include all information of local details in the original images. However, despite the blurriness, note that RQ-VAE with D = 4 (the third column) much improves the quality of reconstructed images than VQ-VAE (or RQ-VAE with D = 1, the second column).</p><p>Although the adversarial and perceptual losses are used to improve the quality of image reconstruction, RQ is still important to generate high-quality reconstructed images with low distortion. When the adversarial and perceptual losses are used in the training of RQ-VAEs (the fourth and fifth columns), the reconstructed images are much clear and include fine-grained details of the original images. However, the reconstructed images by VQ-VAE (or RQ-VAE with D = 1, the fourth column) include the unrealistic artifacts and the high distortion of the original images. Contrastively, when RQ with D = 4 is used to encode the information of the original images, the reconstructed images by RQ-VAE (the fifth column) are significantly realistic and do not distort the visual information in the original images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Using D Non-Shared Codebooks of Size D/K for RQ-VAE</head><p>As mentioned in Section 3.1.2, a single codebook C of size K is shared for every quantization depth D instead of D non-shared codebooks of size D/K. When we replace the shared codebook of size 16,384 with four non-shared codebooks of size 4,096, rFID of RQ-VAE increases from 4.73 to 5.73, since the non-shared codebooks can approximate at most (K/D) D clusters only. In fact, a shared codebook with K=4,096 has 5.94 of rFID, which is similar to 5.73 above. Thus, the shared codebook is more effective to increase the quality of image reconstruction with limited codebook size than non-shared codebooks.            watch : what has earth looked like ? <ref type="figure" target="#fig_4">Figure 18</ref>. Additional examples of text-conditioned image generation by our model trained on CC-3M. Text prompts are randomly chosen from the validation data. Best 1 of 16 with re-ranking as in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b36">37]</ref>.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Generated 256?256 images by our models. First row: LSUN-{cat, bedroom}. Second row: LSUN-church and FFHQ. Third row: ImageNet and CC-3M. The text conditions of CC-3M are "Mountains and hills reflecting over a surface," "A sunset over the skyline of a city," and "Businessman with a paper bag on head," respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The sampling speed of RQ-Transformer with 1.4B parameters according to batch size and code map shape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>do not include the decoding time of the stage 1 model to focus on the effect of RQ-Transformer architecture. The decoding time of VQ-GAN and RQ-VAE is about 0.008 sec/image. For the batch size of 100 and 200, RQ-Transformer shows 4.1? and 5.6? speed-up compared with VQ-GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>The distribution of used codes at each quantization depth. The blue bar plot represents the code distribution according to the norm of embeddings. ImageNet validation data is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>B. 1 .</head><label>1</label><figDesc>Additional Examples of Unconditional Image Generation for LSUNs and FFHQ We show the additional examples of unconditional image generation by RQ-VAEs trained on LSUN-{cat, bedroom, church} and FFHQ. Figure 7, 8, 9, and 10 show the results of LSUN-cat, LSUN-bedroom LSUN-church, and FFHQ, respectively. For the top-k (top-p) sampling, 512 (0.9), 8192 (0.85), 1400 (1.0), and 2048 (0.95) are used respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Additional examples of unconditional image generation by our model trained on LSUN-bedroom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Additional examples of unconditional image generation by our model trained on LSUN-church.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Additional examples of unconditional image generation by our model trained on FFHQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>Visualization of nearest neighbors in the FFHQ training samples to our generated samples. In each row, the first image is our generation. The nearest neighbors to the first image are visualized according to the similarity of VGG-16 features in descending order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>Additional examples of conditional image generation by 1.4B parameters of RQ-Transformer trained on ImageNet. Top: Tench (0). Middle: Ostrich (9). Bottom: Bald eagle (22).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 .</head><label>13</label><figDesc>Additional examples of conditional image generation by 1.4B parameters of RQ-Transformer trained on ImageNet. Top: Lorikeet (90). Middle: Tibetan terrier (200). Bottom: Tiger beetle (300).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 .</head><label>14</label><figDesc>Additional examples of conditional image generation by 1.4B parameters of RQ-Transformer trained on ImageNet. Top: Coffee pot (505). Middle: Space shuttle (812). Bottom: Cheeseburger (933).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 .</head><label>15</label><figDesc>Additional examples of conditional image generation by 3.8B parameters of RQ-Transformer trained on ImageNet. The classes of images in each line are tench (0), ostrich (9), bald eagle (22), lorikeet (90), tibetan terrier (200), tiger beetle (300), coffee pot (505), space shuttle (812), and cheeseburger (933), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 .</head><label>16</label><figDesc>Additional examples of conditional image generation by 1.4B parameters of RQ-Transformer trained on ImageNet with rejection sampling. All images are sampled with top-p=1.0 with different acceptance rates of the rejection sampling and top-k values. (Top) Acceptance rate=0.5 and top-k=512, Middle: Acceptance rate=0.25 and top-k=1024. Bottom: Acceptance rate=0.05 and top-k=2048.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 17 .</head><label>17</label><figDesc>Additional examples of text-conditional image generation by our model trained on CC-3M. The text conditions are customized prompts, which are unseen during the training of RQ-Transformer. All images are sampled with top-k 1024 and top-p 0.9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>farmer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 21 .</head><label>21</label><figDesc>FID of rejection-sampled 50K samples of RQ-Transformer (1.4B) against the training and the validation split of ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 22 .</head><label>22</label><figDesc>0.258 0.258 0.259 0.259 0.259 0.258 0.258 0.258 0.259 0.259 0.258 0.258 0FID and CLIP score of RQ-Transformer (654M) on CC-3M, evaluated against the validation set. Images are generated conditioned on each sentence in the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 23 .</head><label>23</label><figDesc>Additional examples of coarse-to-fine approximation by RQ-VAE with the 8?8?4 code map. The first example in each row is the original image, and the others are constructed from? (d) as d increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 24 .</head><label>24</label><figDesc>Reconstruction images by RQ-VAE with and without adversarial training. The first image in each row is the original image. The second and third images are reconstructed images by RQ-VAE without adversarial training. The second image is reconstructed by RQ-VAE using 8?8?1 code map, and the third image is reconstructed by RQ-VAE using 8?8?4 code map. The fourth and fifth images are reconstructed images by RQ-VAE with adversarial training. The fourth images are reconstructed by 8?8?1 code map, and the fifth images are reconstructed by 8?8?4 code map,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of FIDs for unconditional image generation on LSUN-{Cat, Bedroom, Church}<ref type="bibr" target="#b47">[48]</ref> and FFHQ<ref type="bibr" target="#b24">[25]</ref>.</figDesc><table><row><cell></cell><cell>Cat</cell><cell cols="3">Bedroom Church FFHQ</cell></row><row><cell>VDVAE [7]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>28.5</cell></row><row><cell>DDPM [21]</cell><cell>19.75</cell><cell>4.90</cell><cell>7.89</cell><cell>-</cell></row><row><cell>ImageBART [13]</cell><cell>15.09</cell><cell>5.51</cell><cell>7.32</cell><cell>9.57</cell></row><row><cell>StyleGAN2 [26]</cell><cell>7.25</cell><cell>2.35</cell><cell>3.86</cell><cell>3.8</cell></row><row><cell>BigGAN [3]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>12.4</cell></row><row><cell>DCT [33]</cell><cell>-</cell><cell>6.40</cell><cell>7.56</cell><cell>13.06</cell></row><row><cell>VQ-GAN [14]</cell><cell>17.31</cell><cell>6.35</cell><cell>7.81</cell><cell>11.4</cell></row><row><cell>RQ-Transformer</cell><cell>8.64</cell><cell>3.04</cell><cell>7.45</cell><cell>10.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Comparison of FIDs and ISs for class-conditioned image generation on ImageNet<ref type="bibr" target="#b8">[9]</ref> 256?256. ? denotes a model without our stochastic sampling and soft labeling. ? denotes the use of rejection sampling or gradient guidance by pretrained classifier. * denotes the use of RQ-VAE trained for 50 epochs. Comparison of FID and CLIP score<ref type="bibr" target="#b35">[36]</ref> on the validation data of CC-3M<ref type="bibr" target="#b42">[43]</ref> for text-conditioned image generation.</figDesc><table><row><cell></cell><cell>Params</cell><cell>FID</cell><cell>IS</cell></row><row><cell cols="4">without rejection sampling or gradient guidance</cell></row><row><cell>ADM [11]</cell><cell>554M</cell><cell>10.94</cell><cell>101.0</cell></row><row><cell>ImageBART [13]</cell><cell>3.5B</cell><cell>21.19</cell><cell>61.6</cell></row><row><cell>BigGAN [3]</cell><cell>164M</cell><cell>7.53</cell><cell>168.6</cell></row><row><cell>BigGAN-deep [3]</cell><cell>112M</cell><cell>6.84</cell><cell>203.6</cell></row><row><cell>VQ-VAE2 [39]</cell><cell>13.5B</cell><cell>?31</cell><cell>?45</cell></row><row><cell>DCT [33]</cell><cell>738M</cell><cell>36.5</cell><cell>n/a</cell></row><row><cell>VQ-GAN [14]</cell><cell>1.4B</cell><cell>15.78</cell><cell>74.3</cell></row><row><cell>RQ-Transformer</cell><cell>480M</cell><cell>15.72</cell><cell>86.8?1.4</cell></row><row><cell>RQ-Transformer  ?</cell><cell>821M</cell><cell>14.06</cell><cell>95.8?2.1</cell></row><row><cell>RQ-Transformer</cell><cell>821M</cell><cell cols="2">13.11 104.3?1.5</cell></row><row><cell>RQ-Transformer</cell><cell>1.4B</cell><cell cols="2">11.56 112.4?1.1</cell></row><row><cell>RQ-Transformer  *</cell><cell>1.4B</cell><cell>8.71</cell><cell>119.0?2.5</cell></row><row><cell>RQ-Transformer  *</cell><cell>3.8B</cell><cell>7.55</cell><cell>134.0?3.0</cell></row><row><cell cols="4">with rejection sampling or gradient guidance</cell></row><row><cell>ADM  ? [11]</cell><cell>554M</cell><cell>4.59</cell><cell>186.7</cell></row><row><cell>ImageBART  ? [13]</cell><cell>3.5B</cell><cell>7.44</cell><cell>273.5?4.1</cell></row><row><cell>VQ-VAE2  ? [39]</cell><cell>13.5B</cell><cell>?10</cell><cell>?330</cell></row><row><cell>VQ-GAN  ? [14]</cell><cell>1.4B</cell><cell>5.20</cell><cell>280.3?5.5</cell></row><row><cell>RQ-Transformer  ?</cell><cell>1.4B</cell><cell>4.45</cell><cell>326.0?3.5</cell></row><row><cell>RQ-Transformer  *  ?</cell><cell>1.4B</cell><cell>3.89</cell><cell>337.5?4.6</cell></row><row><cell>RQ-Transformer  *  ?</cell><cell>3.8B</cell><cell>3.80</cell><cell>323.7?2.8</cell></row><row><cell>Validation Data</cell><cell>-</cell><cell>1.62</cell><cell>234.0</cell></row><row><cell></cell><cell>Params</cell><cell>FID</cell><cell>CLIP-s</cell></row><row><cell>VQ-GAN [14]</cell><cell>600M</cell><cell>28.86</cell><cell>0.20</cell></row><row><cell>ImageBART [13]</cell><cell>2.8B</cell><cell>22.61</cell><cell>0.23</cell></row><row><cell>RQ-Transformer</cell><cell>654M</cell><cell>12.33</cell><cell>0.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of FIDs between ImageNet validation images and their reconstructed images according to codebook size (K) and the shape of code map H ? W ? D. ? denotes the reproduced performance, and * denotes 50 epochs of training.</figDesc><table><row><cell></cell><cell>H ? W ? D</cell><cell>K</cell><cell>rFID</cell></row><row><cell>VQ-GAN [14]</cell><cell>16?16?1</cell><cell>16,384</cell><cell>4.90</cell></row><row><cell>VQ-GAN  ?</cell><cell>16?16?1</cell><cell>16,384</cell><cell>4.32</cell></row><row><cell>VQ-GAN</cell><cell>8?8?1</cell><cell>16,384</cell><cell>17.95</cell></row><row><cell>VQ-GAN</cell><cell>8?8?1</cell><cell>65,536</cell><cell>17.66</cell></row><row><cell>VQ-GAN</cell><cell>8?8?1</cell><cell cols="2">131,072 17.09</cell></row><row><cell>RQ-VAE</cell><cell>8?8?2</cell><cell>16,384</cell><cell>10.77</cell></row><row><cell>RQ-VAE</cell><cell>8?8?4</cell><cell>16,384</cell><cell>4.73</cell></row><row><cell>RQ-VAE  *</cell><cell>8?8?4</cell><cell>16,384</cell><cell>3.20</cell></row><row><cell>RQ-VAE</cell><cell>8?8?8</cell><cell>16,384</cell><cell>2.69</cell></row><row><cell>RQ-VAE</cell><cell>8?8?16</cell><cell>16,384</cell><cell>1.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>17.49 16.39 15.30 14.54 14.07 13.54 13.11 13.17 13.52 15.96 15.01 14.21 13.73 13.28 13.27 13.52 13.95 15.23 15.12 14.24 13.63 13.37 13.50 13.89 14.95 16.80 19.68 14.62 13.78 13.43 13.57 14.07 15.38 17.41 20.59 25.01 14.37 13.76 13.61 13.79 14.49 16.50 19.63 23.17 29.03 14.05 13.70 13.53 14.03 15.20 16.99 19.98 24.34 30.33 14.15 13.51 13.66 14.07 15.03 17.16 20.14 24.14 31.05 16.58 15.46 14.37 13.54 12.98 12.38 11.87 11.79 11.94 14.96 13.99 13.15 12.56 11.98 11.84 11.89 12.17 13.17 14.03 13.07 12.37 11.98 11.93 12.14 12.96 14.47 17.01 13.42 12.51 12.01 11.98 12.27 13.30 15.02 17.84 21.85 13.10 12.41 12.09 12.09 12.57 14.24 16.99 20.20 25.58 12.84 12.33 12.00 12.27 13.16 14.69 17.32 21.28 26.81 12.90 12.16 12.12 12.30 13.01 14.83 17.42 21.09 27.47Figure 19. FID of 50K generated samples of RQ-Transformer (821M) against the training and the validation split of ImageNet.14.92 14.28 13.33 12.75 12.30 11.90 11.58 11.56 11.93 13.80 13.07 12.50 11.90 11.61 11.63 11.74 12.52 13.55 12.96 12.47 11.97 11.73 11.91 12.44 13.32 15.00 17.64 12.44 12.13 11.93 11.79 12.54 13.50 15.48 18.41 22.78 12.33 11.95 11.85 12.19 12.95 14.76 17.27 21.08 26.52 12.27 11.95 11.87 12.20 13.29 15.41 18.21 22.28 27.84 12.26 11.88 11.87 12.28 13.34 15.43 18.22 22.33 28.26 14.33 13.63 12.70 12.05 11.51 11.01 10.61 10.45 10.60 13.11 12.33 11.66 10.98 10.58 10.45 10.41 10.95 11.71 12.15 11.56 10.92 10.56 10.57 10.88 11.53 12.90 15.20 11.53 11.08 10.74 10.46 10.94 11.65 13.32 15.90 19.84 11.38 10.85 10.54 10.71 11.23 12.71 14.88 18.30 23.31 11.27 10.83 10.56 10.70 11.50 13.28 15.72 19.41 24.53 11.27 10.76 10.56 10.77 11.55 13.29 15.73 19.47 24.92Figure 20. FID of 50K generated samples of RQ-Transformer (1.4B) against the training and the validation split of ImageNet.</figDesc><table><row><cell cols="2">256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell cols="2">256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">512</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">512</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">1024 2048 4096 top-k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20 22 24 FID (vs. train)</cell><cell cols="2">1024 2048 4096 top-k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 20 22 FID (vs. valid)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell></row><row><cell cols="2">8192</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell cols="2">8192</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>14</cell></row><row><cell cols="2">16384</cell><cell>0.6</cell><cell>0.65</cell><cell>0.7</cell><cell>0.75</cell><cell>0.8 top-p</cell><cell>0.85</cell><cell>0.9</cell><cell>0.95</cell><cell>1.0</cell><cell>14</cell><cell cols="2">16384</cell><cell>0.6</cell><cell>0.65</cell><cell>0.7</cell><cell>0.75</cell><cell>0.8 top-p</cell><cell>0.85</cell><cell>0.9</cell><cell>0.95</cell><cell>1.0</cell><cell>12</cell></row><row><cell cols="2">256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28</cell><cell cols="2">256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">512</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24</cell><cell cols="2">512</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>22</cell></row><row><cell cols="2">1024 2048 4096 top-k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18 20 22 FID (vs. train)</cell><cell cols="2">1024 2048 4096 top-k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20 16 18 FID (vs. valid)</cell></row><row><cell cols="2">8192</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell cols="2">8192</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>14</cell></row><row><cell cols="2">16384</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>12 14</cell><cell cols="2">16384</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>12</cell></row><row><cell></cell><cell></cell><cell>0.6</cell><cell>0.65</cell><cell>0.7</cell><cell>0.75</cell><cell>0.8 top-p</cell><cell>0.85</cell><cell>0.9</cell><cell>0.95</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell>0.65</cell><cell>0.7</cell><cell>0.75</cell><cell>0.8 top-p</cell><cell>0.85</cell><cell>0.9</cell><cell>0.95</cell><cell>1.0</cell></row><row><cell></cell><cell>1.00</cell><cell></cell><cell cols="8">11.93 13.55 17.64 22.78 26.52 27.84 28.26</cell><cell>25</cell><cell></cell><cell>1.00</cell><cell></cell><cell cols="8">10.60 11.71 15.20 19.84 23.31 24.53 24.92</cell></row><row><cell>acc. ratio</cell><cell>0.50 0.25</cell><cell></cell><cell>7.12 6.15</cell><cell>7.08 5.40</cell><cell cols="6">8.59 11.34 13.53 14.40 14.79 5.62 6.79 8.00 8.53 8.83</cell><cell>10 15 20 FID (vs. train)</cell><cell>acc. ratio</cell><cell>0.50 0.25</cell><cell></cell><cell>7.25 7.13</cell><cell>6.73 5.98</cell><cell>7.63 5.64</cell><cell cols="5">9.85 11.74 12.51 12.83 6.37 7.29 7.72 7.96</cell></row><row><cell></cell><cell>0.05</cell><cell></cell><cell>6.71</cell><cell>5.48</cell><cell>4.65</cell><cell>4.45</cell><cell>4.60</cell><cell></cell><cell>4.65</cell><cell>4.66</cell><cell></cell><cell></cell><cell>0.05</cell><cell></cell><cell>8.90</cell><cell>7.38</cell><cell>6.17</cell><cell>5.61</cell><cell>5.52</cell><cell></cell><cell>5.48</cell><cell>5.49</cell></row><row><cell></cell><cell></cell><cell></cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048 top-k</cell><cell>4096</cell><cell></cell><cell cols="2">8192 16384</cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048 top-k</cell><cell>4096</cell><cell></cell><cell cols="2">8192 16384</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>17.80 16.48 15.57 14.59 14.03 13.20 17.02 15.60 14.69 13.80 13.04 12.68 12.69 15.26 14.32 13.41 12.81 12.33 12.59 13.31 14.58 13.59 12.86 12.45 12.65 13.26 15.32 13.87 13.14 12.60 12.52 12.96 14.26 16.58 13.82 13.04 12.49 12.59 13.31 14.63 17.29</figDesc><table><row><cell>256</cell><cell>19.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>512</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1024 2048 top-k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4096</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8192</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell>0.75</cell><cell>0.8</cell><cell>0.85 top-p</cell><cell>0.9</cell><cell>0.95</cell><cell>1.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 7. Additional examples of unconditional image generation by our model trained on LSUN-cat.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Additive quantization for extreme vector compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="931" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03099</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">API design for machine learning software: experiences from the scikit-learn project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Buitinck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaques</forename><surname>Grobler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Layton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ga?l</forename><surname>Varoquaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD Workshop: Languages for Data Mining and Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="108" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Very deep vaes generalize autoregressive models and can outperform them on images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The computational complexity of probabilistic inference using bayesian belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="393" to="405" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Jukebox: A generative model for music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00341</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05233</idno>
		<title level="m">Diffusion models beat gans on image synthesis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Imagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Regularized residual quantization: a multilayer sparse dictionary learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohrab</forename><surname>Ferdowsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slava</forename><surname>Voloshynovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimche</forename><surname>Kostadinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00522</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Magazine</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="29" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scaling laws for autoregressive generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14701</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11239</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multiple stage vector quantization for speech coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biing-Hwang Juang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP&apos;82. IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Trq: Ternary neural networks with residual quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenrui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunlei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8538" to="8546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Performance guaranteed network acceleration via high-order residual quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zefan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2584" to="2592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Stacked quantizers for compositional vector compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2173</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generating images with sparse representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>139 of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4797" to="4805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pixelcnn++: A pixelcnn implementation with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Coding theorems for a discrete source with a fidelity criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Nat. Conv. Rec</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
