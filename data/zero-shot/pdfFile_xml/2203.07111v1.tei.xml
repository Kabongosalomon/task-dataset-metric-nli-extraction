<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disentangled Representation Learning for Text-Video Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhao</forename><surname>Zhang</surname></persName>
							<email>yanhao.zyh@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Pan Pan,</roleName><forename type="first">Yun</forename><surname>Zheng</surname></persName>
							<email>zhengyun.zy@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
							<email>xiansheng.hxs@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Disentangled Representation Learning for Text-Video Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Retrieval</term>
					<term>Cross-modality Interaction</term>
					<term>Decorrelation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-modality interaction is a critical component in Text-Video Retrieval (TVR), yet there has been little examination of how different influencing factors for computing interaction affect performance. This paper first studies the interaction paradigm in depth, where we find that its computation can be split into two terms, the interaction contents at different granularity and the matching function to distinguish pairs with the same semantics. We also observe that the singlevector representation and implicit intensive function substantially hinder the optimization. Based on these findings, we propose a disentangled framework to capture a sequential and hierarchical representation. Firstly, considering the natural sequential structure in both text and video inputs, a Weighted Token-wise Interaction (WTI) module is performed to decouple the content and adaptively exploit the pair-wise correlations. This interaction can form a better disentangled manifold for sequential inputs. Secondly, we introduce a Channel DeCorrelation Regularization (CDCR) to minimize the redundancy between the components of the compared vectors, which facilitate learning a hierarchical representation. We demonstrate the effectiveness of the disentangled representation on various benchmarks, e.g., surpassing CLIP4Clip largely by +2.9%, +3.1%, +7.9%, +2.3%, +2.8% and +6.5% R@1 on the MSR-VTT, MSVD, VATEX, LSMDC, AcitivityNet, and DiDeMo, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text-Video Retrieval (TVR) has significant worth with the explosive growth of video content on the internet. Alignment between different modalities requires particular consideration of both intra-modal representation and cross-modality interaction. The inherent asymmetry across different modalities raises an expression challenge for Text-Video Retrieval. The pioneering works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">26]</ref> incorporate multi-modality features and aggregate information from different pre-trained experts to boost performance. While the open-ended text queries and diverse vision contents require tremendous labor.</p><p>With the great success of NLP pre-training <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref>, Vision-Language Pre-Training (VLPT) has received increasing attention <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b1">2]</ref>. Recently,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumption (1) : Local Context Assumption (2) :Semantic Hierarchy</head><p>Query #2: a man is riding a motor bike in beach.</p><p>Query #1: ad of the mrf tyres. Query #1: A woman falls off a bridge.</p><p>-CUcALZfNIA_000017_000027 VATEX f-24IxG9ijw_25_40 MSVD the Contrastive Language-Image Pre-training (CLIP <ref type="bibr" target="#b35">[35]</ref>) leverages a massive amount of image-caption pairs to learn generic vision and language representations, showing impressive improvements in diverse vision-language (VL) tasks. Researchers <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref> explore the power of this design principle to textvideo retrieval with additional temporal fusion module. CLIP4Clip <ref type="bibr" target="#b29">[29]</ref> utilizes a temporal transformer to aggregate sequential features into a single highdimensional representation, and retrievals by dot-product search.</p><p>However, the text-video retrieval is uniquely characterized by its sequential frames. Thus we highlight the common underlying assumptions behind TVR in <ref type="figure" target="#fig_0">Figure 1:</ref> (1) the description related with the local segment still needs to be retrieved, and (2) human-generated sentences naturally have a hierarchical structure and can describe in different views. Based on these perspectives, we can re-examine the existing algorithmic framework, especially the interaction components. <ref type="figure" target="#fig_1">Figure 2</ref> shows typical interaction archetypes and the proposed variations. In determining the process flow for the interaction block, only a few properties are commonly considered. One is the granularity of input content, and the other is the interaction function.</p><p>The single-vector representation is widely used in the fields of biometrics <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b46">46</ref>] and text retrieval <ref type="bibr" target="#b14">[15]</ref>, and its retrieval process is extremely concise and efficient. While as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (a), the over-abstract representation will introduce a lack of fine-grained matching capabilities. Therefore, the Mixtures of Experts (MoE) approaches <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">25]</ref> take advantage of individual domains to integrate generalizable aggregated features. MMT <ref type="bibr" target="#b11">[12]</ref> and CE <ref type="bibr" target="#b26">[26]</ref> explicitly construct a multi-expert fusion mechanism to boost retrieval performance. In addition, HIT <ref type="bibr" target="#b25">[25]</ref> performs hierarchical cross-modality contrastive matching at both feature-level and semantic-level. They can all be summed up as a hierarchical fusion architecture <ref type="figure" target="#fig_1">(Figure 2(b)</ref>).</p><p>In contrast to the above parameter-free dot-product function, the deeplycontextualized interaction has emerged that fine-tunes MLP models for estimating relevance <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b48">48]</ref>  <ref type="figure" target="#fig_1">(Figure 2(c)</ref>). The recent cross transformer interaction approaches <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b29">29]</ref> can inference the complex relationship between arbitrarylength text and video. However, the neural network interaction lacks the typical inductive bias for matching and usually suffers from optimization difficulty and performance degradation <ref type="bibr" target="#b29">[29]</ref>. Furthermore, these heavy interactions will bring a prohibitively computational cost for real-world deployments.</p><p>We present a disentangled framework to address the above challenges, where a novel token-wise interaction and channel decorrelation regularization collaboratively decouples the sequential and hierarchical representation. Concretely, we propose a lightweight token-wise interaction that fully interacts with all sentence tokens and video frame tokens <ref type="figure" target="#fig_1">(Figure 2</ref>(e-f)). Compared with the single-vector interaction <ref type="figure" target="#fig_1">(Figure 2</ref>(a)(c)) and multi-level interaction <ref type="figure" target="#fig_1">(Figure 2(b)</ref>), our method can preserve more fine-grained clues. Compared to the cross transformer interaction ( <ref type="figure" target="#fig_1">Figure 2(d)</ref>), the proposed interaction mechanism significantly ease the optimization difficulty and computational overhead. In addition to the interaction mechanism, we employ a Channel DeCorrelation Regularization (CDCR) to minimize the redundancy between the components of the compared vectors, which facilitate learning a hierarchical representation. The proposed modules are orthogonal to the existing pretraining techniques <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b1">2]</ref> and can be easily implemented by a few lines of code in modern libraries.</p><p>We validate the retrieval ability of our approach on multiple text-video retrieval datasets. Our experimental results outperform the state-of-the-art methods under widely used benchmarks, e.g., surpassing CLIP4Clip <ref type="bibr" target="#b29">[29]</ref> largely by +2.9%, +3.1%, +7.9%, +2.3%, +2.8% and +6.5% R@1 on the MSR-VTT <ref type="bibr" target="#b44">[44]</ref>, MSVD <ref type="bibr" target="#b43">[43]</ref>, VATEX <ref type="bibr" target="#b42">[42]</ref>, LSMDC <ref type="bibr" target="#b39">[39]</ref>, AcitivityNet <ref type="bibr" target="#b9">[10]</ref>, and DiDeMo <ref type="bibr" target="#b0">[1]</ref>, respectively. Notably, using ViT-B/16 <ref type="bibr" target="#b35">[35]</ref> and QB-Norm <ref type="bibr" target="#b2">[3]</ref> post processing, our best model can achieve 53.3% T2V R@1 and 56.2% V2T R@1, surpassing all existing single-model entries.</p><p>Our empirical analysis suggests that there is vast room for improvement in the design of interaction mechanisms. The findings used in this paper make some initial headway in this direction. We hope that this study will spur further investigation into the operational mechanisms used in modeling interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Feature Representation for Text-Video Retrieval. In recent studies <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b29">29]</ref>, the text and video inputs are usually considered separately for efficient deployment through a Bi-Encoder architecture <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b18">18]</ref>. The text encoder absorbs progress in the NLP field, and is upgraded from the early Word2Vec <ref type="bibr" target="#b30">[30]</ref> to the BERT-like models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">27]</ref>. While for the video input, due to its rich semantic content, researchers utilize multi-modality features <ref type="bibr" target="#b11">[12]</ref> and a variety of pre-trained experts <ref type="bibr" target="#b26">[26]</ref> to boost performance. Recently, CLIP <ref type="bibr" target="#b35">[35]</ref> proposes a concise contrastive learning method and trains on a large-scale dataset with 400 million pairs to obtain the generic cross-modal representation. Indeed, in the span of just a few months, several CLIP-based TVR methods <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>   from existing pre-training, while with mainly focusing on the design of the interaction module. We conduct extensive and fair ablation studies to evaluate the effectiveness of our approach. Interaction Mechanism for Text-Video Retrieval. Over the past few years, most TVR studies have focused on improving performance by increasing the power of text and visual encoders <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref>. Thus the simple dot-product interaction is widely adopted for computing global similarity <ref type="bibr" target="#b29">[29]</ref>. The time-series video features are aggregated with average pooling, LSTM <ref type="bibr" target="#b13">[14]</ref>, or temporal transformer <ref type="bibr" target="#b29">[29]</ref>. Recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">26]</ref> on the Mixture of Experts (MoE <ref type="bibr" target="#b15">[16]</ref>) show that integrating diverse domain experts can improve the overall model's capability with a voting procedure. In <ref type="bibr" target="#b25">[25]</ref>, hierarchical feature alignment methods help models utilize information from different dimensions. The pioneering work, JSFusion <ref type="bibr" target="#b48">[48]</ref>, proposes to measure dense semantic similarity between all sequence data and learn a sophisticated attentions network. The Cross Transformer <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29]</ref> establish the multi-modality correspondence by joint encoding texts and videos, which can capture both intra-and inter-modality context. We conduct the empirical study on the latest instantiation of interaction. Our work is also related to several approaches that analyze the interaction mechanism on Information Retrieval (IR) <ref type="bibr" target="#b18">[18]</ref> and Text-Image Retrieval <ref type="bibr" target="#b19">[19]</ref>. This work targets a deeper understanding of the interaction mechanism for Text-Video Retrieval in a new perspective. Contrastive Learning for Text-Video Retrieval. A common underlying theme that unites retrieval methods is that they aim to learn closer representations for the labelled text-video pairs. The triplet loss based models <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b26">26]</ref> use a max-margin approach to separate positive from negative examples and perform intra-batch comparisons. With the popularity of self-supervised learning (SSL) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b31">31]</ref>, InfoNCE <ref type="bibr" target="#b31">[31]</ref> have dominated recent retrieval tasks, which mainly maximizes the diagonal similarity and achieves superior performance for noisy data. The Barlow Twins method <ref type="bibr" target="#b49">[49]</ref> have recently been proposed as a new solution for SSL. They use a covariance matrix to indicate inter-channel redundancy. We employ a sequential channel decorrelation regularization to reduce inter-channel redundancy and competition, which fits the assumptions of hierarchical semantics in Text-Video Retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The framework of the proposed Text-Video Retrieval (TVR) is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Our method first generates sequential feature representation for both text and video. In the cross-modality interaction stage, the network will reason the dense correlation for all pair-wise features and dynamically adjust weights based on the token contents. We additionally introduce a channel decorrelation regularization to minimize the redundancy of these features. Consequently, given a text query t = {t i } Nt i=1 with N t tokens, and a set of video documents V = {v n } N n=1 , the text-video retrieval is formulated as a cross-modality similarity measurement, S(t, v). We can extract text features e t and video features e v through text encoder E t (t, ? t ) and video encoder E v (v, ? v ), respectively. Different from the global dot-product operation with single vector, our embedding features retain the sequential structure of text and video frames with e t ? R Nt?D , e v ? R Nv?D , where N v is the sampled video frames and D is the feature dimension. We further introduce a Weighted Token-wise Interaction (WTI) module to estimate the cross-modality correlation and discover potential activate tokens. In order to improve the domain generalization, we leverage the covariance matrix to indicate the redundancy of features and employ a simple Channel Decorrelation Regularization (CDCR) to improve the performance. The decorrelation loss implicitly alleviates an overlooked channel competition, achieving impressive performance in retrieval more complex sentences. <ref type="table">Table 1</ref>. Comparisons of different interaction mechanisms. N denotes number of video documents (N is large and depends on applications); D denotes representation dimension (D = 512, by default); Nt and Nv denote length of text token and frame token, Nv+t is the sum of Nt and Nv (Nt = 32, Nv = 12); L denotes number of network layer and S denotes feature levels (L = 4, S = 3, by default).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction Contents Function Computational Complexity</head><formula xml:id="formula_0">Memory DP single parameter-free O(N D) O(N D) HI multi-level light-parameter O(N SD) O(N SD) MLP single black-box O(N (D 2 L + D)) O(N D) XTI token-wise black-box O(N (D 2 (Nt+v) + N 2 t+v D)L) O(N NvD) TI token-wise parameter-free O(N NtNvD) O(N NvD) WTI token-wise light-parameter O(N (NtNvD + Nt+v)) O(N Nv(D + 1))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Extractor</head><p>We utilize the efficient Bi-Encoder architecture for feature extraction. For each query sentence t, we add the [CLS] and [SEP] token to indicate the start and the end of the sentence, and adopt the pretrained BERT-BASE <ref type="bibr" target="#b7">[8]</ref> to encode the text representation e t = E t (t). For each video v, we uniformly select N v frames as keyframes and employ off-the-shelf transformer-based networks, e.g. ViT <ref type="bibr" target="#b8">[9]</ref> , to extract sequential features e v = E v (v). Retrieval across modalities benefits from large-scale pre-training tasks <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b35">35]</ref>. In this paper, we mainly focus on the design of the interaction module rather than the pretrained network. Thus our feature extractors are initialized from the CLIP <ref type="bibr" target="#b35">[35]</ref>, and we finetune in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Study of Interaction Mechanisms</head><p>To facilitate our study, we develop a generalized interaction formulation that is able to represent various module designs and show a comparison in <ref type="table">Table 1</ref>. We then show how the existing interaction mechanisms can be represented within this formulation, and how ablations can be conducted using this formulation with respect to different interaction module elements. Single Vector Dot-product Interaction: When measuring feature similarity, researchers use simple dot-product operation, which is an intuitive solution. Specifically, two global representations are compared in the ? 2 normalized embedding space:</p><formula xml:id="formula_1">DP(e t , e v ) = (e [CLS] t ) ? ?? v ?e [CLS] s ? 2 ? ?? v ? 2 ,<label>(1)</label></formula><p>where e</p><formula xml:id="formula_2">[CLS] t is the text feature for [CLS] token and? v = 1 Nv N V i=1 e i v</formula><p>is the average video representation. The dot-product method is widely used in biometrics <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b46">46]</ref>, text-image retrieval <ref type="bibr" target="#b47">[47]</ref>, and text document retrieval <ref type="bibr" target="#b14">[15]</ref>. This interaction method is extremely efficient and can be accelerated by the ANN library, e.g., FAISS <ref type="bibr" target="#b17">[17]</ref>. While, due to the single-vector representation, the sequential structure is heavily coupled in encoder. Compressing the long sequence into a single vector will push the network to learn extremely complex abstractions and may easily miss out fine-grained clues. The dot-product function also encourages smooth latent spaces, which will discourage the hierarchical semantics learning. Hierarchical Interaction: Similar to the global feature representation, multilayer <ref type="bibr" target="#b25">[25]</ref>, multi-modal <ref type="bibr" target="#b11">[12]</ref> and multi-experts <ref type="bibr" target="#b26">[26]</ref> use the gated fusion mechanism to ensemble the hierarchical representation:</p><formula xml:id="formula_3">HI(e t , e v ) = S s=1 w(e [CLS],s t,s ) ?? v,s ,<label>(2)</label></formula><p>where w s can be a normalized weight or a binarized gated unit. We note that this hierarchical interaction actually contains two essential factors: one is the retrieval-oriented feature pool, and the other is the dynamic fusion block. The content of the comparison has been greatly enriched from expert models. While the learning of the experts requires labor intensive manual annotation. MLP on Global Vector: In order to improve the nonlinear measurement ability of the interaction block, researchers <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b29">29]</ref> propose to use neural network to learn the metric, among which the representative work <ref type="bibr" target="#b41">[41]</ref> builds a Multilayer Perceptron (MLP) measurement. They directly concatenate the compared features as a whole, and optimize the similarity through MLP:</p><formula xml:id="formula_4">MLP(e t , e v ) = f ? ([e [CLS] t ,? v ]).<label>(3)</label></formula><p>where [, ] denotes concatenation operation. Although the neural network brings a nonlinear metric space, it also operates in a black-box setting. The pureparameterizative method needs to consume massive amount of labelled data in the training process. And when deployed on large-scale (billions) documents, even two fully connected layers will bring prohibitively computational overhead. Cross Transformer Interaction: The standard self-attention [40] is adopted for cross-modality matching, which can handle variable-length inputs:</p><formula xml:id="formula_5">XTI(e t , e v ) = MHA ? ([e t , e v ]).<label>(4)</label></formula><p>The Cross Transformer aims to capture and model the inter-modal relations between texts and videos by exchanging key-value pairs in the multi-headed attention (MHA) mechanism. In Section 4.2, we observe that the cross transformer interaction is difficult to optimize and usually occurs performance degradation on video retrieval datasets. Furthermore, compared with the dot-product operation, multi-head attention brings thousand times of the computational overhead, as shown in <ref type="table">Table 1 and Table 2</ref>. Token-wise Interaction: Recently, ColBERT <ref type="bibr" target="#b18">[18]</ref> and FILIP <ref type="bibr" target="#b45">[45]</ref> propose token-wise interaction for document and image retrieval. We introduce this token-wise interaction to TVR, which cleverly solves the local context matching problem (Assumption (1) in <ref type="figure" target="#fig_0">Figure 1</ref>):</p><formula xml:id="formula_6">TI(e t , e v ) = ? ? Nt i=1 Nv max j=1 (? i t ) ??j v + Nv j=1 Nt max i=1 (? i t ) ??j v ? ? /2,<label>(5)</label></formula><p>Algorithm 1 PyTorch-style pseudocode for Weighted Token-wise Interaction. where? = e i /?e i ? 2 is the channel-wise normalization operation. The tokenwise interaction is a parameter-free operation and allows efficient storing and indexing <ref type="bibr" target="#b18">[18]</ref>. Weighted Token-wise Interaction: Intuitively, not all words and video frames contribute equally. We provide an adaptive approach to adjust the weight magnitude for each token:</p><formula xml:id="formula_7">WTI(e t , e v ) = ? ? Nt i=1 f i tw,? (e t ) Nv max j=1 (? i t ) ? e j v + Nv j=1 f j vw,? (e v ) Nt max i=1 (? i t ) ??j v ? ? /2,<label>(6)</label></formula><p>where f tw,? and f vw,? are composed of classic MLP and a SoftMax function. The adaptive block is lightweight and takes a single-modality input, allowing offline pre-computation for large-scale video documents. In the online process, the attention module introduces negligible computational overhead, as shown in <ref type="table">Table 2</ref>. Our method inherits the general matching priors and is empirically verified to be more effective and efficient.</p><p>WTI can be easily implemented by a few lines of code in modern libraries. Algorithm 1 shows a simplified code based on PyTorch <ref type="bibr" target="#b32">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Channel Decorrelation Regularization</head><p>Given a batch of B video-text pairs, WTI generates an B ? B similarity matrix. The Text-Video Retrieval is trained in a supervised way. We employ the InfoNCE loss <ref type="bibr" target="#b31">[31]</ref> to maximize the similarity between labelled video-text pairs and minimize the similarity for other pairs:</p><formula xml:id="formula_8">L InfoNCE = L v2t + L t2v L v2t = ? 1 B B i log exp (WTI (e t,i , e v,i ) /? ) B j exp (WTI (e t,i , e v,j ) /? ) L t2v = ? 1 B B i log exp (WTI (e t,i , e v,i ) /? ) B j exp (WTI (e t,j , e v,i ) /? ) ,<label>(7)</label></formula><p>where ? is temperature hyper-parameter.</p><p>The contrastive loss provides a macro objective to optimize the global similarity. Referring expression comprehension, the multi-modality retrieval often requires semantic information from micro-views, e.g., the channel-level. Inspired from self-supervised learning methods <ref type="bibr" target="#b49">[49]</ref>, we utilize the covariance matrix to measure the redundancy between features and employ a simple ? 2 -norm minimization to optimize the hierarchical representation:</p><formula xml:id="formula_9">L CDCR = i 1 ? C ii 2 + ? i j? =i (C ij ) 2 C ij ? b e (i) t,b e (j) v,b b e (i) t,b 2 b e (j) v,b 2 ,<label>(8)</label></formula><p>where e</p><formula xml:id="formula_10">(i) t,b is the i-th channel of b-th text feature e t,b</formula><p>. The coefficient ? controls the magnitude of the redundancy term. The total training loss L all is defined as:</p><formula xml:id="formula_11">L all = L InfoNCE + ?L CDCR ,<label>(9)</label></formula><p>where ? is the weighting parameter. Surprisingly, empirical results show that the cross-modality channel decorrelation regularization brings significant improvements for all interaction mechanisms, as shown in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we firstly present the configuration details of our algorithm and then conduct thorough ablation experiments to explore the relative importance of each component in our method on MSR-VTT <ref type="bibr" target="#b44">[44]</ref>. Finally we conduct comprehensive experiments on six benchmarks: MSR-VTT <ref type="bibr" target="#b44">[44]</ref>, MSVD <ref type="bibr" target="#b43">[43]</ref>, VA-TEX <ref type="bibr" target="#b42">[42]</ref>, LSMDC <ref type="bibr" target="#b39">[39]</ref>, ActivityNet <ref type="bibr" target="#b9">[10]</ref> and DiDeMo <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Dataset:</p><p>We conduct experiments on six benchmarks for video-text retrieval tasks including:</p><p>-MSR-VTT <ref type="bibr" target="#b44">[44]</ref>  , and the text tokens are concatenated as inputs to a 12-layer linguistic transformer <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">35]</ref>. We leverage the MLP to capture the weight factor for tokens, and our ablation studies suggest a 2-layer structure. The adaptive module is initialized from scratch with random weights.</p><p>Training Schedule: For fair comparisons with our baseline, we follow training schedules from CLIP4Clip <ref type="bibr" target="#b29">[29]</ref>. The network is optimized by Adam <ref type="bibr" target="#b20">[20]</ref> with a batch size of 128 in 5 epochs. The initial learning rate for vision encoder and text encoder are set 1 ? 10 ?7 , and the initial learning rate for the temporal transformer and the adaptive module are set to 1 ? 10 ?4 . All learning rates follow the cosine learning rate schedule with a linear warmup. We apply a weight decay regularization to all parameters except for bias, layer normalization, token embedding, positional embedding and temperature. During training, we set the temperature ? = 100, CDCR weight ? = 0.06 and overall weighting parameter ? = 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>We quantitatively evaluate the key components, Weighted Token-wise Interaction (WTI) and Channel DeCorrelation Regularization (CDCR), on MSR-VTT 1K <ref type="bibr" target="#b44">[44]</ref>. <ref type="table">Table 2</ref> summarizes the ablation results. Effect of Weighted Token-wise Interaction: Compared with the Single-Vector Dot-Product Interaction (DP), the Token-wise Interaction (TI) only adds a dense correlation, while the performance boosts +2.0% R@1. Our Weighted Token-wise Interaction (WTI) dramatically improves the performance by an R@1 of 3.5%, which shows that the proposed interaction structure indeed helps the model to adequately exploit pair-wise correlations. Compared with other light-parameter function, our method improves by +2.8% R@1 over Hierarchical Interaction (HI). An explanation is that the token representation is more valuable than the multi-level features for the single-scale ViT <ref type="bibr" target="#b8">[9]</ref>. As observed in <ref type="bibr" target="#b29">[29]</ref>, the MLP interaction (MLP) and Cross Transformer Interaction (XTI), under black-box function, would lead to degradation problems. Further, by adopting ViT-B/16 <ref type="bibr" target="#b35">[35]</ref>, WTI greatly promotes the T2V R@1 to 48.8%. Effect of Channel DeCorrelation Regularization: In <ref type="table">Table 2</ref>, we evaluate the importance of Channel Decorrelation Regularization on different interaction structures except block-box function. Surprisingly, CDCR increases performance significantly for all interactions, i.e. by +1.4%, +0.6%, +0.7% and +1.1% for DP, HI, TI and WTI respectively. The micro channel-level regularization makes it easier to leverage semantic hierarchy. Effect of Dual-path Token-wise Retrieval: In <ref type="table">Table 3</ref>, we test the importance of dual-path interaction of t2v and v2t, subset of Eq. 5. By adopting dual-path to compute the logits, our model provides stable gains by +1.4% and +1.0% for TI and WTI, respectively.  Effect of MLP layers: By default, we simply set the weighted function with MLP, which is composed of FC+ReLU blocks. In <ref type="table">Table 3</ref>, we observe that R@1 is slightly better when applying 2FC layers and drop by 3FC, likely because a heavier structure suffers from over-fitting.</p><p>Hyper-parameter Selection for DeCorrelation: The parameter ? and ? specifies the scale and importance of the Channel DeCorrelation Loss. We evaluate the scale range setting ? ? [0, 0.1] and ? ? [0, 0.002] as shown in <ref type="figure">Figure 4</ref>. We find that R@1 is improved from 44.8% to 45.5% when ? = 0.06 and saturated with ? = 0.07. We can get the best R@1 when the ? is around 0.001. As a result, we adopt ? = 0.06 and ? = 0.001 in our model to achieve the best performance. Training Convergence Analysis: To further reveal the source of the performance improvement, we present the training loss in <ref type="figure">Figure 5</ref>. The training of pure-parameterizative interaction (Cross Transformer) is not trivial since it lacks the basic inductive biases for distance measurement. Inspired by this observation, we seek a light-weight way to leverage the heuristic property for better convergence, and therefore achieve a faster convergence rate. Benefit for More Frames: <ref type="figure">Figure 5</ref>(b) shows performance for DP and WTI with different frames. Compared with the na?ve baseline DP, our WTI can achieve more significant gains, +4.9% v.s. +2.8%, with more frames. For fair comparisons with CLIP4Clip <ref type="bibr" target="#b29">[29]</ref>, we mainly report results at 12 frames. Inference Speed: Besides recall metric, we report the inference speed of all the interaction structure in <ref type="table">Table 2</ref>. Our lightweight interaction introduces negligible computational overhead, bringing remarkable improvements. Our approach reduce the inference time by several orders of magnitude, compared with the heavy cross transformer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-Arts</head><p>In this subsection, we compare the proposed model with recent state-of-theart methods on the six benchmarks, MSR-VTT <ref type="bibr" target="#b44">[44]</ref>, MSVD <ref type="bibr" target="#b43">[43]</ref>, VATEX <ref type="bibr" target="#b42">[42]</ref>, LSMDC <ref type="bibr" target="#b39">[39]</ref>, ActivityNet <ref type="bibr" target="#b9">[10]</ref> and DiDeMo <ref type="bibr" target="#b0">[1]</ref>. For MSR-VTT in <ref type="table" target="#tab_4">Table 4</ref>, our model significantly surpasses the Clip-BERT <ref type="bibr" target="#b21">[21]</ref> by absolute 25.5% R@1, reaching 47.4% R@1 on MSR-VTT, indicating the benefits and necessity of large scale image-text pre-training for videotext retrieval. We achieve 2.9% improvement compared to CLIP4Clip <ref type="bibr" target="#b29">[29]</ref>, which shows the benefit from token-wise interaction. Our WTI, employing ViT-B/16 and QB-Norm <ref type="bibr" target="#b2">[3]</ref>, yields a remarkable T2V R@1 53.3%. <ref type="table">Table 5</ref> shows results for other benchmarks. For MSVD, our model also demonstrates competitive performance with top-performing CLIP based model <ref type="bibr" target="#b29">[29]</ref>. For VATEX, our approach achieves +7.6% R@1 improvement for text-video retrieval and +3.8% improvement for video-text retrieval. For LSMDC, our approach achieves +2.3% R@1 improvements for text-video retrieval.</p><p>For ActivityNet, we outperform the state-of-the-art method by a large margin of +2.8% R@1 on text-video retrieval. For DiDeMo, we achieve remarkable performance 47.9% R@1 and a relative performance improvement of 16.5% in T2V compared to CLIP4Clip. It is worth noting that long sentences are used in ActivityNet and DiDeMo. The significant improvements on both datasets further proves the advantages of our disentangled representation.</p><p>Overall, the consistent improvements across different benchmarks strongly demonstrate the effectiveness of our algorithm. We hope that our studies will spur further investigation in modeling interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we present an empirical study to give a better general understanding of interaction mechanisms for Text-Video Retrieval. Then we propose a novel <ref type="table">Table 5</ref>. Retrieval results on the validation set of MSVD <ref type="bibr" target="#b43">[43]</ref>, VATEX <ref type="bibr" target="#b42">[42]</ref>, LSMDC <ref type="bibr" target="#b39">[39]</ref>, ActivityNet <ref type="bibr" target="#b9">[10]</ref> and DiDeMo <ref type="bibr" target="#b0">[1]</ref>. disentangled representation method, which are collaboratively implemented by a Weighted Token-wise Interaction (WTI) to solve sequential matching problem from macro-view and a Channel DeCorrelation Regularization (CDCR) that reduces feature redundancy from a micro-view. We demonstrate the effectiveness of the proposed approach for modeling better sequential and hierarchical clues on six datasets. We wish our cross-modality interaction will inspire more theoretical researches towards more powerful interaction design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This document brings additional details of Weighted Token-wise Interaction. Additional qualitative and quantitative results are also given for completeness. <ref type="figure" target="#fig_6">Figure 6</ref> shows the internal operating mechanism of WTI, and also powerfully explains the performance improvement of our algorithm.</p><p>[CLS]  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>How Weighted Token-wise Interaction (WTI) improve performance? In this section, we discuss the reasons for considering token-wise interaction and the adaptive weight module. First, the text and video input are naturally sequential, and keeping the sequential structure can reduce the information loss and preserve more fine-grained cues. Using the sequential representation for interaction is essentially an ensemble learning method. The Max operation in our token-wise interaction can select matching video clips, as shown in <ref type="figure" target="#fig_6">Fig. 6</ref>, which can help us resolve local context assumptions in the main paper. Otherwise, if using average pooling <ref type="bibr" target="#b29">[29]</ref>, the overall similarity will be pulled down by irrelevant segments. Quantitatively, we analyze the relationship between recall improvement and the number of scenes in a video <ref type="figure">(Fig. 7)</ref>, and find that our algorithm achieves most improvements at 3 scenes. Therefore, token-wise interaction is intuitively a sensible practice.</p><p>Second, learning of adaptive weights is necessary, especially when CLIP <ref type="bibr" target="#b35">[35]</ref> is used as a pre-trained network. CLIP performs large-scale training (on 256 GPUs for 2 weeks) with large-scale dataset (400 millions), and we cannot completely change the distribution of position embedding and features by a smallscale fine-tuning dataset. The default global token, [SEP], shows a dominant role, achieving top1 important token for 98.7% videos and an average weight of 0.2838, far exceeding the average weight of other tokens (0.0674) on MSR-VTT 1K validation set. Simply averaging all sequential features is unfair to [SEP], resulting in a suboptimal solution. On the other hand, the importance of words in the text are also quite different, and our experimental analysis shows that nouns/verbs account for the main weight, as shown in <ref type="figure">Fig. 8</ref>.</p><p>Therefore, a good practice is to consider the token-wise interaction and adaptive weight module and devising a solution to effectively fuse these two factors (as WTI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">More Implementation Details</head><p>In this section, we present more implementation details that were omitted in the main paper for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Pseudocode with Mask Inputs</head><p>Please note we omit the mask inputs at the main paper for brevity in Section 3.2.</p><p>Here we present complete pseudo-codes for our Weighted Token-wise Interaction (WTI) and Channel Decorrelation Regularization (CDCR) in Algorithm 2-4. We believe the pseudocode would aid an independent researcher to better replicate the proposed interaction. <ref type="figure" target="#fig_6">Figure 6</ref> shows details of the network architectures for learning weights. We use the single-modal input for each branch to avoid coupling between modalities. nn.Linear(512, 1) nn.Linear(512, 1) nn.Linear(512, 1)  <ref type="figure">Fig. 9</ref>. Network architectures for weight branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Network Architecture for Weight Branch</head><formula xml:id="formula_12">softmax !,# ? ? N_tx512 $,# ? ? N_vx512 !" ( !,$ ) ? ? N_tx1 %" ( %,$ ) ? ? N_tx1</formula><formula xml:id="formula_13">softmax !,# ? ? N_tx512 $,# ? ? N_vx512 !" ( !,$ ) ? ? N_tx1 %" ( %,$ ) ? ?</formula><formula xml:id="formula_14">softmax !,# ? ? N_tx512 $,# ? ? N_vx512 !" ( !,$ ) ? ? N_tx1 %" ( %,$ ) ? ?</formula><p>Algorithm 2 PyTorch-style pseudocode for Weighted Token-wise Interaction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Complexity Analysis</head><p>Single Vector Dot-product Interaction: For each video, we only need to store a D-dimensional feature, and the the overall feature storage complexity is O(N D). During the query process, a similarity calculation consists of D multiplication operations. The overall computational complexity is O(N D) . Hierarchical Interaction: For the hierarchical interaction, we consider the simplest case, directly using the constant weighted average to fuse the multi-layer features. Therefore, both computation and storage are increased by a factor of S compared to DP. MLP on Global Vector: We adopt the structure of stacking L? FC+ReLU blocks, where the input channel of the first layer is 2D, the hidden channels size is D, and the final output is a single similarity. The computational complexity is O(N (2DD + DD(L ? 2) + D)) = O(N (LD 2 + D)). We can observe that with the default configuration (D = 512, L = 4), the computational complexity of MLP Algorithm 3 PyTorch-style pseudocode for Channel DeCorrelation Regularization with single-vector representation.  <ref type="table">Table 6</ref>. Comparisons of different interaction mechanisms. N denotes number of video documents (N is large and depends on applications); D denotes representation dimension (D = 512, by default); Nt and Nv denote length of text token and frame token, Nv+t is the sum of Nt and Nv (Nt = 32, Nv = 12); L denotes number of network layer and S denotes feature levels (L = 4, S = 3, by default). is about 2,049? of the DP, which is why neural network-based interactions are difficult to deploy. They are usually designed to be used as a fine-grained ranking module. The storage complexity of MLP is same as DP. query sentence, we need to calculate an additional MLP to generate the text weights, which brings O(D 2 LN s ). For video documents, we can pre-compute sequential video features and the corresponding video weights. The total storage overhead is O(N (N v D + N v )). Note that during the online process, there is no need to dynamically calculate the weights for each video. Compared with TI, only two additional weighting operations are added in the online process. Therefore, the overall computational complexity is O(N (N t N v D + N t+v )). Compared to XTI, our computational complexity is reduced 63.69 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction Contents Function Computational Complexity</head><formula xml:id="formula_15">Memory DP single parameter-free O(N D) O(N D) HI multi-level light-parameter O(N SD) O(N SD) MLP single black-box O(N (D 2 L + D)) O(N D)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Visualizations</head><p>More visualization results of WTI are shown in <ref type="figure" target="#fig_0">Fig. 10 and 11</ref>.  <ref type="figure" target="#fig_0">Fig. 11</ref>. Qualitative results on images from MSR-VTT <ref type="bibr" target="#b44">[44]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustrations of common underlying assumptions for Text-Video Retrieval. (1) The description could be relevant to the local video context. (2) Multiple distinct sentences can hint at the same video. Our disentangled representation addresses both assumptions through token-wise interaction and channel decorrelation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Illustrations of the text-video retrieval architecture and six categories of interaction methods. Each subplot shows a typical interaction module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Weighted Token-wise Interaction(macro-view)    Channel Decorrelation Regularization (micro-view) Overview of the proposed disentangled representation learning design for Text-Video Retrieval. The pipeline of our method consists of two components: the Weighted Token-wise Interaction (WTI) block that exhaustively matching all sequential representation for the sentence and video; the Channel DeCorrelation Regularization (CDCR) minimizes the redundancy for the sequential representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc># t : text input v : video input # f_t : text encoder network f_v : video encoder network # f_tw : text weight network f_vw : video weight network # B : batch size D : dime nsional ity of the embeddings # N_t : text token size N_v : frame token size def w e i g h t e d _ t o k e n _ w i s e _ i n t e r a c t i o n (t , v ): # compute embeddings e_t = f_t ( t ) # BxN_txD e_v = f_v ( v ) # BxN_vxD # generate fusion weights text_weight = torch . softmax ( f_tw ( e_t ) , dim = -1) # BxN_t video_weight = torch . softmax ( f_tv ( e_v ) , dim = -1) # BxN_v # normalize representa tion e_t = e_t / e_t . norm ( dim = -1 , keepdim = True ) # BxN_txD e_v = e_v / e_v . norm ( dim = -1 , keepdim = True ) # BxN_vxD # token interaction logits = torch . einsum ( " atc , bvc -&gt; abtv " , [ e_t , e_v ]) # BxBxN_txN_v t2v_logits = logits . max ( dim = -1)[0] # BxBxN_txN_v -&gt; BxBxN_t t2v_logits = torch . einsum ( " abt , at -&gt; ab " , [ t2v_logits , text_weight ]) # BxBxN_t -&gt; BxB v2t_logits = logits . max ( dim = -2)[0] # BxBxN_v v2t_logits = torch . einsum ( " abv , bv -&gt; ab " , [ v2t_logits , video_weight ]) # BxBxN_v -&gt; BxB r e trieval_logits = ( t2v_logits + v2t_logits ) / 2.0 # BxB return retrieval_l o g i t s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Hyper-parameter for feature decorrelation. (a) ? for LCDCR (b) ? for L all . The training loss comparisons (a) and the influence of frame number (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Visualization of the internal operating mechanism of WTI on MSR-VTT<ref type="bibr" target="#b44">[44]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>The improvement with different scenes number on the MSR-VTT 1K val<ref type="bibr" target="#b44">[44]</ref>. The word cloud of top-1 weighted ordinary tokens on the MSR-VTT 1K val<ref type="bibr" target="#b44">[44]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc># t : text input v : video input # mask_t : text mask mask_v : video mask # f_t : text encoder network f_v : video encoder network # f_tw : text weight network f_vw : video weight network # B : batch size D : dime nsional ity of the embeddings # N_t : text token size N_v : frame token sizedef w e i g h t e d _ t o k e n _ w i s e _ i n t e r a c t i o n (t , mask_t , v , mask_v ): # compute embeddings e_t = f_t ( t ) # BxN_txD e_v = f_v ( v ) # BxN_vxD # generate fusion weights text_weight = f_tw ( e_t ). squeeze () # BxN_t # fill masked text with -inf text_weight . masked_fill_ (1 -mask_t , float ( " -inf " )) # BxN_v text_weight = torch . softmax ( text_weight , dim = -1) # BxN_t video_weight = f_tv ( e_v ). squeeze () # BxN_v # fill masked video with -inf vision_weight . masked_fill_ (1 -mask_v , float ( " -inf " )) # BxN_v video_weight = torch . softmax ( vision_weight , dim = -1) # BxN_v # normalize representa tion e_t = e_t / e_t . norm ( dim = -1 , keepdim = True ) # BxN_txD e_v = e_v / e_v . norm ( dim = -1 , keepdim = True ) # BxN_vxD # token interaction logits = torch . einsum ( " atc , bvc -&gt; abtv " , [ e_t , e_v ]) # BxBxN_txN_v # mask for logits logits = torch . einsum ( ' abtv , at -&gt; abtv ' , [ logits , mask_t ]) logits = torch . einsum ( ' abtv , bv -&gt; abtv ' , [ logits , mask_v ]) t2v_logits , t2v_max_idx = logits . max ( dim = -1) # BxBxN_txN_v -&gt; BxBxN_t t2v_logits = torch . einsum ( " abt , at -&gt; ab " , [ t2v_logits , text_weight ]) # BxBxN_t -&gt; BxB v2t_logits , v2t_max_idx = logits . max ( dim = -2) # BxBxN_txN_v -&gt; BxBxN_v v2t_logits = torch . einsum ( " abv , bv -&gt; ab " , [ v2t_logits , video_weight ]) # BxBxN_v -&gt; BxB r e trieval_logits = ( t2v_logits + v2t_logits ) / 2.0 # BxB return retrieval_l o g i t s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>#</head><label></label><figDesc>e_t : text feature e_v : video feature # B : batch size D : dime nsional ity of the embeddings # alpha : the magnitude of the redundancy term def c h a n n e l _ d e c o r r e l a t i o n _ r e g u l a r i z a t i o n _ s i n g l e ( e_t , e_v ): # batch norm t_norm = ( e_t -e_t . mean (0)) / e_t . std (0) # NxD v_norm = ( e_v -e_v . mean (0)) / e_v . std (0) # NxD B , D = t_norm . shape cov = torch . einsum ( 'ac , ad -&gt; cd ' , t_norm , v_norm ) / B # DxD # loss on_diag = torch . diagonal ( cov ). add_ ( -1). pow_ (2). sum () off_diag = cov . flatten ()[1:]. view ( D -1 , D + 1)[: , : -1]. pow_ (2). sum () cdcr = on_diag + off_diag * alpha return cdcr</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>2</head><label>2</label><figDesc>Nt+v + N 2 t+v D)L) O(N NvD) TI token-wise parameter-free O(N NtNvD) O(N NvD) WTI token-wise light-parameter O(N (NtNvD + Nt+v)) O(N Nv(D + 1))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Cross Transformer Interaction: It is usually assumed that a Multi-Head Attention block (MHA) consists of linear layer and QKV structure. The length of the sequence input is N t+v , so the computational complexity of the linear layer is O(D 2 N t+v ). An dot-product version of the QKV operation requires O(N 2 t+v D) complexity. Therefore, the overall computational complexity for L? MHA is O(N (D 2 N t+v + N 2 t+v D)L). With the default configuration (D = 512, L = 4, N t = 32, N v = 12, N t+v = 44), the computational complexity of XTI is about 24,464? of the DP. At the same time, due to the use of sequential representation for video, the storage complexity rises to O(N N v D). Token-wise Interaction: Token-wise interactions need to calculate an N t ? N v size of similarity matrix for text and video tokens. The computational complexity is O(N (N t N v D)) and the storage complexity is O(N N v D). Weighted Token-wise Interaction: For weighted token-wise interaction, the complexity for the online search process is a little more complicated. For the Algorithm 4 PyTorch-style pseudocode for Channel DeCorrelation Regularization with sequential representation.# e_t : text feature e_v : video feature # mask_t : text mask mask_v : video mask # B : batch size D : dime nsional ity of the embeddings # N_t : text token size N_v : frame token size def c h a n n e l _ d e c o r r e l a t i o n _ r e g u l a r i z a t i o n _ s e q u e n t i a l ( e_t , mask_t , e_v , mask_v ): # selecet max indexs for each text -video pair i4t = t2v_max_idx [ torch . arange ( B ) , torch . arange ( B )] # BxBxN_t -&gt; BxN_t i4v = v2t_max_idx [ torch . arange ( B ) , torch . arange ( B )] # BxBxN_v -&gt; BxN_v e_4_t = e_v [ torch . arange ( B ). r e p e a t _ i n t e r l e a v e ( N_t ) , i4t . flatten ()] # ( BxN_t ) xD e_4_v = e_t [ torch . arange ( B ). r e p e a t _ i n t e r l e a v e ( N_v ) , i4v . flatten ()] # ( BxN_v ) xD e_t = e_t . reshape ( -1 , D ) # ( BxN_t ) xD e_v = e_v . reshape ( -1 , D ) # ( BxN_v ) xD mask_t = mask_t . flatten (). type ( torch . bool ) mask_v = mask_v . flatten (). type ( torch . bool ) e_t = e_t [ mask_t ] e_v = e_v [ mask_v ] e_4_t = e_4_t [ mask_t ] e_4_v = e_4_v [ mask_v ] # cov for t2v t_norm = ( e_t -e_t . mean (0)) / e_t . std (0) # XxD v_norm = ( e_4_t -e_4_t . mean (0)) / e_4_t . std (0) # XxD X = t_norm . shape cov1 = torch . einsum ( 'ac , ad -&gt; cd ' , t_norm , v_norm ) / B # DxD # cov for v2t v_norm = ( e_t -e_t . mean (0)) / e_t . std (0) # XxD t_norm = ( e_4_v -e_4_v . mean (0)) / e_4_v . std (0) # XxD X = t_norm . shape [0] cov2 = torch . einsum ( 'ac , ad -&gt; cd ' , t_norm , v_norm ) / B # DxD cov = ( cov1 + cov2 )/2 # loss on_diag = torch . diagonal ( cov ). add_ ( -1). pow_ (2). sum () off_diag = cov . flatten ()[1:]. view ( D -1 , D + 1)[: , : -1]. pow_ (2). sum () cdcr = on_diag + off_diag * alpha return cdcr</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 10 .</head><label>10</label><figDesc>Qualitative results on images from MSR-VTT<ref type="bibr" target="#b44">[44]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>contains 10,000 videos with 20 captions for each. We report results on 1k-A which adopts 9,000 videos with all corresponding captions for training and utilizes 1,000 video-text pairs as test.-MSVD<ref type="bibr" target="#b43">[43]</ref> includes 1,970 videos with 80,000 captions. We report results on split set, where train, validation and test are 1200, 100 and 670 videos. -VATEX [42] contains 34,991 videos with multilingual annotations. The training split contains 25,991 videos. We report the results on the split set includes 1500 videos for validation and 1500 videos for test. -LSMDC [39] contains 118081 videos and equal captions extracted from 202 movies with a split of 109673, 7408, and 1000 as the train, validation, and test set. Every video is selected from movies ranging from 2 to 30 seconds. -ActivityNet [10] consists of 20,000 YouTube videos. We concatenate all descriptions of a video to a single query and evaluate on the 'val1' split. -DiDeMo [1] contains 10,000 videos annotated with 40,000 sentences. All sentence descriptions for a video are also concatenated into a single query for text-video retrieval. Concretely, the vision encoder is comprised of a vanilla ViT-B/32 [9] and 4-layers of temporal transformer blocks. Each block employs 8 heads and 512 hidden channels. The temporal position embedding and network weight parameters are initialized from the CLIP's text encoder. The fixed video length and caption lengths are 12 and 32 for MSR-VTT, MSVD, VATEX, LSMDC and 64 and 64 for ActivityNet and DiDeMo. Following [35], the special tokens, [CLS] and [SEP]</figDesc><table /><note>Evaluation Metric: We follow the standard retrieval task [29] and adopt Recall at rank K (R@K), median rank (MdR) and mean rank (MnR) as metrics. Higher R@K and lower MdR or MnR indicates better performance. Implementation Details: We utilize the standard Bi-Encoder from CLIP [35] as pre-trained feature extractor.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Ablation of Disentangled Representation on the 1K validation set of MSR-VTT [44]. Time shows inference speed for indexing 1 million video documents on a Tesla V100 GPU. WTI +ViT-B/16 48.8 76.1 84.3 13.5 50.2 76.5 84.7 12.4 565 Effect of dual-path and layers for weight model on MSR-VTT 1K [44]. WTI 45.4 72.3 81.7 13.4 45.3 74.6 83.3 13.6 46.3 73.7 83.2 13.0 Layers 1FC 2FC 3FC WTI 46.3 73.9 82.9 13.7 46.3 73.7 83.2 13.0 45.7 72.9 81.4 14.0</figDesc><table><row><cell>Interaction</cell><cell cols="6">InfoNCE [31] R@1? R@5? R@10? MnR? R@1? R@5? R@10? MnR? +CDCL</cell><cell>Time (ms)</cell></row><row><cell>DP</cell><cell>42.8 72.1 81.4</cell><cell cols="4">16.3 44.2 72.7 82.0</cell><cell cols="2">14.5 415</cell></row><row><cell>HI</cell><cell>43.5 72.9 81.7</cell><cell cols="4">16.1 44.1 72.6 82.8</cell><cell>14.1</cell><cell>531</cell></row><row><cell>MLP</cell><cell>29.3 54.8 64.2</cell><cell>33.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>25,304</cell></row><row><cell>XTI</cell><cell>41.8 71.2 82.7</cell><cell>16.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80,453</cell></row><row><cell>TI</cell><cell>44.8 73.7 82.9</cell><cell cols="4">13.5 45.5 72.0 82.5</cell><cell>13.3</cell><cell>536</cell></row><row><cell>WTI</cell><cell cols="7">46.3 73.7 83.2 13.0 47.4 74.6 83.8 12.8 565</cell></row><row><cell>DP +ViT-B/16</cell><cell>45.9 73.8 82.3</cell><cell cols="4">13.8 46.6 73.3 82.8</cell><cell cols="2">13.4 415</cell></row><row><cell>TI +ViT-B/16</cell><cell cols="5">47.3 76.7 84.8 13.7 49.1 75.7 85.1</cell><cell>12.7</cell><cell>536</cell></row><row><cell>Dual</cell><cell>t2v</cell><cell>v2t</cell><cell></cell><cell></cell><cell></cell><cell cols="2">t2v+v2t</cell></row></table><note>R@1? R@5? R@10? MnR? R@1? R@5? R@10? MnR? R@1? R@5? R@10? MnR? TI 43.4 71.0 80.2 16.1 43.5 71.8 82.7 14.0 44.8 73.7 82.9 13.5</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Retrieval results on the validation set of MSR-VTT 1K<ref type="bibr" target="#b44">[44]</ref>.</figDesc><table><row><cell>Method</cell><cell>Text? Video</cell><cell></cell><cell></cell><cell cols="2">Video ?Text</cell><cell></cell></row><row><cell></cell><cell cols="6">R@1? R@5? R@10? MdR? R@1? R@5? R@10? MdR?</cell></row><row><cell>HERO [23]</cell><cell>16.8 43.4 57.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UniVL [28]</cell><cell>21.2 49.6 63.1</cell><cell>6.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ClipBERT [21] 22.0 46.8 59.9</cell><cell>6.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">MDMMT [12] 26.6 57.1 69.6</cell><cell>4.0</cell><cell cols="3">27.0 57.5 69.7</cell><cell>3.7</cell></row><row><cell cols="2">SUPPORT [33] 27.4 56.3 67.7</cell><cell>3.0</cell><cell cols="3">26.6 55.1 67.5</cell><cell>3.0</cell></row><row><cell>FROZEN [2]</cell><cell>31.0 59.5 70.5</cell><cell>3.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">CLIP4Clip [29] 44.5 71.4 81.6</cell><cell cols="4">2.0 42.7 70.9 80.6</cell><cell>2.0</cell></row><row><cell>Ours</cell><cell>47.4 74.6 83.8</cell><cell cols="4">2.0 45.3 73.9 83.3</cell><cell>2.0</cell></row><row><cell>+ViT-B/16</cell><cell>50.2 76.5 84.7</cell><cell>1.0</cell><cell cols="3">48.9 76.3 85.4</cell><cell>2.0</cell></row><row><cell cols="2">+QB-Norm [3] 53.3 80.3 87.6</cell><cell>1.0</cell><cell cols="3">56.2 79.9 87.4</cell><cell>1.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5803" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1728" to="1738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Bogolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12777</idno>
		<title level="m">Cross modal retrieval with querybank normalisation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Teachtext: Crossmodal generalized distillation for text-video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Bogolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11583" to="11593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Activitynet: A largescale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11097</idno>
		<title level="m">Clip2video: Mastering video-text retrieval via image clip</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="214" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05610</idno>
		<title level="m">Clip2tv: An empirical study on transformer-based methods for video-text retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<publisher>Xian-Sheng Hua</publisher>
			<pubPlace>Pan Pan</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Colbert: Efficient and effective passage search via contextualized late interaction over bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7331" to="7341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Hero: Hierarchical encoder for video+ language omni-representation pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hit: Hierarchical transformer with momentum contrast for video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11915" to="11925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
		<title level="m">Use what you have: Video retrieval using representations from collaborative experts</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Univl: A unified video and language pre-training model for multimodal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<title level="m">Clip4clip: An empirical study of clip for end to end video clip retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<title level="m">Representation learning with contrastive predictive coding. arXiv e-prints pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1807</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02824</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A straightforward framework for video retrieval using clip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Portillo-Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ortiz-Bayliss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Terashima-Mar?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mexican Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<title level="m">Learning transferable visual models from natural language supervision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m">Sentence-bert: Sentence embeddings using siamese bertnetworks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A dataset for movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3202" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spm-tracker: Series-parallel matching for real-time visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3643" to="3652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Vatex: A large-scale, high-quality multilingual dataset for video-and-language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4581" to="4591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning for video classification and captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers of multimedia research</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07783</idno>
		<title level="m">Filip: Fine-grained interactive language-image pre-training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep learning for person re-identification: A survey and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12310" to="12320" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
