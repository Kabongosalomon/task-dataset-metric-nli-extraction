<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Global Entity Disambiguation with BERT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Megagon Labs ? NAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Washio</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Megagon Labs ? NAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Megagon Labs ? NAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Megagon Labs ? NAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Studio</forename><surname>Ousia</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Megagon Labs ? NAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Riken</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Megagon Labs ? NAIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Global Entity Disambiguation with BERT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a global entity disambiguation (ED) model based on BERT (Devlin et al.,  2019). To capture global contextual information for ED, our model treats not only words but also entities as input tokens, and solves the task by sequentially resolving mentions to their referent entities and using resolved entities as inputs at each step. We train the model using a large entityannotated corpus obtained from Wikipedia. We achieve new state-of-the-art results on five standard ED datasets: AIDA-CoNLL, MSNBC, AQUAINT, ACE2004, and WNED-WIKI. The source code and model checkpoint are available at https://github.com/ studio-ousia/luke.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entity disambiguation (ED) refers to the task of assigning mentions in a document to corresponding entities in a knowledge base (KB). This task is challenging because of the ambiguity between mentions (e.g., "World Cup") and the entities they refer to (e.g., FIFA World Cup or Rugby World Cup). ED models typically rely on local contextual information based on words that co-occur with the mention and global contextual information based on the entity-based coherence of the disambiguation decisions. A key to improve the performance of ED is to effectively combine both local and global contextual information <ref type="bibr">(Ganea and Hofmann, 2017;</ref><ref type="bibr">Le and Titov, 2018)</ref>.</p><p>In this study, we propose a global ED model based on <ref type="bibr">BERT (Devlin et al., 2019)</ref>. Our model treats words and entities in the document as input tokens, and is trained by predicting randomly masked entities in a large entity-annotated corpus obtained from Wikipedia. This training enables the model to learn how to disambiguate masked entities based on words and non-masked entities. * Work done at RIKEN. <ref type="bibr">[MASK]</ref> [MASK]</p><p>Lionel Messi <ref type="bibr">[CLS]</ref> played in the world cup Input: Messi played in the World Cup mess ##i <ref type="bibr">[SEP]</ref> Lionel Messi <ref type="bibr">[MASK]</ref> FIFA World Cup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Words Entities</head><p>Transformer Lionel Messi Result: <ref type="bibr">[CLS]</ref> played in the world cup mess ##i <ref type="bibr">[SEP]</ref> Figure 1: The inference procedure of our model with the input text "Messi played in the World Cup." Given mentions ("Messi" and "World Cup"), our model sequentially resolves them to their referent entities, and uses the resolved entities as contexts at each step.</p><p>At the inference time, our model disambiguates mentions sequentially using words and already resolved entities (see <ref type="figure">Figure 1</ref>). This sequential inference effectively accumulates the global contextual information and enhances the coherence of disambiguation decisions <ref type="bibr" target="#b11">(Yang et al., 2019)</ref>. We conducted extensive experiments using six standard ED datasets, i.e., AIDA-CoNLL, MSNBC, AQUAINT, ACE2004, WNED-WIKI, and WNED-CWEB. As a result, the global contextual information consistently improved the performance. Furthermore, we achieved new state of the art on all datasets except for WNED-CWEB. The source code and model checkpoint are available at https://github.com/ studio-ousia/luke.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Transformer-based ED. Several recent studies have proposed ED models based on Transformer <ref type="bibr" target="#b7">(Vaswani et al., 2017)</ref> trained with a large entity-annotated corpus obtained from Wikipedia <ref type="bibr" target="#b2">(Broscheit, 2019;</ref><ref type="bibr">Ling et al., 2020;</ref><ref type="bibr">F?vry et al., 2020;</ref><ref type="bibr" target="#b3">Cao et al., 2021;</ref><ref type="bibr" target="#b0">Barba et al., 2022)</ref>. <ref type="bibr" target="#b2">Broscheit (2019)</ref>  </p><formula xml:id="formula_0">A [CLS] C word D 0 A mess C word D 1 A ##i C word D 2 A played C word D 3 [CLS] mess ##i played A in C word D 4 in A the C word D 5 the A world C word D 6 world A cup C word D 7 cup A [SEP] C word D 8 [SEP] BLionel_Messi C entity (E 1 + E 2 ) / 2 Lionel Messi B[MASK] C entity (E 6 + E 7 ) / 2 [MASK]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Words Entities</head><p>Input Position emb.  <ref type="bibr">et al., 2020)</ref> to generate referent entity titles of target mentions in an autoregressive manner. <ref type="bibr" target="#b0">Barba et al. (2022)</ref> formulated ED as a text extraction problem; they fed the document and candidate entity titles to BART and Longformer <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref> and disambiguated a mention in the document by extracting the referent entity title of the mention. However, unlike our model, these models addressed the task based only on local contextual information.</p><p>Treating entities as inputs of Transformer. Recent studies <ref type="bibr" target="#b13">(Zhang et al., 2019;</ref><ref type="bibr" target="#b9">Yamada et al., 2020;</ref><ref type="bibr" target="#b5">Sun et al., 2020)</ref> have proposed Transformerbased models that treat entities as input tokens to enrich their expressiveness using additional information contained in the entity embeddings. However, these models were designed to solve general NLP tasks and not tested on ED. We treat entities as input tokens to capture the global context that is shown to be highly effective for ED. ED as sequential decision task. Past studies <ref type="bibr" target="#b11">(Yang et al., 2019;</ref><ref type="bibr">Fang et al., 2019)</ref> have solved ED by casting it as a sequential decision task to capture global contextual information. We adopt a similar method with an enhanced Transformer architecture, a training task, and an inference method to implement the global ED model based on BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Given a document with N mentions, each of which has K entity candidates, our model solves ED by selecting a correct referent entity from the entity candidates for each mention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>Our model is based on BERT and takes words and entities (Wikipedia entities or the [MASK] entity). The input representation of a word or an entity is constructed by summing the token, token type, and position embeddings (see <ref type="figure">Figure 2</ref>):</p><p>Token embedding is the embedding of the corresponding token. The matrices of the word and entity token embeddings are represented as A ? R Vw?H and B ? R Ve?H , respectively, where H is the size of the hidden states of BERT, and V w and V e are the number of items in the word vocabulary and that of the entity vocabulary, respectively.</p><p>Token type embedding represents the type of token, namely word (C word ) or entity (C entity ).</p><p>Position embedding represents the position of the token in a word sequence. A word and an entity appearing at the i-th position in the sequence are represented as D i and E i , respectively. If an entity mention contains multiple words, its position embedding is computed by averaging the embeddings of the corresponding positions (see <ref type="figure">Figure 2</ref>). Following Devlin et al. <ref type="formula" target="#formula_1">(2019)</ref>, we tokenize the document text using the BERT's wordpiece tokenizer, and insert [CLS] and [SEP] tokens as the first and last words, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Task</head><p>Similar to the masked language model (MLM) objective adopted in BERT, our model is trained by predicting randomly masked entities. Specifically, we randomly replace some percentage of the entities with special [MASK] entity tokens and then trains the model to predict masked entities.</p><p>We adopt a model equivalent to the one used to predict words in MLM. Formally, we predict the original entity corresponding to a masked entity by </p><formula xml:id="formula_1">y = softmax(Bm e + b o ) (1) m e = layernorm gelu(W f h e + b f )<label>(2)</label></formula><p>where h e ? R H is the output embedding corresponding to the masked entity, (2) and predicts the entity using softmax over the K entity candidates: </p><formula xml:id="formula_2">W f ? R H?H is a matrix, b o ? R Ve and b f ? R H are</formula><formula xml:id="formula_3">y ED = softmax(B * m e + b * o ),<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Modeling Details</head><p>Our model is based on BERT LARGE <ref type="bibr">(Devlin et al., 2019)</ref>. The parameters shared with BERT are initialized using BERT, and the other parameters are initialized randomly. We treat the hyperlinks in Wikipedia as entity annotations and randomly mask 30% of all entities. We train the model by maximizing the log likelihood of entity predictions. Further details are described in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experimental setup follows <ref type="bibr">Le and Titov (2018)</ref>. In particular, we test the proposed ED models using six standard datasets: AIDA-CoNLL (CoNLL) (Hoffart et al., 2011), MSNBC, AQUAINT, ACE2004, WNED-CWEB (CWEB), and WNED-WIKI (WIKI) (Guo and <ref type="bibr">Barbosa, 2018)</ref>. We consider only the mentions that refer to valid entities in Wikipedia. For all datasets, we use the KB+YAGO entity candidates and their associatedp(e|m) (Ganea and Hofmann, 2017), and use the top 30 candidates based onp(e|m). For the CoNLL dataset, we also test the performance using PPRforNED entity candidates <ref type="bibr" target="#b4">(Pershina et al., 2015)</ref>. We report the in-KB accuracy for the CoNLL dataset and the micro F1 score (averaged per mention) for the other datasets. Further details of the datasets are provided in Appendix C. Furthermore, we optionally fine-tune the model by maximizing the log likelihood of the ED pre-Name MSNBC AQUAINT ACE2004 CWEB WIKI Average Baselines:</p><p>Ganea and Hofmann <ref type="formula" target="#formula_1">(2017)</ref>     <ref type="formula" target="#formula_1">(2019)</ref> Our global models consistently perform better than the local model, demonstrating the effectiveness of using global contextual information even if local contextual information is captured using 1 All models listed in <ref type="table" target="#tab_6">Table 2</ref> use Wikipedia as training data which partly overlap with the WIKI dataset. expressive BERT model. Moreover, the confidenceorder model performs better than the natural-order model on most datasets. An analysis investigating why the confidence-order model outperforms the natural-order model is provided in the next section.</p><p>The fine-tuning on the CoNLL dataset significantly improves the performance on this dataset <ref type="table" target="#tab_2">(Table 1)</ref>. However, it generally degrades the performance on the other datasets <ref type="table" target="#tab_6">(Table 2)</ref>. This suggests that Wikipedia entity annotations are more suitable than the CoNLL dataset to train generalpurpose ED models.</p><p>Additionally, our models perform worse than <ref type="bibr" target="#b12">Yang et al. (2018)</ref> on the CWEB dataset. This is because this dataset is significantly longer on average than other datasets, i.e., approximately 1,700 words per document on average, which is more than three times longer than the 512-word limit that can be handled by BERT-based models including ours. <ref type="bibr" target="#b12">Yang et al. (2018)</ref> achieved excellent performance on this dataset because their model uses various hand-engineered features capturing document-level contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>To investigate how global contextual information helps our model to improve performance, we manually analyze the difference between the predictions of the local, natural-order, and confidence-order models. We use the fine-tuned model using the CoNLL dataset with the YAGO+KB candidates. Although all models perform well on most mentions, the local model often fails to resolve mentions of common names referring to specific entities (e.g., "New York" referring to New York Knicks). Global models are generally better to resolve such difficult cases because of the presence of strong global contextual information (e.g., mentions referring to basketball teams).</p><p>Furthermore, we find that the confidence-order model works especially well for mentions that require a highly detailed context to resolve. For example, a mention of "Matthew Burke" can refer to two different former Australian rugby players. Although the local and natural-order models incorrectly resolve this mention to the player who has the larger number of occurrences in our Wikipediabased corpus, the confidence-order model successfully resolves this by disambiguating its contextual mentions, including his teammates, in advance. We provide detailed inference sequence of the corresponding document in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance for Rare Entities</head><p>We examine whether our model learns effective embeddings for rare entities using the CoNLL dataset. Following Ganea and Hofmann (2017), we use the mentions of which entity candidates contain their gold entities and measure the performance by dividing the mentions based on the frequency of their entities in the Wikipedia annotations used to train the embeddings.</p><p>As presented in <ref type="table" target="#tab_7">Table 3</ref>, our models achieve enhanced performance for rare entities. Furthermore, the global models consistently outperform the local model both for rare and frequent entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We propose a new global ED model based on BERT.</p><p>Our extensive experiments on a wide range of ED datasets demonstrate its effectiveness.</p><p>One limitation of our model is that, similar to existing ED models, our model cannot handle entities that are not included in the vocabulary. In our future work, we will investigate the method to compute the embeddings of such entities using a post-hoc training with an extended vocabulary <ref type="bibr" target="#b6">(Tai et al., 2020)</ref>. As the input corpus for training our model, we use the December 2018 version of Wikipedia, comprising approximately 3.5 billion words and 11 million entity annotations. We generate input sequences by splitting the content of each page into sequences comprising ? 512 words and their entity annotations (i.e., hyperlinks). The input text is tokenized using BERT's tokenizer with its vocabulary consisting of V w = 30, 000 words. Similar to Ganea and Hofmann <ref type="formula" target="#formula_1">(2017)</ref>, we create an entity vocabulary consisting of V e = 128, 040 entities, which are contained in the entity candidates in the datasets used in our experiments. Our model consists of approximately 440 million parameters. To reduce the training time, the parameters that are shared with BERT are initialized using BERT. The other parameters are initialized randomly. The model is trained via iterations over Wikipedia pages in a random order for seven epochs. To stabilize the training, we update only those parameters that are randomly initialized (i.e., fixed the parameters initialized using BERT) at the first epoch, and update all parameters in the remaining six epochs. We implement the model using PyTorch <ref type="bibr">(Paszke et al., 2019)</ref> and Hugging Face Transformers <ref type="bibr" target="#b8">(Wolf et al., 2020)</ref>, and the training takes approximately ten days using eight Tesla V100 GPUs. We optimize the model using AdamW. The hyper-parameters used in the training are detailed in <ref type="table" target="#tab_11">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Fine-tuning on CoNLL Dataset</head><p>The hyper-parameters used in the fine-tuning on the CoNLL dataset are detailed in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of ED Datasets</head><p>The statistics of the ED datasets used in our experiments are provided in <ref type="table" target="#tab_14">Table 6</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Example of Inference by</head><p>Confidence-order Model <ref type="figure" target="#fig_1">Figure 3</ref> shows an example of the inference performed by our confidence-order model fine-tuned on the CoNLL dataset. The document is obtained from the test set of the CoNLL dataset. As shown in the figure, the model starts with unambiguous player names to recognize the topic of the document, and subsequently resolves the mentions that are challenging to resolve. Notably, the model correctly resolves the mention "Nigel Walker" to the corresponding former rugby player instead of a football player, and the mention "Matthew Burke" to the correct former Australian rugby player born in 1973 instead of   the former Australian rugby player born in 1964. This is accomplished by resolving other contextual mentions, including their colleague players, in advance. These two mentions are denoted in red in the figure. Note that our local model fails to resolve both mentions, and our natural-order model fails to resolve "Matthew Burke."</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, Ling et al. (2020), F?vry et al. (2020), Cao et al. (2021), and Barba et al. (2022). 1 Furthermore, on the CoNLL dataset, our confidence-order model trained only on our Wikipedia-based corpus outperforms Yamada et al. (2016) and Ganea and Hofmann (2017) trained on its in-domain training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>An illustrative example showing the inference performed by our fine-tuned confidence-order model on a document in the CoNLL dataset. Mentions are shown as underlined. Numbers in boldface represent the selection order of the confidence-order model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Algorithm 1 :</head><label>1</label><figDesc>Algorithm of our global ED model. Input: Words and mentions m1, . . . mN . Initialize: ei ? [MASK], i = 1 . . . N repeat N times For all [MASK]s, obtain predictions using Eq.(3) with words and entities e1, ..., eN as inputs Select a mention mj and its prediction?j with the highest probability ej ??j end return {e1, . . . , eN } applying softmax over all entities:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Our local ED model takes words and N [MASK] tokens corresponding to the mentions in the document. The model then computes the embedding m e ? R H for each [MASK] token using Eq.</figDesc><table><row><cell>bias vectors,</cell></row><row><cell>gelu(?) is the gelu activation function (Hendrycks</cell></row><row><cell>and Gimpel, 2016), and layernorm(?) is the layer</cell></row><row><cell>normalization function (Lei Ba et al., 2016).</cell></row><row><cell>3.3 ED Model</cell></row><row><cell>Local ED Model.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>where B * ? R K?H and b * o ? R K consist of the entity token embeddings and the bias corresponding to the entity candidates, respectively. Note that B * and b * o are the subsets of B and b o , respectively.Global ED Model. Our global ED model resolves mentions sequentially for N steps (see Algorithm 1). First, the model initializes the entity of each mention using the [MASK] token. Then, for each step, it predicts an entity for each[MASK]   token, selects the prediction with the highest probability produced by the softmax function inEq.(3), and resolves the corresponding mention by assigning the predicted entity to it. This model is denoted as confidence-order. We also test a model that selects mentions according to their order of appearance in the document and denote it by naturalorder.</figDesc><table><row><cell>Name</cell><cell>Accuracy (KB+YAGO)</cell><cell>Accuracy (PPRforNED)</cell></row><row><cell>Baselines:</cell><cell></cell><cell></cell></row><row><cell>Yamada et al. (2016)</cell><cell>91.5</cell><cell>93.1</cell></row><row><cell>Ganea and Hofmann (2017)</cell><cell>92.2</cell><cell>-</cell></row><row><cell>Yang et al. (2018)</cell><cell>93.0</cell><cell>95.9</cell></row><row><cell>Le and Titov (2018)</cell><cell>93.1</cell><cell>-</cell></row><row><cell>Fang et al. (2019)</cell><cell>94.3</cell><cell>-</cell></row><row><cell>Yang et al. (2019)</cell><cell>94.6</cell><cell></cell></row><row><cell>Broscheit (2019)</cell><cell>87.9</cell><cell>-</cell></row><row><cell>Ling et al. (2020)</cell><cell>-</cell><cell>94.9</cell></row><row><cell>F?vry et al. (2020)</cell><cell>92.5</cell><cell>96.7</cell></row><row><cell>Cao et al. (2021)</cell><cell>93.3</cell><cell>-</cell></row><row><cell>Barba et al. (2022)</cell><cell>92.6</cell><cell>-</cell></row><row><cell>Our model w/o fine-tuning:</cell><cell></cell><cell></cell></row><row><cell>confidence-order</cell><cell>92.4</cell><cell>94.6</cell></row><row><cell>natural-order</cell><cell>91.7</cell><cell>94.0</cell></row><row><cell>local</cell><cell>90.8</cell><cell>94.0</cell></row><row><cell>Our model w/ fine-tuning:</cell><cell></cell><cell></cell></row><row><cell>confidence-order</cell><cell>95.0</cell><cell>97.1</cell></row><row><cell>natural-order</cell><cell>94.8</cell><cell>97.0</cell></row><row><cell>local</cell><cell>94.5</cell><cell>96.8</cell></row></table><note>Table 1: In-KB accuracy on the CoNLL dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Micro F1 score on the MSNBC, AQUAINT, ACE2004, CWEB, and WIKI datasets.</figDesc><table><row><cell cols="5">#annotations confidence-order natural-order local G&amp;H2017</cell></row><row><cell>0</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>0.8</cell></row><row><cell>1-10</cell><cell>95.55</cell><cell>95.55</cell><cell cols="2">95.55 91.93</cell></row><row><cell>11-50</cell><cell>96.98</cell><cell>96.70</cell><cell cols="2">96.43 92.44</cell></row><row><cell>?51</cell><cell>96.64</cell><cell>96.38</cell><cell cols="2">95.80 94.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>dictions (? ED ) using the training set of the CoNLL</cell></row><row><cell>dataset with the KB+YAGO candidates. We mask</cell></row><row><cell>90% of the mentions and fix the entity token em-</cell></row><row><cell>beddings (B and B  *  ) and the bias (b o and b  *  o ).</cell></row><row><cell>The model is trained for two epochs using AdamW.</cell></row><row><cell>Additional details are provided in Appendix B.</cell></row><row><cell>4.1 Results</cell></row><row><cell>Table 1 and Table 2 present our experimental</cell></row><row><cell>results. We achieve new state of the art on</cell></row><row><cell>all datasets except the CWEB dataset by outper-</cell></row><row><cell>forming strong Transformer-based ED models,</cell></row><row><cell>i.e, Broscheit</cell></row></table><note>Accuracy on the CoNLL dataset split by the frequency of entity annotations. Our models were fine- tuned using the CoNLL dataset. G&amp;H2017: The re- sults of Ganea and Hofmann (2017).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>We select these hyper-parameters from the search space described in Devlin et al. (2019) based on the accuracy on the development set of the CoNLL dataset. A document is split if it is longer than 512 words, which is the maximum word length of the BERT model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Hyper-parameters used for training on Wikipedia entity annotations.</figDesc><table><row><cell>Name</cell><cell>Value</cell></row><row><cell>maximum word length</cell><cell>512</cell></row><row><cell>number of epochs</cell><cell>2</cell></row><row><cell>batch size</cell><cell>16</cell></row><row><cell>learning rate</cell><cell>2e-5</cell></row><row><cell>learning rate decay</cell><cell>linear</cell></row><row><cell>warmup proportion</cell><cell>0.1</cell></row><row><cell>dropout</cell><cell>0.1</cell></row><row><cell>weight decay</cell><cell>0.01</cell></row><row><cell>gradient clipping</cell><cell>1.0</cell></row><row><cell>adam ? 1</cell><cell>0.9</cell></row><row><cell>adam ? 2</cell><cell>0.999</cell></row><row><cell>adam</cell><cell>1e-6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc>Hyper-parameters during fine-tuning on the CoNLL dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc>Statistics of ED datasets.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix for "Global Entity Disambiguation with BERT"</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ExtEnD: Extractive Entity Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Procopio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The Long-Document Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150v2</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="677" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autoregressive Entity Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Personalized Page Rank for Named Entity Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pershina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="238" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CoLAKE: Contextualized Language and Knowledge Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaru</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3660" to="3670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">exBERT: Extending Pretrained Models with Domain-specific Vocabulary Under Constrained Training Resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Comiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Fu</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1433" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-Art Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LUKE: Deep Contextualized Entity Representations with Entityaware Self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Dynamic Context Augmentation for Global Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="271" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Collective Entity Disambiguation with Structured Gradient Tree Boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi Shefaet</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="777" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Australia have won all four tests against 46: Italy, 47: Scotland, 48: Ireland and 45: Wales, and scored 414 points at an average of almost 35 points a game. League duties restricted the 28: Barbarians&apos; selectorial options but they still boast 13 internationals including 44: England full-back 16: Tim Stimpson and recalled wing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
			<affiliation>
				<orgName type="collaboration">captain</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
			<affiliation>
				<orgName type="collaboration">captain</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
			<affiliation>
				<orgName type="collaboration">captain</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
			<affiliation>
				<orgName type="collaboration">captain</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
			<affiliation>
				<orgName type="collaboration">captain</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
			<affiliation>
				<orgName type="collaboration">captain</orgName>
			</affiliation>
		</author>
		<idno>Australia -15 -53</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Scotland; England; Pontypridd; New Zealand; Leicester; New Zealand; Matthew Burke; David Campese; Pat Howard, 9 -Sam Payne</addrLine></address></meeting>
		<imprint>
			<publisher>Owen Finegan</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="5" to="21" />
		</imprint>
	</monogr>
	<note>: Nick Popplewell (49: Ireland). David Giffin, 4 -Tim Gavin, 3 -Andrew Blades, 2 -Marco Caputo, 1 -6: Dan Crowley. Order of Inference by Confidence-order Model</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>England</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Wales ! England</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>England</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Wales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Wales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Wales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>England</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Scotland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Wales ! Pontypridd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Leicester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Australia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>England</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Wales ! Italy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Scotland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Ireland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">! Nigel</forename><surname>Ireland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walker</surname></persName>
		</author>
		<editor>Allan Bateman ! Rob Howley ! Nick Popplewell ! Tony Underwood ! Darren Garforth ! Dan Crowley ! Tim Stimpson ! Neil Back ! Joe Roff ! Gregor Townsend ! Craig Quinnell ! All Black ! Owen Finegan ! Norm Hewitt ! Scott Quinnell ! Tim Stimpson ! Australia ! Norm Hewitt ! Dale McIntosh ! Tim Horan ! David Giffin ! Tony Underwood ! David Campese ! Ian Jones ! Ian Jones ! Daniel Herbert ! Barbarians ! Barbarians ! Pat Howard ! David Wilson !</editor>
		<imprint>
			<pubPlace>New Zealand ! New Zealand ! Matthew Burke</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
