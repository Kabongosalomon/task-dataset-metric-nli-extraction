<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Matryoshka Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kusupati</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gantavya</forename><surname>Bhatt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Rege</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wallingford</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Sinha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Ramanujan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Howard-Snyder</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
							<email>prajain@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Matryoshka Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks. When training such representations, it is often the case that computational and statistical constraints for each downstream task are unknown. In this context, rigid fixed-capacity representations can be either over or under-accommodating to the task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution is Matryoshka Representation Learning (MRL) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks. MRL minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. MRL learns coarse-to-fine representations that are at least as accurate and rich as independently trained low-dimensional representations. The flexibility within the learned Matryoshka Representations offer: (a) up to 14? smaller embedding size for ImageNet-1K classification at the same level of accuracy; (b) up to 14? real-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for long-tail few-shot classification, all while being as robust as the original representations. Finally, we show that MRL extends seamlessly to web-scale datasets (ImageNet, JFT) across various modalities -vision (ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and pretrained models are open-sourced at https://github.com/RAIVNLab/MRL. * Equal contribution -AK led the project with extensive support from GB and AR for experimentation.</p><p>36th Conference on Neural Information Processing Systems (NeurIPS 2022).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learned representations <ref type="bibr" target="#b55">[55]</ref> are fundamental building blocks of real-world ML systems <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b86">86]</ref>. Trained once and frozen, d-dimensional representations encode rich information and can be used to perform multiple downstream tasks <ref type="bibr" target="#b3">[4]</ref>. The deployment of deep representations has two steps: <ref type="bibr" target="#b0">(1)</ref> an expensive yet constant-cost forward pass to compute the representation <ref type="bibr" target="#b26">[27]</ref> and (2) utilization of the representation for downstream applications <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b84">84]</ref>. Compute costs for the latter part of the pipeline scale with the embedding dimensionality as well as the data size (N ) and label space (L). At web-scale <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b80">80]</ref> this utilization cost overshadows the feature computation cost. The rigidity in these representations forces the use of high-dimensional embedding vectors across multiple tasks despite the varying resource and accuracy constraints that require flexibility.</p><p>Human perception of the natural world has a naturally coarse-to-fine granularity <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>. However, perhaps due to the inductive bias of gradient-based training <ref type="bibr" target="#b79">[79]</ref>, deep learning models tend to diffuse "information" across the entire representation vector. The desired elasticity is usually enabled in the existing flat and fixed representations either through training multiple low-dimensional models <ref type="bibr" target="#b26">[27]</ref>, jointly optimizing sub-networks of varying capacity <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b95">95]</ref> or post-hoc compression <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b57">57]</ref>. Each of these techniques struggle to meet the requirements for adaptive large-scale deployment either due to training/maintenance overhead, numerous expensive forward passes through all of the data, storage and memory cost for multiple copies of encoded data, expensive on-the-fly feature selection or a significant drop in accuracy. By encoding coarse-to-fine-grained representations, which are as accurate as the independently trained counterparts, we learn with minimal overhead a representation that can be deployed adaptively at no additional cost during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We introduce</head><p>Matryoshka Representation Learning (MRL) to induce flexibility in the learned representation. MRL learns representations of varying capacities within the same high-dimensional vector through explicit optimization of O(log(d)) lower-dimensional vectors in a nested fashion, hence the name Matryoshka. MRL can be adapted to any existing representation pipeline and is easily extended to many standard tasks in computer vision and natural language processing. <ref type="figure" target="#fig_15">Figure 1</ref> illustrates the core idea of Matryoshka Representation Learning (MRL) and the adaptive deployment settings of the learned Matryoshka Representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive Retrieval</head><p>Shortlisting Re-ranking</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive Classification</head><p>Training Inference </p><formula xml:id="formula_0">f T u Z p d T z X i b + 5 3 U S E 1 x 4 K Z d x Y p i k s 0 N B I r C J c F Y B 7 n P F q B F j S w h V 3 G b F d E g U o c Y W V b Q l u P N f X i T N 0 6 r r V N 3 b s 3 L t M q + j A I d w B B V w 4 R x q c A 1 1 a A A F B c / w C m / o E b 2 g d / Q x G 1 1 C + c 4 B / A H 6 / A G Z E J H n &lt; / l a t e x i t &gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>Matryoshka Representation Learning is adaptable to any representation learning setup and begets a Matryoshka Representation z by optimizing the original loss L(.) at O(log(d)) chosen representation sizes. Matryoshka Representation can be utilized effectively for adaptive deployment across environments and downstream tasks.</p><p>The first m-dimensions, m ? [d], of the Matryoshka Representation is an information-rich low-dimensional vector, at no additional training cost, that is as accurate as an independently trained m-dimensional representation. The information within the Matryoshka Representation increases with the dimensionality creating a coarse-to-fine grained representation, all without significant training or additional deployment overhead. MRL equips the representation vector with the desired flexibility and multifidelity that can ensure a near-optimal accuracy-vs-compute trade-off. With these advantages, MRL enables adaptive deployment based on accuracy and compute constraints.</p><p>The Matryoshka Representations improve efficiency for large-scale classification and retrieval without any significant loss of accuracy. While there are potentially several applications of coarse-tofine Matryoshka Representations, in this work we focus on two key building blocks of real-world ML systems: large-scale classification and retrieval. For classification, we use adaptive cascades with the variable-size representations from a model trained with MRL, significantly reducing the average dimension of embeddings needed to achieve a particular accuracy. For example, on ImageNet-1K, MRL + adaptive classification results in up to a 14? smaller representation size at the same accuracy as baselines (Section 4.2.1). Similarly, we use MRL in an adaptive retrieval system. Given a query, we shortlist retrieval candidates using the first few dimensions of the query embedding, and then successively use more dimensions to re-rank the retrieved set. A simple implementation of this approach leads to 128? theoretical (in terms of FLOPS) and 14? wall-clock time speedups compared to a single-shot retrieval system that uses a standard embedding vector; note that MRL's retrieval accuracy is comparable to that of single-shot retrieval (Section 4.3.1). Finally, as MRL explicitly learns coarse-to-fine representation vectors, intuitively it should share more semantic information among its various dimensions ( <ref type="figure" target="#fig_7">Figure 5</ref>). This is reflected in up to 2% accuracy gains in long-tail continual learning settings while being as robust as the original embeddings. Furthermore, due to its coarse-to-fine grained nature, MRL can also be used as method to analyze hardness of classification among instances and information bottlenecks.</p><p>We make the following key contributions:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">We introduce</head><p>Matryoshka Representation Learning (MRL) to obtain flexible representations (Matryoshka Representations) for adaptive deployment (Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Representation Learning. Large-scale datasets like ImageNet <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b71">71]</ref> and JFT <ref type="bibr" target="#b80">[80]</ref> enabled the learning of general purpose representations for computer vision <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b93">93]</ref>. These representations are typically learned through supervised and un/self-supervised learning paradigms. Supervised pretraining <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b77">77]</ref> casts representation learning as a multi-class/label classification problem, while un/self-supervised learning learns representation via proxy tasks like instance classification <ref type="bibr" target="#b92">[92]</ref> and reconstruction <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b60">60]</ref>. Recent advances <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28]</ref> in contrastive learning <ref type="bibr" target="#b24">[25]</ref> enabled learning from web-scale data <ref type="bibr" target="#b20">[21]</ref> that powers large-capacity cross-modal models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b96">96]</ref>. Similarly, natural language applications are built <ref type="bibr" target="#b37">[38]</ref> on large language models <ref type="bibr" target="#b7">[8]</ref> that are pretrained <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b70">70]</ref> in a un/self-supervised fashion with masked language modelling <ref type="bibr" target="#b18">[19]</ref> or autoregressive training <ref type="bibr" target="#b66">[66]</ref>.</p><p>Matryoshka Representation Learning (MRL) is complementary to all these setups and can be adapted with minimal overhead (Section 3). MRL equips representations with multifidelity at no additional cost which enables adaptive deployment based on the data and task (Section 4).</p><p>Efficient Classification and Retrieval. Efficiency in classification and retrieval during inference can be studied with respect to the high yet constant deep featurization costs or the search cost which scales with the size of the label space and data. Efficient neural networks address the first issue through a variety of algorithms <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b52">52]</ref> and design choices <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b82">82]</ref>. However, with a strong featurizer, most of the issues with scale are due to the linear dependence on number of labels (L), size of the data (N ) and representation size (d), stressing RAM, disk and processor all at the same time.</p><p>The sub-linear complexity dependence on number of labels has been well studied in context of compute <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b65">65]</ref> and memory <ref type="bibr" target="#b19">[20]</ref> using Approximate Nearest Neighbor Search (ANNS) <ref type="bibr" target="#b59">[59]</ref> or leveraging the underlying hierarchy <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b53">53]</ref>. In case of the representation size, often dimensionality reduction <ref type="bibr" target="#b72">[72,</ref><ref type="bibr" target="#b83">83]</ref>, hashing techniques <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b73">73]</ref> and feature selection <ref type="bibr" target="#b61">[61]</ref> help in alleviating selective aspects of the O(d) scaling at a cost of significant drops in accuracy. Lastly, most real-world search systems <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref> are often powered by large-scale embedding based retrieval <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b62">62]</ref> that scales in cost with the ever increasing web-data. While categorization <ref type="bibr" target="#b84">[84,</ref><ref type="bibr" target="#b94">94]</ref> clusters similar things together, it is imperative to be equipped with retrieval capabilities that can bring forward every instance <ref type="bibr" target="#b6">[7]</ref>. Approximate Nearest Neighbor Search (ANNS) <ref type="bibr" target="#b39">[40]</ref> makes it feasible with efficient indexing <ref type="bibr" target="#b13">[14]</ref> and traversal <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> to present the users with the most similar documents/images from the database for a requested query. Widely adopted HNSW <ref type="bibr" target="#b59">[59]</ref> (O(d log(N ))) is as accurate as exact retrieval (O(dN )) at the cost of a graph-based index overhead for RAM and disk <ref type="bibr" target="#b41">[42]</ref>.</p><p>MRL tackles the linear dependence on embedding size, d, by learning multifidelity Matryoshka Representations. Lower-dimensional Matryoshka Representations are as accurate as independently trained counterparts without the multiple expensive forward passes. Matryoshka Representations provide an intermediate abstraction between high-dimensional vectors and their efficient ANNS indices through the adaptive embeddings nested within the original representation vector (Section 4). All other aforementioned efficiency techniques are complementary and can be readily applied to the learned Matryoshka Representations obtained from MRL.</p><p>Several works in efficient neural network literature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b88">88,</ref><ref type="bibr" target="#b95">95]</ref> aim at packing neural networks of varying capacity within the same larger network. However, the weights for each progressively smaller network can be different and often require distinct forward passes to isolate the final representations. This is detrimental for adaptive inference due to the need for re-encoding the entire retrieval database with expensive sub-net forward passes of varying capacities. Finally, ordered representations proposed by Rippel et al. <ref type="bibr" target="#b69">[69]</ref> use nested dropout in the context of autoencoders to learn nested representations.  Matryoshka Representation Learning representation of the datapoint x. We obtain z using a deep neural network F ( ? ; ? F ) : X ? R d parameterized by learnable weights ? F , i.e., z := F (x; ? F ). The multi-granularity is captured through the set of the chosen dimensions M, that contains less than log(d) elements, i.e., |M| ? log(d) .</p><p>The usual set M consists of consistent halving until the representation size hits a low information bottleneck. We discuss the design choices in Section 4 for each of the representation learning settings.</p><p>For the ease of exposition, we present the formulation for fully supervised representation learning via multi-class classification. Matryoshka Representation Learning modifies the typical setting to become a multi-scale representation learning problem on the same task. For example, we train ResNet50 <ref type="bibr" target="#b26">[27]</ref> on ImageNet-1K <ref type="bibr" target="#b71">[71]</ref> which embeds a 224 ? 224 pixel image into a d = 2048 representation vector and then passed through a linear classifier to make a prediction,? among the L = 1000 labels. For MRL, we choose M = {8, 16, . . . , 1024, 2048} as the nesting dimensions.</p><p>Suppose we are given a labelled dataset</p><formula xml:id="formula_1">D = {(x 1 , y 1 ), . . . , (x N , y N )} where x i ? X is an input point and y i ? [L] is the label of x i for all i ? [N ]</formula><p>. MRL optimizes the multi-class classification loss for each of the nested dimension m ? M using standard empirical risk minimization using a separate linear classifier, parameterized by W (m) ? R L?m . All the losses are aggregated after scaling with their relative importance (c m ? 0) m?M respectively. That is, we solve</p><formula xml:id="formula_2">min {W (m) } m?M , ? F 1 N i?[N ] m?M c m ? L W (m) ? F (x i ; ? F ) 1:m ; y i ,<label>(1)</label></formula><p>where L : R L ? [L] ? R + is the multi-class softmax cross-entropy loss function. This is a standard optimization problem that can be solved using sub-gradient descent methods. We set all the importance scales, c m = 1 for all m ? M; see Section 5 for ablations. Lastly, despite only optimizing for O(log(d)) nested dimensions, MRL results in accurate representations, that interpolate, for dimensions that fall between the chosen granularity of the representations (Section 4.2).</p><p>We call this formulation as Matryoshka Representation Learning (MRL). A natural way to make this efficient is through weight-tying across all the linear classifiers, i.e., by defining W (m) = W 1:m for a set of common weights W ? R L?d . This would reduce the memory cost due to the linear classifiers by almost half, which would be crucial in cases of extremely large output spaces <ref type="bibr" target="#b84">[84,</ref><ref type="bibr" target="#b94">94]</ref>. This variant is called Efficient Matryoshka Representation Learning (MRL-E). Refer to Alg 1 and Alg 2 in Appendix A for the building blocks of Matryoshka Representation Learning (MRL).</p><p>Adaptation to Learning Frameworks. MRL can be adapted seamlessly to most representation learning frameworks at web-scale with minimal modifications (Section 4.1). For example, MRL's adaptation to masked language modelling reduces to MRL-E due to the weight-tying between the input embedding matrix and the linear classifier. For contrastive learning, both in context of vision &amp; vision + language, MRL is applied to both the embeddings that are being contrasted with each other. The presence of normalization on the representation needs to be handled independently for each of the nesting dimension for best results (see Appendix C for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Applications</head><p>In this section, we discuss Matryoshka Representation Learning (MRL) for a diverse set of applications along with an extensive evaluation of the learned multifidelity representations. Further, we showcase the downstream applications of the learned Matryoshka Representations for flexible large-scale deployment through (a) Adaptive Classification (AC) and (b) Adaptive Retrieval (AR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Representation Learning</head><p>We   <ref type="figure" target="#fig_2">Figure 3</ref> showcases the comparison of learned representation quality through 1-NN accuracy on ImageNet-1K (trainset with 1.3M samples as the database and validation set with 50K samples as the queries). Matryoshka Representations are up to 2% more accurate than their fixed-feature counterparts for the lower-dimensions while being as accurate elsewhere. 1-NN accuracy is an excellent proxy, at no additional training cost, to gauge the utility of learned representations in the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classification</head><p>We also evaluate the quality of the representations from training ViT-B/16 on JFT-300M alongside the ViT-B/16 vision encoder of the ALIGN model -two web-scale setups. Due to the expensive nature of these experiments, we only train the highest capacity fixed feature model and choose random features for evaluation in lower-dimensions. Web-scale is a compelling setting for MRL due to its relatively inexpensive training overhead while providing multifidelity representations for downstream tasks. <ref type="figure" target="#fig_6">Figure 4</ref>, evaluated with 1-NN on ImageNet-1K, shows that all the MRL models for JFT and ALIGN are highly accurate while providing an excellent cost-vs-accuracy trade-off at lower-dimensions. These experiments show that MRL seamlessly scales to large-scale models and web-scale datasets while providing the otherwise prohibitively expensive multi-granularity in the process. We also have similar observations when pretraining BERT; please see Appendix D.2 for more details. Our experiments also show that post-hoc compression (SVD), linear probe on random features, and sub-net style slimmable networks drastically lose accuracy compared to MRL as the representation size decreases. Finally, <ref type="figure" target="#fig_7">Figure 5</ref> shows that, while MRL explicitly optimizes O(log(d)) nested representations -removing the O(d) dependence [69] -, the coarse-to-fine grained information is interpolated across all d dimensions providing highest flexibility for adaptive deployment.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Adaptive Classification</head><p>The flexibility and coarse-to-fine granularity within Matryoshka Representations allows model cascades <ref type="bibr" target="#b85">[85]</ref> for Adaptive Classification (AC) <ref type="bibr" target="#b25">[26]</ref>. Unlike standard model cascades <ref type="bibr" target="#b90">[90]</ref>, MRL does not require multiple expensive neural network forward passes. To perform AC with an MRL trained model, we learn thresholds on the maximum softmax probability <ref type="bibr" target="#b30">[31]</ref> for each nested classifier on a holdout validation set. We then use these thresholds to decide when to transition to the higher dimensional representation (e.g 8 ? 16 ? 32) of the MRL model. Appendix D.1 discusses the implementation and learning of thresholds for cascades used for adaptive classification in detail. <ref type="figure">Figure 6</ref> shows the comparison between cascaded MRL representations (MRL-AC) and independently trained fixed feature (FF) models on ImageNet-1K with ResNet50. We computed the expected representation size for MRL-AC based on the final dimensionality used in the cascade. We observed that MRL-AC was as accurate, 76.30%, as a 512-dimensional FF model but required an expected dimensionality of ? 37 while being only 0.8% lower than the 2048-dimensional FF baseline. Note that all MRL-AC models are significantly more accurate than the FF baselines at comparable representation sizes. MRL-AC uses up to ? 14? smaller representation size for the same accuracy which affords computational efficiency as the label space grows <ref type="bibr" target="#b84">[84]</ref>. Lastly, our results with MRL-AC indicate that instances and classes vary in difficulty which we analyze in Section 5 and Appendix J.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Retrieval</head><p>Nearest neighbour search with learned representations powers a plethora of retrieval and search applications <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b86">86,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b62">62]</ref>. In this section, we discuss the image retrieval performance of the pretrained ResNet50 models (Section 4.1) on two large-scale datasets ImageNet-1K <ref type="bibr" target="#b71">[71]</ref> and ImageNet-4K. ImageNet-1K has a database size of ?1.3M and a query set of 50K samples uniformly spanning 1000 classes. We also introduce ImageNet-4K which has a database size of ?4.2M and query set of ?200K samples uniformly spanning 4202 classes (see Appendix B for details). A single forward pass on ResNet50 costs 4 GFLOPs while exact retrieval costs 2.6 GFLOPs per query for ImageNet-1K. Although retrieval overhead is 40% of the total cost, retrieval cost grows linearly with the size of the database. ImageNet-4K presents a retrieval benchmark where the exact search cost becomes the computational bottleneck (8.6 GFLOPs per query). In both these settings, the memory and disk usage are also often bottlenecked by the large databases. However, in most real-world applications exact search, O(dN ), is replaced with an approximate nearest neighbor search (ANNS) method like HNSW <ref type="bibr" target="#b59">[59]</ref>, O(d log(N )), with minimal accuracy drop at the cost of additional memory overhead.</p><p>The goal of image retrieval is to find images that belong to the same class as the query using representations obtained from a pretrained model. In this section, we compare retrieval performance using mean Average Precision @ 10 (mAP@10) which comprehensively captures the setup of relevant image retrieval at scale. We measure the cost per query using exact search in MFLOPs. All embeddings are unit normalized and retrieved using the L2 distance metric. Lastly, we report 14x smaller representation size <ref type="figure">Figure 6</ref>: Adaptive classification on MRL ResNet50 using cascades results in 14? smaller representation size for the same level of accuracy on ImageNet-1K (? 37 vs 512 dims for 76.3%).  MRL models are capable of performing accurate retrieval at various granularities without the additional expense of multiple model forward passes for the web-scale databases. FF models also generate independent databases which become prohibitively expense to store and switch in between. Matryoshka Representations enable adaptive retrieval (AR) which alleviates the need to use full-capacity representations, d = 2048, for all data and downstream tasks. Lastly, all the vector compression techniques <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b42">43]</ref> used as part of the ANNS pipelines are complimentary to Matryoshka Representations and can further improve the efficiency-vs-accuracy trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Adaptive Retrieval</head><p>We benchmark MRL in the adaptive retrieval setting (AR) <ref type="bibr" target="#b47">[48]</ref>. For a given query image, we obtained a shortlist, K = 200, of images from the database using a lower-dimensional representation, e.g. D s = 16 followed by reranking with a higher capacity representation, e.g. D r = 2048. In real-world scenarios where top ranking performance is the key objective, measured with mAP@k where k covers a limited yet crucial real-estate, AR provides significant compute and memory gains over single-shot retrieval with representations of fixed dimensionality. Finally, the most expensive part of AR, as with any retrieval pipeline, is the nearest neighbour search for shortlisting. For example, even naive re-ranking of 200 images with 2048 dimensions only costs 400 KFLOPs. While we report exact search cost per query for all AR experiments, the shortlisting component of the pipeline can be sped-up using ANNS (HNSW). Appendix I has a detailed discussion on compute cost for exact search, memory overhead of HNSW indices and wall-clock times for both implementations. We note that using HNSW with 32 neighbours for shortlisting does not decrease accuracy during retrieval. <ref type="figure" target="#fig_10">Figure 8</ref> showcases the compute-vs-accuracy trade-off for adaptive retrieval using Matryoshka Representations compared to single-shot using fixed features with ResNet50 on ImageNet-1K. We observed that all AR settings lied above the Pareto frontier of single-shot retrieval with varying representation sizes. In particular for ImageNet-1K, we show that the AR model with D s = 16 &amp; D r = 2048 is as accurate as single-shot retrieval with d = 2048 while being ? 128? more efficient in theory and ? 14? faster in practice (compared using HNSW on the same hardware). We show similar trends with ImageNet-4K, but note that we require D s = 64 given the increased difficulty of the dataset. This results in ? 32? and ? 6? theoretical and in-practice speedups respectively. Lastly, while K = 200 works well for our adaptive retrieval experiments, we ablated over the shortlist size k in Appendix K.2 and found that the accuracy gains stopped after a point, further strengthening the use-case for Matryoshka Representation Learning and adaptive retrieval.</p><p>Even with adaptive retrieval, it is hard to determine the choice of D s &amp; D r . In order to alleviate this issue to an extent, we propose Funnel Retrieval, a consistent cascade for adaptive retrieval. Funnel thins out the initial shortlist by a repeated re-ranking and shortlisting with a series of increasing capacity representations. Funnel halves the shortlist size and doubles the representation size at every step of re-ranking. For example on ImageNet-1K, a funnel with the shortlist progression of 200 ? 100 ? 50 ? 25 ? 10 with the cascade of 16 ? 32 ? 64 ? 128 ? 256 ? 2048 representation sizes within Matryoshka Representation is as accurate as the single-shot 2048-dim retrieval while being ? 128? more efficient theoretically (see Appendix F for more results). All these results showcase the potential of MRL and AR for large-scale multi-stage search systems <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Further Analysis and Ablations</head><p>Robustness. We evaluate the robustness of the MRL models trained on ImageNet-1K on out-ofdomain datasets, ImageNetV2/R/A/Sketch <ref type="bibr" target="#b68">[68,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b89">89]</ref>, and compare them to the FF baselines. <ref type="table" target="#tab_2">Table 17</ref> in Appendix H demonstrates that Matryoshka Representations for classification are at least as robust as the original representation while improving the performance on ImageNet-A by 0.6% -a 20% relative improvement. We also study the robustness in the context of retrieval by using ImageNetV2 as the query set for ImageNet-1K database. <ref type="table" target="#tab_10">Table 9</ref> in Appendix E shows that MRL models have more robust retrieval compared to the FF baselines by having up to 3% higher mAP@10 performance. This observation also suggests the need for further investigation into robustness using nearest neighbour based classification and retrieval instead of the standard linear probing setup. We also find that the zero-shot robustness of ALIGN-MRL <ref type="table" target="#tab_2">(Table 18</ref> in Appendix H) agrees with the observations made by Wortsman et al. <ref type="bibr" target="#b91">[91]</ref>. Lastly, <ref type="table" target="#tab_7">Table 6</ref> in Appendix D.2 shows that MRL also improves the cosine similarity span between positive and random image-text pairs.</p><p>Few-shot and Long-tail Learning. We exhaustively evaluated few-shot learning on MRL models using nearest class mean <ref type="bibr" target="#b74">[74]</ref>. <ref type="table" target="#tab_2">Table 15</ref> in Appendix G shows that that representations learned through MRL perform comparably to FF representations across varying shots and number of classes.</p><p>Matryoshka Representations realize a unique pattern while evaluating on FLUID <ref type="bibr" target="#b87">[87]</ref>, a long-tail sequential learning framework. We observed that MRL provides up to 2% accuracy higher on novel classes in the tail of the distribution, without sacrificing accuracy on other classes <ref type="table" target="#tab_2">(Table 16</ref> in Appendix G). Additionally we find the accuracy between low-dimensional and high-dimensional representations is marginal for pretrain classes. We hypothesize that the higher-dimensional representations are required to differentiate the classes when few training examples of each are known. This results provides further evidence that different tasks require varying capacity based on their difficulty.</p><p>Disagreement across Dimensions. The information packing in Matryoshka Representations often results in gradual increase of accuracy with increase in capacity. However, we observed that -dim models incorrectly focus on the eyes of the doll ("sunglasses") and not the "sweatshirt" which is correctly in focus at higher dimensions; MRL fails gracefully in these scenarios and shows potential use cases of disagreement across dimensions.</p><p>this trend was not ubiquitous and certain instances and classes were more accurate when evaluated with lower-dimensions ( <ref type="figure" target="#fig_4">Figure 12</ref> in Appendix J). With perfect routing of instances to appropriate dimension, MRL can gain up to 4.6% classification accuracy. At the same time, the low-dimensional models are less accurate either due to confusion within the same superclass <ref type="bibr" target="#b22">[23]</ref> of the ImageNet hierarchy or presence of multiple objects of interest. <ref type="figure" target="#fig_11">Figure 9</ref> showcases 2 such examples for 8dimensional representation. These results along with Appendix J put forward the potential for MRL to be a systematic framework for analyzing the utility and efficiency of information bottlenecks.</p><p>Superclass Accuracy. As the information bottleneck becomes smaller, the overall accuracy on fine-grained classes decreases rapidly ( <ref type="figure" target="#fig_2">Figure 3</ref>). However, the drop-off is not as significant when evaluated at a superclass level ( <ref type="table" target="#tab_3">Table 24</ref> in Appendix J). <ref type="figure" target="#fig_12">Figure 10</ref> presents that this phenomenon   occurs with both MRL and FF models; MRL is more accurate across dimensions. This shows that tight information bottlenecks while not highly accurate for fine-grained classification, do capture required semantic information for coarser classification that could be leveraged for adaptive routing for retrieval and classification. Mutifidelity of Matryoshka Representation naturally captures the underlying hierarchy of the class labels with one single model. Lastly, <ref type="figure" target="#fig_14">Figure 11</ref> showcases the accuracy trends per superclass with MRL. The utility of additional dimensions in distinguishing a class from others within the same superclass is evident for "garment" which has up to 11% improvement for 8 ? 16 dimensional representation transition. We also observed that superclasses such as "oscine (songbird)" had a clear visual distinction between the object and background and thus predictions using 8 dimensions also led to a good inter-class separability within the superclass. <ref type="table" target="#tab_3">Table 26</ref> in Appendix K presents that Matryoshka Representations can be enabled within off-theshelf pretrained models with inexpensive partial finetuning thus paving a way for ubiquitous adoption of MRL. At the same time, <ref type="table" target="#tab_3">Table 27</ref> in Appendix C indicates that with optimal weighting of the nested losses we could improve accuracy of lower-dimensions representations without accuracy loss. <ref type="table" target="#tab_3">Tables 28 and 29</ref> in Appendix C ablate over the choice of initial granularity and spacing of the granularites. <ref type="table" target="#tab_3">Table 28</ref> reaffirms the design choice to shun extremely low dimensions that have poor classification accuracy as initial granularity for MRL while <ref type="table" target="#tab_3">Table 29</ref> confirms the effectiveness of logarthmic granularity spacing inspired from the behaviour of accuracy saturation across dimensions over uniform. Lastly, Tables 30 and 31 in Appendix K.2 show that the retrieval performance saturates after a certain shortlist dimension and length depending on the complexity of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Conclusions</head><p>The results in Section 5.1 reveal interesting weaknesses of MRL that would be logical directions for future work. (1) Optimizing the weightings of the nested losses to obtain a Pareto optimal accuracy-vs-efficiency trade-off -a potential solution could emerge from adaptive loss balancing aspects of anytime neural networks <ref type="bibr" target="#b38">[39]</ref>. <ref type="bibr" target="#b1">(2)</ref> Using different losses at various fidelities aimed at solving a specific aspect of adaptive deployment -e.g. high recall for 8-dimension and robustness for 2048-dimension. (3) Learning a search data-structure, like differentiable k-d tree, on top of Matryoshka Representation to enable dataset and representation aware retrieval. (4) Finally, the joint optimization of multi-objective MRL combined with end-to-end learnable search data-structure to have data-driven adaptive large-scale retrieval for web-scale search applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In conclusion, we presented</head><p>Matryoshka Representation Learning (MRL), a flexible representation learning approach that encodes information at multiple granularities in a single embedding vector. This enables the MRL to adapt to a downstream task's statistical complexity as well as the available compute resources. We demonstrate that MRL can be used for large-scale adaptive classification as well as adaptive retrieval. On standard benchmarks, MRL matches the accuracy of the fixed-feature baseline despite using 14? smaller representation size on average. Furthermore, the Matryoshka Representation based adaptive shortlisting and re-ranking system ensures comparable mAP@10 to the baseline while being 128? cheaper in FLOPs and 14? faster in wall-clock time. Finally, most of the efficiency techniques for model inference and vector search are complementary to MRL further assisting in deployment at the compute-extreme environments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Matryoshka Representation Learning Model Training</head><p>We trained all ResNet50-MRL models using the efficient dataloaders of FFCV <ref type="bibr" target="#b54">[54]</ref>. We utilized the rn50_40_epochs.yaml configuration file of FFCV to train all MRL models defined below:</p><p>? MRL: ResNet50 model with the fc layer replaced by MRL_Linear_Layer(efficient=False)</p><p>? MRL-E: ResNet50 model with the fc layer replaced by MRL_Linear_Layer(efficient=True)</p><p>? FF-k: ResNet50 model with the fc layer replaced by torch.nn.Linear(k, num_classes), where k ? <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024,</ref><ref type="bibr">2048]</ref>. We will henceforth refer to these models as simply FF, with the k value denoting representation size.</p><p>We trained all ResNet50 models with a learning rate of 0.475 with a cyclic learning rate schedule <ref type="bibr" target="#b78">[78]</ref>. This was after appropriate scaling (0.25?) of the learning rate specified in the configuration file to accommodate for 2xA100 NVIDIA GPUs available for training, compared to the 8xA100 GPUs utilized in the FFCV benchmarks. We trained with a batch size of 256 per GPU, momentum <ref type="bibr" target="#b81">[81]</ref> of 0.9, and an SGD optimizer with a weight decay of 1e-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our code (Appendix A) makes minimal modifications to the training pipeline provided by FFCV to learn Matryoshka Representations.</head><p>We trained ViT-B/16 models for JFT-300M on a 8x8 cloud TPU pod <ref type="bibr" target="#b46">[47]</ref> using Tensorflow <ref type="bibr" target="#b0">[1]</ref> with a batchsize of 128 and trained for 300K steps. Similarly, ALIGN models were trained using Tensorflow on 8x8 cloud TPU pod for 1M steps with a batchsize of 64 per TPU. Both these models were trained with adafactor optimizer <ref type="bibr" target="#b76">[76]</ref> with a linear learning rate decay starting at 1e-3.</p><p>Lastly, we trained a BERT-Base model on English Wikipedia and BookCorpus. We trained our models in Tensorflow using a 4x4 cloud TPU pod with a total batchsize of 1024. We used AdamW <ref type="bibr" target="#b58">[58]</ref> optimizer with a linear learning rate decay starting at 1e-4 and trained for 450K steps.</p><p>In each configuration/case, if the final representation was normalized in the FF implementation, MRL models adopted the same for each nested dimension for a fair comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Classification Results</head><p>We show the top-1 classification accuracy of ResNet50-MRL models on ImageNet-1K in <ref type="table" target="#tab_2">Table 1</ref> and <ref type="figure" target="#fig_4">Figure 2</ref>. We compare the performance of MRL models (MRL, MRL-E) to several baselines:</p><p>? FF: We utilize the FF-k models described in Appendix C for k ? {8, ...2048}.</p><p>? SVD: We performed a low rank approximation of the 1000-way classification layer of FF-2048, with rank = 1000. ? Rand. LP: We compared against a linear classifier fit on randomly selected features <ref type="bibr" target="#b27">[28]</ref>. ? Slim. Net: We take pretrained slimmable neural networks <ref type="bibr" target="#b95">[95]</ref> which are trained with a flexible width backbone (25%, 50%, 75% and full width). For each representation size, we consider the first k dimensions for classification. Note that training of slimmable neural networks becomes unstable when trained below 25% width due to the hardness in optimization and low complexity of the model.</p><p>At lower dimensions ( d ? 128), MRL outperforms all baselines significantly, which indicates that pretrained models lack the multifidelity of Matryoshka Representations and are incapable of fitting an accurate linear classifier at low representation sizes.</p><p>We compared the performance of MRL models at various representation sizes via 1-nearest neighbors (1-NN) image classification accuracy on ImageNet-1K in <ref type="table" target="#tab_3">Table 2</ref> and <ref type="figure" target="#fig_2">Figure 3</ref>. We provide detailed information regarding the k-NN search pipeline in Appendix E. We compared against a baseline of attempting to enforce nesting to a FF-2048 model by 1) Random Feature Selection (Rand. FS): considering the first m dimensions of FF-2048 for NN lookup, and 2) FF+SVD: performing SVD on the FF-2048 representations at the specified representation size, 3) FF+JL: performing random projection according to the Johnson-Lindenstrauss lemma <ref type="bibr" target="#b45">[46]</ref> on the FF-2048 representations at the specified representation size. We also compared against the 1-NN accuracy of slimmable neural nets <ref type="bibr" target="#b95">[95]</ref> as an additional baseline. We observed these baseline models to perform very poorly at lower dimensions, as they were not explicitly trained to learn Matryoshka Representations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Adaptive Classification (MRL-AC)</head><p>In an attempt to use the smallest representation that works well for classification for every image in the ImageNet-1K validation set, we learned a policy to increase the representation size from m i to m i+1 using a 10K sized subset of the ImageNet-1K validation set. This policy is based on whether the prediction confidence p i using representation size m i exceeds a learned threshold t * i . If p i ? t * i , we used predictions from representation size m i otherwise, we increased to representation size m i+1 . To learn the optimal threshold t * i , we performed a grid search between 0 and 1 (100 samples). For each threshold t k , we computed the classification accuracy over our 10K image subset. We set t * i equal to the smallest threshold t k that gave the best accuracy. We use this procedure to obtain thresholds for successive models, i.e., {t * j | j ? {8, 16, 32, 64, . . . , 2048}}. To improve reliability of threshold based greedy policy, we use test time augmentation which has been used successfully in the past <ref type="bibr" target="#b77">[77]</ref>.</p><p>For inference, we used the remaining held-out 40K samples from the ImageNet-1K validation set. We began with smallest sized representation (m = 8) and compared the computed prediction confidence p 8 to learned optimal threshold t * 8 . If p 8 ? t * 8 , then we increased m = 16, and repeated this procedure until m = d = 2048. To compute the expected dimensions, we performed early stopping at m = {16, 32, 64, . . . 2048} and computed the expectation using the distribution of representation sizes. As shown in <ref type="table" target="#tab_4">Table 3</ref> and <ref type="figure">Figure 6</ref>, we observed that in expectation, we only needed a ? 37 sized representation to achieve 76.3% classification accuracy on ImageNet-1K, which was roughly 14? smaller than the FF-512 baseline. Even if we computed the expectation as a weighted average over the cumulative sum of representation sizes {8, 24, 56, . . .}, due to the nature of multiple linear heads for MRL, we ended up with an expected size of 62 that still provided a roughly 8.2? efficient representation than the FF-512 baseline. However, MRL-E alleviates this extra compute with a minimal drop in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 JFT, ALIGN and BERT</head><p>We examine the k-NN classification accuracy of learned Matryoshka Representations via ALIGN-MRL and JFT-ViT-MRL in <ref type="table" target="#tab_5">Table 4</ref>. For ALIGN <ref type="bibr" target="#b43">[44]</ref>, we observed that learning Matryoshka Representations via ALIGN-MRL improved classification accuracy at nearly all dimensions when compared to ALIGN. We observed a similar trend when training ViT-B/16 <ref type="bibr" target="#b21">[22]</ref> for JFT-300M <ref type="bibr" target="#b80">[80]</ref> classification, where learning Matryoshka Representations via MRL and MRL-E on top of JFT-ViT improved classification accuracy for nearly all dimensions, and significantly for lower ones. This demonstrates that training to learn Matryoshka Representations is feasible and extendable even for extremely large scale datasets. We also demonstrate that Matryoshka Representations are learned at interpolated dimensions for both ALIGN and JFT-ViT, as shown in <ref type="table" target="#tab_6">Table 5</ref>, despite not being trained explicitly at these dimensions. Lastly, <ref type="table" target="#tab_7">Table 6</ref> shows that MRL training leads to a increase in the cosine similarity span between positive and random image-text pairs.</p><p>We also evaluated the capability of Matryoshka Representations to extend to other natural language processing via masked language modeling (MLM) with BERT <ref type="bibr" target="#b18">[19]</ref>, whose results are tabulated in <ref type="table" target="#tab_8">Table 7</ref>. Without any hyper-parameter tuning, we observed Matryoshka Representations to be within 0.5% of FF representations for BERT MLM validation accuracy. This is a promising initial result that could help with large-scale adaptive document retrieval using BERT-MRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Image Retrieval</head><p>We evaluated the strength of Matryoshka Representations via image retrieval on ImageNet-1K (the training distribution), as well as on out-of-domain datasets ImageNetV2 and ImageNet-4K for all  For metrics, we utilized corrected mean average precision (mAP@k) <ref type="bibr" target="#b53">[53]</ref> and precision (P@k):</p><formula xml:id="formula_3">P @k = correct_pred k</formula><p>where correct_pred is the average number of retrieved NN with the correct label over the entire query set using a shortlist of length k.</p><p>We performed retrieval with FAISS <ref type="bibr" target="#b44">[45]</ref>, a library for efficient similarity search. To obtain a shortlist of k-NN, we built an index to search the database. We performed an exhaustive NN search with the L2 distance metric with faiss.IndexFlatL2, as well as an approximate NN search (ANNS) via HNSW <ref type="bibr" target="#b44">[45]</ref> with faiss.IndexHNSWFlat. We used HNSW with M = 32 unless otherwise mentioned, and henceforth referred to as HNSW32. The exact search index was moved to the GPU for fast k-NN search computation, whereas the HNSW index was kept on the CPU as it currently lacks GPU support. We show the wall clock times for building the index as well as the index size in <ref type="table" target="#tab_3">Table 20</ref>. We observed exact search to have a smaller index size which was faster to build when compared to HNSW, which trades off a larger index footprint for fast NN search (discussed in more detail in Appendix K). The database and query vectors are normalized with faiss.normalize_L2 before building the index and performing search.</p><p>Retrieval performance on ImageNet-1K, i.e. the training distribution, is shown in <ref type="table" target="#tab_9">Table 8</ref>. MRL outperforms FF models for nearly all representation size for both top-1 and mAP@10, and especially at low representation size (D s ? 32). MRL-E loses out to FF significantly only at D s = 8. This indicates that training ResNet50 models via the MRL training paradigm improves retrieval at low representation size over models explicitly trained at those representation size (FF-8...2048).</p><p>We carried out all retrieval experiments at D s ? {8, <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512</ref>, 1024, 2048}, as these were the representation sizes which were a part of the nesting_list at which losses were added during training, as seen in Algorithm 1, Appendix A. To examine whether MRL is able to learn Matryoshka Representations at dimensions in between the representation size for which it was trained, we also tabulate the performance of MRL at interpolated D s ? {12, <ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b96">96,</ref><ref type="bibr">192,</ref><ref type="bibr">384,</ref><ref type="bibr">768</ref>, 1536} as MRL-Interpolated and MRL-E-Interpolated (see <ref type="table" target="#tab_9">Table 8</ref>). We observed that performance scaled nearly monotonically between the original representation  We examined the robustness of MRL for retrieval on out-of-domain datasets ImageNetV2 and ImageNet-4K, as shown in <ref type="table" target="#tab_10">Table 9</ref> and <ref type="table" target="#tab_2">Table 10</ref> respectively. On ImageNetV2, we observed that MRL outperformed FF at all D s on top-1 Accuracy and mAP@10, and MRL-E outperformed FF at all D s except D s = 8. This demonstrates the robustness of the learned Matryoshka Representations for out-of-domain image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Adaptive Retrieval</head><p>The time complexity of retrieving a shortlist of k-NN often scales as O(d), where d =D s , for a fixed k and N . We thus will have a theoretical 256? higher cost for D s = 2048 over D s = 8. We discuss search complexity in more detail in Appendix I. In an attempt to replicate performance at higher D s while using less FLOPs, we perform adaptive retrieval via retrieving a k-NN shortlist with representation size D s , and then re-ranking the shortlist with representations of size D r . Adaptive retrieval for a shortlist length k = 200 is shown in <ref type="table" target="#tab_2">Table 11</ref> for ImageNet-1K, and in <ref type="table" target="#tab_2">Table 12</ref> for ImageNet-4K. On ImageNet-1K, we are able to achieve comparable performance to retrieval with D s = 2048 (from <ref type="table" target="#tab_9">Table 8</ref>) with D s = 16 at 128? less MFLOPs/Query (used interchangeably with MFLOPs). Similarly, on ImageNet-4K, we are able to achieve comparable performance to retrieval with D s = 2048 (from <ref type="table" target="#tab_2">Table 10</ref>) with D s = 64 on ImageNet-1K and ImageNet-4K, at 32? less MFLOPs. This demonstrates the value of intelligent routing techniques which utilize appropriately sized Matryoshka Representations for retrieval. Funnel Retrieval. We also designed a simple cascade policy which we call funnel retrieval to successively improve and refine the k-NN shortlist at increasing D s . This was an attempt to remove the dependence on manual choice of D s &amp; D r . We retrieved a shortlist at D s and then re-ranked the shortlist five times while simultaneously increasing D r (rerank cascade) and decreasing the shortlist length (shortlist cascade), which resembles a funnel structure. We tabulate the performance of funnel retrieval in various configurations in <ref type="table" target="#tab_2">Table 13</ref> on ImageNet-1K, and in <ref type="table" target="#tab_2">Table 14</ref> on ImageNet-4K. With funnel retrieval on ImageNet-1K, we were able to achieve top-1 accuracy within 0.1% of retrieval with D s = 2048 (as in <ref type="table" target="#tab_9">Table 8</ref>) with a funnel with D s = 16, with 128? less MFLOPs. Similarly, we are able to achieve equivalent top-1 accuracy within 0.15% of retrieval at D s = 2048 (as in <ref type="table" target="#tab_2">Table 10</ref>) with funnel retrieval at D s = 32 on ImageNet-4K, with 64? less MFLOPs. This demonstrates that with funnel retrieval, we can emulate the performance of retrieval with D s = 2048 with a fraction of the MFLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Few-shot and Sample Efficiency</head><p>We compared MRL, MRL-E, and FF on various benchmarks to observe the effect of representation size on sample efficiency. We used Nearest Class Means <ref type="bibr" target="#b74">[74]</ref> for classification which has been shown to be effective in the few-shot regime <ref type="bibr" target="#b12">[13]</ref>.</p><p>ImageNetV2. Representations are evaluated on ImageNetV2 with the n-shot k-way setup. Ima-geNetV2 is a dataset traditionally used to evaluate the robustness of models to natural distribution shifts. For our experiments we evaluate accuracy of the model given n examples from the Ima-geNetV2 distribution. We benchmark representations in the traditional small-scale (10-way) and  large-scale (1000-way) setting. We evaluate for n ? 1, 3, 5, 7, 9 with 9 being the maximum value for n because there are 10 images per class.</p><p>We observed that MRL had equal performance to FF across all representation sizes and shot numbers. We also found that for both MRL and FF, as the shot number decreased, the required representation size to reach optimal accuracy decreased ( <ref type="table" target="#tab_2">Table 15</ref>). For example, we observed that 1-shot performance at 32 representation size had equal accuracy to 2048 representation size.</p><p>FLUID. For the long-tailed setting we evaluated MRL on the FLUID benchmark <ref type="bibr" target="#b87">[87]</ref> which contains a mixture of pretrain and new classes. <ref type="table" target="#tab_2">Table 16</ref> shows the evaluation of the learned representation on FLUID. We observed that MRL provided up to 2% higher accuracy on novel classes in the tail of the distribution, without sacrificing accuracy on other classes. Additionally we found the accuracy between low-dimensional and high-dimensional representations was marginal for pretrain classes. For example, the 64-dimensional MRL performed ? 1% lower in accuracy compared to the 2048-dimensional counterpart on pretrain-head classes (84.46% vs 85.60%). However for noveltail classes the gap was far larger (6.22% vs 12.88%). We hypothesize that the higher-dimensional representations are required to differentiate the classes when few training examples of each are known.   shortlist length. To examine real-world performance, we tabulated wall clock search time for every query in the ImageNet-1K and ImageNet-4K validation sets over all representation sizes d in <ref type="table" target="#tab_2">Table 19</ref> for both Exact Search and HNSW32, and ablated wall clock query time over shortlist length k on the ImageNet-1K validation set in <ref type="table" target="#tab_2">Table 21</ref>. The wall clock time to build the index and the index size is also shown in <ref type="table" target="#tab_3">Table 20</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Analysis of Model Disagreement</head><p>Class Trends Does increasing representation size necessarily help improve classification performance across all classes in ImageNet-1K? We studied this question by examining trends in performance with increasing representation size from d = 8, ...2048. For MRL models, we observed that 244 classes showed a monotonic improvement in performance with increasing d, 177 classes first improved but then observed a slight dip (one or two misclassifications per class), 49 classes showed a decline first and then an improvement, and the remaining classes did not show a clear trend. When we repeated this experiment with independently trained FF models, we noticed that 950 classes did not show a clear trend. This motivated us to leverage the disagreement as well as gradual improvement of accuracy at different representation sizes by training Matryoshka Representations. <ref type="figure" target="#fig_4">Figure 12</ref> showcases the progression of relative per-class accuracy distribution compared to the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matryoshka Representation</head><p>Learning-2048 dimensional model. This also showed that some instances and classes could benefit from lower-dimensional representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion of Oracle Accuracy</head><p>Based on our observed model disagreements for different representation sizes d, we defined an optimal oracle accuracy <ref type="bibr" target="#b56">[56]</ref> for MRL. We labeled an image as correctly predicted if classification using any representation size was correct. The percentage of total samples of ImageNet-1K that were firstly correctly predicted using each representation size d is shown in <ref type="table" target="#tab_3">Table 22</ref>. This defined an upper bound on the performance of MRL models, as 18.46% of the ImageNet-1K validation set were incorrectly predicted ?d ? {8, 16, . . . , 2048}. We show the oracle performance on MRL models for ImageNet-1K/V2/A/R/Sketch datasets in <ref type="table" target="#tab_3">Table 23</ref>.</p><p>In an attempt to derive an optimal routing policy to emulate oracle accuracy, we designed the adaptive classification via cascading method as discussed in Appendix D.1. This led to an interesting   We leave the design and learning of a more optimal policy for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grad-CAM Examples</head><p>We analyzed the nature of model disagreement across representation sizes with MRL models with the help of Grad-CAM visualization <ref type="bibr" target="#b75">[75]</ref>. We observed there were certain classes in ImageNet-1K such as "tools", "vegetables" and "meat cutting knife" which were occasionally located around multiple objects and a cluttered environment. In such scenarios, we observed that smaller representation size models would often get confused due to other objects and fail to extract the object of interest which generated the correct label. We also observed a different nature  of disagreement arising when the models got confused within the same superclass. For example, ImageNet-1K has multiple "snake" classes, and models often confuse a snake image for an incorrect species of snake.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Superclass Performance</head><p>We created a 30 superclass subset of the validation set based on wordnet hierarchy <ref type="table" target="#tab_3">(Table 24</ref>) to quantify the performance of MRL model on ImageNet-1K superclasses. <ref type="table" target="#tab_3">Table 25</ref> quantifies the performance with different representation size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.1 MRL Training Paradigm</head><p>Matryoshka Representations via Finetuning. To observe if nesting can be induced in models that were not explicitly trained with nesting from scratch, we loaded a pretrained FF-2048 ResNet50 model and initialized a new MRL layer, as defined in Algorithm 2, Appendix C. We then unfroze different layers of the backbone to observe how much non-linearity in the form of unfrozen conv layers needed to be present to enforce nesting into a pretrained FF model. A description of these layers can be found in the ResNet50 architecture <ref type="bibr" target="#b26">[27]</ref>. All models were finetuned with the FFCV pipeline, with same training configuration as in the end-to-end training aside from changing lr = 0.1 and epochs = 10. We observed that finetuning the linear layer alone was insufficient to learn Matryoshka Representations at lower dimensionalities. Adding more and more non-linear conv+ReLU layers steadily improved classification accuracy of d = 8 from 5% to 60% after finetuning, which was only 6% less than training MRL end-to-end for 40 epochs. This difference was successively less pronounced as we increased dimensionality past d = 64, to within 1.5% for all larger dimensionalities. The full results of this ablation can be seen in <ref type="table" target="#tab_3">Table 26</ref>.</p><p>Relative Importance. We performed an ablation of MRL over the relative importance, c m , of different nesting dimensions m ? M, as defined in Sec. 3. In an attempt to improve performance at lower dimensionalities, we boosted the relative importance c m of training loss at lower dimensions as in Eq. 1 with two models, MRL-8boost and MRL-8+16boost. The MRL-8boost model had c m?M = [2, 1, 1, 1, 1, 1, 1, 1, 1] and the MRL-8+16boost model had c m?M = [2, 1.5, 1, 1, 1, 1, 1, 1, 1]. The relative importance list c m?M had a 1-to-1 correspondence with nesting dimension set M. In <ref type="table" target="#tab_3">Table 27</ref>, we observed that MRL-8boost improves top-1 accuracy by 3% at d = 8, and also improves top-1 accuracy of all representation scales from 16 to 256 over MRL, while only hurting the performance at 512 to 2048 representation scales by a maximum of 0.1%. This suggests that the relative importance c m can be tuned/set for optimal accuracy for all m ? M, but we leave this extension for future work.   Matryoshka Representations at Arbitrary Granularities. To train MRL, we used nested dimensions at logarithmic granularities M = {8, 16, . . . , 1024, 2048} as detailed in Section 3. We made this choice for two empirically-driven reasons: a) The accuracy improvement with increasing representation size was more logarithmic than linear (as shown by FF models in <ref type="figure" target="#fig_4">Figure 2</ref>). This indicated that optimizing for granularities increasing in a non-logarithmic fashion would be sub-optimal both for maximum performance and expected efficiency; b) If we have m arbitrary granularities, the expected cost of the linear classifier to train MRL scales as O(L * (m 2 )) while logarithmic granularities result in O(L * 2log(d)) space and compute costs.</p><p>To demonstrate this effect, we learned Matryoshka Representations with uniform (MRL-Uniform) nesting dimensions m ? M = {8, 212, 416, 620, 824, 1028, 1232, 1436, 1640, 1844, 2048}. We evaluated this model at the standard (MRL-log) dimensions m ? M = {8, 16, 32, 64, 128, 256, 512, 1024, 2048} for ease of comparison to reported numbers using 1-NN accuracy (%). As shown in <ref type="table" target="#tab_3">Table 29</ref>, we observed that while performance interpolated, MRL-Uniform suffered at low dimensions as the logarithmic spacing of MRL-log resulted in tighter packing of information in these initial dimensions. The higher nesting dimensions of MRL-Uniform did not help in significant accuracy improvement due to accuracy saturation, which is often logarithmic in representation size as shown by FF models. Note that the slight improvement at dimensions higher than 512 for MRL-Uniform is due to multiple granularities around them compared to just three for MRL-log, which are not useful in practice for efficiency.</p><p>Lower Dimensionality. We experimented with training MRL with smaller nesting dimension than m = 8, as shown in <ref type="table" target="#tab_3">Table 28</ref>, with two models: MRL-4 and MRL-6. We found that using lower than 8-dimensions to train MRL, i.e. m 0 ? {4, 6} for MRL-4 and MRL-6 respectively, did not affect the top-1 accuracy of other granularities significantly. However, granularities smaller than 8-dimensions had very low accuracy and were often unusable for deployment along with additional training difficulty. We also observed a small dip in accuracy at higher dimensions which we attribute to the joint loss that now also included the harder optimization of the smallest dimension. Lastly, we hypothesize the dimensionality of 8 is an empirically validated design choice due to the considerable accuracy it provided along with the ease of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.2 Retrieval</head><p>Adaptive Retrieval. To examine the effect of increasing shortlist lengths on search time, we performed a reranking ablation over shortlist lengths for D s = 16 and D r = 2048 over ImageNet-1K in <ref type="table" target="#tab_4">Table 30</ref>, and over ImageNet-4K in <ref type="table" target="#tab_2">Table 31</ref>. We observed that using a larger shortlist k saturated ImageNet-1K performance at k=200. But using larger shortlists until k = 2048, the maximum value    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e h 9 h k + p e B k d s P Y 6 v + r 4 r O N m x Y L Y = " &gt; A A A B 7 n i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B U 9 k V o R 6 L X j x W s B / Q L i W b Z t v Q b B K S r F C W / g g v H h T x 6 u / x 5 r 8 x 2 + 5 B W x 8 M P N 6 b Y W Z e p D g z 1 v e / v d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p G J l q Q t t E c q l 7 E T a U M 0 H b l l l O e 0 p T n E S c d q P p X e 5 3 n 6 g 2 T I p H O 1 M 0 T P B Y s J g R b J 3 U H U j F U 1 M Z V m t + 3 V 8 A r Z O g I D U o 0 B p W v w Y j S d K E C k s 4 N q Y f + M q G G d a W E U 7 n l U F q q M J k i s e 0 7 6 j A C T V h t j h 3 j i 6 c M k K x 1 K 6 E R Q v 1 9 0 S G E 2 N m S e Q 6 E 2 w n Z t X L x f + 8 f m r j m z B j Q q W W C r J c F K c c W Y n y 3 9 G I a U o s n z m C i W b u V k Q m W G N i X U J 5 C M H q y + u k c 1 U P / H r w c F 1 r 3 h Z x l O E M z u E S A m h A E + 6 h B W 0 g M I V n e I U 3 T 3 k v 3 r v 3 s W w t e c X M K f y B 9 / k D B r K P W Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s z v l z T D Y Q E H 5 M / G S B t p 0 D 9 X a T i U = " &gt; A A A B / H i c b V D L S s N A F L 2 p r 1 p f 0 S 7 d D B a h b k o i g u K q 6 M a F i w r 2 A W 0 I k 8 m 0 H T p 5 M D M R Y q i / 4 s a F I m 7 9 E H f + j Z M 2 C 2 0 9 M H A 4 5 1 7 u m e P F n E l l W d 9 G a W V 1 b X 2 j v F n Z 2 t 7 Z 3 T P 3 D z o y S g S h b R L x S P Q 8 L C l n I W 0 r p j j t x Y L i w O O 0 6 0 2 u c 7 / 7 Q I V k U X i v 0 p g 6 A R 6 F b M g I V l p y z e o g w G p M M E e 3 9 U c 3 s y / 9 6 Y l r 1 q y G N Q N a J n Z B a l C g 5 Z p f A z 8 i S U B D R T i W s m 9 b s X I y L B Q j n E 4 r g 0 T S G J M J H t G + p i E O q H S y W f g p O t a K j 4 a R 0 C 9 U a K b + 3 s h w I G U a e H o y j y o X v V z 8 z + s n a n j h Z C y ME 0 V D M j 8 0 T D h S E c q b Q D 4 T l C i e a o K J Y D o r I m M s M F G 6 r 4 o u w V 7 8 8 j L p n D Z s q 2 H f n d W a V 0 U d Z T i E I 6 i D D e f Q h B t o Q R s I p P A M r / B m P B k v x r v x M R 8 t G c V O F f 7 A + P w B d 6 W T / A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E D z x x Y G d F H E 0 O T / 8 r 1 y z v d u i K k Y = " &gt; A A A B / n i c b V D L S s N A F L 2 p r 1 p f U X H l Z r A I d V O T I i i u i m 5 c u K h g H 9 C G M J l M 2 6 G T B z M T o Y a C v + L G h S J u / Q 5 3 / o 2 T N g t t P T B w O O d e 7 p n j x Z x J Z V n f R m F p e W V 1 r b h e 2 t j c 2 t 4 x d / d a M k o E o U 0 S 8 U h 0 P C w p Z y F t K q Y 4 7 c S C 4 s D j t O 2 N r j O / / U C F Z F F 4 r 8 Y x d Q I 8 C F m f E a y 0 5 J o H v Q C r I c E c 3 V Y e 3 d S + 9 E 9 r k x P X L F t V a w q 0 S O y c l C F H w z W / e n 5 E k o C G i n A s Z d e 2 Y u W k W C h G O J 2 U e o m k M S Y j P K B d T U M c U O m k 0 / g T d K w V H / U j o V + o 0 F T 9 v Z H i Q M px 4 O n J L K y c 9 z L x P 6 + b q P 6 F k 7 I w T h Q N y e x Q P + F I R S j r A v l M U K L 4 W B N M B N N Z E R l i g Y n S j Z V 0 C f b 8 l x d J q 1 a 1 r a p 9 d 1 a u X + V 1 F O E Q j q A C N p x D H W 6 g A U 0 g k M I z v M K b 8 W S 8 G O / G x 2 y 0 Y O Q 7 + / A H x u c P Y d O U c Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G l g g P M D 8 z 4 l B + h i I v M 4 R 0 N c w G w o = " &gt; A A A B / n i c b V D L S s N A F L 2 p r 1 p f U X H l Z r A I d V M T K S i u i m 5 c u K h g H 9 C G M J l M 2 6 G T B z M T o Y a C v + L G h S J u / Q 5 3 / o 2 T N g t t P T B w O O d e 7 p n j x Z x J Z V n f R m F p e W V 1 r b h e 2 t j c 2 t 4 x d / d a M k o E o U 0 S 8 U h 0 P C w p Z y F t K q Y 4 7 c S C 4 s D j t O 2 N r j O / / U C F Z F F 4 r 8 Y x d Q I 8 C F m f E a y 0 5 J o H v Q C r I c E c 3 V Y e 3 d S + 9 E 9 r k x P X L F t V a w q 0 S O y c l C F H w z W / e n 5 E k o C G i n A s Z d e 2 Y u W k W C h G O J 2 U e o m k M S Y j P K B d T U M c U O m k 0 / g T d K w V H / U j o V + o 0 F T 9 v Z H i Q M p x 4 O n J L K y c 9 z L x P 6 + b q P 6 F k 7 I w T h Q N y e x Q P + F I R S j r A v l M U K L 4 W B N M B N N Z E R l i g Y n S j Z V 0 C f b 8 l x d J 6 6 x q W 1 X 7 r l a u X + V 1 F O E Q j q A C N p x D H W 6 g A U 0 g k M I z v M K b 8 W S 8 G O / G x 2 y 0 Y O Q 7 + / A H x u c P Z N + U c w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t E t I n X K d 9 m q m i / o F c t u / V j S e + v 0 = " &gt; A A A B / n i c b V D L S s N A F L 2 p r 1 p f U X H l Z r A I d V M T E S y u i m 5 c u K h g H 9 C G M J l M 2 6 G T B z M T o Y a C v + L G h S J u / Q 5 3 / o 2 T N g t t P T B w O O d e 7 p n j x Z x J Z V n f R m F p e W V 1 r b h e 2 t j c 2 t 4 x d / d a M k o E o U 0 S 8 U h 0 P C w p Z y F t K q Y 4 7 c S C 4 s D j t O 2 N r j O / / U C F Z F F 4 r 8 Y x d Q I 8 C F m f E a y 0 5 J o H v Q C r I c E c 3 V Y e 3 d S + 9 E 9 r k x P X L F t V a w q 0 S O y c l C F H w z W / e n 5 E k o C G i n A s Z d e 2 Y u W k W C h G O J 2 U e o m k M S Y j P K B d T U M c U O m k 0 / g T d K w V H / U j o V + o 0 F T 9 v Z H i Q M p x 4 O n J L K y c 9 z L x P 6 + b q H 7 N S V k Y J 4 q G Z H a o n 3 C k I p R 1 g X w m K F F 8 r A k m g u m s i A y x w E T p x k q 6 B H v + y 4 u k d V a 1 r a p 9 d 1 6 u X + V 1 F O E Q j q A C N l x A H W 6 g A U 0 g k M I z v M K b 8 W S 8 G O / G x 2 y 0 Y O Q 7 + / A H x u c P a v e U d w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d y C s Z / n y 7 r Q z K c X z t j E l U t g 2 Q P g = " &gt; A A A B / 3 i c b V D L S g M x F M 3 U V 6 2 v U c G N m 2 A R 6 q Z O R F R c F d 2 4 c F H B P q A d h k w m b U M z m S H J C H X s w l 9 x 4 0 I R t / 6 G O / / G T D s L r R 4 I H M 6 5 l 3 t y / J g z p R 3 n y y r M z S 8 s L h W X S y u r a + s b 9 u Z W U 0 W J J L R B I h 7 J t o 8 V 5 U z Q h m a a 0 3 Y s K Q 5 9 T l v + 8 D L z W 3 d U K h a J W z 2 K q R v i v m A 9 R r A 2 k m f v d E O s B w R z e F 2 5 9 1 J 0 H h y i k / G B Z 5 e d q j M B / E t Q T s o g R 9 2 z P 7 t B R J K Q C k 0 4 V q q D n F i 7 K Z a a E U 7 H p W 6 i a I z J E P d p x 1 C B Q 6 r c d J J / D P e N E s B e J M 0 T G k 7 U n x s p D p U a h b 6 Z z N K q W S 8 T / / M 6 i e 6 d u S k T c a K p I N N D v Y R D H c G s D B g w S Y n m I 0 M w k c x k h W S A J S b a V F Y y J a D Z L / 8 l z a M q c q r o 5 r h c u 8 j r K I J d s A c q A I F T U A N X o A 4 a g I A H 8 A R e w K v 1 a D 1 b b 9 b 7 d L R g 5 T v b 4 B e s j 2 / e C Z S w &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O P H M 4 A C s G r 0 V I 7 q M p D g o N + t 2 I C I = " &gt; A A A B 9 X i c b V D L S g M x F L 3 x W e u r 6 t J N s A h 1 U 2 Z E 0 G X R j Q s X F e w D 2 r F k 0 k w b m s k M S U a p Q / / D j Q t F 3 P o v 7 v w b M + 0 s t P V A 4 H D O v d y T 4 8 e C a + M 4 3 2 h p e W V 1 b b 2 w U d z c 2 t 7 Z L e 3 t N 3 W U K M o a N B K R a v t E M 8 E l a x h u B G v H i p H Q F 6 z l j 6 4 y v / X A l O a R v D P j m H k h G U g e c E q M l e 6 7 I T H D Q J E R v q k 8 n f R K Z a f q T I E X i Z u T M u S o 9 0 p f 3 X 5 E k 5 B J Q w X R u u M 6 s f F S o g y n g k 2 K 3 U S z m N A R G b C O p Z K E T H v p N P U E H 1 u l j 4 N I 2 S c N n q q / N 1 I S a j 0 O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>MRL differentiates itself in formulation by optimizing only for O(log(d)) nesting dimensions instead of O(d). Despite this, MRL diffuses information to intermediate dimensions interpolating between the optimized Matryoshka Representation sizes accurately (Figure 5); making web-scale feasible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3</head><label>3</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>For</head><label></label><figDesc>d ? N, consider a set M ? [d] of representation sizes. For a datapoint x in the input domain X , our goal is to learn a d-dimensional representation vector z ? R d . For every m ? M, Matryoshka Representation Learning (MRL) enables each of the first m dimensions of the embedding vector, z 1:m ? R m to be independently capable of being a transferable and general purpose</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2</head><label>2</label><figDesc>compares the linear classification accuracy of ResNet50 models trained and evaluated on ImageNet-1K. ResNet50-MRL model is at least as accurate as each FF model at every representation size in M while MRL-E is within 1% starting from 16-dim. Similarly,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>ImageNet-1K 1-NN accuracy for ViT-B/16 models trained on JFT-300M &amp; as part of ALIGN. MRL scales seamlessly to web-scale with minimal training overhead.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Despite optimizing MRL only for O(log(d)) dimensions for ResNet50 and ViT-B/16 models; the accuracy in the intermediate dimensions shows interpolating behaviour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>mAP@10 for Image Retrieval on ImageNet-1K with ResNet50. MRL consistently produces better retrieval performance over the baselines across all the representation sizes. an extensive set of metrics spanning mAP@k and P@k for k = {10, 25, 50, 100} and real-world wall-clock times for exact search and HNSW. See Appendices E and F for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7</head><label>7</label><figDesc>compares the mAP@10 performance of ResNet50 representations on ImageNet-1K across dimensionalities for MRL, MRL-E, FF, slimmable networks along with post-hoc compression of vectors using SVD and random feature selection. Matryoshka Representations are often the most accurate while being up to 3% better than the FF baselines. Similar to classification, post-hoc compression and slimmable network baselines suffer from significant drop-off in retrieval mAP@10 with ? 256 dimensions. Appendix E discusses the mAP@10 of the same models on ImageNet-4K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>real-world speed-up 32x theoretical speed-up (a) ImageNet-1K (b) ImageNet-4K The trade-off between mAP@10 vs MFLOPs/Query for Adaptive Retrieval (AR) on ImageNet-1K (left) and ImageNet-4K (right). Every combination of D s &amp; D r falls above the Pareto line (orange dots) of single-shot retrieval with a fixed representation size while having configurations that are as accurate while being up to 14? faster in real-world deployment. Funnel retrieval is almost as accurate as the baseline while alleviating some of the parameter choices of Adaptive Retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Grad-CAM [75] progression of predictions in MRL model across 8, 16, 32 and 2048 dimensions. (a) 8-dimensional representation confuses due to presence of other relevant objects (with a larger field of view) in the scene and predicts "shower cap" ; (b) 8-dim model confuses within the same super-class of "boa" ; (c) 8 and 16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>31-way ImageNet-1K superclass classification across representation size for MRL &amp; FF models showing the capture of underlying hierarchy through tight information bottlenecks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Diverse per-superclass accuracy trends across representation sizes for ResNet50-MRL on ImageNet-1K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Checklist 1 .</head><label>1</label><figDesc>For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section 6 (c) Did you discuss any potential negative societal impacts of your work? [N/A] Our workdoes not have any additional negative societal impact on top of the existing impact of representation learning. However, a study on the trade-off between representation size and the tendency to encode biases is an interesting future direction along the lines of existing literature<ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. A part of this is already presented in Section 5.(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See supplemental material and Appendix A. All the code and public models will be open sourced. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 4 and Appendix C. (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] We benchmarked on large-scale datasets like ImageNet-1K, JFT-300M and ALIGN data with models like ResNet and ViT making it extremely expensive to run things multiple times. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix C and Appendix I. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [No] All the non-proprietary datasets and code used are public under MIT, BSD or CC licenses. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We created a new subset of ImageNet-21K for downstream evaluation of retrieval performance at scale. See Section 4.3 and Appendix B (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 12 :</head><label>12</label><figDesc>Progression of relative per-class accuracy vs MRL-2048. As the dimensionality increases, the spread shrinks while the class marked (x) (Madagascar cat) loses accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>NN) accuracy. Experiments show that MRL models remove the dependence on |M| resource-intensive independently trained models for the coarse-to-fine representations while being as accurate. Lastly, we show that despite optimizing only for |M| dimensions, MRL models diffuse the information, in an interpolative fashion, across all the d dimensions providing the finest granularity required for adaptive deployment.</figDesc><table><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top-1 Accuracy (%)</cell><cell>50 60 70</cell><cell></cell><cell></cell><cell></cell><cell>MRL MRL-E FF SVD Slim. Net Rand. LP</cell><cell>1-NN Accuracy (%)</cell><cell>50 60 70</cell><cell></cell><cell></cell><cell>MRL MRL-E FF SVD Slim. Net Rand. FS</cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell></row><row><cell></cell><cell>8</cell><cell>1 6</cell><cell>3 2</cell><cell>6 4</cell><cell>1 2 8 2 5 6 5 1 2 1 0 2 4 2 0 4 8</cell><cell></cell><cell>8</cell><cell>1 6</cell><cell>3 2</cell><cell>6 4</cell><cell>1 2 8 2 5 6 5 1 2 1 0 2 4 2 0 4 8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Representation Size</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Representation Size</cell></row><row><cell cols="6">Figure 2: ImageNet-1K linear classification ac-</cell><cell cols="5">Figure 3: ImageNet-1K 1-NN accuracy of</cell></row><row><cell cols="6">curacy of ResNet50 models. MRL is as accurate</cell><cell cols="5">ResNet50 models measuring the representation</cell></row><row><cell cols="6">as the independently trained FF models for every</cell><cell cols="5">quality for downstream task. MRL outperforms</cell></row><row><cell cols="4">representation size.</cell><cell></cell><cell></cell><cell cols="5">all the baselines across all representation sizes.</cell></row><row><cell cols="11">sentation while ViT-B/16 and BERT-Base output 768-dimensional embeddings for each data point.</cell></row><row><cell cols="11">We use M = {8, 16, 32, 64, 128, 256, 512, 1024, 2048} and M = {12, 24, 48, 96, 192, 384, 768} as</cell></row><row><cell cols="11">the explicitly optimized nested dimensions respectively. Lastly, we extensively compare the MRL</cell></row><row><cell cols="11">and MRL-E models to independently trained low-dimensional (fixed feature) representations (FF),</cell></row><row><cell cols="11">dimensionality reduction (SVD), sub-net method (slimmable networks [95]) and randomly selected</cell></row><row><cell cols="6">features of the highest capacity FF model.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">In section 4.2, we evaluate the quality and capacity of the learned representations through linear</cell></row><row><cell cols="8">classification/probe (LP) and 1-nearest neighbour (1-</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="10">adapt Matryoshka Representation Learning (MRL) to various representation learning setups</cell></row><row><cell cols="11">(a) Supervised learning for vision: ResNet50 [27] on ImageNet-1K [71] and ViT-B/16 [22] on</cell></row><row><cell cols="11">JFT-300M [80], (b) Contrastive learning for vision + language: ALIGN model with ViT-B/16 vision</cell></row><row><cell cols="11">encoder and BERT language encoder on ALIGN data [44] and (c) Masked language modelling:</cell></row><row><cell cols="11">BERT [19] on English Wikipedia and BooksCorpus [97]. Please refer to Appendices B and C for</cell></row><row><cell cols="11">details regarding the model architectures, datasets and training specifics.</cell></row><row><cell cols="11">We do not search for best hyper-parameters for all MRL experiments but use the same hyper-</cell></row><row><cell cols="11">parameters as the independently trained baselines. ResNet50 outputs a 2048-dimensional repre-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Representation Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 4.2 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 4.2.1 Adaptive Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 4.3 Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 4.3.1 Adaptive Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Adaptive Classification (MRL-AC) . . . . . . . . . . . . . . . . . . . . . . . . . 21 D.2 JFT, ALIGN and BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 We use Alg 1 and 2 provided below to train supervised ResNet50-MRL models on ImageNet-1K. We provide this code as a template to extend MRL to any domain. Pytorch code for Matryoshka Cross-Entropy Loss Pytorch code for MRL Linear Layer 1K [71] contains 1,281,167 labeled train images, and 50,000 labelled validation images across 1,000 classes. The images were transformed with standard procedures detailed by FFCV [54]. ImageNet-4K dataset was constructed by selecting 4,202 classes, non-overlapping with ImageNet-1K, from ImageNet-21K [16] with 1,050 or more examples. The train set contains 1,000 examples and the query/validation set contains 50 examples per class totalling to ?4.2M and ?200K respectively.We will release the list of images curated together to construct ImageNet-4K.</figDesc><table><row><cell>Contents B Datasets</cell><cell>(MRL)</cell><cell></cell></row><row><cell cols="3">1 Introduction 2 Related Work 3 Matryoshka Representation Learning 4 Applications Algorithm 1 class Matryoshka_CE_Loss(nn.Module): def __init__(self, relative_importance, **kwargs): ImageNet-JFT-300M [80] is a large-scale multi-label dataset with 300M images labelled across 18,291 cate-1 3 3 4 super(Matryoshka_CE_Loss, self).__init__() gories. self.criterion = nn.CrossEntropyLoss(**kwargs) self.relative_importance = relative_importance # usually set to all ones def forward(self, output, target): loss=0 for i in range(len(output)): loss+= self.relative_importance[i] * self.criterion(output[ i], target) ALIGN [44] utilizes a large scale noisy image-text dataset containing 1.8B image-text pairs. ImageNet Robustness Datasets We experimented on the following datasets to examine the robust-ness of MRL models: ImageNetV2 [68] is a collection of 10K images sampled a decade after the original construction of ImageNet [16]. ImageNetV2 contains 10 examples each from the 1,000 classes of ImageNet-1K. 4.1 5 Further Analysis and Ablations return loss ImageNet-A [33] contains 7.5K real-world adversarially filtered images from 200 ImageNet-8 1K classes.</cell></row><row><cell cols="2">5.1 6 Discussion and Conclusions A Code for Matryoshka Representation Learning B Datasets Algorithm 2 1) (MRL) self.num_classes=num_classes</cell><cell>10 19 20</cell></row><row><cell cols="2">self.is_efficient=efficient # flag for MRL-E</cell><cell></cell></row><row><cell cols="2">C Matryoshka Representation Learning Model Training</cell><cell>20</cell></row><row><cell cols="2">if not is_efficient:</cell><cell></cell></row><row><cell cols="2">D Classification Results for i, num_feat in enumerate(self.nesting_list): setattr(self, f"nesting_classifier_{i}", nn.Linear(</cell><cell>21</cell></row><row><cell cols="2">num_feat, self.num_classes, **kwargs)) setattr(self, "nesting_classifier_0", nn.Linear(self. else: nesting_list[-1], self.num_classes, **kwargs)) # D.1 E Image Retrieval Instantiating one nn.Linear layer for MRL-E</cell><cell>22</cell></row><row><cell>def forward(self, x):</cell><cell></cell><cell></cell></row><row><cell cols="2">F Adaptive Retrieval nesting_logits = () for i, num_feat in enumerate(self.nesting_list):</cell><cell>24</cell></row><row><cell cols="2">if(self.is_efficient):</cell><cell></cell></row><row><cell>G Few-shot and Sample Efficiency</cell><cell cols="2">efficient_logit = torch.matmul(x[:, :num_feat], 25 (self.nesting_classifier_0.weight[:, :</cell></row><row><cell></cell><cell>num_feat]).t())</cell><cell></cell></row><row><cell>H Robustness Experiments else:</cell><cell>nesting_logits.append(getattr(self, f"</cell><cell>27</cell></row><row><cell></cell><cell cols="2">nesting_classifier_{i}")(x[:, :num_feat]))</cell></row><row><cell>I In Practice Costs</cell><cell></cell><cell>27</cell></row><row><cell cols="2">if(self.is_efficient):</cell><cell></cell></row><row><cell cols="2">J Analysis of Model Disagreement nesting_logits.append(efficient_logit)</cell><cell>29</cell></row><row><cell cols="2">return nesting_logits</cell><cell></cell></row><row><cell>K Ablation Studies</cell><cell></cell><cell>32</cell></row></table><note>K.1 MRL Training Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 K.2 Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 A Code for Matryoshka Representation Learningclass MRL_Linear_Layer(nn.Module): def __init__(self, nesting_list: List, num_classes=1000, efficient= False, **kwargs): super(MRL_Linear_Layer, self).__init__() self.nesting_list=nesting_list # set of m in M (Eq.ImageNet-R [32] contains 30K artistic image renditions for 200 of the original ImageNet-1K classes. ImageNet-Sketch [89] contains 50K sketches, evenly distributed over all 1,000 ImageNet-1K classes. ObjectNet [2] contains 50K images across 313 object classes, each containing ?160 images each.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Top-1 classification accuracy (%) for ResNet50 MRL and baseline models on ImageNet-1K.</figDesc><table><row><cell cols="3">Rep. Size Rand. LP SVD</cell><cell>FF</cell><cell cols="3">Slim. Net MRL MRL-E</cell></row><row><cell>8</cell><cell>4.56</cell><cell cols="2">2.34 65.29</cell><cell>0.42</cell><cell>66.63</cell><cell>56.66</cell></row><row><cell>16</cell><cell>11.29</cell><cell cols="2">7.17 72.85</cell><cell>0.96</cell><cell>73.53</cell><cell>71.94</cell></row><row><cell>32</cell><cell>27.21</cell><cell cols="2">20.46 74.60</cell><cell>2.27</cell><cell>75.03</cell><cell>74.48</cell></row><row><cell>64</cell><cell>49.47</cell><cell cols="2">48.10 75.27</cell><cell>5.59</cell><cell>75.82</cell><cell>75.35</cell></row><row><cell>128</cell><cell>65.70</cell><cell cols="2">67.24 75.29</cell><cell>14.15</cell><cell>76.30</cell><cell>75.80</cell></row><row><cell>256</cell><cell>72.43</cell><cell cols="2">74.59 75.71</cell><cell>38.42</cell><cell>76.47</cell><cell>76.22</cell></row><row><cell>512</cell><cell>74.94</cell><cell cols="2">76.78 76.18</cell><cell>69.80</cell><cell>76.65</cell><cell>76.36</cell></row><row><cell>1024</cell><cell>76.10</cell><cell cols="2">76.87 76.63</cell><cell>74.61</cell><cell>76.76</cell><cell>76.48</cell></row><row><cell>2048</cell><cell>76.87</cell><cell>-</cell><cell>76.87</cell><cell>76.26</cell><cell>76.80</cell><cell>76.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Rep. Size Rand. FS SVD</cell><cell>JL</cell><cell>FF</cell><cell cols="3">Slimmable MRL MRL-E</cell></row><row><cell>8</cell><cell>2.36</cell><cell cols="3">19.14 0.11 58.93</cell><cell>1.00</cell><cell>62.19</cell><cell>57.45</cell></row><row><cell>16</cell><cell>12.06</cell><cell cols="3">46.02 0.09 66.77</cell><cell>5.12</cell><cell>67.91</cell><cell>67.05</cell></row><row><cell>32</cell><cell>32.91</cell><cell cols="3">60.78 0.06 68.84</cell><cell>16.95</cell><cell>69.46</cell><cell>68.6</cell></row><row><cell>64</cell><cell>49.91</cell><cell cols="3">67.04 0.05 69.41</cell><cell>35.60</cell><cell>70.17</cell><cell>69.61</cell></row><row><cell>128</cell><cell>60.91</cell><cell cols="3">69.63 0.06 69.35</cell><cell>51.16</cell><cell>70.52</cell><cell>70.12</cell></row><row><cell>256</cell><cell>65.75</cell><cell cols="3">70.67 0.04 69.72</cell><cell>60.61</cell><cell>70.62</cell><cell>70.36</cell></row><row><cell>512</cell><cell>68.77</cell><cell cols="3">71.06 0.03 70.18</cell><cell>65.82</cell><cell>70.82</cell><cell>70.74</cell></row><row><cell>1024</cell><cell>70.41</cell><cell>71.22</cell><cell>-</cell><cell>70.34</cell><cell>67.19</cell><cell>70.89</cell><cell>71.07</cell></row><row><cell>2048</cell><cell>71.19</cell><cell>71.21</cell><cell>-</cell><cell>71.19</cell><cell>66.10</cell><cell>70.97</cell><cell>71.21</cell></row></table><note>1-NN accuracy (%) on ImageNet-1K for various ResNet50 models.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Threshold-based adaptive classification performance of ResNet50 MRL on a 40K sized held-out subset of the ImageNet-1K validation set. Results are averaged over 30 random held-out subsets.</figDesc><table><row><cell>Expected Rep. Size</cell><cell>Accuracy</cell></row><row><cell>13.43 ? 0.81</cell><cell>73.79 ? 0.10</cell></row><row><cell>18.32 ? 1.36</cell><cell>75.25 ? 0.11</cell></row><row><cell>25.87 ? 2.41</cell><cell>76.05 ? 0.15</cell></row><row><cell>36.26 ? 4.78</cell><cell>76.28 ? 0.16</cell></row><row><cell>48.00 ? 8.24</cell><cell>76.43 ? 0.18</cell></row><row><cell>64.39 ? 12.55</cell><cell>76.53 ? 0.19</cell></row><row><cell>90.22 ? 20.88</cell><cell>76.55 ? 0.20</cell></row><row><cell>118.85 ? 33.37</cell><cell>76.56 ? 0.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>ViT-B/16 and ViT-B/16-MRL top-1 and top-5 k-NN accuracy (%) for ALIGN and JFT. Top-1 entries where MRL-E and MRL outperform baselines are bolded for both ALIGN and JFT-ViT. 28.05 43.57 67.36 27.07 48.57 53.61 75.30 51.54 73.94 24 33.35 55.58 56.44 78.19 48.64 70.20 62.80 81.51 62.40 81.36 48 51.32 73.15 62.33 82.30 63.58 81.80 67.24 84.37 66.89 83.80 96 61.82 81.97 65.72 84.61 68.56 85.13 69.74 85.86 68.80 85.13 192 66.71 85.27 67.00 85.36 71.32 86.21 71.34 86.62 70.41 86.01 384 67.65 85.70 67.70 85.73 71.67 86.98 71.73 87.08 71.18 86.46 768 68.00 86.10 67.85 85.85 72.10 87.20 71.85 86.92 71.31 86.62</figDesc><table><row><cell>Rep. Size</cell><cell>ALIGN</cell><cell>ALIGN-MRL</cell><cell>JFT-ViT</cell><cell cols="2">JFT-ViT-MRL JFT-ViT-MRL-E</cell></row><row><cell></cell><cell cols="4">Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1</cell><cell>Top-5</cell></row><row><cell>12</cell><cell>11.90</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Examining top-1 and top-5 k-NN accuracy (%) at interpolated hidden dimensions for ALIGN and JFT. This indicates that MRL is able to scale classification accuracy as hidden dimensions increase even at dimensions that were not explicitly considered during training.</figDesc><table><row><cell>Interpolated</cell><cell cols="2">ALIGN-MRL JFT-ViT-MRL</cell></row><row><cell>Rep. Size</cell><cell cols="2">Top-1 Top-5 Top-1 Top-5</cell></row><row><cell>16</cell><cell>49.06 72.26 58.35</cell><cell>78.55</cell></row><row><cell>32</cell><cell>58.64 79.96 64.98</cell><cell>82.89</cell></row><row><cell>64</cell><cell>63.90 83.39 68.19</cell><cell>84.85</cell></row><row><cell>128</cell><cell>66.63 85.00 70.35</cell><cell>86.24</cell></row><row><cell>256</cell><cell>67.10 85.30 71.57</cell><cell>86.77</cell></row><row><cell>512</cell><cell>67.64 85.72 71.55</cell><cell>86.67</cell></row></table><note>MRL ResNet50 models. We generated the database and query sets, containing N and Q samples respectively, with a standard PyTorch [63] forward pass on each dataset. We specify the representation size at which we retrieve a shortlist of k-nearest neighbors (k-NN) by D s . The database is a thus a [N , D s ] array, the query set is a [Q, D s ] array, and the neighbors set is a [Q, k] array.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Cosine similarity between embeddings</figDesc><table><row><cell>Avg. Cosine Similarity</cell><cell cols="2">ALIGN ALIGN-MRL</cell></row><row><cell>Positive Text to Image</cell><cell>0.27</cell><cell>0.49</cell></row><row><cell>Random Text to Image</cell><cell>8e-3</cell><cell>-4e-03</cell></row><row><cell>Random Image to Image</cell><cell>0.10</cell><cell>0.08</cell></row><row><cell>Random Text to Text</cell><cell>0.22</cell><cell>0.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Masked Language Modelling (MLM) accuracy(%) of FF and MRL models on the validation set.</figDesc><table><row><cell cols="3">Rep. Size BERT-FF BERT-MRL</cell></row><row><cell>12</cell><cell>60.12</cell><cell>59.92</cell></row><row><cell>24</cell><cell>62.49</cell><cell>62.05</cell></row><row><cell>48</cell><cell>63.85</cell><cell>63.40</cell></row><row><cell>96</cell><cell>64.32</cell><cell>64.15</cell></row><row><cell>192</cell><cell>64.70</cell><cell>64.58</cell></row><row><cell>384</cell><cell>65.03</cell><cell>64.81</cell></row><row><cell>768</cell><cell>65.54</cell><cell>65.00</cell></row></table><note>size and the interpolated representation size as we increase D s , which demonstrates that MRL is able to learn Matryoshka Representations at nearly all representation size m ? [8, 2048] despite optimizing only for |M| nested representation sizes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Retrieve a shortlist of 200-NN with D s sized representations on ImageNet-1K via exact search with L2 distance metric. Top-1 and mAP@10 entries (%) where MRL-E and MRL outperform FF at their respective representation sizes are bolded.</figDesc><table><row><cell>Model</cell><cell>Ds</cell><cell cols="11">MFlops Top-1 Top-5 Top-10 mAP@10 mAP@25 mAP@50 mAP@100 P@10 P@25 P@50 P@100</cell></row><row><cell></cell><cell>8</cell><cell>10</cell><cell cols="2">58.93 75.76</cell><cell>80.25</cell><cell>53.42</cell><cell>52.29</cell><cell>51.84</cell><cell>51.57</cell><cell cols="3">59.32 59.28 59.25</cell><cell>59.21</cell></row><row><cell></cell><cell>16</cell><cell>20</cell><cell cols="2">66.77 80.88</cell><cell>84.40</cell><cell>61.63</cell><cell>60.51</cell><cell>59.98</cell><cell>59.62</cell><cell cols="3">66.76 66.58 66.43</cell><cell>66.27</cell></row><row><cell></cell><cell>32</cell><cell>41</cell><cell cols="2">68.84 82.58</cell><cell>86.14</cell><cell>63.35</cell><cell>62.08</cell><cell>61.36</cell><cell>60.76</cell><cell cols="3">68.43 68.13 67.83</cell><cell>67.48</cell></row><row><cell></cell><cell>64</cell><cell>82</cell><cell cols="2">69.41 83.56</cell><cell>87.33</cell><cell>63.26</cell><cell>61.64</cell><cell>60.63</cell><cell>59.67</cell><cell cols="3">68.49 67.91 67.38</cell><cell>66.74</cell></row><row><cell>FF</cell><cell>128</cell><cell>164</cell><cell cols="2">69.35 84.23</cell><cell>88.24</cell><cell>62.30</cell><cell>60.16</cell><cell>58.73</cell><cell>57.29</cell><cell cols="3">67.84 66.83 65.96</cell><cell>64.92</cell></row><row><cell></cell><cell>256</cell><cell>328</cell><cell cols="2">69.72 84.71</cell><cell>88.54</cell><cell>61.47</cell><cell>58.85</cell><cell>57.02</cell><cell>55.13</cell><cell cols="3">67.19 65.82 64.64</cell><cell>63.24</cell></row><row><cell></cell><cell>512</cell><cell>656</cell><cell cols="2">70.18 85.04</cell><cell>88.91</cell><cell>61.37</cell><cell>58.41</cell><cell>56.26</cell><cell>53.98</cell><cell cols="3">67.12 65.49 64.07</cell><cell>62.35</cell></row><row><cell></cell><cell>1024</cell><cell>1312</cell><cell cols="2">70.34 85.38</cell><cell>89.19</cell><cell>61.13</cell><cell>57.87</cell><cell>55.47</cell><cell>52.90</cell><cell cols="3">66.93 65.08 63.43</cell><cell>61.45</cell></row><row><cell></cell><cell>2048</cell><cell>2624</cell><cell cols="2">71.19 85.66</cell><cell>89.17</cell><cell>62.90</cell><cell>60.06</cell><cell>57.99</cell><cell>55.76</cell><cell>68.46</cell><cell>66.9</cell><cell>65.52</cell><cell>63.83</cell></row><row><cell></cell><cell>8</cell><cell>10</cell><cell cols="2">57.39 74.18</cell><cell>79.16</cell><cell>51.80</cell><cell>50.41</cell><cell>49.60</cell><cell>48.86</cell><cell cols="3">57.50 57.16 56.81</cell><cell>56.36</cell></row><row><cell></cell><cell>16</cell><cell>20</cell><cell cols="2">67.08 81.38</cell><cell>85.15</cell><cell>61.60</cell><cell>60.36</cell><cell>59.66</cell><cell>59.04</cell><cell cols="3">66.79 66.53 66.24</cell><cell>65.87</cell></row><row><cell></cell><cell>32</cell><cell>41</cell><cell cols="2">68.62 82.92</cell><cell>86.44</cell><cell>63.34</cell><cell>61.97</cell><cell>61.14</cell><cell>60.39</cell><cell cols="3">68.49 68.06 67.65</cell><cell>67.17</cell></row><row><cell></cell><cell>64</cell><cell>82</cell><cell cols="2">69.56 83.49</cell><cell>86.85</cell><cell>63.84</cell><cell>62.33</cell><cell>61.43</cell><cell>60.57</cell><cell>68.93</cell><cell>68.4</cell><cell>67.96</cell><cell>67.38</cell></row><row><cell>MRL-E</cell><cell>128</cell><cell>164</cell><cell cols="2">70.13 83.63</cell><cell>87.07</cell><cell>64.15</cell><cell>62.58</cell><cell>61.61</cell><cell>60.70</cell><cell cols="3">69.19 68.62 68.11</cell><cell>67.50</cell></row><row><cell></cell><cell>256</cell><cell>328</cell><cell>70.39</cell><cell>83.8</cell><cell>87.28</cell><cell>64.35</cell><cell>62.76</cell><cell>61.76</cell><cell>60.82</cell><cell cols="3">69.36 68.79 68.26</cell><cell>67.63</cell></row><row><cell></cell><cell>512</cell><cell>656</cell><cell cols="2">70.74 83.91</cell><cell>87.33</cell><cell>64.69</cell><cell>63.05</cell><cell>62.06</cell><cell>61.14</cell><cell cols="3">69.63 69.00 68.50</cell><cell>67.88</cell></row><row><cell></cell><cell>1024</cell><cell>1312</cell><cell cols="2">71.05 84.13</cell><cell>87.46</cell><cell>64.85</cell><cell>63.22</cell><cell>62.19</cell><cell>61.26</cell><cell cols="3">69.78 69.16 68.60</cell><cell>67.99</cell></row><row><cell></cell><cell>2048</cell><cell>2624</cell><cell cols="2">71.17 84.27</cell><cell>87.67</cell><cell>64.99</cell><cell>63.33</cell><cell>62.29</cell><cell>61.33</cell><cell cols="3">69.90 69.24 68.68</cell><cell>68.05</cell></row><row><cell></cell><cell>12</cell><cell>15</cell><cell cols="2">64.25 79.21</cell><cell>83.29</cell><cell>58.83</cell><cell>57.50</cell><cell>56.71</cell><cell>56.02</cell><cell cols="3">64.10 63.78 63.42</cell><cell>63.02</cell></row><row><cell></cell><cell>24</cell><cell>31</cell><cell cols="2">68.28 82.31</cell><cell>85.89</cell><cell>62.75</cell><cell>61.41</cell><cell>60.62</cell><cell>59.92</cell><cell cols="3">67.89 67.49 67.11</cell><cell>66.69</cell></row><row><cell></cell><cell>48</cell><cell>61</cell><cell cols="2">69.20 83.15</cell><cell>86.67</cell><cell>63.58</cell><cell>62.12</cell><cell>61.23</cell><cell>60.42</cell><cell cols="3">68.71 68.19 67.75</cell><cell>67.22</cell></row><row><cell>MRL-E</cell><cell>96</cell><cell>123</cell><cell cols="2">70.05 83.63</cell><cell>87.11</cell><cell>64.04</cell><cell>62.46</cell><cell>61.52</cell><cell>60.63</cell><cell cols="3">69.10 68.51 68.04</cell><cell>67.45</cell></row><row><cell>Interpolated</cell><cell>192</cell><cell>246</cell><cell cols="2">70.36 83.72</cell><cell>87.21</cell><cell>64.26</cell><cell>62.65</cell><cell>61.65</cell><cell>60.72</cell><cell cols="3">69.26 68.67 68.15</cell><cell>67.53</cell></row><row><cell></cell><cell>384</cell><cell>492</cell><cell cols="2">70.54 83.88</cell><cell>87.28</cell><cell>64.55</cell><cell>62.94</cell><cell>61.93</cell><cell>61.01</cell><cell cols="3">69.51 68.92 68.40</cell><cell>67.78</cell></row><row><cell></cell><cell>768</cell><cell>984</cell><cell cols="2">70.96 84.05</cell><cell>87.44</cell><cell>64.79</cell><cell>63.15</cell><cell>62.15</cell><cell>61.22</cell><cell cols="3">69.72 69.10 68.56</cell><cell>67.95</cell></row><row><cell></cell><cell>1536</cell><cell>1968</cell><cell cols="2">71.19 84.17</cell><cell>87.57</cell><cell>64.94</cell><cell>63.29</cell><cell>62.26</cell><cell>61.32</cell><cell cols="3">69.85 69.21 68.66</cell><cell>68.04</cell></row><row><cell></cell><cell>8</cell><cell>10</cell><cell cols="2">62.19 77.05</cell><cell>81.34</cell><cell>56.74</cell><cell>55.47</cell><cell>54.76</cell><cell>54.12</cell><cell cols="3">62.06 61.81 61.54</cell><cell>61.17</cell></row><row><cell></cell><cell>16</cell><cell>20</cell><cell cols="2">67.91 81.44</cell><cell>85.00</cell><cell>62.94</cell><cell>61.79</cell><cell>61.16</cell><cell>60.64</cell><cell cols="3">67.93 67.71 67.48</cell><cell>67.20</cell></row><row><cell></cell><cell>32</cell><cell>41</cell><cell cols="2">69.46 83.01</cell><cell>86.30</cell><cell>64.21</cell><cell>62.96</cell><cell>62.22</cell><cell>61.58</cell><cell cols="3">69.18 68.87 68.54</cell><cell>68.17</cell></row><row><cell></cell><cell>64</cell><cell>82</cell><cell cols="2">70.17 83.53</cell><cell>86.95</cell><cell>64.69</cell><cell>63.33</cell><cell>62.53</cell><cell>61.80</cell><cell cols="3">69.67 69.25 68.89</cell><cell>68.42</cell></row><row><cell>MRL</cell><cell>128</cell><cell>164</cell><cell cols="2">70.52 83.98</cell><cell>87.25</cell><cell>64.94</cell><cell>63.50</cell><cell>62.63</cell><cell>61.83</cell><cell cols="3">69.93 69.44 69.02</cell><cell>68.50</cell></row><row><cell></cell><cell>256</cell><cell>328</cell><cell cols="2">70.62 84.17</cell><cell>87.38</cell><cell>65.04</cell><cell>63.56</cell><cell>62.66</cell><cell>61.81</cell><cell cols="3">70.02 69.52 69.07</cell><cell>68.50</cell></row><row><cell></cell><cell>512</cell><cell>656</cell><cell cols="2">70.82 84.31</cell><cell>87.55</cell><cell>65.14</cell><cell>63.57</cell><cell>62.62</cell><cell>61.73</cell><cell cols="3">70.12 69.53 69.04</cell><cell>68.45</cell></row><row><cell></cell><cell>1024</cell><cell>1312</cell><cell cols="2">70.89 84.44</cell><cell>87.68</cell><cell>65.16</cell><cell>63.58</cell><cell>62.60</cell><cell>61.68</cell><cell cols="3">70.14 69.54 69.01</cell><cell>68.41</cell></row><row><cell></cell><cell>2048</cell><cell>2624</cell><cell cols="2">70.97 84.41</cell><cell>87.74</cell><cell>65.20</cell><cell>63.57</cell><cell>62.56</cell><cell>61.60</cell><cell cols="3">70.18 69.52 68.98</cell><cell>68.35</cell></row><row><cell></cell><cell>12</cell><cell>15</cell><cell cols="2">65.89 80.04</cell><cell>83.68</cell><cell>60.84</cell><cell>59.66</cell><cell>58.98</cell><cell>58.37</cell><cell cols="3">65.94 65.72 65.45</cell><cell>65.08</cell></row><row><cell></cell><cell>24</cell><cell>31</cell><cell cols="2">68.76 82.48</cell><cell>85.87</cell><cell>63.64</cell><cell>62.42</cell><cell>61.74</cell><cell>61.13</cell><cell cols="3">68.64 68.35 68.07</cell><cell>67.71</cell></row><row><cell></cell><cell>48</cell><cell>61</cell><cell cols="2">69.96 83.40</cell><cell>86.65</cell><cell>64.58</cell><cell>63.2</cell><cell>62.42</cell><cell>61.72</cell><cell cols="3">69.53 69.10 68.75</cell><cell>68.32</cell></row><row><cell>MRL</cell><cell>96</cell><cell>123</cell><cell cols="2">70.40 83.83</cell><cell>87.04</cell><cell>64.86</cell><cell>63.46</cell><cell>62.62</cell><cell>61.84</cell><cell cols="3">69.82 69.38 68.98</cell><cell>68.48</cell></row><row><cell>Interpolated</cell><cell>192</cell><cell>246</cell><cell cols="2">70.64 84.09</cell><cell>87.37</cell><cell>65.00</cell><cell>63.53</cell><cell>62.66</cell><cell>61.83</cell><cell cols="3">69.98 69.49 69.05</cell><cell>68.50</cell></row><row><cell></cell><cell>384</cell><cell>492</cell><cell cols="2">70.69 84.25</cell><cell>87.41</cell><cell>65.09</cell><cell>63.56</cell><cell>62.64</cell><cell>61.76</cell><cell cols="3">70.05 69.51 69.04</cell><cell>68.46</cell></row><row><cell></cell><cell>768</cell><cell>984</cell><cell cols="2">70.84 84.40</cell><cell>87.63</cell><cell>65.16</cell><cell>63.59</cell><cell>62.62</cell><cell>61.71</cell><cell cols="3">70.14 69.55 69.03</cell><cell>68.44</cell></row><row><cell></cell><cell>1536</cell><cell>1968</cell><cell cols="2">70.88 84.39</cell><cell>87.71</cell><cell>65.18</cell><cell>63.59</cell><cell>62.58</cell><cell>61.64</cell><cell cols="3">70.16 69.54 68.99</cell><cell>68.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Retrieve a shortlist of 200-NN with D s sized representations on ImageNetV2 via exact search with L2 distance metric. Top-1 and mAP@10 entries (%) where MRL-E outperforms FF are bolded. MRL outperforms FF at all D s and is thus not bolded.</figDesc><table><row><cell>Config</cell><cell>Ds</cell><cell cols="9">MFLOPs Top-1 Top-5 Top-10 mAP@10 mAP@25 mAP@50 mAP@100 P@10 P@25 P@50 P@100</cell></row><row><cell></cell><cell>8</cell><cell>10</cell><cell>48.79 64.70</cell><cell>69.72</cell><cell>43.04</cell><cell>41.89</cell><cell>41.42</cell><cell>41.17</cell><cell>48.43 48.27 48.25</cell><cell>48.19</cell></row><row><cell></cell><cell>16</cell><cell>20</cell><cell>55.08 69.50</cell><cell>74.08</cell><cell>49.63</cell><cell>48.53</cell><cell>48.06</cell><cell>47.75</cell><cell>54.76 54.64 54.53</cell><cell>54.39</cell></row><row><cell></cell><cell>32</cell><cell>41</cell><cell>56.69 71.10</cell><cell>76.47</cell><cell>51.11</cell><cell>49.85</cell><cell>49.17</cell><cell>48.65</cell><cell>56.23 55.96 55.71</cell><cell>55.42</cell></row><row><cell></cell><cell>64</cell><cell>82</cell><cell>57.37 72.71</cell><cell>77.48</cell><cell>51.28</cell><cell>49.75</cell><cell>48.85</cell><cell>47.99</cell><cell>56.65 56.14 55.71</cell><cell>55.15</cell></row><row><cell>FF</cell><cell>128</cell><cell>164</cell><cell>57.17 73.31</cell><cell>78.64</cell><cell>50.07</cell><cell>48.09</cell><cell>46.79</cell><cell>45.58</cell><cell>55.75 54.89 54.12</cell><cell>53.28</cell></row><row><cell></cell><cell>256</cell><cell>328</cell><cell>57.09 74.04</cell><cell>79.24</cell><cell>49.11</cell><cell>46.66</cell><cell>44.99</cell><cell>43.35</cell><cell>55.02 53.77 52.74</cell><cell>51.53</cell></row><row><cell></cell><cell>512</cell><cell>656</cell><cell>57.12 73.91</cell><cell>79.32</cell><cell>48.95</cell><cell>46.25</cell><cell>44.37</cell><cell>42.42</cell><cell>54.88 53.49 52.29</cell><cell>50.83</cell></row><row><cell></cell><cell>1024</cell><cell>1312</cell><cell>57.53 74.17</cell><cell>79.55</cell><cell>48.27</cell><cell>45.41</cell><cell>43.36</cell><cell>41.26</cell><cell>54.31 52.84 51.49</cell><cell>49.87</cell></row><row><cell></cell><cell>2048</cell><cell>2624</cell><cell>57.84 74.59</cell><cell>79.45</cell><cell>49.99</cell><cell>47.47</cell><cell>45.66</cell><cell>43.87</cell><cell>55.89 54.63 53.45</cell><cell>52.12</cell></row><row><cell></cell><cell>8</cell><cell>10</cell><cell>47.05 62.53</cell><cell>67.60</cell><cell>40.79</cell><cell>39.47</cell><cell>38.78</cell><cell>38.16</cell><cell>46.03 45.77 45.54</cell><cell>45.17</cell></row><row><cell></cell><cell>16</cell><cell>20</cell><cell>55.73 70.54</cell><cell>74.86</cell><cell>49.86</cell><cell>48.57</cell><cell>47.84</cell><cell>47.26</cell><cell>54.97 54.71 54.44</cell><cell>54.10</cell></row><row><cell></cell><cell>32</cell><cell>41</cell><cell>57.33 71.61</cell><cell>76.64</cell><cell>51.26</cell><cell>49.92</cell><cell>49.09</cell><cell>48.42</cell><cell>56.46 56.11 55.70</cell><cell>55.30</cell></row><row><cell></cell><cell>64</cell><cell>82</cell><cell>57.90 72.55</cell><cell>77.44</cell><cell>51.89</cell><cell>50.29</cell><cell>49.34</cell><cell>48.53</cell><cell>57.06 56.45 55.97</cell><cell>55.43</cell></row><row><cell>MRL-E</cell><cell>128</cell><cell>164</cell><cell>57.73 72.79</cell><cell>77.28</cell><cell>52.02</cell><cell>50.38</cell><cell>49.49</cell><cell>48.62</cell><cell>57.13 56.58 56.15</cell><cell>55.58</cell></row><row><cell></cell><cell>256</cell><cell>328</cell><cell>58.22 72.77</cell><cell>77.67</cell><cell>52.16</cell><cell>50.61</cell><cell>49.67</cell><cell>48.81</cell><cell>57.30 56.79 56.33</cell><cell>55.77</cell></row><row><cell></cell><cell>512</cell><cell>656</cell><cell>58.46 73.00</cell><cell>77.88</cell><cell>52.52</cell><cell>50.97</cell><cell>50.02</cell><cell>49.16</cell><cell>57.65 57.10 56.64</cell><cell>56.08</cell></row><row><cell></cell><cell>1024</cell><cell>1312</cell><cell>58.71 73.29</cell><cell>78.00</cell><cell>52.70</cell><cell>51.13</cell><cell>50.17</cell><cell>49.30</cell><cell>57.83 57.26 56.77</cell><cell>56.20</cell></row><row><cell></cell><cell>2048</cell><cell>2624</cell><cell>58.86 73.17</cell><cell>78.00</cell><cell>52.88</cell><cell>51.25</cell><cell>50.26</cell><cell>49.36</cell><cell>57.95 57.35 56.85</cell><cell>56.25</cell></row><row><cell></cell><cell>8</cell><cell>10</cell><cell>50.41 65.56</cell><cell>70.27</cell><cell>45.51</cell><cell>44.38</cell><cell>43.71</cell><cell>43.17</cell><cell>50.55 50.44 50.17</cell><cell>49.91</cell></row><row><cell></cell><cell>16</cell><cell>20</cell><cell>56.64 70.19</cell><cell>74.61</cell><cell>50.98</cell><cell>49.76</cell><cell>49.16</cell><cell>48.69</cell><cell>55.90 55.66 55.52</cell><cell>55.29</cell></row><row><cell></cell><cell>32</cell><cell>41</cell><cell>57.96 71.88</cell><cell>76.41</cell><cell>52.06</cell><cell>50.78</cell><cell>50.09</cell><cell>49.54</cell><cell>57.18 56.83 56.57</cell><cell>56.27</cell></row><row><cell></cell><cell>64</cell><cell>82</cell><cell>58.94 72.74</cell><cell>77.17</cell><cell>52.65</cell><cell>51.24</cell><cell>50.44</cell><cell>49.76</cell><cell>57.72 57.29 56.94</cell><cell>56.52</cell></row><row><cell>MRL</cell><cell>128</cell><cell>164</cell><cell>59.13 73.07</cell><cell>77.49</cell><cell>52.94</cell><cell>51.42</cell><cell>50.53</cell><cell>49.74</cell><cell>58.00 57.47 57.05</cell><cell>56.55</cell></row><row><cell></cell><cell>256</cell><cell>328</cell><cell>59.18 73.64</cell><cell>77.75</cell><cell>52.96</cell><cell>51.45</cell><cell>50.52</cell><cell>49.70</cell><cell>58.01 57.53 57.06</cell><cell>56.54</cell></row><row><cell></cell><cell>512</cell><cell>656</cell><cell>59.40 73.85</cell><cell>77.97</cell><cell>53.01</cell><cell>51.39</cell><cell>50.46</cell><cell>49.61</cell><cell>58.11 57.49 57.04</cell><cell>56.48</cell></row><row><cell></cell><cell>1024</cell><cell>1312</cell><cell>59.11 73.77</cell><cell>77.92</cell><cell>52.98</cell><cell>51.37</cell><cell>50.40</cell><cell>49.54</cell><cell>58.13 57.51 57.00</cell><cell>56.45</cell></row><row><cell></cell><cell>2048</cell><cell>2624</cell><cell>59.63 73.84</cell><cell>77.97</cell><cell>52.96</cell><cell>51.34</cell><cell>50.34</cell><cell>49.44</cell><cell>58.07 57.48 56.95</cell><cell>56.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Retrieve a shortlist of 200-NN with D s sized representations on ImageNet-4K via exact search with L2 distance metric. MRL-E and FF models are omitted for clarity and compute/inference time costs. All entries are in %.</figDesc><table><row><cell>Config</cell><cell>Ds</cell><cell cols="11">MFLOPs Top-1 Top-5 Top-10 mAP@10 mAP@25 mAP@50 mAP@100 P@10 P@25 P@50 P@100</cell></row><row><cell></cell><cell>8</cell><cell>34</cell><cell>10.60 26.23</cell><cell>35.57</cell><cell>5.32</cell><cell>4.29</cell><cell>3.76</cell><cell>3.36</cell><cell>9.13</cell><cell>8.77</cell><cell>8.46</cell><cell>8.13</cell></row><row><cell></cell><cell>16</cell><cell>67</cell><cell>16.74 36.91</cell><cell>47.28</cell><cell>8.64</cell><cell>6.83</cell><cell>5.84</cell><cell>5.05</cell><cell cols="3">13.82 12.79 12.04</cell><cell>13.27</cell></row><row><cell></cell><cell>32</cell><cell>134</cell><cell>21.54 43.75</cell><cell>54.11</cell><cell>11.36</cell><cell>8.88</cell><cell>7.47</cell><cell>6.31</cell><cell cols="3">17.25 15.67 14.47</cell><cell>13.27</cell></row><row><cell></cell><cell>64</cell><cell>269</cell><cell>25.00 47.97</cell><cell>58.25</cell><cell>13.38</cell><cell>10.40</cell><cell>8.67</cell><cell>7.23</cell><cell cols="3">19.68 17.64 16.14</cell><cell>14.65</cell></row><row><cell>MRL</cell><cell>128</cell><cell>538</cell><cell>27.27 50.35</cell><cell>60.47</cell><cell>14.77</cell><cell>11.47</cell><cell>9.53</cell><cell>7.91</cell><cell cols="3">21.25 18.95 17.26</cell><cell>15.59</cell></row><row><cell></cell><cell>256</cell><cell>1076</cell><cell>28.53 51.95</cell><cell>61.90</cell><cell>15.66</cell><cell>12.19</cell><cell>10.12</cell><cell>8.38</cell><cell cols="3">22.28 19.81 18.01</cell><cell>16.22</cell></row><row><cell></cell><cell>512</cell><cell>2151</cell><cell>29.46 53.03</cell><cell>62.81</cell><cell>16.29</cell><cell>12.70</cell><cell>10.55</cell><cell>8.72</cell><cell cols="3">22.96 20.42 18.54</cell><cell>16.68</cell></row><row><cell></cell><cell>1024</cell><cell>4303</cell><cell>30.23 53.72</cell><cell>63.45</cell><cell>16.76</cell><cell>13.08</cell><cell>10.86</cell><cell>8.97</cell><cell cols="3">23.48 20.88 18.93</cell><cell>17.00</cell></row><row><cell></cell><cell>2048</cell><cell>8606</cell><cell>30.87 54.32</cell><cell>64.02</cell><cell>17.20</cell><cell>13.43</cell><cell>11.14</cell><cell>9.19</cell><cell cols="3">23.97 21.28 19.28</cell><cell>17.30</cell></row><row><cell></cell><cell>12</cell><cell>50</cell><cell>14.04 32.56</cell><cell>42.71</cell><cell>7.16</cell><cell>5.70</cell><cell>4.92</cell><cell>4.32</cell><cell cols="3">11.81 11.08 10.52</cell><cell>9.94</cell></row><row><cell></cell><cell>24</cell><cell>101</cell><cell>19.49 40.82</cell><cell>51.26</cell><cell>10.17</cell><cell>7.98</cell><cell>6.75</cell><cell>5.75</cell><cell cols="3">15.76 14.43 13.42</cell><cell>12.40</cell></row><row><cell></cell><cell>48</cell><cell>202</cell><cell>23.51 46.23</cell><cell>56.56</cell><cell>12.49</cell><cell>9.72</cell><cell>8.13</cell><cell>6.81</cell><cell cols="3">18.62 16.75 15.39</cell><cell>14.04</cell></row><row><cell>MRL-</cell><cell>96</cell><cell>403</cell><cell>26.25 49.32</cell><cell>59.48</cell><cell>14.15</cell><cell>11.00</cell><cell>9.15</cell><cell>7.61</cell><cell cols="3">20.55 18.36 16.78</cell><cell>15.17</cell></row><row><cell>Interpolated</cell><cell>192</cell><cell>807</cell><cell>27.94 51.32</cell><cell>61.32</cell><cell>15.29</cell><cell>11.89</cell><cell>9.88</cell><cell>8.18</cell><cell cols="3">21.86 19.46 17.71</cell><cell>15.96</cell></row><row><cell></cell><cell>384</cell><cell>1614</cell><cell>29.03 52.53</cell><cell>62.45</cell><cell>15.99</cell><cell>12.46</cell><cell>10.35</cell><cell>8.56</cell><cell cols="3">22.64 20.14 18.29</cell><cell>16.47</cell></row><row><cell></cell><cell>768</cell><cell>3227</cell><cell>29.87 53.36</cell><cell>63.13</cell><cell>16.54</cell><cell>12.90</cell><cell>10.71</cell><cell>8.85</cell><cell cols="3">23.23 20.67 18.75</cell><cell>16.85</cell></row><row><cell></cell><cell>1536</cell><cell>6454</cell><cell>30.52 54.02</cell><cell>63.79</cell><cell>16.99</cell><cell>13.27</cell><cell>11.01</cell><cell>9.08</cell><cell cols="3">23.73 21.09 19.12</cell><cell>17.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Retrieve a shortlist of k-NN with D s sized representations on ImageNet-1K with MRL representations, and then re-order the neighbors shortlist with L2 distances using D r sized representations. Top-1 and mAP@10 entries (%) that are within 0.1% of the maximum value achievable without reranking on MRL representations, as seen inTable 8, are bolded.</figDesc><table><row><cell>D s</cell><cell>D r</cell><cell>MFLOPs Top-1 mAP@10 mAP@25 mAP@50 mAP@100 P@10 P@25 P@50 P@100</cell></row><row><cell></cell><cell>16</cell><cell></cell></row><row><cell>8</cell><cell></cell><cell></cell></row><row><cell>Shortlist Length = 200</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Retrieve a shortlist of k-NN with Ds sized representations on ImageNet-4K with MRL representations, and then re-order the neighbors shortlist with L2 distances using Dr sized representations. Top-1 and mAP@10 entries (%) that are within 0.1% of the maximum value achievable without reranking on MRL representations, as seen inTable 10, are bolded.</figDesc><table><row><cell></cell><cell>D s</cell><cell>D r</cell><cell cols="8">MFLOPs Top-1 mAP@10 mAP@25 mAP@50 mAP@100 P@10 P@25 P@50 P@100</cell></row><row><cell></cell><cell></cell><cell>16</cell><cell></cell><cell>16.84</cell><cell>8.70</cell><cell>6.88</cell><cell>5.88</cell><cell>5.08</cell><cell>13.86 12.80 11.98</cell><cell>11.10</cell></row><row><cell></cell><cell></cell><cell>32</cell><cell></cell><cell>20.73</cell><cell>10.66</cell><cell>8.19</cell><cell>6.77</cell><cell>5.61</cell><cell>16.18 14.39 13.02</cell><cell>11.61</cell></row><row><cell></cell><cell></cell><cell>64</cell><cell></cell><cell>23.11</cell><cell>11.91</cell><cell>9.03</cell><cell>7.36</cell><cell>6.00</cell><cell>17.56 15.34 13.67</cell><cell>11.99</cell></row><row><cell></cell><cell>8</cell><cell>128 256</cell><cell>34</cell><cell>24.63 25.5</cell><cell>12.71 13.24</cell><cell>9.59 9.96</cell><cell>7.76 8.03</cell><cell>6.25 6.42</cell><cell>18.42 15.94 14.08 19.00 16.35 14.36</cell><cell>12.22 12.37</cell></row><row><cell></cell><cell></cell><cell>512</cell><cell></cell><cell>26.07</cell><cell>13.59</cell><cell>10.21</cell><cell>8.20</cell><cell>6.53</cell><cell>19.37 16.62 14.54</cell><cell>12.46</cell></row><row><cell></cell><cell></cell><cell>1024</cell><cell></cell><cell>26.52</cell><cell>13.85</cell><cell>10.40</cell><cell>8.34</cell><cell>6.61</cell><cell>19.65 16.80 14.68</cell><cell>12.53</cell></row><row><cell></cell><cell></cell><cell>2048</cell><cell></cell><cell>26.94</cell><cell>14.11</cell><cell>10.57</cell><cell>8.45</cell><cell>6.68</cell><cell>19.92 16.98 14.79</cell><cell>12.58</cell></row><row><cell></cell><cell></cell><cell>32</cell><cell></cell><cell>21.44</cell><cell>11.24</cell><cell>8.72</cell><cell>7.26</cell><cell>6.02</cell><cell>17.02 15.30 13.92</cell><cell>12.41</cell></row><row><cell></cell><cell></cell><cell>64</cell><cell></cell><cell>24.36</cell><cell>12.78</cell><cell>9.75</cell><cell>7.96</cell><cell>6.43</cell><cell>18.72 16.41 14.63</cell><cell>12.74</cell></row><row><cell></cell><cell></cell><cell>128</cell><cell></cell><cell>26.08</cell><cell>13.70</cell><cell>10.39</cell><cell>8.39</cell><cell>6.69</cell><cell>19.68 17.07 15.05</cell><cell>12.94</cell></row><row><cell></cell><cell>16</cell><cell>256</cell><cell>67</cell><cell>26.99</cell><cell>14.27</cell><cell>10.79</cell><cell>8.67</cell><cell>6.85</cell><cell>20.27 17.48 15.31</cell><cell>13.07</cell></row><row><cell>Shortlist Length = 200</cell><cell>32</cell><cell>512 1024 2048 64 128 256 512 1024</cell><cell>134</cell><cell>27.60 28.12 28.56 24.99 27.17 28.33 29.12 29.78</cell><cell>14.66 14.94 15.21 13.35 14.61 15.37 15.88 16.25</cell><cell>11.06 11.26 11.43 10.35 11.27 11.83 12.20 12.47</cell><cell>8.86 8.99 9.11 8.59 9.26 9.67 9.94 10.13</cell><cell>6.97 7.05 7.12 7.09 7.51 7.77 7.93 8.05</cell><cell>20.67 17.75 15.50 20.96 17.95 15.62 21.23 18.13 15.73 19.61 17.52 15.92 20.99 18.52 16.62 21.80 19.12 17.05 22.33 19.51 17.32 22.71 19.79 17.5</cell><cell>13.16 13.22 13.27 14.21 14.59 14.81 14.94 15.03</cell></row><row><cell></cell><cell></cell><cell>2048</cell><cell></cell><cell>30.33</cell><cell>16.59</cell><cell>12.72</cell><cell>10.30</cell><cell>8.16</cell><cell>23.07 20.05 17.66</cell><cell>15.11</cell></row><row><cell></cell><cell></cell><cell>128</cell><cell></cell><cell>27.27</cell><cell>14.76</cell><cell>11.47</cell><cell>9.51</cell><cell>7.85</cell><cell>21.25 18.92 17.20</cell><cell>15.40</cell></row><row><cell></cell><cell></cell><cell>256</cell><cell></cell><cell>28.54</cell><cell>15.64</cell><cell>12.15</cell><cell>10.05</cell><cell>8.21</cell><cell>22.24 19.71 17.81</cell><cell>15.76</cell></row><row><cell></cell><cell>64</cell><cell>512</cell><cell>269</cell><cell>29.45</cell><cell>16.25</cell><cell>12.62</cell><cell>10.40</cell><cell>8.44</cell><cell>22.88 20.24 18.20</cell><cell>15.97</cell></row><row><cell></cell><cell></cell><cell>1024</cell><cell></cell><cell>30.19</cell><cell>16.69</cell><cell>12.96</cell><cell>10.66</cell><cell>8.60</cell><cell>23.35 20.61 18.46</cell><cell>16.10</cell></row><row><cell></cell><cell></cell><cell>2048</cell><cell></cell><cell>30.81</cell><cell>17.10</cell><cell>13.27</cell><cell>10.88</cell><cell>8.74</cell><cell>23.79 20.93 18.69</cell><cell>16.21</cell></row><row><cell></cell><cell></cell><cell>256</cell><cell></cell><cell>28.54</cell><cell>15.66</cell><cell>12.19</cell><cell>10.12</cell><cell>8.36</cell><cell>22.28 19.81 18.00</cell><cell>16.16</cell></row><row><cell></cell><cell>128</cell><cell>512 1024</cell><cell>538</cell><cell>29.45 30.22</cell><cell>16.29 16.76</cell><cell>12.69 13.07</cell><cell>10.53 10.83</cell><cell>8.66 8.86</cell><cell>22.96 20.41 18.50 23.47 20.84 18.83</cell><cell>16.48 16.68</cell></row><row><cell></cell><cell></cell><cell>2048</cell><cell></cell><cell>30.86</cell><cell>17.19</cell><cell>13.41</cell><cell>11.09</cell><cell>9.03</cell><cell>23.95 21.22 19.12</cell><cell>16.84</cell></row><row><cell></cell><cell></cell><cell>512</cell><cell></cell><cell>29.45</cell><cell>16.29</cell><cell>12.70</cell><cell>10.55</cell><cell>8.71</cell><cell>22.97 20.42 18.54</cell><cell>16.66</cell></row><row><cell></cell><cell>256</cell><cell>1024</cell><cell>1076</cell><cell>30.21</cell><cell>16.76</cell><cell>13.08</cell><cell>10.86</cell><cell>8.95</cell><cell>23.48 20.87 18.92</cell><cell>16.94</cell></row><row><cell></cell><cell></cell><cell>2048</cell><cell></cell><cell>30.85</cell><cell>17.20</cell><cell>13.43</cell><cell>11.14</cell><cell>9.15</cell><cell>23.97 21.27 19.26</cell><cell>17.16</cell></row><row><cell></cell><cell>512</cell><cell>1024 2048</cell><cell>2152</cell><cell>30.22 30.87</cell><cell>16.76 17.20</cell><cell>13.08 13.43</cell><cell>10.86 11.14</cell><cell>8.97 9.19</cell><cell>23.48 20.88 18.93 23.97 21.28 19.28</cell><cell>17.00 17.28</cell></row><row><cell></cell><cell cols="2">1024 2048</cell><cell>4303</cell><cell>30.87</cell><cell>17.20</cell><cell>13.43</cell><cell>11.15</cell><cell>9.19</cell><cell>23.97 21.28 19.28</cell><cell>17.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>Retrieve a shortlist of k-NN with D s sized representations on ImageNet-1K with MRL. This shortlist is then reranked with funnel retrieval, which uses a rerank cascade with a one-toone mapping with a monotonically decreasing shortlist length as shown in the shortlist cascade. Top-1 and mAP@10 entries (%) within 0.1% of the maximum achievable without reranking on MRL representations, as seen inTable 8, are bolded.</figDesc><table><row><cell>D s</cell><cell>Rerank Cascade</cell><cell>Shortlist Cascade</cell><cell cols="5">MFLOPs Top-1 Top-5 Top-10 mAP@10 P@10</cell></row><row><cell></cell><cell></cell><cell>200?100?50?25?10</cell><cell>10.28</cell><cell>70.22 82.63</cell><cell>85.49</cell><cell>64.06</cell><cell>68.65</cell></row><row><cell>8</cell><cell>16?32?64?128?2048</cell><cell>400?200?50?25?10</cell><cell>10.29</cell><cell>70.46 83.13</cell><cell>86.08</cell><cell>64.43</cell><cell>69.10</cell></row><row><cell></cell><cell></cell><cell>800?400?200?50?10</cell><cell>10.31</cell><cell>70.58 83.54</cell><cell>86.53</cell><cell>64.62</cell><cell>69.37</cell></row><row><cell></cell><cell></cell><cell>200?100?50?25?10</cell><cell>20.54</cell><cell>70.90 83.96</cell><cell>86.85</cell><cell>65.19</cell><cell>69.97</cell></row><row><cell>16</cell><cell>32?64?128?256?2048</cell><cell>400?200?50?25?10</cell><cell>20.56</cell><cell>70.95 84.05</cell><cell>87.04</cell><cell>65.18</cell><cell>70.00</cell></row><row><cell></cell><cell></cell><cell>800?400?200?50?10</cell><cell>20.61</cell><cell>70.96 84.18</cell><cell>87.22</cell><cell>65.14</cell><cell>70.01</cell></row><row><cell></cell><cell></cell><cell>200?100?50?25?10</cell><cell>41.07</cell><cell>70.96 84.32</cell><cell>87.47</cell><cell>65.21</cell><cell>70.11</cell></row><row><cell cols="2">32 64?128?256?512?2048</cell><cell>400?200?50?25?10</cell><cell>41.09</cell><cell>70.97 84.32</cell><cell>87.47</cell><cell>65.19</cell><cell>70.11</cell></row><row><cell></cell><cell></cell><cell>800?400?200?50?10</cell><cell>41.20</cell><cell>70.97 84.36</cell><cell>87.53</cell><cell>65.18</cell><cell>70.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 :</head><label>14</label><figDesc>Retrieve a shortlist of k-NN with D s sized representations on ImageNet-4K with MRL. This shortlist is then reranked with funnel retrieval, which uses a rerank cascade with a one-toone mapping with a monotonically decreasing shortlist length as shown in the shortlist cascade. Top-1 and mAP@10 entries (%) within 0.15% of the maximum achievable without reranking on MRL representations, as seen inTable 10, are bolded.</figDesc><table><row><cell>D s</cell><cell>Rerank Cascade</cell><cell>Shortlist Cascade</cell><cell cols="5">MFLOPs Top-1 Top-5 Top-10 mAP@10 P@10</cell></row><row><cell></cell><cell></cell><cell>200?100?50?25?10</cell><cell>33.65</cell><cell>26.20 46.45</cell><cell>54.12</cell><cell>12.79</cell><cell>17.85</cell></row><row><cell>8</cell><cell>16?32?64?128?2048</cell><cell>400?200?50?25?10</cell><cell>33.66</cell><cell>26.55 47.02</cell><cell>54.72</cell><cell>13.02</cell><cell>18.15</cell></row><row><cell></cell><cell></cell><cell>800?400?200?50?10</cell><cell>33.68</cell><cell>26.83 47.54</cell><cell>55.35</cell><cell>13.24</cell><cell>18.44</cell></row><row><cell></cell><cell></cell><cell>200?100?50?25?10</cell><cell>67.28</cell><cell>29.51 51.44</cell><cell>59.56</cell><cell>15.27</cell><cell>21.03</cell></row><row><cell>16</cell><cell>32?64?128?256?2048</cell><cell>400?200?50?25?10</cell><cell>67.29</cell><cell>29.66 51.71</cell><cell>59.88</cell><cell>15.42</cell><cell>21.22</cell></row><row><cell></cell><cell></cell><cell>800?400?200?50?10</cell><cell>67.34</cell><cell>29.79 52.00</cell><cell>60.25</cell><cell>15.55</cell><cell>21.41</cell></row><row><cell></cell><cell></cell><cell>200?100?50?25?10</cell><cell>134.54</cell><cell>30.64 53.52</cell><cell>62.16</cell><cell>16.45</cell><cell>22.64</cell></row><row><cell>32</cell><cell>64?128?256?512?2048</cell><cell>400?200?50?25?10</cell><cell>134.56</cell><cell>30.69 53.65</cell><cell>62.31</cell><cell>16.51</cell><cell>22.73</cell></row><row><cell></cell><cell></cell><cell>800?400?200?50?10</cell><cell>134.66</cell><cell>30.72 53.78</cell><cell>62.43</cell><cell>16.55</cell><cell>22.79</cell></row><row><cell></cell><cell></cell><cell>200?100?50?25?10</cell><cell>269.05</cell><cell>30.81 54.06</cell><cell>63.15</cell><cell>16.87</cell><cell>23.34</cell></row><row><cell cols="2">64 128?256?512?1024?2048</cell><cell>400?200?50?25?10</cell><cell>269.10</cell><cell>30.84 54.20</cell><cell>63.31</cell><cell>16.92</cell><cell>23.42</cell></row><row><cell></cell><cell></cell><cell>800?400?200?50?10</cell><cell>269.31</cell><cell>30.87 54.27</cell><cell>63.42</cell><cell>16.95</cell><cell>23.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 15 :</head><label>15</label><figDesc>Few-shot accuracy (%) on ImageNetV2 for 1000-way classification. MRL performs equally to FF across all shots and representation sizes. We also observed that accuracy saturated at a lower dimension for lower shot numbers. E.g. for 1-shot, 32-dim performed comparably to 2048-dim.</figDesc><table><row><cell cols="7">Rep. Size Method 1-Shot 3-Shot 5-Shot 7-Shot 9-Shot</cell></row><row><cell>8</cell><cell>FF MRL</cell><cell>35.41 35.37</cell><cell>45.73 45.69</cell><cell>49.23 49.25</cell><cell>50.89 50.85</cell><cell>51.72 51.73</cell></row><row><cell>16</cell><cell>FF MRL</cell><cell>40.88 40.90</cell><cell>53.96 53.94</cell><cell>57.36 57.37</cell><cell>58.72 58.65</cell><cell>59.39 59.29</cell></row><row><cell>32</cell><cell>FF MRL</cell><cell>41.41 41.40</cell><cell>54.88 54.91</cell><cell>58.28 58.30</cell><cell>59.63 59.65</cell><cell>60.40 60.45</cell></row><row><cell>64</cell><cell>FF MRL</cell><cell>41.25 41.28</cell><cell>54.83 54.80</cell><cell>58.29 58.32</cell><cell>59.82 59.77</cell><cell>60.61 60.69</cell></row><row><cell>128</cell><cell>FF MRL</cell><cell>41.36 41.38</cell><cell>54.90 54.95</cell><cell>58.50 58.50</cell><cell>60.05 60.06</cell><cell>60.90 60.83</cell></row><row><cell>256</cell><cell>FF MRL</cell><cell>41.36 41.38</cell><cell>54.90 54.95</cell><cell>58.50 58.50</cell><cell>60.05 60.06</cell><cell>60.90 60.83</cell></row><row><cell>512</cell><cell>FF MRL</cell><cell>41.36 41.34</cell><cell>55.05 55.14</cell><cell>58.70 58.78</cell><cell>60.19 60.40</cell><cell>61.02 61.18</cell></row><row><cell>1024</cell><cell>FF MRL</cell><cell>41.32 41.31</cell><cell>55.20 55.24</cell><cell>58.85 58.86</cell><cell>60.46 60.42</cell><cell>61.38 61.34</cell></row><row><cell>2048</cell><cell>FF MRL</cell><cell>41.18 41.16</cell><cell>55.09 55.10</cell><cell>58.77 58.77</cell><cell>60.38 60.40</cell><cell>61.34 61.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 16 :</head><label>16</label><figDesc>Accuracy (%) categories indicates whether classes were present during ImageNet pretraining and head/tail indicates classes that have greater/less than 50 examples in the streaming test set. We observed that MRL performed better than the baseline on novel tail classes by ? 2% on average.</figDesc><table><row><cell cols="2">Rep. Size Method</cell><cell>Pretrain -Head (&gt;50)</cell><cell>Novel -Head (&gt;50)</cell><cell>Pretrain -Tail (&lt;50)</cell><cell>Novel -Tail (&lt;50)</cell><cell>Mean Per Class Acc.</cell><cell>Acc.</cell></row><row><cell></cell><cell>FF</cell><cell>68.04</cell><cell>11.30</cell><cell>33.18</cell><cell>0.36</cell><cell>16.29</cell><cell>28.47</cell></row><row><cell>8</cell><cell>MRL</cell><cell>71.75</cell><cell>10.70</cell><cell>38.29</cell><cell>0.19</cell><cell>17.15</cell><cell>29.34</cell></row><row><cell></cell><cell>MRL-E</cell><cell>57.40</cell><cell>6.25</cell><cell>23.14</cell><cell>0.04</cell><cell>11.78</cell><cell>22.81</cell></row><row><cell></cell><cell>FF</cell><cell>80.74</cell><cell>19.12</cell><cell>63.29</cell><cell>2.78</cell><cell>25.65</cell><cell>37.61</cell></row><row><cell>16</cell><cell>MRL</cell><cell>81.79</cell><cell>17.90</cell><cell>61.39</cell><cell>1.95</cell><cell>24.73</cell><cell>37.59</cell></row><row><cell></cell><cell>MRL-E</cell><cell>79.08</cell><cell>9.15</cell><cell>60.33</cell><cell>0.08</cell><cell>20.45</cell><cell>30.24</cell></row><row><cell></cell><cell>FF</cell><cell>83.67</cell><cell>24.30</cell><cell>66.66</cell><cell>4.23</cell><cell>28.86</cell><cell>42.40</cell></row><row><cell>32</cell><cell>MRL</cell><cell>83.46</cell><cell>23.26</cell><cell>65.82</cell><cell>3.75</cell><cell>28.16</cell><cell>41.90</cell></row><row><cell></cell><cell>MRL-E</cell><cell>81.42</cell><cell>10.47</cell><cell>68.01</cell><cell>0.23</cell><cell>22.31</cell><cell>32.17</cell></row><row><cell></cell><cell>FF</cell><cell>84.12</cell><cell>27.49</cell><cell>68.20</cell><cell>5.17</cell><cell>30.64</cell><cell>45.18</cell></row><row><cell>64</cell><cell>MRL</cell><cell>84.46</cell><cell>27.61</cell><cell>67.59</cell><cell>6.22</cell><cell>31.03</cell><cell>45.35</cell></row><row><cell></cell><cell>MRL-E</cell><cell>82.57</cell><cell>13.23</cell><cell>70.18</cell><cell>0.52</cell><cell>23.83</cell><cell>34.74</cell></row><row><cell></cell><cell>FF</cell><cell>84.87</cell><cell>29.96</cell><cell>68.79</cell><cell>5.54</cell><cell>31.84</cell><cell>47.06</cell></row><row><cell>128</cell><cell>MRL</cell><cell>84.88</cell><cell>30.86</cell><cell>68.58</cell><cell>8.41</cell><cell>33.23</cell><cell>47.79</cell></row><row><cell></cell><cell>MRL-E</cell><cell>82.76</cell><cell>18.93</cell><cell>64.46</cell><cell>2.22</cell><cell>25.75</cell><cell>39.19</cell></row><row><cell></cell><cell>FF</cell><cell>84.77</cell><cell>32.78</cell><cell>69.96</cell><cell>7.21</cell><cell>33.65</cell><cell>49.15</cell></row><row><cell>256</cell><cell>MRL</cell><cell>85.10</cell><cell>32.91</cell><cell>69.39</cell><cell>9.99</cell><cell>34.74</cell><cell>49.39</cell></row><row><cell></cell><cell>MRL-E</cell><cell>82.96</cell><cell>22.63</cell><cell>64.55</cell><cell>3.59</cell><cell>27.64</cell><cell>41.96</cell></row><row><cell></cell><cell>FF</cell><cell>85.62</cell><cell>35.27</cell><cell>70.27</cell><cell>9.05</cell><cell>35.42</cell><cell>51.14</cell></row><row><cell>512</cell><cell>MRL</cell><cell>85.62</cell><cell>34.67</cell><cell>70.24</cell><cell>11.43</cell><cell>36.11</cell><cell>50.79</cell></row><row><cell></cell><cell>MRL-E</cell><cell>82.86</cell><cell>25.62</cell><cell>64.34</cell><cell>4.99</cell><cell>29.22</cell><cell>44.20</cell></row><row><cell></cell><cell>FF</cell><cell>86.30</cell><cell>37.49</cell><cell>71.12</cell><cell>10.92</cell><cell>37.14</cell><cell>52.88</cell></row><row><cell>1024</cell><cell>MRL</cell><cell>85.64</cell><cell>35.88</cell><cell>70.02</cell><cell>12.19</cell><cell>36.80</cell><cell>51.58</cell></row><row><cell></cell><cell>MRL-E</cell><cell>83.03</cell><cell>27.78</cell><cell>64.58</cell><cell>6.32</cell><cell>30.57</cell><cell>45.71</cell></row><row><cell></cell><cell>FF</cell><cell>86.40</cell><cell>37.09</cell><cell>71.74</cell><cell>10.77</cell><cell>37.04</cell><cell>52.67</cell></row><row><cell>2048</cell><cell>MRL</cell><cell>85.60</cell><cell>36.83</cell><cell>70.34</cell><cell>12.88</cell><cell>37.46</cell><cell>52.18</cell></row><row><cell></cell><cell>MRL-E</cell><cell>83.01</cell><cell>29.99</cell><cell>65.37</cell><cell>7.60</cell><cell>31.97</cell><cell>47.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 17 :</head><label>17</label><figDesc>Top-1 classification accuracy (%) on out-of-domain datasets (ImageNet-V2/R/A/Sketch) to examine robustness of Matryoshka Representation Learning. Note that these results are without any fine tuning on these datasets.</figDesc><table><row><cell></cell><cell></cell><cell>ImageNet-V1</cell><cell></cell><cell>ImageNet-V2</cell><cell></cell><cell>ImageNet-R</cell><cell></cell><cell>ImageNet-A</cell><cell cols="2">ImageNet-Sketch</cell></row><row><cell>Rep. Size</cell><cell>FF</cell><cell>MRL-E MRL</cell><cell>FF</cell><cell>MRL-E MRL</cell><cell>FF</cell><cell>MRL-E MRL</cell><cell>FF</cell><cell>MRL-E MRL</cell><cell>FF</cell><cell>MRL-E MRL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 18 :</head><label>18</label><figDesc>Zero-shot top-1 image classification accuracy (%) of a ALIGN-MRL model on ImageNet-V1/V2/R/A and ObjectNet.</figDesc><table><row><cell>Rep. Size</cell><cell>V1</cell><cell>V2</cell><cell>A</cell><cell>R</cell><cell>ObjectNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 20 :</head><label>20</label><figDesc>FAISS<ref type="bibr" target="#b44">[45]</ref> index size and build times for exact k-NN search with L2 Distance metric and approximate k-NN search with HNSW32<ref type="bibr" target="#b59">[59]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Exact Search</cell><cell></cell><cell></cell><cell cols="2">HNSW32</cell></row><row><cell>Rep. Size</cell><cell cols="2">ImageNet-1K</cell><cell cols="2">ImageNet-4K</cell><cell cols="2">ImageNet-1K</cell><cell cols="2">ImageNet-4K</cell></row><row><cell></cell><cell>Index Size</cell><cell>Index Build</cell><cell>Index Size</cell><cell>Index Build</cell><cell>Index Size</cell><cell>Index Build</cell><cell>Index Size</cell><cell>Index Build</cell></row><row><cell></cell><cell>(MB)</cell><cell>Time (s)</cell><cell>(MB)</cell><cell>Time (s)</cell><cell>(MB)</cell><cell>Time (s)</cell><cell>(MB)</cell><cell>Time (s)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 21 :</head><label>21</label><figDesc>Retrieval k-NN wall clock search times (s) over entire validation (query) set of ImageNet-1K over various shortlist lengths k.</figDesc><table><row><cell>Index</cell><cell cols="5">k = 50 k = 100 k = 200 k = 500 k = 1000 k = 2048</cell></row><row><cell cols="2">Exact L2 0.4406 0.4605</cell><cell>0.5736</cell><cell>0.6060</cell><cell>1.2781</cell><cell>2.7047</cell></row><row><cell cols="2">HNSW32 0.1193 0.1455</cell><cell>0.1833</cell><cell>0.2145</cell><cell>0.2333</cell><cell>0.2670</cell></row><row><cell cols="6">observation on the expected dimensionality for 76.30% top-1 classification accuracy being just</cell></row><row><cell>d ? 37.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 22 :</head><label>22</label><figDesc>Percentage of ImageNet-1K validation set that is first correctly predicted using each representation size d. We note that 18.46% of the samples cannot be correctly predicted by any representation size. The remaining 81.54% constitutes the oracle accuracy.</figDesc><table><row><cell>Rep. Size</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>128 256 512 1024 2048</cell><cell>Always Wrong</cell></row><row><cell>Correctly Predicted</cell><cell cols="5">67.46 8.78 2.58 1.35 0.64 0.31 0.20 0.12 0.06</cell><cell>18.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 23 :</head><label>23</label><figDesc>Oracle classification accuracy of various evaluation datasets for ResNet50-MRL model trained on ImageNet-1K.</figDesc><table><row><cell>Top-1</cell><cell cols="5">ImageNetV1 ImageNetV2 ImageNet-A ImageNet-R ImageNet-Sketch</cell></row><row><cell>FF-2048</cell><cell>76.9</cell><cell>64.9</cell><cell>3.6</cell><cell>35.1</cell><cell>23.7</cell></row><row><cell>MRL-Oracle</cell><cell>81.5</cell><cell>70.6</cell><cell>8.7</cell><cell>39.8</cell><cell>28.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 24 :</head><label>24</label><figDesc><ref type="bibr" target="#b29">30</ref> Superclasses in ImageNet-1K corresponding to the performance inTable 25.</figDesc><table><row><cell>insect</cell><cell>motor vehicle</cell><cell>artiodactyl</cell><cell>vegetable</cell><cell>game equipment</cell></row><row><cell>terrier</cell><cell>serpent</cell><cell>machine</cell><cell>measuring device</cell><cell>sheepdog</cell></row><row><cell cols="3">protective covering sporting dog vessel, watercraft</cell><cell>building</cell><cell>lizard</cell></row><row><cell>garment</cell><cell>hound</cell><cell>monkey</cell><cell>home appliance</cell><cell>wind instrument</cell></row><row><cell>vessel</cell><cell>fish</cell><cell>nourishment</cell><cell>electronic equipment</cell><cell>oscine</cell></row><row><cell>furniture</cell><cell>wading bird</cell><cell>tool</cell><cell>canine</cell><cell>mechanism</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 25 :</head><label>25</label><figDesc>Performance of MRL model on 31-way classification (1 extra class is for reject token) on ImageNet-1K superclasses. MRL 85.57 88.67 89.48 89.82 89.97 90.11 90.18 90.22 90.21</figDesc><table><row><cell>Rep. Size</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024 2048</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 26 :</head><label>26</label><figDesc>Top-1 classification accuracy (%) on ImageNet-1K of various ResNet50 models which are finetuned on pretrained FF-2048 model. We observed that adding more non-linearities is able to induce nesting to a reasonable extent even if the model was not pretrained with nesting in mind.</figDesc><table><row><cell>Rep. Size</cell><cell>fc</cell><cell>4.2 conv3, fc</cell><cell>4.2 conv2, conv3, fc</cell><cell>4.2 full, fc</cell><cell>All (MRL)</cell></row><row><cell>8</cell><cell>5.15</cell><cell>36.11</cell><cell>54.78</cell><cell>60.02</cell><cell>66.63</cell></row><row><cell>16</cell><cell>13.79</cell><cell>58.42</cell><cell>67.26</cell><cell>70.10</cell><cell>73.53</cell></row><row><cell>32</cell><cell>32.52</cell><cell>67.81</cell><cell>71.62</cell><cell>72.84</cell><cell>75.03</cell></row><row><cell>64</cell><cell>52.66</cell><cell>72.42</cell><cell>73.61</cell><cell>74.29</cell><cell>75.82</cell></row><row><cell>128</cell><cell>64.60</cell><cell>74.41</cell><cell>74.67</cell><cell>75.03</cell><cell>76.30</cell></row><row><cell>256</cell><cell>69.29</cell><cell>75.30</cell><cell>75.23</cell><cell>75.38</cell><cell>76.47</cell></row><row><cell>512</cell><cell>70.51</cell><cell>75.96</cell><cell>75.47</cell><cell>75.64</cell><cell>76.65</cell></row><row><cell>1024</cell><cell>70.19</cell><cell>76.18</cell><cell>75.70</cell><cell>75.75</cell><cell>76.76</cell></row><row><cell>2048</cell><cell>69.72</cell><cell>76.44</cell><cell>75.96</cell><cell>75.97</cell><cell>76.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 27 :</head><label>27</label><figDesc>An ablation over boosting training loss at lower nesting dimensions, with top-1 and top-5 accuracy (%). The models are described in Appendix K.1.</figDesc><table><row><cell>Model</cell><cell>MRL</cell><cell>MRL-8boost</cell><cell cols="2">MRL-8+16boost</cell></row><row><cell cols="4">Rep. Size Top-1 Top-5 Top-1 Top-5 Top-1</cell><cell>Top-5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 30 :</head><label>30</label><figDesc>Adaptive retrieval ablation over shortlist length k for D s = 16, D r = 2048 on ImageNet-1K with exact search. Entries with the highest P@1 and mAP@10 across all k are in bold.</figDesc><table><row><cell>Shortlist Length</cell><cell cols="9">P@1 mAP@10 mAP@25 mAP@50 mAP@100 P@10 P@25 P@50 P@100</cell></row><row><cell>100</cell><cell>70.88</cell><cell>65.19</cell><cell>63.62</cell><cell>62.59</cell><cell>61.24</cell><cell cols="3">69.96 69.24 68.53</cell><cell>67.20</cell></row><row><cell>200</cell><cell>70.90</cell><cell>65.27</cell><cell>63.73</cell><cell>62.82</cell><cell>61.97</cell><cell cols="3">70.10 69.44 68.90</cell><cell>68.21</cell></row><row><cell>400</cell><cell>70.94</cell><cell>65.26</cell><cell>63.71</cell><cell>62.81</cell><cell>62.03</cell><cell cols="3">70.15 69.51 69.02</cell><cell>68.47</cell></row><row><cell>800</cell><cell>70.96</cell><cell>65.23</cell><cell>63.64</cell><cell>62.69</cell><cell>61.85</cell><cell cols="3">70.16 69.52 69.02</cell><cell>68.45</cell></row><row><cell>1600</cell><cell>70.96</cell><cell>65.20</cell><cell>63.58</cell><cell>62.58</cell><cell>61.66</cell><cell>70.16</cell><cell>69.5</cell><cell>68.97</cell><cell>68.36</cell></row><row><cell>2048</cell><cell>70.97</cell><cell>65.20</cell><cell>63.57</cell><cell>62.58</cell><cell>61.64</cell><cell>70.16</cell><cell>69.5</cell><cell>68.97</cell><cell>68.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 31 :</head><label>31</label><figDesc>Adaptive retrieval ablation over shortlist length k for D s = 16, D r = 2048 on ImageNet-4K with exact search.</figDesc><table><row><cell>Shortlist Length</cell><cell cols="7">P@1 mAP@10 mAP@25 mAP@50 mAP@100 P@10 P@25 P@50 P@100</cell></row><row><cell>100</cell><cell>27.70</cell><cell>14.38</cell><cell>10.62</cell><cell>8.26</cell><cell>6.07</cell><cell>20.12 16.87 14.29</cell><cell>11.26</cell></row><row><cell>200</cell><cell>28.56</cell><cell>15.21</cell><cell>11.43</cell><cell>9.11</cell><cell>7.12</cell><cell>21.23 18.13 15.73</cell><cell>13.27</cell></row><row><cell>400</cell><cell>29.34</cell><cell>15.83</cell><cell>12.06</cell><cell>9.76</cell><cell>7.79</cell><cell>22.08 19.09 16.83</cell><cell>14.54</cell></row><row><cell>800</cell><cell>29.86</cell><cell>16.30</cell><cell>12.53</cell><cell>10.23</cell><cell>8.26</cell><cell>22.72 19.83 17.65</cell><cell>15.45</cell></row><row><cell>1600</cell><cell>30.24</cell><cell>16.63</cell><cell>12.86</cell><cell>10.56</cell><cell>8.60</cell><cell>23.18 20.36 18.23</cell><cell>16.11</cell></row><row><cell>2048</cell><cell>30.35</cell><cell>16.73</cell><cell>12.96</cell><cell>10.65</cell><cell>8.69</cell><cell>23.31 20.50 18.40</cell><cell>16.30</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to Srinadh Bhojanapalli, Lovish Madaan, Raghav Somani and Ludwig Schmidt for helpful discussions and feedback. Aditya Kusupati also thanks Tom Duerig and Rahul Sukthankar for their support. Part of the paper's large-scale experimentation is supported through a research GCP credit award from Google Cloud and Google Research. Gantavya Bhatt is supported in part by the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA. Sham Kakade acknowledges funding from the NSF award CCF-1703574 and ONR N00014-22-1-2377. Ali Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543 and gifts from Allen Institute for Artificial Intelligence.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>These results provide further evidence that different tasks require varying capacity based on their difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Robustness Experiments</head><p>We evaluated the robustness of MRL models on out-of-domain datasets (ImageNetV2/R/A/Sketch) and compared them to the FF baseline. Each of these datasets is described in Appendix B. The results in <ref type="table">Table 17</ref> demonstrate that learning Matryoshka Representations does not hurt out-ofdomain generalization relative to FF models, and Matryoshka Representations in fact improve the performance on ImageNet-A. For a ALIGN-MRL model, we examine the the robustness via zero-shot retrieval on out-of-domain datasets, including ObjectNet, in <ref type="table">Table 18</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I In Practice Costs</head><p>All approximate NN search experiments via HNSW32 were run on an Intel Xeon 2.20GHz CPU with 24 cores. All exact search experiments were run with CUDA 11.0 on 2xA100-SXM4 NVIDIA GPUs with 40G RAM each.</p><p>MRL models. As MRL makes minimal modifications to the ResNet50 model in the final fc layer via multiple heads for representations at various scales, it has only an 8MB storage overhead when compared to a standard ResNet50 model. MRL-E has no storage overhead as it has a shared head for logits at the final fc layer.</p><p>Retrieval Exact search has a search time complexity of O(dkN ), and HNSW has a search time complexity of O(dk log(N )), where N is the database size, d is the representation size, and k is the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/.Softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Label embedding trees for large multi-class tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML workshop on unsupervised and transfer learning</title>
		<meeting>ICML workshop on unsupervised and transfer learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">K-d trees for semidynamic point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth annual symposium on Computational geometry</title>
		<meeting>the sixth annual symposium on Computational geometry</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cover trees for nearest neighbor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09791</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pre-training tasks for embeddingbased large-scale retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03932</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extreme multi-label learning for semantic matching in product search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kolluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shandilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ievgrafov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2643" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Meta-baseline: exploring simple meta-learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9062" to="9071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Locality-sensitive hashing scheme based on p-stable distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immorlica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twentieth annual symposium on Computational geometry</title>
		<meeting>the twentieth annual symposium on Computational geometry</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Challenges in building large-scale information retrieval systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Keynote of the 2nd ACM International Conference on Web Search and Data Mining (WSDM)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical semantic indexing for large scale image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="785" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Virtex: Learning visual representations from textual annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11162" to="11173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Solving multiclass learning problems via error-correcting output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bakiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="263" to="286" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning everything about anything: Weblysupervised visual concept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3270" to="3277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robustness</surname></persName>
		</author>
		<ptr target="https://github.com/MadryLab/robustness" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>python library</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A survey of quantization methods for efficient neural network inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13630</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Coarse-grained information dominates fine-grained information in judgments of time-to-contact from retinal flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Giachritsis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="601" to="611" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<title level="m">Masked autoencoders are scalable vision learners</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Time course of visual perception: coarse-to-fine processing and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hegd?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in neurobiology</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="405" to="439" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02136</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8340" to="8349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Natural adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15262" to="15271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05248</idno>
		<title level="m">What do compressed deep neural networks forget? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Moorosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03058</idno>
		<title level="m">Characterising bias in compressed models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Analysis of a complex of statistical variables into principal components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">417</biblScope>
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning anytime predictions in neural networks via adaptive loss balancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3812" to="3821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors: towards removing the curse of dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirtieth annual ACM symposium on Theory of computing</title>
		<meeting>the thirtieth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Slice: Scalable linear extreme classifiers trained on 100 million labels for related searches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chunduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="528" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Diskann: Fast accurate billion-point nearest neighbor search on a single node</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Devvrit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Simhadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishnawamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kadekodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Extensions of lipschitz mappings into a hilbert space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemp. Math</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="189" to="206" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th annual international symposium on computer architecture</title>
		<meeting>the 44th annual international symposium on computer architecture</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Vertex ai matching engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Sato</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Microsoft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blog</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/blog/topics/developers-practitioners/find-anything-blazingly-fast-googles-vector-search-technology" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast similarity search for learned metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2143" to="2157" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fastgrnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Soft threshold weight reparameterization for learnable sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanujan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Somani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5544" to="5555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Llc: Accurate, multi-purpose learnt low-dimensional binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wallingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanujan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Somani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pillutla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Leclerc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<ptr target="https://github.com/libffcv/ffcv/,2022.commit607d117" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Deep learning. nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Stochastic multiple choice learning for training diverse deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushwalkam Shiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">An algorithm for vector quantizer design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on communications</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Yashunin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="824" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Stacked convolutional auto-encoders for hierarchical feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cire?an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection using feature similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="312" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Understanding searches better than ever before</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nayak</surname></persName>
		</author>
		<ptr target="https://blog.google/products/search/search-language-understanding-bert/" />
	</analytic>
	<monogr>
		<title level="j">Google AI Blog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
		<ptr target="https://aclanthology.org/N18-1202" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Extreme regression for dynamic search advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="456" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/language-unsupervised/" />
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5389" to="5400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning ordered representations with nested dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelbart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Transfer learning in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: Tutorials</title>
		<meeting>the 2019 conference of the North American chapter of the association for computational linguistics: Tutorials</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning a nonlinear embedding by preserving class neighbourhood structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="412" to="419" />
			<date type="published" when="2007" />
			<publisher>PMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="969" to="978" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">On the use of neighbourhood-based non-parametric classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Ferri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1179" to="1186" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4596" to="4604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE winter conference on applications of computer vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">The implicit bias of gradient descent on separable data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Nacson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2822" to="2878" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Dimensionality reduction: a comparative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Postma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Den Herik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Extreme classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="44" to="45" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE computer society conference on computer vision and pattern recognition</title>
		<meeting>the 2001 IEEE computer society conference on computer vision and pattern recognition</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">As search needs evolve, microsoft makes ai tools for better search available to researchers and developers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Waldburger</surname></persName>
		</author>
		<ptr target="https://blogs.microsoft.com/ai/bing-vector-search/" />
	</analytic>
	<monogr>
		<title level="m">Microsoft AI Blog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Are we overfitting to experimental setups in recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wallingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alizadeh-Vahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02519</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Task adaptive parameter sharing for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wallingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.16708</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10506" to="10518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Multiple networks are more efficient than one: Fast and accurate models via ensembles and cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eban</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.01988</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Robust fine-tuning of zero-shot models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01903</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning via non-parametric instancelevel discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01978</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Pecos: Prediction for enormous and correlated output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">98</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08928</idno>
		<title level="m">Slimmable neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02639</idno>
		<title level="m">Merlot reserve: Neural script knowledge through vision and language and sound</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Retrieval k-NN wall clock search times (s) over the entire validation (query) set of ImageNet-1K and ImageNet-4K, containing 50K and 200K samples respectively</title>
		<idno>MRL-4 MRL-6 MRL-8</idno>
	</analytic>
	<monogr>
		<title level="m">Rep. Size ImageNet-1K ImageNet-4K 2. Rep. Size</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">An ablation over training MRL with nesting list at uniformly distributed granularities. Entries in the MRL-Uniform column are evaluated at logarithmic dimensions for a fair comparison to MRL-Log (standard MRL) with 1-NN accuracy (%)</title>
	</analytic>
	<monogr>
		<title level="m">Rep. Size MRL-Log MRL-Uniform</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">This is likely due to the increased database size, but could also indicate a correlation with ImageNet-4K being slightly out-of-distribution making the task at hand harder</title>
	</analytic>
	<monogr>
		<title level="m">FAISS framework, steadily improved performance on ImageNet-4K</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
