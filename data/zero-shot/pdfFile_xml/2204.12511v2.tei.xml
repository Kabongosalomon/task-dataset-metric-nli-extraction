<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">POLYLOSS: A POLYNOMIAL EXPANSION PERSPEC- TIVE OF CLASSIFICATION LOSS FUNCTIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoqi</forename><surname>Leng</surname></persName>
							<email>lengzhaoqi@waymo.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Waymo LLC 2 Google LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
							<email>tanmingxing@waymo.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Waymo LLC 2 Google LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
							<email>cxliu@waymo.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Waymo LLC 2 Google LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Waymo LLC 2 Google LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Shi</surname></persName>
							<email>xiaojies@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Waymo LLC 2 Google LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Cheng</surname></persName>
							<email>shuyangcheng@waymo.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Waymo LLC 2 Google LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
							<email>dragomir@waymo.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Waymo LLC 2 Google LLC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">POLYLOSS: A POLYNOMIAL EXPANSION PERSPEC- TIVE OF CLASSIFICATION LOSS FUNCTIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022 Task ImageNet classification COCO det. and seg. Waymo Open Dataset 3D detection Default loss Cross-entropy Cross-entropy Focal loss Model ENetV2-L(21K) ENetV2-L(1K) Mask R-CNN PointPillars Car PointPillars Ped RSN Car RSN Ped Baseline 45.8 86.8 47.2 42.3 63.3 68.9 78.4 79.4 PolyLoss 46.4 (+0.6)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-entropy loss and focal loss are the most common choices when training deep neural networks for classification problems. Generally speaking, however, a good loss function can take on much more flexible forms, and should be tailored for different tasks and datasets. Motivated by how functions can be approximated via Taylor expansion, we propose a simple framework, named PolyLoss, to view and design loss functions as a linear combination of polynomial functions. Our PolyLoss allows the importance of different polynomial bases to be easily adjusted depending on the targeting tasks and datasets, while naturally subsuming the aforementioned cross-entropy loss and focal loss as special cases. Extensive experimental results show that the optimal choice within the PolyLoss is indeed dependent on the task and dataset. Simply by introducing one extra hyperparameter and adding one line of code, our Poly-1 formulation outperforms the crossentropy loss and focal loss on 2D image classification, instance segmentation, object detection, and 3D object detection tasks, sometimes by a large margin. Table 1: PolyLoss outperforms cross-entropy and focal loss on various models and tasks. Results are for the simplest Poly-1, which has only a single hyperparameter. On ImageNet (Deng et al., 2009), our PolyLoss improves both pretraining and finetuning for the recent EfficientNetV2 (Tan &amp; Le, 2021); on COCO (Lin et al., 2014), PolyLoss improves both 2D detection and segmentation AR for Mask-RCNN (He et al., 2017); on Waymo Open Dataset (WOD) (Sun et al., 2020), PolyLoss improves 3D detection AP for the widely used PointPillars (Lang et al., 2019) and the very recent Range Sparse Net (RSN) (Sun et al., 2021). Details are in</p><p>Table 4, 5, 7. arXiv:2204.12511v2 [cs.CV] 10 May 2022</p><p>Published as a conference paper at ICLR 2022</p><p>Our study shows that, in order to achieve better results, it is necessary to adjust polynomial coefficients ? j for different tasks and datasets. Since it is impossible to adjust an infinite number of ? j , we explore various strategies with a small degree of freedom. Perhaps surprisingly, we observe that simply adjusting the single polynomial coefficient for the leading polynomial, which we denote L Poly-1 , is sufficient to achieve significant improvements over the commonly used cross-entropy loss and focal loss. Overall, our contribution can be summarized as:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Loss functions are important in training neural networks. In principle, a loss function could be any (differentiable) function that maps predictions and labels to a scalar. Therefore, designing a good loss function is generally challenging due to its large design space, and designing a universal loss function that works across different tasks and datasets is even more challenging: for example, L1 / L2 losses are commonly used for regression tasks, but they are rarely used for classification tasks; focal loss is often used to alleviate the overfitting issue of cross-entropy loss for imbalanced object detection datasets <ref type="bibr" target="#b21">(Lin et al., 2017)</ref>, but it is not shown to consistently help other tasks. Many recent works have also explored new loss functions via meta-learning, ensembling or compositing different losses <ref type="bibr" target="#b11">(Hajiabadi et al., 2017;</ref><ref type="bibr" target="#b38">Xu et al., 2018;</ref><ref type="bibr" target="#b10">Gonzalez &amp; Miikkulainen, 2020b;</ref><ref type="bibr">a;</ref>.</p><p>In this paper, we propose PolyLoss: a novel framework for understanding and designing loss functions. Our key insight is to decompose commonly used classification loss functions, such as crossentropy loss and focal loss, into a series of weighted polynomial bases. They are decomposed in the form of ? j=1 ? j (1 ? P t ) j , where ? j ? R + is the polynomial coefficient and P t is the prediction probability of the target class label. Each polynomial base (1 ? P t ) j is weighted by a corresponding polynomial coefficient ? j , which enables us to easily adjust the importance of different bases for different applications. When ? j = 1/j for all j, our PolyLoss becomes equivalent to the commonly used cross-entropy loss, but this coefficient assignment may not be optimal.</p><p>? Insights on common losses: We propose a unified framework, named PolyLoss, to rethink and redesign loss functions. This framework helps to explain cross-entropy loss and focal loss as two special cases of the PolyLoss family (by horizontally shifting polynomial coefficients), which was not recognized before. This new finding motivates us to investigate new loss functions that vertically adjust polynomial coefficients, shown in <ref type="figure" target="#fig_0">Figure 1</ref>. ? New loss formulation: We evaluate different ways of vertically manipulating polynomial coefficients to simplify the hyperparameters search space. We propose a simple and effective Poly-1 loss formulation which only introduces one hyperparameter and one line of code. ? New findings: We identify that focal loss, though effective for many detection tasks, is suboptimal for the imbalanced ImageNet-21K. We find the leading polynomial contributes to a large portion of the gradient during training, and its coefficient correlates to the prediction confidence P t . In addition, we provide an intuitive explanation on how to leverage this correlation to design good PolyLoss tailored to imbalanced datasets. ? Extensive experiments: We evaluate our PolyLoss on different tasks, models, and datasets. Results show PolyLoss consistently improves the performance on all fronts, summarized in <ref type="table">Table 1</ref>, which includes the state-of-the-art classifiers EfficientNetV2 and detectors RSN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Cross-entropy loss is used in popular and current state-of-the-art models for perception tasks such as classification, detection and semantic segmentation <ref type="bibr" target="#b32">(Tan &amp; Le, 2021;</ref><ref type="bibr" target="#b14">He et al., 2017;</ref><ref type="bibr" target="#b41">Zoph et al., 2020;</ref>. Various losses are proposed to improve cross-entropy loss <ref type="bibr" target="#b21">(Lin et al., 2017;</ref><ref type="bibr" target="#b17">Law &amp; Deng, 2018;</ref><ref type="bibr" target="#b2">Cui et al., 2019;</ref><ref type="bibr" target="#b40">Zhao et al., 2021)</ref>. Unlike prior works, the goal of this paper is to provide a unified framework for systematically designing a better classification loss function. Loss for class imbalance Training detection models, especially single-stage detectors, is difficult due to class imbalance. Common approaches such as hard example mining and reweighing are developed to address the class imbalance issue <ref type="bibr" target="#b30">(Sung, 1996;</ref><ref type="bibr" target="#b35">Viola &amp; Jones, 2001;</ref><ref type="bibr" target="#b5">Felzenszwalb et al., 2010;</ref><ref type="bibr" target="#b27">Shrivastava et al., 2016;</ref><ref type="bibr" target="#b22">Liu et al., 2016;</ref><ref type="bibr" target="#b1">Bulo et al., 2017)</ref>. As one of these approaches, focal loss is designed to mitigate the class imbalance issue by focusing on the hard examples and is used to train state-of-the-art 2D and 3D detectors <ref type="bibr" target="#b21">(Lin et al., 2017;</ref><ref type="bibr" target="#b4">Du et al., 2020;</ref><ref type="bibr" target="#b26">Shi et al., 2020;</ref>. In our work, we found that focal loss is suboptimal for the imbalanced ImageNet-21K. Using the PolyLoss framework, we discover a better loss function, which performs the opposite role of focal loss. We further provide intuitive understanding of why it is important to design different loss functions tailored to different imbalanced datasets using the PolyLoss framework. Robust loss to label noise Another direction of research is to design loss functions that are robust to label noise <ref type="bibr" target="#b7">(Ghosh et al., 2015;</ref><ref type="bibr" target="#b39">Zhang &amp; Sabuncu, 2018;</ref><ref type="bibr" target="#b36">Wang et al., 2019;</ref><ref type="bibr">Oksuz et al., 2020;</ref><ref type="bibr" target="#b23">Menon et al., 2019)</ref>. A commonly used approach is to incorporate noise robust loss function such as Mean Absolute Error (MAE) into cross-entropy loss. In particular, Taylor cross entropy loss is proposed to unify MAE and cross-entropy loss by expanding the cross-entropy loss in (1 ? P t ) j polynomial bases <ref type="bibr" target="#b6">(Feng et al., 2020)</ref>. By truncating the higher-order polynomials, they show truncated cross-entropy loss function is closer to MAE, which is more robust to label noise on datasets with synthetic label noise. In contrast, our PolyLoss provides a more general framework to design loss functions for different datasets by manipulating polynomial coefficients, which includes dropping higher-order polynomials proposed in <ref type="bibr" target="#b6">Feng et al. (2020)</ref>. Our experiments in subsection 4.1 show the loss proposed in <ref type="bibr" target="#b6">Feng et al. (2020)</ref> performs worse than cross-entropy loss on the clean ImageNet dataset. Learned loss functions Several recent works demonstrate learning the loss function during training via gradient descent or meta learning <ref type="bibr" target="#b11">(Hajiabadi et al., 2017;</ref><ref type="bibr" target="#b38">Xu et al., 2018;</ref><ref type="bibr" target="#b9">Gonzalez &amp; Miikkulainen, 2020a;</ref>. Notably, TaylorGLO utilizes CMA-ES to optimize multivariate Taylor parameterization of a loss function and learning rate schedule during training <ref type="bibr" target="#b12">(Hansen &amp; Ostermeier, 1996;</ref><ref type="bibr" target="#b10">Gonzalez &amp; Miikkulainen, 2020b</ref>   j=1 ? j (1 ? P t ) j is a more general framework, where P t stands for prediction probability of the target class. Left: Polyloss is more flexible: it can be steeper (deep red) than cross-entropy loss (black) or flatter (light red) than focal loss (green). Right: Polynomial coefficients of different loss functions in the bases of (1 ? P t ) j , where j ? Z + . Black dash lines are drawn to show the trend of polynomial coefficients. In the PolyLoss framework, focal loss can only shift the polynomial coefficients horizontally (green arrow), see Equation 2, whereas the proposed PolyLoss framework is more general, which also allows vertical adjustment (red arrows) of the polynomial coefficient for each polynomial term. of polynomials, the paper demonstrates that using the third-order parameterization (8 parameters), the learned loss function schedule outperforms cross-entropy loss on 10-class classification problems. Our paper <ref type="figure">(Figure 2a</ref>), on the other hand, shows for 1000-class classification tasks, hundreds of polynomials are needed. This results in a prohibitively large search space. Our proposed Poly-1 formulation mitigates the challenge of the large search space and do not rely on advanced black-box optimization algorithms. Instead, we show a simple grid search over one hyperparameter can lead to significant improvement on all tasks that we investigate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">POLYLOSS</head><p>PolyLoss provides a framework for understanding and improving the commonly used cross-entropy loss and focal loss, visualized in <ref type="figure" target="#fig_0">Figure 1</ref>. It is inspired from the Taylor expansion of cross-entropy loss (Equation 1) and focal loss (Equation 2) in the bases of (1 ? P t ) j :</p><formula xml:id="formula_0">L CE = ? log(P t ) = ? j=1 1/j(1 ? P t ) j = (1 ? P t ) + 1/2(1 ? P t ) 2 ...</formula><p>(1)</p><formula xml:id="formula_1">L FL = ?(1 ? P t ) ? log(P t ) = ? j=1 1/j(1 ? P t ) j+? = (1 ? P t ) 1+? + 1/2(1 ? P t ) 2+? ...<label>(2)</label></formula><p>where P t is the model's prediction probability of the target ground-truth class.</p><p>Cross-entropy loss as PolyLoss Using the gradient descent method to optimize the cross-entropy loss requires taking the gradient with respect to P t . In the PolyLoss framework, an interesting observation is that the coefficients 1/j exactly cancel the jth power of the polynomial bases, see Equation <ref type="formula">1</ref>. Thus, the gradient of cross-entropy loss is simply the sum of polynomials (1 ? P t ) j , shown in Equation <ref type="formula" target="#formula_2">3</ref>.</p><formula xml:id="formula_2">? dL CE dP t = ? j=1 (1 ? P t ) j?1 = 1 + (1 ? P t ) + (1 ? P t ) 2 ...<label>(3)</label></formula><p>The polynomial terms in the gradient expansion capture different sensitivity with respect to P t . The leading gradient term is 1, which provides a constant gradient regardless of the value of P t . On the contrary, when j 1, the jth gradient term is strongly suppressed when P t gets closer to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Focal loss as PolyLoss</head><p>In the PolyLoss framework, Equation 2, it is apparent that the focal loss simply shifts the power j by the power of a modulating factor ?. This is equivalent to horizontally shifting all the polynomial coefficients by ? as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. To understand the focal loss from a gradient prospective, we take the gradient of the focal loss (Equation 2) with respect to P t :</p><formula xml:id="formula_3">? dL FL dP t = ? j=1 (1 + ?/j)(1 ? P t ) j+??1 = (1 + ?)(1 ? P t ) ? + (1 + ?/2)(1 ? P t ) 1+? ...<label>(4)</label></formula><p>For a positive ?, the gradient of focal loss drops the constant leading gradient term, 1, in the crossentropy loss, see Equation <ref type="formula" target="#formula_2">3</ref>. As discussed in the previous paragraph, this constant gradient term causes the model to emphasize the majority class, since its gradient is simply the total number of Polynomial expansion in the basis of (1 ? Pt) Loss examples for each class. By shifting the power of all the polynomial terms by ?, the first term then becomes (1 ? P t ) ? , which is suppressed by the power of ? to avoid overfitting to the already confident (meaning P t close to 1) majority class. More details are shown in section 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-entropy loss</head><formula xml:id="formula_4">(1 ? Pt) + 1/2(1 ? Pt) 2 + ... + 1/N (1 ? Pt) N + 1/(N + 1)(1 ? Pt) N +1 + ... LCE = ? log(Pt) Drop poly. (Sec 4.1) (1 ? Pt) + 1/2(1 ? Pt) 2 + ... + 1/N (1 ? Pt) N (drop the remaining terms) LDrop = LCE ? ? j=N 1/j(1 ? Pt) j Poly-N (Sec 4.2) ( 1 + 1)(1 ? Pt) + ... + ( N + 1/N )D N t + 1/(N + 1)(1 ? Pt) N +1 + ... LPoly-N = LCE + N j=1 j (1 ? Pt) i Poly-1 (Sec 4.3) ( 1 + 1)(1 ? Pt) + 1/2(1 ? Pt) 2 + ... + 1/N (1 ? Pt) N + 1/(N + 1)(1 ? Pt) N +1 + ... LPoly-1 = LCE + 1(1 ? Pt)</formula><p>Connection to regression and general form Representing the loss function in the PolyLoss framework provides an intuitive connection to regression. For classification tasks where y = 1 is the effective probability of the ground-truth label, the polynomial bases (1 ? P t ) j can be expressed as (y ? P t ) j . Thus both cross-entropy loss and focal loss can be interpreted as a weighted ensemble of distances between the prediction and label to the jth power. However, a fundamental question in those losses: Are the coefficients in front of the regression terms optimal?</p><p>In general, PolyLoss is a monotone decreasing function 1 on [0, 1] which can be expressed as ? j=1 ? j (1 ? P t ) j and provides a flexible framework to adjust each coefficient 2 . PolyLoss can be generalized to non-integer j, but for simplicity we only focus on integer power (j ? Z + ) in this paper. In the next section, we investigate several strategies on designing better loss functions in the PolyLoss framework via manipulating ? j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">UNDERSTANDING THE EFFECT OF POLYNOMIAL COEFFICIENTS</head><p>In the previous section, we established the PolyLoss framework and showed that cross-entropy loss and focal loss simply correspond to different polynomial coefficients, where focal loss horizontally shifts the polynomial coefficients of cross-entropy loss.</p><p>In this section, we propose the final loss formulation Poly-1. We study in depth how vertically adjusting polynomial coefficients, shown in <ref type="figure" target="#fig_0">Figure 1</ref>, may affect training. Specifically, we explore three different strategies in assigning polynomial coefficients: dropping higher-order terms; adjusting multiple leading polynomial coefficients; and adjusting the first polynomial coefficient, summarized in <ref type="table" target="#tab_2">Table 2</ref>. We find adjusting the first polynomial coefficient (Poly-1 formulation) leads to maximal gain while requiring minimal code change and hyperparameter tuning.</p><p>In these explorations, we experiment with 1000-class ImageNet <ref type="bibr" target="#b3">(Deng et al., 2009)</ref> classification. We abbreviate it as ImageNet-1K to differentiate it from the full version, which contains 21K classes. We use ResNet-50 <ref type="bibr" target="#b13">(He et al., 2016)</ref> and its training hyperparameters without modification. 3 4.1 L Drop : REVISITING DROPPING HIGHER-ORDER POLYNOMIAL TERMS Prior works <ref type="bibr" target="#b6">(Feng et al., 2020;</ref><ref type="bibr" target="#b10">Gonzalez &amp; Miikkulainen, 2020b)</ref> have shown dropping the higherorder polynomials and tuning the leading polynomials can improve model robustness and performance. We adopt the same loss formulation L Drop = N j=1 1/j(1 ? P t ) j , as in <ref type="bibr" target="#b6">Feng et al. (2020)</ref>, and compare their performance with the baseline cross-entropy loss on ImageNet-1K. As shown in <ref type="figure">Figure 2a</ref>, we need to sum up more than 600 polynomial terms to match the accuracy of crossentropy loss. Notably, removing higher-order polynomials cannot simply be interpreted as adjusting the learning rate. To verify this, <ref type="figure">Figure 2b</ref> compares the performance for different learning rates with various cutoffs: no matter we increase or decrease the learning rate from the original value of 0.1, the accuracy worsens. Additional hyperparameter tuning is shown in section 9. <ref type="bibr">1</ref> We only consider the case all ?j ? 0 in this paper for simplicity. There exist monotone decreasing functions on [0, 1] with some ?j negative, for example sin(1 ? Pt) = ? j=0 (?1) j /(2j + 1)!(1 ? Pt) 2j+1 . 2 To ensure series converges, we require 1/ lim sup j?? j |?j| ? 1 for Pt in (0, 1]. For Pt = 0 we don't require point-wise convergence; in fact cross-entropy and focal loss both go to +?.</p><p>3 Code at https://github.com/tensorflow/tpu/tree/master/models/official/   <ref type="figure">Figure 2</ref>: Training ResNet-50 on ImageNet-1K requires hundreds of polynomial terms to reproduce the same accuracy as cross-entropy loss.</p><p>To understand why higher-order terms are important, we consider the residual sum after removing the first N polynomial terms from cross-entropy loss:</p><formula xml:id="formula_5">R N = L CE ? L Drop = ? j=N +1 1/j(1 ? P t ) j . Theorem 1. For any small ? &gt; 0, ? &gt; 0 if N &gt; log 1?? (? ? ?), then for any p ? [?, 1], we have |R N (p)| &lt; ? and |R N (p)| &lt; ?. (Proof in section 7)</formula><p>Hence, taking a large N is necessary to ensure L Drop is uniformly close to L CE in the perspectives of loss and loss derivative on <ref type="bibr">[?, 1]</ref>. For a fixed ?, as ? approaches 0, N grows rapidly. Our experimental results align with the theorem. The higher-order (j &gt; N + 1) polynomials play an important role during the early stages of training, where P t is typically close to zero. For example, when P t ? 0.001, according to Equation 3, the coefficient of the 500th term's gradient is 0.999 499 ? 0.6, which is fairly large. Different from aforementioned prior works, our results show that we cannot easily reduce the number of polynomial coefficients ? j by excluding the higher-order polynomials.</p><p>Dropping higher order polynomials is equivalent to pushing all the higher order (j &gt; N +1) polynomial coefficients ? j vertically to zero in the PolyLoss framework. Since simply setting coefficients to zero is suboptimal for training ImageNet-1K, in the following sections, we investigate how to manipulate polynomial coefficient beyond setting them to zero in the PolyLoss framework. In particular, we aim to propose a simple and effective loss function that requires minimal tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">L POLY-N : PERTURBING LEADING POLYNOMIAL COEFFICIENTS</head><p>In this paper, we propose an alternative way of designing a new loss function in the PolyLoss framework, where we adjust the coefficients of each polynomial. In general, there are infinitely many polynomial coefficients ? j need to be tuned. Thus, it is infeasible to optimize the most general loss:</p><formula xml:id="formula_6">L Poly = ? 1 (1 ? P t ) + ? 2 (1 ? P t ) 2 + ... + ? N (1 ? P t ) N + ... = ? j=1 ? j (1 ? P t ) j<label>(5)</label></formula><p>The previous section (subsection 4.1) has shown that hundreds of polynomials are required in training to do well on tasks such as ImageNet-1K classification. If we naively truncate the infinite sum in Equation 5 to the first few hundreds terms, tuning coefficients for so many polynomials still results in a prohibitively large search space. In addition, collectively tuning many coefficients also does not outperform cross-entropy loss, details in section 10.</p><p>To tackle this challenge, we propose to perturb the leading polynomial coefficients in cross-entropy loss, while keeping the rest the same. We denote the proposed loss formulation as Poly-N, where N stands for the number of leading coefficients that will be tuned.  Here, we replace the jth polynomial coefficient in crossentropy loss</p><formula xml:id="formula_7">L Poly-N = ( 1 + 1)(1 ? P t ) + ... + ( N + 1/N )(1 ? P t ) N perturbed by j + 1/(N + 1)(1 ? P t ) N +1 + ... same as LCE = ? log(P t ) + N j=1 j (1 ? P t ) j<label>(6)</label></formula><formula xml:id="formula_8">1/j with 1/j + j , where j ? [?1/j, ?)</formula><p>is the perturbation term. This allows us to pinpoint the first N polynomials without the need to worry about the infinitely many higher-order (j &gt; N + 1) coefficients, as in Equation <ref type="formula" target="#formula_6">5</ref>.  The first polynomial (1?P t ) contributes more than half of the cross-entropy gradient at the last 65% of the training steps, which highlights the importance of tuning the first polynomial. The red dash line shows the crossover. <ref type="table" target="#tab_5">Table 3</ref> shows L Poly-N outperforms the baseline cross-entropy loss accuracy. We explore Ndimensional grid search and greedy grid search of j in L Poly-N up to N = 3 and find that simply adjusting the coefficient of the first polynomial (N = 1) leads to better classification accuracy. Performing 2D grid search (N = 2) can further boost the accuracy. However, the additional gain is small (+0.1) compared to adjusting only the first polynomial (+0.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">L POLY-1 : SIMPLE AND EFFECTIVE</head><p>As shown in the previous section, we find tuning the first polynomial term leads to the most significant gain. In this section, we further simplify the Poly-N formulation and focus on evaluating Poly-1, where only the first polynomial coefficient in cross-entropy loss is modified.</p><formula xml:id="formula_9">L Poly-1 = (1 + 1 )(1 ? P t ) + 1/2(1 ? P t ) 2 + ... = ? log(P t ) + 1 (1 ? P t )<label>(7)</label></formula><p>We study the effect of different first term scaling on the accuracy and observe that increasing the first polynomial coefficient can systematically increase the ResNet-50 accuracy, as shown in <ref type="figure" target="#fig_4">Figure 3a</ref>. This result suggests that the cross-entropy loss is suboptimal in terms of polynomial coefficient values, and increasing the first polynomial coefficient leads to consistent improvement, which is comparable to other training techniques (section 11). <ref type="figure" target="#fig_4">Figure 3b</ref> shows the leading polynomial contributes to more than half of the cross-entropy gradient during training for the majority of the time, which highlights the significance of the first polynomial term (1 ? P t ) compared to the rest of the infinite many terms. Therefore, in the remaining of the paper, we adopt the form of L Poly-1 and primarily focus on adjusting the leading polynomial coefficient. As is evident from Equation 7, it only modifies the original loss implementation by a single line of code (adding a 1 (1 ? P t ) term on top of cross-entropy loss).</p><p>Note that, all the training hyperparameters are optimized for cross-entropy loss. Even so, a simple grid search on the first polynomial coefficients in the Poly-1 formulation significantly increases the classification accuracy. We find optimizing other hyperparameters for L Poly-1 leads to higher accuracy, and show more details in section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>In this section, we compare our PolyLoss against the commonly used cross-entropy loss and focal loss on various tasks, models, and datasets. For the following experiments, we adopt the default training hyperparameters in the public repositories without any tuning. Nevertheless, Poly-1 formulation leads to consistent advantage over default loss functions at the cost of a simple grid search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">L POLY-1 IMPROVES 2D IMAGE CLASSIFICATION ON IMAGENET</head><p>Image classification is a fundamental problem in computer vision, and progress on image classification has led to progress on many related computer vision tasks. In terms of the network architecture, in addition to the ResNet-50 already used in section 4, we also experiment with the state-of-the-art EfficientNetV2 <ref type="bibr" target="#b32">(Tan &amp; Le, 2021)</ref>. We use the ImageNet settings in <ref type="bibr" target="#b32">(Tan &amp; Le, 2021)</ref> except for replacing the original cross-entropy loss with our PolyLoss L P oly?1 with different values of 1 . In terms of the dataset, in addition to the ImageNet-1K dataset already used in section 4, we also consider ImageNet-21K, which has about 13M training images with 21,841 classes. We will study both the ImageNet-21K pretraining results and the ImageNet-1K finetuning results.</p><p>Pretraining EfficientNetV2-L on ImageNet-21K, then finetuning it on ImageNet-1K can improve classification accuracy <ref type="bibr" target="#b32">(Tan &amp; Le, 2021)</ref>. Here, we follow the same pretraining and finetuning schedule as reported in <ref type="bibr" target="#b32">Tan &amp; Le (2021)</ref> without modification 4 but replace the cross-entropy loss with L Poly-1 = ? log(P t ) + 1 (1 ? P t ). We reserve 25,000 images from the training set as minival to search the optimal 1 . Pretraining on ImageNet-21K <ref type="figure" target="#fig_5">Figure 4</ref> highlights the importance of using tailored loss function when pretraining model on ImageNet-21K dataset. A simple grid search over 1 ? {0, 1, 2, . . . , 7} in L Poly-1 without changing other default hyperparameters leads to around 1% accuracy gain for all SOTA EfficientNetV2 models with different sizes. The accuracy improvement of using a better loss function nearly matches the improvement of scaling up the model architecture (S to M and M to L).</p><p>Surprisingly, see <ref type="figure" target="#fig_7">Figure 5a</ref>, increasing the weight of the leading polynomial coefficient improves the accuracy of pretraining on ImageNet-21K (+0.6), whereas reducing it lowers the accuracy (-0.9). Setting 1 = ?1 truncates the leading polynomial term in the cross-entropy loss (Equation 1), which is similar to having a focal loss with ? = 1 (Equation 2). However, the opposite change, where 1 &gt; 0, improves the accuracy on the imbalanced ImageNet-21K.</p><p>We hypothesize the prediction of the imbalanced ImageNet-21K is not confident enough (P t is small), and using positive 1 PolyLoss leads to more confident predictions. To validate our hypothesis, we plot P t as a function of training steps in <ref type="figure" target="#fig_7">Figure 5b</ref>. We observe that 1 directly controls the mean P t over all classes. Using positive 1 PolyLoss leads to more confident prediction (higher P t ).</p><p>On the other hand, negative 1 PolyLoss lowers the confidence.    <ref type="table">Table 4</ref>: PolyLoss improves classification accuracy on ImageNet validation set. We set 1 = 2 for both.</p><p>Fine tuning on ImageNet-1K After pretraining on ImageNet-21K, we take the EfficientNetV2-L checkpoint and finetune it on ImageNet-1K, using the same procedure as Tan &amp; Le (2021) except for replacing the original cross-entropy loss with the Poly-1 formulation. PolyLoss improves the finetuning accuracy by 0.4%, advancing the ImageNet-1K top-1 accuracy from 86.8% to 87.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">L POLY-1 IMPROVES 2D INSTANCE SEGMENTATION AND OBJECT DETECTION ON COCO</head><p>Instance segmentation and object detection require localizing objects in an image in addition to recognizing them: the former in the form of arbitrary shapes and the latter in the form of bounding boxes. For both instance segmentation and object detection, we use the popular COCO <ref type="bibr" target="#b20">(Lin et al., 2014)</ref> dataset, which contains 80 object classes. We choose Mask R-CNN <ref type="bibr" target="#b14">(He et al., 2017)</ref> as the representative model for instance segmentation and object detection. These models optimize multiple losses, e.g. L MaskRCNN = L cls + L box + L mask . For the following experiments, we only replace the L cls with PolyLoss and leave other losses intact. Results are summarized in <ref type="table" target="#tab_8">Table 5</ref>.  Reducing the leading polynomial coefficient improves Mask R-CNN AP and AR. In training Mask R-CNN, we use the training schedule optimized for cross-entropy loss, 5 and replace the crossentropy loss with L P oly?1 = ? log(P t ) + 1 (1 ? P t ) for the classification loss L cls , where 1 ? {?1.0, ?0.8, ?0.6, ?0.4, ?0.2, 0, 0.5, 1.0}. We ensure the leading coefficient is positive, i.e. 1 ? ?1. Our results in <ref type="figure" target="#fig_9">Figure 6a</ref> show systematic improvements of box AP, box AR, mask AP, and mask AR as we reduce the weight of the first polynomial by using negative 1 values. Note that Poly-1 ( = ?1) not only improves AP but also significantly increases AR, shown in <ref type="table" target="#tab_8">Table 5</ref>.  Tailoring loss function to datasets and tasks is important. ImageNet-21K and COCO are both imbalanced but the optimal for PolyLoss are opposite in sign, i.e. = 2 for ImageNet-21K classification and = ?1 for Mask R-CNN detection. We plot the P t of the Mask R-CNN classification head and found the original prediction is overly confident (P t is close to 1) on the imbalanced COCO dataset, thus using a negative lowers the prediction confidence, as shown in <ref type="figure" target="#fig_9">Figure 6b</ref>. This effect is similar to label smoothing  and confidence penalty <ref type="bibr" target="#b25">(Pereyra et al., 2017)</ref>, but unlike those methods, as long as 0 &gt; &gt; ?1, PolyLoss lowers the gradients of overconfident predictions but will not encourage incorrect predictions or directly penalize prediction confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">L POLY-1 IMPROVES 3D OBJECT DETECTION ON WAYMO OPEN DATASET</head><p>Polynomial expansion in the basis of (1 ? P t ) Loss <ref type="table">Table 6</ref>: PolyLoss vs. focal loss for 3D detection models. Differences are highlighted in red. We found the best Poly-1 for PointPillars is 1 = ?1, which is equivalent to dropping the first term. Therefore, for RSN, we drop the first term and tune the new leading polynomial (1 ? P t ) ?+2 .</p><formula xml:id="formula_10">Focal loss (1 ? P t ) ?+1 + 1/2(1 ? P t ) ?+2 + 1/3(1 ? P t ) ?+3 + ... L FL = ?(1 ? P t ) ? log(P t ) Poly-1 (PointPillars) ( 1 + 1)(1 ? P t ) ?+1 + 1/2(1 ? P t ) ?+2 + 1/3(1 ? P t ) ?+3 + ... L FL Poly-1 = L FL + 1 (1 ? P t ) ?+1 Poly-1 * (RSN) (drop first) (1/2 + 2 )(1 ? P t ) ?+2 + 1/3(1 ? P t ) ?+3 + ... L FL Poly-1 * = L FL ? (1 ? P t ) ?+1 + 2 (1 ? P t ) ?+2</formula><p>Detecting 3D objects from LiDAR point clouds is an important topic and can directly benefit autonomous driving applications. We conduct these experiments on the Waymo Open Dataset <ref type="bibr" target="#b28">(Sun et al., 2020)</ref>. Similar to 2D detectors, 3D detection models are commonly based on single-stage and two-stage architectures. Here, we evaluate our PolyLoss on two models: a popular single-stage PointPillars model <ref type="bibr" target="#b16">(Lang et al., 2019)</ref>; and a state-of-the-art two-stage Range Sparse Net (RSN) model . Both models rely on multi-task loss functions during training. Here, we focus on improving the classification focal loss by replacing it with PolyLoss. Similar to the 2D perception cases, we adopt the Poly-1 formulation to improve upon focal loss, shown in <ref type="table">Table 6</ref>.</p><p>PolyLoss improves single-stage PointPillars model. The PointPillars model converts the raw 3D point cloud to a 2D top-down pseudo image, and then detect 3D bounding boxes from the 2D image in a similar way to RetinaNet <ref type="bibr" target="#b21">(Lin et al., 2017)</ref>. Here, we replace the classification   <ref type="bibr" target="#b16">(Lang et al., 2019)</ref> and two-stage SOTA RSN  are evaluated. Bird's eye view (BEV) and 3D detection average precision (AP) and average precision with heading (APH) at Level 1 (L1) and Level 2 (L2) difficulties are reported. The IoU threshold is set to 0.7 for vehicle detection and 0.5 for pedestrian detection.</p><p>focal loss (? = 2) with L FL Poly-1 = ?(1 ? P t ) 2 log P t + 1 (1 ? P t ) 3 and adopt the same training schedule optimized for focal loss without any modification 6 . <ref type="table" target="#tab_10">Table 7</ref> shows that L FL Poly-1 with = ?1 leads to significant improvement on all the metrics for both vehicle and pedestrian models.</p><p>(1 P t ) 3 (1 P t ) 4 (1 P t ) 5 (1 P t ) <ref type="bibr">6</ref> Polynomial terms 0.0 0.5 1.0  Advancing the state-of-the-art with RSN. RSN segments foreground points from the 3D point cloud in the first stage, and then applies sparse convolution to predict 3D bounding boxes from the selected foreground points. RSN uses the same focal loss as the PointPillars model, i.e., L FL = ?(1?P t ) 2 log P t . Since the optimal L FL Poly-1 for PointPillars ( 1 = ?1) is equivalent to dropping the first polynomial, we adapt the same loss formulation for RSN and tune the new leading polynomial (1 ? P t ) 4 by defining L FL Poly-1 * = ?(1 ? P t ) 2 log(P t ) ? (1 ? P t ) 3 + 2 (1 ? P t ) 4 , shown in <ref type="figure" target="#fig_10">Figure 7</ref>. We follow the same training schedule optimized for focal loss described in  without adjustment. Our results, in <ref type="table" target="#tab_10">Table 7</ref>, show that tuning the new leading polynomial improves all metrics (except vehicle detection BEV APH L2) for the SOTA 3D detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose the PolyLoss framework, which provides a unified view on common loss functions for classification problems. We recognize that, under polynomial expansion, focal loss is a horizontal shift of the polynomial coefficients compared to the cross-entropy loss. This new insight motivates us to explore an alternative dimension. i.e. vertically modify the polynomial coefficients.</p><p>Our PolyLoss framework provides flexible ways of changing the loss function shape by adjusting the polynomial coefficients. In this framework, we propose a simple and effective Poly-1 formulation. By simply adjusting the coefficient of the leading polynomial coefficient with just one extra hyperparameter 1 , we show our simple Poly-1 improves a variety of models across multiple tasks and datasets. We hope Poly-1 formulation's simplicity (one extra line of code) and effectiveness will lead to adoption in more applications of classification than the ones we have managed to explore.</p><p>More importantly, our work highlights the limitation of common loss functions, and simple modification could lead to improvements even on well established state-of-the-art models. We hope these findings will encourage exploring and rethinking the loss function design beyond the commonly used cross-entropy and focal loss, as well as the simplest Poly-1 loss proposed in this work. Proof.</p><formula xml:id="formula_11">|R N (p)| = ? j=N +1 1/j(1 ? p) j ? ? j=N +1 (1 ? p) j = (1 ? p) N +1 p ? (1 ? ?) N +1 ? ? (1 ? ?) N ? |R N (p)| = ? j=N (1 ? p) j = (1 ? p) N p ? (1 ? ?) N ? 8 ADJUSTING OTHER TRAINING HYPERPARAMETERS LEADS TO HIGHER GAIN.</formula><p>All the experiments shown in the main text are based on hyperparameters optimized for the baseline loss function, which actually puts PolyLoss at a disadvantage. Here we use weight decay rate for ResNet50 as an example. The default weight decay (1e-4) is optimized for cross-entropy loss.</p><p>Adjusting the decay rate may reduce the model performance of cross-entropy loss but leads to much higher gain for PolyLoss (+0.8%), which is better than the best accuracy (76.3%) trained using cross-entropy loss (+0.8%).  Here, we add additional ablation studies on COCO detection using RetinaNet. The optimal ? and ? balance values for Focal loss are (2.0, 0.25) <ref type="bibr" target="#b21">(Lin et al., 2017)</ref>. Since all the hyperparameters are optimized with respect to the optimal (?, ?) values, we observe no improvement when tuning the leading polynomial term. We suspect the detection AP is at a 'local maximum' of hyperparameters. By adjusting (?, ?) values, we show PolyLoss consistently outperforms the best Focal Loss AP (33.4), i.e., adjusting only ? value (column 3, 4) or both ? and ? values (column 5, 6).   <ref type="figure">(N = 2)</ref>, besides adjusting the learning rate, we further tune the coefficient (?) of the second polynomial, similar to a prior work <ref type="bibr" target="#b10">(Gonzalez &amp; Miikkulainen, 2020b)</ref>, and weight decay.</p><formula xml:id="formula_12">L Drop* = (1 ? P t ) + ?(1 ? P t ) 2<label>(8)</label></formula><p>Unlike <ref type="bibr" target="#b6">Feng et al. (2020)</ref>, where ? = 0.5 after dropping all higher-order polynomial, we find the optimal ? = 8, while the optimal learning rate is the same as the default setting (0.1). This alone increases the accuracy to 70.9, which shows simply dropping polynomial terms is not enough and adjusting the polynomial coefficients is critical. Further tuning weight decay leads to less than 0.1% model quality improvement.</p><p>Comparing to prior works <ref type="bibr" target="#b10">(Gonzalez &amp; Miikkulainen, 2020b;</ref><ref type="bibr" target="#b6">Feng et al., 2020)</ref>, Poly-1 is more effective and only contains one hyperparameter. Tuning weight decay of Poly-1 further increases the accuracy while having less hyperparameters compared to L Drop * , shown in <ref type="table" target="#tab_18">Table 10</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-entropy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">COLLECTIVELY TUNING MULTIPLE POLYNOMIAL COEFFICIENTS</head><p>Besides adjusting individual polynomial coefficients, in this section, we explore collectively tuning multiple polynomial coefficients in the PolyLoss framwork. In particular, we change the coefficients in the original cross-entropy loss from 1/j (Equation 1) to exponential decay. Here, we define</p><formula xml:id="formula_13">L exp = 2N j=1 e ?(j?1)/N (1 ? P t ) j<label>(9)</label></formula><p>where we cut off the infinite sum at twice the decay factor N . We performed 2D grid search on N ? {5, 20, 80, 320} and learning rate ? {0.1, 0.4, 1.6, 6.4}. The best accuracy is 72.3, where N = 80 and learning rate = 1.6, shown in <ref type="table" target="#tab_20">Table 11</ref>.  Though Poly-1 is better than using L exp , there are a lot more possibilities besides using exponential decay. We believe understanding how collectively tuning multiple coefficients affects the training is an important topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-entropy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">COMPARING TO OTHER TRAINING TECHNIQUES</head><p>As shown in recent works <ref type="bibr" target="#b15">(He et al., 2019;</ref><ref type="bibr" target="#b0">Bello et al., 2021;</ref><ref type="bibr" target="#b37">Wightman et al., 2021)</ref>, though independent novel training techniques often lead to sub 1% improvement, combining them could lead to significant overall improvements. To put things into perspective, Poly-1 achieves similar improvements as other commonly used training techniques, such as label smoothing and dropout on FC, shown in  Focal loss was first developed for single-stage detector RetinaNet to address strong class imbalance presented in object detection <ref type="bibr" target="#b21">(Lin et al., 2017)</ref>. Here, we provide an additional ablation study on how to systemically discover focal loss in the PolyLoss framework and investigate how the leading terms affect training in the presence of class imbalance.</p><p>Rediscovering the concept of focal loss from crossentropy loss. Here, we take a step back and attempt to systematically rediscover the concept of focal loss via our PolyLoss framework. Focal loss is commonly used for training detection models. Coming up with such an insight to address the class imbalance issue in detection requires strong domain expertise. We start with the PolyLoss representation of crossentropy loss and improve it from the PolyLoss gradient perspective.  <ref type="figure">Figure 9</ref>: Dropping leading polynomials reduces overfitting to the majority class. P t during RetinaNet training are plotted. Top: overall. Bottom left: background. Bottom right: foreground object. Dark blue curves represents P t for cross-entropy loss. Blue curves represents dropping the first polynomial in the cross-entropy loss. Light blue curves represents dropping both the first and second polynomials in the cross-entropy loss.</p><p>We start with the cross-entropy loss and define PolyLoss by dropping the first N polynomials in cross-entropy loss, i.e. L Drop-front = ? j=N +1 1/j(1 ? P t ) j = L CE ? N j=1 1/j(1 ? P t ) j . Dropping the first two polynomial terms (1 ? P t ) significantly improves both the detection AP and AR, see <ref type="figure">Figure 8</ref>. Dropping the first two polynomials (N = 2) leads to the best RetinaNet performance, which is similar to setting ? = 2 in focal loss, i.e. focal loss ? = 2 pushes all the polynomial coefficients to the right by 2, shown in <ref type="figure" target="#fig_0">Figure 1</ref> right, which is similar to truncating the first two polynomial terms.</p><p>Leading polynomials cause overfitting to the majority class. In the PolyLoss framework, the leading polynomial of cross-entropy loss is a constant, shown in Equation 3. For binary classification, the leading gradient for each class is simply N background ?N object , where N background and N object are the counts of background and object instances in the training mini-batch. When the class counts are extremely imbalanced, the majority class will dominate the gradient which will lead to significant bias towards optimizing the majority class.</p><p>Dropping polynomials reduces the extremely confident prediction P t , see <ref type="figure">Figure 9</ref>. To examine the composition of the overall prediction confidence, we also plot the P t for background only and P t for object only. Due to the extreme imbalance between the background and the object class, the overall P t is dominated by the background only P t . So reducing the overall P t decreases the background P t . On the other hand, reducing overfitting to the majority background class leads to more confident prediction P t on the object class.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Unified view of cross-entropy loss, focal loss, and PolyLoss. PolyLoss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>?</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Truncating the infinite sum of polynomials in cross-entropy loss to N reduces accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>log(Pt) + 1(1 Pt) LCE (a) PolyLoss family LPoly-1 = ? log(Pt) + 1(1 ? Pt), where 1 ? {?1,0, 1, .. . , 8}. Percentage of gradient from the first polynomial versus the rest (infinitely many) polynomials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The first polynomial plays an important role for training ResNet-50 on ImageNet-1K. (a) Increasing the coefficient of the first polynomial term ( 1 &gt; 0) consistently improves the ResNet50 prediction accuracy. Red dash line shows the accuracy when using cross-entropy loss. Mean and stdev of three runs are plotted. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>PolyLoss improves EfficientNetV2 family on the speed-accuracy Pareto curve. Validation accuracy of EfficientNetV2 models pretrained on ImageNet-21K are plotted. Poly-Loss outperforms cross-entropy loss with about ?2 speed-up.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Validation accuracy of EfficientNetV2-L on ImageNet-21K. PolyLoss with positive 1 outperforms baseline cross-entropy loss (red dash line). Positive 1 = 1 (dark) increases the prediction confidence, while negative 1 = ?1 (light) decreases the prediction confidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>PolyLoss improves EfficientNetV2-L by increasing prediction confidence P t .EfficientNetV2-L L CE L Poly-1 Improv.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Bound box AP, AR and Mask AP, AR increase as 1 decreases. Negative 1 outperforms cross-entropy loss (red dash line). ) + (1 Pt) log(Pt) log(Pt) (1 Pt) (b) Negative 1 = ?1 (light) reduces the overconfident prediction Pt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>PolyLoss improves Mask R-CNN by lowering overconfident predictions. Mean and stdev of three runs are plotted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Visualizing L F L Poly-1 and L F L Poly-1 * in the PolyLoss framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparing different losses in the PolyLoss framework.</figDesc><table><row><cell>Dropping higher order poly-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Adjusting the learning rate (default 0.1) of LDrop does not improve the classification accuracy.</figDesc><table><row><cell>2 4 6 Accuracy</cell><cell></cell><cell>lr =6.4 lr =1.6 lr =0.4 lr =0.1</cell><cell cols="2">lr =0.02 lr =0.005 lr =0.001</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4 Cutoff index N 5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>L Poly-N outperforms cross- entropy loss on ImageNet-1K.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>35.0? 0.09 47.2? 0.16 31.3 ? 0.09 42.3 ? 0.02 Mask R-CNN L Poly-1 ? log(Pt) ? (1 ? Pt) 35.3 ? 0.12 49.7? 0.07 31.6 ? 0.11 44.4 ? 0.07</figDesc><table><row><cell></cell><cell>Loss</cell><cell>Box</cell><cell></cell><cell>Mask</cell><cell></cell></row><row><cell></cell><cell></cell><cell>AP</cell><cell>AR</cell><cell>AP</cell><cell>AR</cell></row><row><cell>Mask R-CNN L CE</cell><cell>? log(Pt)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Improvement</cell><cell>-</cell><cell>+0.3</cell><cell>+2.5</cell><cell>+0.3</cell><cell>+2.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>PolyLoss improves detection results on COCO validation set. Bounding box and instance segmentation mask average-precision (AP) and average-recall (AR) are reported for Mask R-CNN model with a ResNet-50 backbone. Mean and stdev of three runs are reported.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>PolyLoss improves detection results on Waymo Open Dataset validation set. Two detection models: single-stage PointPillars</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Theorem 1. For any small ? &gt; 0, ? &gt; 0 if N &gt; log 1?? (? ? ?), then for any p ? [?, 1], we have |R N (p)| &lt; ? and |R N (p)| &lt; ?.</figDesc><table><row><cell>SUPPLEMENTARY MATERIAL</cell></row><row><cell>7 PROOF OF THEOREM 1</cell></row><row><cell>def poly1_cross_entropy(logits, labels, epsilon):</cell></row><row><cell># epsilon &gt;=-1.</cell></row><row><cell># pt, CE, and Poly1 have shape [batch].</cell></row><row><cell>pt = tf.reduce_sum(labels * tf.nn.softmax(logits), axis=-1)</cell></row><row><cell>CE = tf.nn.softmax_cross_entropy_with_logits(labels, logits)</cell></row><row><cell>Poly1 = CE + epsilon * (1 -pt)</cell></row><row><cell>return Poly1</cell></row><row><cell>Example code for L CE Poly-1 with ? label smoothing is shown below.</cell></row><row><cell>def poly1_cross_entropy(logits, labels, epsilon, alpha = 0.1):</cell></row><row><cell># epsilon &gt;=-1.</cell></row><row><cell># one minus pt, CE, and Poly1 have shape [batch].</cell></row><row><cell>num_classes = labels.get_shape().as_list()[-1]</cell></row><row><cell>smooth labels = labels * (1-alpha) + alpha/num classes</cell></row><row><cell>one_minus_pt = tf.reduce_sum(</cell></row><row><cell>smooth labels * (1 -tf.nn.softmax(logits)), axis=-1)</cell></row><row><cell>CE_loss = tf.keras.losses.CategoricalCrossentropy(</cell></row><row><cell>from_logits=True, label_smoothing=alpha, reduction='none')</cell></row><row><cell>CE = CE_loss(labels, logits)</cell></row><row><cell>Poly1 = CE + epsilon * one minus pt</cell></row><row><cell>return Poly1</cell></row><row><cell>Example code for L FL Poly-1 with sigmoid activation is shown below.</cell></row><row><cell>def poly1_focal_loss(logits, labels, epsilon, gamma=2.0):</cell></row><row><cell># epsilon &gt;=-1.</cell></row><row><cell># p, pt, FL, and Poly1 have shape [batch, num of classes].</cell></row><row><cell>p = tf.math.sigmoid(logits)</cell></row><row><cell>pt = labels * p + (1 -labels) * (1 -p)</cell></row><row><cell>FL = focal_loss(pt, gamma)</cell></row><row><cell>Poly1 = FL + epsilon * tf.math.pow(1 -pt, gamma + 1)</cell></row><row><cell>return Poly1</cell></row><row><cell>Example code for L FL Poly-1 with ? balance is shown below.</cell></row><row><cell>def poly1_focal_loss(logits, labels, epsilon, gamma=2.0, alpha=0.25):</cell></row><row><cell># epsilon &gt;=-1.</cell></row><row><cell># p, pt, FL, weight, and Poly1 have shape [batch, num of classes].</cell></row><row><cell>p = tf.math.sigmoid(logits)</cell></row><row><cell>pt = labels * p + (1 -labels) * (1 -p)</cell></row><row><cell>FL = focal_loss(pt, gamma, alpha)</cell></row><row><cell>weight = labels * alpha + (1 -labels) * (1 -alpha)</cell></row><row><cell>Poly1 = FL + epsilon * tf.math.pow(1 -pt, gamma + 1) * weight</cell></row><row><cell>return Poly1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>+0.8 +0.6 Improv. compared to the best L CE (76.3%) +0.4 +0.8 +0.4</figDesc><table><row><cell>Weight decay</cell><cell>1e-4  ? 2e-4 9e-5</cell></row><row><cell>Cross-entropy</cell><cell>76.3 76.3 76.1</cell></row><row><cell>PolyLoss</cell><cell>76.7 77.1 76.7</cell></row><row><cell>Improv. @ the same weight decay</cell><cell>+0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>ResNet50 performances on ImageNet-1K using different weight decays. ? The default weight decay value is 1e-4.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>RetinaNet (ResNet50 backbone) performances on COCO using different Focal loss (?, ?). ? The default (?, ?) used in Focal loss is (2.0, 0.25).</figDesc><table /><note>9 L DROP WITH MORE HYPERPARAMETER TUNING For L Drop</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>Poly-1 outperforms L Drop * with hyperparameter tuning. Accuracy of ResNet50 on ImageNet-1K is reported.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 11 :</head><label>11</label><figDesc>Comparing Poly-1 with exponential decay coefficients. Accuracy of ResNet50 on ImageNet-1K is reported.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 12</head><label>12</label><figDesc></figDesc><table><row><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Cross-entropy Poly-1 Label smoothing Dropout on FC</cell></row><row><cell>Accuracy</cell><cell>76.3</cell><cell>76.7</cell><cell>76.7</cell><cell>76.4</cell></row><row><cell>Num. of parameters</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 12 :</head><label>12</label><figDesc>Comparing Poly-1 with common training techniques. Accuracy of ResNet50 on ImageNet-1K is reported.12 REDISCOVERING FOCAL LOSS FROM POLYLOSS</figDesc><table><row><cell>0.30 0.32 Box AP</cell><cell>0</cell><cell>1 Drop first N polynomials 2 3 AP AR</cell><cell>4</cell><cell>0.40 0.45 Box AR</cell></row><row><cell cols="5">Figure 8: Dropping leading polyno-</cell></row><row><cell cols="5">mial terms can improve RetinaNet.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Code at https://github.com/google/automl/tree/master/efficientnetv2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Code at https://github.com/tensorflow/tpu/tree/master/models/official</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Code at https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/car</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank James Philbin, Doug Eck, Tsung-Yi Lin and the rest of Waymo Research and Google Brain teams for valuable feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY STATEMENT</head><p>Our experiments are based on public datasets and open source code repositories, shown in footnote 3-6. We do not tune any default training hyperparameters and only modify the loss functions, which are shown in <ref type="table">Table 2</ref>-7. The proposed final formulation L Poly-1 requires one line of code change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example code for L CE</head><p>Poly-1 with softmax activation is shown below.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07579</idno>
		<title level="m">Revisiting resnets: Improved training and scaling strategies</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Loss max-pooling for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Samuel Rota Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7082" to="7091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spinenet: Learning scale-permuted backbone for recognition and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11592" to="11601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascade object detection with deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2241" to="2248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Can cross entropy loss be robust to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senlin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengmao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Joint Conferences on Artificial Intelligence</title>
		<meeting>the 29th International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2206" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Making risk minimization tolerant to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="93" to="107" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved training speed, accuracy, and data utilization through loss function optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Congress on Evolutionary Computation (CEC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Optimizing loss functions through multivariate taylor polynomial parameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Miikkulainen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00059</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On extending neural networks with loss ensembles for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamideh</forename><surname>Hajiabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Molla-Aliod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Monsefi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05170</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ostermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE international conference on evolutionary computation</title>
		<meeting>IEEE international conference on evolutionary computation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="312" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Am-lfs: Automl for loss function search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8410" to="8419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Auto seg-loss: Searching metric surrogates for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07930</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Can gradient clipping mitigate label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Imbalance problems in object detection: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Baris Can Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akbas</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scalability in perception for autonomous driving: Waymo open dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2446" to="2454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rsn: Range sparse net for efficient, accurate lidar 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gamaleldin</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning and example selection for object and pattern detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kah-Kay</forename><surname>Sung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficientnetv2: Smaller models and faster training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10781" to="10790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10821</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE computer society conference on computer vision and pattern recognition</title>
		<meeting>the 2001 IEEE computer society conference on computer vision and pattern recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00476</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Autoloss: Learning discrete schedules for alternate optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02442</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sabuncu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07836</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Well-classified examples are underestimated in classification with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangxiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06537</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Rethinking pre-training and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
