<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manh-Duy</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">AISIA Research Lab</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Science</orgName>
								<address>
									<settlement>Ho Chi Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Vietnam National University Ho Chi Minh City</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>April 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>scene graphs</term>
					<term>graph embedding</term>
					<term>image retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional approaches to image-text retrieval mainly focus on indexing visual objects appearing in pictures but ignore the interactions between these objects. Such objects occurrences and interactions are equivalently useful and important in this field as they are usually mentioned in the text. Scene graph presentation is a suitable method for the image-text matching challenge and obtained good results due to its ability to capture the inter-relationship information. Both images and text are represented in scene graph levels and formulate the retrieval challenge as a scene graph matching challenge. In this paper, we introduce the Local and Global Scene Graph Matching (LGSGM) model that enhances the state-of-the-art method by integrating an extra graph convolution network to capture the general information of a graph. Specifically, for a pair of scene graphs of an image and its caption, two separate models are used to learn the features of each graph's nodes and edges. Then a Siamese-structure graph convolution model is employed to embed graphs into vector forms. We finally combine the graph-level and the vectorlevel to calculate the similarity of this image-text pair. The empirical experiments show that our enhancement with the combination of levels can improve the performance of the baseline method by increasing the recall by more than 10% on the Flickr30k dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Computer vision and natural language processing are two of the most popular domains of deep learning, each of which has a wide range of applications <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. A fusion of these two areas has raised many fascinating challenges and focused the attention of researchers. One of those topics is image-text retrieval, in which a text (image) query is provided to retrieve relevant images (texts) from a given dataset. This issue's critical point is the clear semantic gap between images and text, presenting a challenging multi-modal data retrieval challenge.</p><p>Many kinds of research have focused on this challenge, and one can summarise earlier works into two types. The first type includes methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> in which each image and textual description are entirely encoded as global representative vectors in the same space. It can be considered a decent solution due to its simplicity and fast retrieval. However, these approaches are only suitable for simple cases where there is only a single object in an image or a short sentence as they neglect the importance of positioning information. The proposed techniques <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> of the second type can deal with this positioning issue. They encode images into several regions based on detected objects. Sentences are also parsed into many fields based on their chunks of words. This approach now can gain more elaborate detail for both data. Although these methods have been shown to achieve better performance than others in the first group, there are still challenges to be solved. Neither type has considered the actions occurring within the data, including the interactions between entities/objects in images and captions. Such information can offer more in-depth insights into images as well as textual queries <ref type="bibr" target="#b9">[10]</ref>, therefore facilitating enhanced image-text retrieval. It should be mentioned that the global embedding methods in the first group also could learn this association information. However, it cannot be in detail since they do not focus directly on the relationship aspect. The introduction of a scene graph structure <ref type="bibr" target="#b9">[10]</ref> introduces a promising way to address this issue. Due to its capacity to capture both objects and their interactions, the design has been applied and achieved excellent results in various fields <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. A scene graph structure is a graph including nodes and edges connecting nodes together. In the image-text retrieval field, a node and an edge represent objects and the associations among objects detected in images or captions, as depicted in <ref type="figure">Figure 1</ref>. Recent research <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> could obtain better results in this retrieval area by utilizing these relations information. Hence there is a promise when applying this useful data structure in the retrieval field. <ref type="figure">Figure 1</ref>. An example of scene graphs generated from an image and its caption. The green rectangles are nodes in the graph indicating detected objects in the image or the caption while the orange rectangles are edges illustrating the relationships between objects.</p><p>In this paper, we employ a state-of-the-art method that applies scene graph structure to solve the retrieval challenge <ref type="bibr" target="#b14">[15]</ref>. However, we advance this baseline by introducing an addition graph convolution network to capture a global form of a graph beside its local representation produced by the baseline. To be more specific, we firstly generate scene graph information of each image and its caption. Our proposed model can extract and learn the insightful features of nodes and edges from both graphs to obtain the local details. These scene graphs are then embedded into vectors by a Siamese graph model to represent the entire image's global information and textual content. The similarity of the two graphs now can be measured at the local and global levels. We evaluate our proposed method by measuring the recall metrics on the well-known Flickr30k <ref type="bibr" target="#b16">[17]</ref> dataset to show the effectiveness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Many models have been introduced for the image-text retrieval problem. Some of them <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> encode entire images and sentences into one space to facilitate the comparison, while other solutions focused on local information instead <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Faghri et al. <ref type="bibr" target="#b3">[4]</ref> followed a typical method to use a CNN-based image encoder to extract visual features of images and an RNN-based textual module to extract those from captions. These features then went through a fully connected layer to be projected into the same vector space to facilitate the comparison. Although it is a simple approach, it still achieves high performance by applying hard negatives triplet loss during training. With different points of view on analyzing sentences, Wang et al. <ref type="bibr" target="#b4">[5]</ref> used a Bag-of-Word technique to extract the information from semantic data. However, one could also use a CNN network to process sentences in a dual-path convolutional model <ref type="bibr" target="#b5">[6]</ref> by entirely embedding each image and sentence into a vector form using two CNN modules separately. In contrast, Huang et al. <ref type="bibr" target="#b6">[7]</ref> focused on the local information in images where semantic concepts and their ordering were extracted by learning with their corresponding sentences. The work from Wang <ref type="bibr" target="#b8">[9]</ref> raised the awareness of the relative position of detected objects within an image which could be useful when matching with the caption. Meanwhile, Lee et al. <ref type="bibr" target="#b7">[8]</ref> presented a novel cross attention mechanism that learned the importance of regions (images) and words (captions) along with their alignment.</p><p>Although achieving considerable accuracy, previous work ignored or underestimated the robust information of objects' interactions within images and sentences. Johnson and colleagues firstly introduced the scene graph for dealing with this issue for the image retrieval challenge <ref type="bibr" target="#b9">[10]</ref>. In their research, a dataset of scene graphs of images was humanannotated manually. An arbitrary graph was made and used as a query to retrieve equivalent images having a similar graph. A Conditional Random Field model and a maximum posterior technique was employed to measure the similarity of graphs. Many algorithms were invented to generate scene graphs from images <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>since this structure and the dataset about it was public. By virtue of them, some studies about image-text retrieval <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> have followed the scene graph approach recently. Shi et al. <ref type="bibr" target="#b15">[16]</ref> created a scene concept graph based on a popular scene graph dataset <ref type="bibr" target="#b21">[22]</ref> and used it to expand the detected concepts in images to extract visual features. Cosine similarity was used to measure the similarity between these features and the semantic features produced by applying an RNN module to captions and rank them to perform the retrieval. The scene graph matching (SGM) model from Wang et al. <ref type="bibr" target="#b14">[15]</ref> takes a scene graph from both images and captions as input then extracts the graph features by using their own designed encoders called visual scene graph and textual scene graph for each modality. Then, one can promptly calculate the similarity between the two graphs based on each part of the first graph's agreement with every part of the second graph. Despite getting state-of-theart results among the scene graph approaches, the SGM could still be enhanced. Firstly, its similarity calculating process only takes the small parts of a graph into account, hence neglect the overall detail. Secondly, the SGM model does not apply any normalization technique, resulting in easy overfitting. Besides, using ResNet <ref type="bibr" target="#b22">[23]</ref> as the backbone of the structure also limits the model's potential performance. There have been several recent CNN-based networks that are more accurate than the ResNet.</p><p>Recently, some researchers have applied the attention technique to both visual and textual data to learn the interaction between the components themselves, then combine with another attention module to fuse two modalities together <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Chen et al. <ref type="bibr" target="#b23">[24]</ref> presented a novel structure that iteratively finds matching compartments between images and sentences and refined them progressively. Meanwhile, the multi-modality cross attention network employs the compelling Transformer <ref type="bibr" target="#b25">[26]</ref> technique on images and semantic data to get the fine-grained relationship information in detail. They both manage to get the best results so far in the domain. Nevertheless, none of them use scene graphs in their approach, which is our main focus of this research.</p><p>In this paper, we try to improve the performance of the SGM method by employing a graph convolution model to capture the overall information of a graph which can be considered as one of its weaknesses mentioned above. Our model utilizes scene graph structures for images and sentences and encodes them into vector forms for storing overall essential information to calculate the similarity between graphs instead of operating at the graph level exclusively. We name this model "Local and Global Scene Graph Matching" (LGSGM).</p><p>Our primary contributions are threefold. Firstly, the SGM measured the similarity between graphs by dividing them into sub-graphs, hence diminishing the overall detail of a graph. We overcome this problem by building a graph embedding module to summarise all information of a graph into a vector form and combine it with the graphical form to calculate the similarity of a pair of graphs. Secondly, the efficient network <ref type="bibr" target="#b26">[27]</ref> is applied to extract the visual features of detected objects in images instead of using conventional Residual structure <ref type="bibr" target="#b22">[23]</ref> in the SGM model as the former network is shown to be more accurate although having a simpler design. Moreover, we also integrate normalization techniques including Dropout and Batch normalization to mitigate the potential of overfitting. Thirdly, we run an evaluation on the Flickr30k dataset for the image-text matching problem to facilitate a comparison with the SGM model <ref type="bibr" target="#b14">[15]</ref> as the baseline, which also uses scene graphs to support retrieval. We choose the SGM model as the baseline due to its state-of-the-art result in this field for models that employ scene graphs in the structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>This section will describe how our LGSGM model approaches the retrieval challenge as a graph similarity ranking problem which is inspired by the SGM. <ref type="figure">Figure 2</ref> depicts the workflow of LGSGM that integrates an extra graph embedding stage to the SGM baseline. Initially, our preprocessing data stage starts with the scene graph construction. The graphs of all images and texts are extracted beforehand and stored in the database. A generated scene graph of a query, which can be an image or a sentence, then goes through a graph encoder module to get a graph feature. Meanwhile, each sample in the remaining modality database also follows the same scheme but separately. All feature graphs now are passed through a Siamese-structured <ref type="bibr" target="#b27">[28]</ref> graph embedding layer to learn the summarised attributes of them, and the vector-level features of those graphs are obtained. The similarity of a pair of graphs, which is a query graph and a sample graph in the database, now can be calculated based on the feature graphs and their embedded vectors. Finally, these scores are ranked descending to find the most relevant answers from the database. <ref type="figure">Figure 2</ref>. Our proposed LGSGM pipeline follows the SGM baseline. A scene graph of an image is firstly extracted at the preprocessing stage. The scene graph is then encoded to get the visual feature graph used to compute the local similarity score. A text also goes through a similar process to get the textual feature graph. Both feature graphs are embedded into vectors by a shared graph embedding model. The global similarity is then calculated based on their vector-level forms and combine with the local score to get the final similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Visual Graph Encoder</head><p>A scene graph of an image can be established beforehand by using any available scene graph generation method. The graph G after extracted can be seen as a tuple G = (O, B, R) where:</p><formula xml:id="formula_0">? O = {o 1 , ..., o N o } is the set of N o semantic labels of detected objects appearing the image. ? B = {b o 1 , ..., b o No |b o i ? R 4 } is the set of bounding boxes where b o i is the coordi- nate of the box of the corresponding object o i in the set O. ? R = {r 1 , ..., r N r } is the set of N r predicted relations between objects in the image.</formula><p>Each r m is a tuple of (o i , p i j , o j ) where p i j is the label of the association between two objects o i and o j . It is noted that R can be seen as the set of edges connecting nodes, which are objects in O, of the scene graph G.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 3</ref>, after the graph is constructed, we embed the label of the nodes and edges into vectors with the help of trainable word embedding layers to gain their semantic information. Besides, there is also rich information from images themselves. We extract an image feature for each object in O based on its associated bounding box in B. Regarding an edge p i j , its image feature can be computed through the union regions of two boxes b o i and b o j . The feature of one node or an edge now is the fusion of its image feature and semantic features. We update these features to learn the connection between nodes and edges features by using a convolutional graph neural network <ref type="bibr" target="#b28">[29]</ref> to get the final visual feature graph. Word Embedding. We exploit the information of semantic labels by embedding names of objects and relations in O and R into vectors with two distinct embedding layers  <ref type="bibr" target="#b26">[27]</ref> in our setting, to get the image features while the scene graph goes through the word embedding layer to get the semantic features of its nodes and edges. These features are updated by combining with corresponding image features by a fusion layer. A convolutional graph network is applied to the graph, and the visual feature graph is obtained. The graph is finally fed to the Siamese graph embedding model to get its global representation. for each. Each object label o i and predicate label p i j is transformed into one-hot vector I o i and I p i j . Their embedding vectors, e o i and e p i j , can be calculated as</p><formula xml:id="formula_1">e o i = W o I o i , W o ? R d W ?C o<label>(1)</label></formula><formula xml:id="formula_2">e p i j = W p I p i j , W p ? R d W ?C p<label>(2)</label></formula><p>where W o and W p are the trainable parameters in the layer, C o and C p is the number of categories of objects and relations supported in the scene graph generation method. We set d W = 300 and utilise the pretrained embedding Glove model to initialise W o and W p . Image Features. An image feature v o i of an object o i can be obtained by applying a pretrained CNN-based network to the cropped region b o i around the object. Similarly, the cropped region for an edge p i j , which is the area covering both b o i and b o j regions, also be used to extract its d I -dimension image feature v i j .</p><p>Fused Features. A simple trainable, fully connected neural network is applied to the semantic feature's concatenated vector and the image feature to get the fused feature that can combine both modality detail. The fused representation of a node and an edge is achieved as followed:</p><formula xml:id="formula_3">u o i = f act (W u [v o i , e o i ])<label>(3)</label></formula><formula xml:id="formula_4">u i j = f act (W u [v i j , e i j ]),<label>(4)</label></formula><p>where [.] is the concatenating operation, f act is an activation function, and W u ?</p><formula xml:id="formula_5">R d F ?(d I + d W )</formula><p>is the trainable parameters. Graph Network. We use a Convolutional Graph Network (GCN) to learn the connection and update the fused features of nodes (u o i ) and edges (u i j ) of the graph. The GCN works similarly to a normal CNN operation but more flexible to many types of graph-structured data than only the grid data of a conventional CNN. In this GCN model, the features of a node are only updated based on itself solely to mitigate noise effects from surrounding nodes <ref type="bibr" target="#b14">[15]</ref>. In contrast, because an edge bridges two nodes together, its features should be related to those nodes' features to learn the association between them. Regarding an n-layer GCN model, updated features of a node and an edge at the l th layer can be formulated as follows:</p><formula xml:id="formula_6">h l o i = MLP o (h l?1 o i ) (5) h l p i j = MLP p ([h l?1 o i , h l?1 p i j , h l?1 o j ]),<label>(6)</label></formula><p>where MLP o and MLP p are two separate neural network models, and h 0 o i = u o i and h 0 p i j = u i j . The final output of the visual feature graph is the updated features of nodes and edges which are denoted as h o i and h p i j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Textual Graph Encoder</head><p>Similarly, a sentence is also converted into a scene graph to describe the relationships in detail. These triplet information, such as "man-wears-hat", can be obtained by using SPICE technique <ref type="bibr" target="#b12">[13]</ref>. However, relying on the graph only will ignore other important clues in the sentence. Therefore two different modules are used to extract the data from both pathways as depicted in <ref type="figure" target="#fig_1">Figure 4</ref>. Initially, every word in the sentence can be encoded by a word embedding module similar to that in the visual feature graph. Two LSTM models <ref type="bibr" target="#b29">[30]</ref> are then employed to learn the features of each word in the sentence as well as the features of each extracted relation triplet. We expand the SGM model by introducing an additional GCN to the graph created from the triplets. This graph is used to summarise information of all relations in the sentence and compared to that of visual modality afterward.</p><p>Word Embedding. It is worth noting that there are a massive number of vocabularies in a real-world deployment. Hence, it is not suitable for the word embedding part to learn all words in the entire semantic modality database as an overfitting issue will occur in the test condition. We create the dictionary from the textual database (excluding the test set) and only learn some popular words. We replace the least common ones, whose appearance frequency less than 4, with the "unknown" word, which is considered a popular vocabulary. During the test phase, words not included in the dictionary are converted to "unknown".</p><p>Word Features. After the embedding stage, the entire sentence goes to the bidirectional LSTM hierarchy to get the features from both forwards and backward ways. Then the representation of a word w i in the sentence, denoted as h w i , is the average of two paths and computed as follows:</p><formula xml:id="formula_7">h w i = ????? LSTM w (e w i , ??? h w i?1 ) + ????? LSTM w (e w i , ??? h w i+1 ) 2 ,<label>(7)</label></formula><p>where LSTM w is the bi-LSTM model, e w i is the word embedding vector whilst ??? h w i?1 and ??? h w i+1 are the hidden states of the word w i from forward and backward directions.</p><p>Graph Network. We organise the set of relations of a caption as T = {t 1 , ...,t N t } with N t is the total number of relations in the sentence parsed by SPICE. Each triplet t m is a sequence of words (w i , w i j , w j ) where the word w i j is the association between two objects w i and w j such as ("man", "wears", "hat"). We use another bidirectional LSTM structure, named LSTM t , and share it among all triplet t i for i ? [1, N t ]. The feature of a word in a triplet is the hidden state itself while the feature of entire triplet is the last state of the sequence model. Specifically, they are calculated as:</p><formula xml:id="formula_8">h w n t k = ? ??? ? LSTM t (e w n t k , ? ?? ? h w n?1 t k ) + ? ??? ? LSTM t (e w n t k , ? ?? ? h w n+1 t k ) 2 ,<label>(8)</label></formula><formula xml:id="formula_9">h t k = ? ? h w i t k + ? ? h w j t k 2 ,<label>(9)</label></formula><p>where triplet t k is a sequence of words started with w i and ended with w j , e w n t k is the word embedding vector of w n in t k . An extra GCN model having a similar structure with that in visual modality (Eq. 5 and Eq. 6) is then applied to the graph formed by the set T . Particularly, the graph will have h w n t k as feature of nodes and h w i j t k for edges which are all go through two neural networks to get the graph-level features of nodes and edges which are denoted as hg w n t k and hg w i j t k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Graph Embedding</head><p>After feature graphs of both modalities are extracted, we embed them into vector forms using an attention mechanism called multi-scale node attention <ref type="bibr" target="#b30">[31]</ref>. Given a graph G having N nodes {h 1 , ..., h N } and M edges {r 1 , ..., r M } where h i and r j ? R d is a feature of a node and an edge accordingly, the embedded vector of G, noted as a G ? R 2d , is obtained by the following formula:</p><formula xml:id="formula_10">a h G = N ? n=1 ? (h T n ReLU(W h ( 1 N N ? m=1 h m )))h n ,<label>(10)</label></formula><formula xml:id="formula_11">a r G = M ? n=1 ? (r T n ReLU(W r ( 1 M M ? m=1 r m )))r n ,<label>(11)</label></formula><formula xml:id="formula_12">a G = [a h G , a r G ]<label>(12)</label></formula><p>where ? and ReLU indicate the sigmoid and rectified linear unit activate function, W h and W r ? R d?d is the training parameter in the model, subscript T denotes the transpose operation, and [.] is the concatenation. We share this attention structure to the visual feature graph and textual feature graph as a siamese-model to get their vector-level representation a V and a T respectively. It is noted that the node features and edge features of a visual graph are h o i and h p i j while those of textual graph are hg w n t k and hg w i j t k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Similarity Function</head><p>The similarity between two scene graphs is measured using both local and global approaches. In the local form, each part of a graph is compared to every part of other graphs; for instance, matching on each node-level feature and on each edge-level feature. Regarding the global approach, the embedding vectors of the entire two graphs are used. This similarity score can be used to rank the retrieved result as higher means more relevant answers. Local similarity. The local score is the sum of a node score and an edge score. The former score is the average of the matching scores of all words in a sentence with their most relevant object detected in an image. The relevant score of a word w i in the caption with an object in the image o k is their dot product h T w i h o k . Assuming there are N w words in the caption and N o objects in the image, the node score is calculated as:</p><formula xml:id="formula_13">S Node = 1 N w N w ? i=1 max k?[1,N o ] h T w i h o k<label>(13)</label></formula><p>Similarly, the edge score is the mean score between relations in the textual data with their most matching edge in the visual graph and is formulated as:</p><formula xml:id="formula_14">S Rel = 1 N t N t ? i=1 max p i j ?R h T t i h p i j<label>(14)</label></formula><p>The local similarity then can be obtained as follows:</p><formula xml:id="formula_15">S Local = S Node + S Rel<label>(15)</label></formula><p>Global similarity. The global vectors provide the overall information of graphs. We use conventional cosine distance to measure the degree of matching between two entire graphs. Specifically, the similarity score, S Global , of two embedding vectors of visual graph a V and textual graph a T is:</p><formula xml:id="formula_16">S Global = a T V a T a V a T<label>(16)</label></formula><p>Finally, the similarity between two graphs takes both local and global details into account, hence is calculated by the sum of them, which is</p><formula xml:id="formula_17">S = S Local + S Global<label>(17)</label></formula><p>This bi-level similarity score is also used in the loss function during the training phase where we apply the hardest negative triplet loss as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>. The loss function is:</p><formula xml:id="formula_18">L(k, l) = max(0, m ? S kl + S kl ) + max(0, m ? S kl + S?k l ),<label>(18)</label></formula><p>where m is a margin hyperparameter, S kl is the similarity score of a true pair of image k and its caption l, andk,l is the least matching image and sentence with l and k in the mini-batch, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dataset and Metrics</head><p>We evaluate our proposed model on the Flickr30k dataset <ref type="bibr" target="#b16">[17]</ref>, which is one of the most popular datasets in this image-text matching field due to its high quality of textual annotation compared to others <ref type="bibr" target="#b31">[32]</ref>. The dataset consists of 31, 783 images, and each of them has five corresponding captions. An example of image-sentence pairs and their generated scene graph can be illustrated in <ref type="figure">Figure 1</ref>. We split the Flickr30k data into training, validating, and testing sets, so that the number of the training images is 29, 783, whilst the two latter subsets both have 1, 000 images. The evaluation metric we choose is a common Recall at K (R@K) value that has been used in many kinds of research <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. The R@K is the proportion of queries that we find their correct matching answers in the top K of the ranking result. In our experiments, we evaluate three values of K, which are 1, 5, and 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>In this section, we run our proposed model on the Flickr30k dataset then compare the baseline that is chosen as the SGM model as both our LGSGM and the SGM have a similar scene graph approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Model Setting</head><p>At the scene graph generation stage, we use Neural Motifs <ref type="bibr" target="#b18">[19]</ref> to get the similar graphs as described in <ref type="bibr" target="#b14">[15]</ref>. We only keep the top N o = 36 detected objects and N r = 25 predicates in the result sorted by their confidence scores for each scene graph. Regarding visual features of objects, we employ the EfficientNet, which is considered to be more accurate in the image classification challenge, although fewer parameters <ref type="bibr" target="#b26">[27]</ref> than ResNet <ref type="bibr" target="#b22">[23]</ref> as used in the baseline. The region surrounding the object is firstly cropped based on its bounding box and resized to the desired format, then goes through the net, which is excluded from the classification layer, to get the feature. We set d I = d F = 2048 and choose the EfficientNet-b5 model pretrained on ImageNet to get the 2048-dim feature vectors as comparable with the baseline. The number of layers in GCN models for both modalities and the LSTM models is 1. The dimension of the output vector from GCN is configured to be d = 1024, which is also the size of the hidden state of LSTM models. We also apply Dropout and Batch-normalisation to avoid the potential overfitting in both hierarchies. We use Swish <ref type="bibr" target="#b32">[33]</ref> as an activate function when creating fused features (Eq. 3 and Eq. 4) and in GCN models. We also tried other activation functions (such as ReLU, Leaky ReLU, or Tanh), but Swish has the best performance among these functions. The margin m in the loss function (Eq. 18) is selected as 0.35. We train the model with the batch size of 128 at the learning rate of 0.0003 with Adam optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with Baseline and other Methods</head><p>Although our primary focus is to compare with the baseline SGM, we also show the result of other state-of-the-art techniques to provide a comprehensive perspective on the performance of our proposed approach. Besides the SGM, other competing models are:</p><p>? PFAN <ref type="bibr" target="#b8">[9]</ref> which uses the position focused attention network to extract the features of the location of objects in images. ? IMRAM <ref type="bibr" target="#b23">[24]</ref> with the attention mechanism to learn the matching fragments between images and sentences. ? MMCA <ref type="bibr" target="#b24">[25]</ref> applying attention and transformer compartment to exploit the relationship between objects in images and words in texts within themselves. ? GSMN <ref type="bibr" target="#b33">[34]</ref> that is also a graph-based approach but connects all detected objects within an image to create a graph then compares to that of a text.</p><p>The LGSGM and others' results on Flickr30k can be depicted in <ref type="table" target="#tab_0">Table 1</ref>. The value in bold is the highest number in that metric. Caption retrieval indicates that a model needs to find texts that are relevant to a query image. On the contrary, image retrieval is used when a query is a sentence. It is important to note that other models' metrics are taken from their original report since we use the same subset for training, validating, and testing.</p><p>It is easier for all models to find captions when an image is given as a query than the reversed retrieving because the caption retrieval section's scores are higher than those in image retrieval. Regarding the caption retrieval, the GSMN using an ensemble setting performs best among all models, where the differences in the R-Sum can be up to more than 10%. This model is more accurate than ours with 3% on average of recall. Both attention-based techniques, IMRAM and MMCA, share similar results as there is no significant gap in all recall metrics. It is also true for SGM and LGSGM models. Although we obtain higher scores at R@5 and R@10 with a margin less than 0.5%, the SGM model has a slightly better R@1 metric than ours, which are 71.8% and 71%, respectively. The PFAN model performs worst with the lowest R@1 and R@10. However, our LGSGM achieves the highest scores on all three recall metrics in the image retrieval field, which are 57.4%, 84.1%, and 90.2% accordingly. It makes the proposed method have the best R-Sum of 231.7%, which is 3% higher than the GSMN in the second place. In specific, our R@5 and R@10 are better than that of GSMN roughly by 1.8% and 1.2% respectively. Compared with the SGM, our improvement creates a huge increase of 12.1% in total recall. The PFAN is still the model with the lowest recall, while the R-Sum of MMCA is 4.5% higher than IMRAM. In general, the proposed LGSGM surpasses the SGM model by a large margin, which is also our main contribution. Our model and GSMN, which is also a graph-based method, are the top-2 methods in the experiment, showing the usefulness of the graph structure in this field. Nevertheless, GSMN achieves higher recall than ours with 496.7% compared to 490.7% of our model. It might be due to its dense graph structure where this model connects all of the objects in an image while the scene graph structure only captures some detected relations between them. Although the attention approaches are better than our scene graph model in the image-to-text retrieval, our network still manages to get the highest score on the remaining experiment. Moreover, LGSGM manages to score better than those models concerning the sum of recall of both image retrieval and caption retrieval experiments. With our state-of-the-art result in the text-to-image retrieval section, it opens a wide range of applications of our structure in the field of finding images that are relevant to the given description. For instance, one potential application is in lifelogging retrieval, where a graph-based method has shown its promising performance <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this research, we address an issue that remained in the state-of-the-art scene graph matching model in which the global detail of graphs is ignored during the graph encoding phrase. We propose a graph embedding module that can address that concern by summarising the overall information of a graph into a vector form. Our LGSGM method, therefore, can measure the similarity between images and captions based on their input scene graphs with both local and global views. Using a lighter and more accurate Effi-cientNet to extract features combining with normalizing techniques to mitigate the over-fitting problem, our model can surpass the baseline and achieve the new state-of-the-art results for those using scene graph as input in image-text retrieval challenge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>This figure presents the workflow of an image in our proposed model. First, regions of detected objects and a scene graph of the image are extracted. The regions are fed into a CNN-based model, which is the EfficientNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>This figure presents the workflow of a sentence in our proposed model. Both scene graphs and each word in the sentence go through a word embedding to learn the semantic features. Two distinct LSTM models are applied to the sentence and the triplet relations in the graph to get the word and the relation features used to compare with the visual feature scene graph to get the local similarity. Each encoded node and edge in the graph after the LSTM model then form into a graph with will be fed into a graph convolutional network to update their features and create the textual feature graph. Finally, this graph is embedded into a vector compared with that of visual data to measure the global similarity score. The N and M in the figure indicate the number of words and number of relations in the sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of models on Flickr30k Dataset. R-Sum is the sum of all recall metrics.</figDesc><table><row><cell>Methods</cell><cell>R@1</cell><cell cols="3">Caption retrieval R@5 R@10 R-Sum</cell><cell cols="3">Image retrieval R@1 R@5 R@10</cell><cell>R-Sum</cell></row><row><cell>PFAN [9]</cell><cell>70</cell><cell>91.8</cell><cell>95.0</cell><cell>256.8</cell><cell>50.4</cell><cell>78.7</cell><cell>86.1</cell><cell>215.2</cell></row><row><cell>IMRAM [24]</cell><cell>74.1</cell><cell>93.0</cell><cell>96.6</cell><cell>263.7</cell><cell>53.9</cell><cell>79.4</cell><cell>87.2</cell><cell>220.5</cell></row><row><cell>MMCA [25]</cell><cell>74.2</cell><cell>92.8</cell><cell>96.4</cell><cell>263.4</cell><cell>54.8</cell><cell>81.4</cell><cell>87.8</cell><cell>224.0</cell></row><row><cell>GSMN [34]</cell><cell>76.4</cell><cell>94.3</cell><cell>97.3</cell><cell>268.0</cell><cell>57.4</cell><cell>82.3</cell><cell>89.0</cell><cell>228.7</cell></row><row><cell>SGM [15]</cell><cell>71.8</cell><cell>91.7</cell><cell>95.5</cell><cell>259.0</cell><cell>53.5</cell><cell>79.6</cell><cell>86.5</cell><cell>219.6</cell></row><row><cell>LGSGM (Ours)</cell><cell>71</cell><cell>91.9</cell><cell>96.1</cell><cell>259.0</cell><cell>57.4</cell><cell>84.1</cell><cell>90.2</cell><cell>231.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head><p>This publication has emanated from research supported in party by research grants from Science Foundation Ireland under grant numbers SFI/12/RC/2289, SFI/13/RC/2106, and 18/CRT/6223.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno type="arXiv">arXiv:1802.01268</idno>
		<title level="m">Accurate brain extraction using active shape model and convolutional neural networks</title>
		<editor>Nguyen Ho Minh Duy, Nguyen Manh Duy, Mai Thanh Nhat Truong, Pham The Bao, and Nguyen Thanh Binh</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d photography using context-aware layered depth inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Li</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Yang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adversarial cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="154" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dual-path convolutional image-text embeddings with instance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Dong</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning semantic concepts and order for image and sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Position focused attention network for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueming</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09748</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3668" to="3678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scene graph captioner: Image captioning based on structural visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An-An</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="477" to="485" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph-based indexing and retrieval of lifelog data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manh-Duy</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Binh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cathal</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="256" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-modal scene graph matching for relationship-aware image-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1508" to="1517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge aware semantic concept expansion for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="670" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5831" to="5840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to compose dynamic tree structures for visual contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6619" to="6628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Factorizable net: an efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="335" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imram: Iterative matching with recurrent attention memory for cross-modal image-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12655" to="12663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-modality cross attention network for image and sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10941" to="10950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<meeting><address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised inductive graph-level representation learning via graph-graph proximity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agustin</forename><surname>Marinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01098</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2641" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph structured network for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10921" to="10930" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
