<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Remember More with Less Memorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Applied AI Institute</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>Geelong</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
							<email>truyen.tran@deakin.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Applied AI Institute</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>Geelong</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
							<email>svetha.venkatesh@deakin.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Applied AI Institute</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>Geelong</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Remember More with Less Memorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Memory-augmented neural networks consisting of a neural controller and an external memory have shown potentials in long-term sequential learning. Current RAM-like memory models maintain memory accessing every timesteps, thus they do not effectively leverage the short-term memory held in the controller. We hypothesize that this scheme of writing is suboptimal in memory utilization and introduces redundant computation. To validate our hypothesis, we derive a theoretical bound on the amount of information stored in a RAM-like system and formulate an optimization problem that maximizes the bound. The proposed solution dubbed Uniform Writing is proved to be optimal under the assumption of equal timestep contributions. To relax this assumption, we introduce modifications to the original solution, resulting in a solution termed Cached Uniform Writing. This method aims to balance between maximizing memorization and forgetting via overwriting mechanisms. Through an extensive set of experiments, we empirically demonstrate the advantages of our solutions over other recurrent architectures, claiming the state-of-the-arts in various sequential modeling tasks.</p><p>paper presents a new approach towards finding optimal operations for MANNs that serve the purpose of learning longer sequences with finite memory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A core task in sequence learning is to capture long-term dependencies amongst timesteps which demands memorization of distant inputs. In recurrent neural networks (RNNs), the memorization is implicitly executed via integrating the input history into the state of the networks. However, learning vanilla RNNs over long distance proves to be difficult due to the vanishing gradient problem <ref type="bibr" target="#b2">(Bengio et al., 1994;</ref><ref type="bibr" target="#b19">Pascanu et al., 2013)</ref>. One alleviation is to introduce skip-connections along the execution path, in the forms of dilated layers <ref type="bibr" target="#b26">(Van Den Oord et al., 2016;</ref><ref type="bibr" target="#b3">Chang et al., 2017)</ref>, attention mechanisms <ref type="bibr" target="#b1">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b27">Vaswani et al., 2017)</ref> and external memory <ref type="bibr" target="#b10">(Graves et al., 2014;</ref><ref type="bibr" target="#b9">2016)</ref>.</p><p>Amongst all, using external memory most resembles human cognitive architecture where we perceive the world sequentially and make decision by consulting our memory. Recent attempts have simulated this process by using RAM-like memory architectures that store information into memory slots. Reading and writing are governed by neural controllers using attention mechanisms. These memory-augmented neural networks (MANN) have demonstrated superior performance over recurrent networks in various synthetic experiments  and realistic applications <ref type="bibr" target="#b14">(Le et al., 2018a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b6">Franke et al., 2018)</ref>.</p><p>Despite the promising empirical results, there is no theoretical analysis or clear understanding on optimal operations that a memory should have to maximize its performance. To the best of our knowledge, no solution has been proposed to help MANNs handle ultra-long sequences given limited memory. This scenario is practical because (i) sequences in the real-world can be very long while the computer resources are limited and (ii) it reflects the ability to compress in human brain to perform life-long learning. Previous attempts such as <ref type="bibr" target="#b22">(Rae et al., 2016)</ref> try to learn ultra-long sequences by expanding the memory, which is not always feasible and do not aim to optimize the memory by some theoretical criterion. This More specifically, upon analyzing RNN and MANN operations we first introduce a measurement on the amount of information that a MANN holds after encoding a sequence. This metric reflects the quality of memorization under the assumption that contributions from timesteps are equally important. We then derive a generic solution to optimize the measurement. We term this optimal solution as Uniform Writing (UW), and it is applicable for any MANN due to its generality. Crucially, UW helps reduce significantly the computation time of MANN. Third, to relax the assumption and enable the method to work in realistic settings, we further propose Cached Uniform Writing (CUW) as an improvement over the Uniform Writing scheme. By combining uniform writing with local attention, CUW can learn to discriminate timesteps while maximizing local memorization. Finally we demonstrate that our proposed models outperform several MANNs and other state-of-the-art methods in various synthetic and practical sequence modeling tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Theoretical Analysis</head><p>Memory-augmented neural networks can be viewed as an extension of RNNs with external memory M . The memory supports read and write operations based on the output o t of the controller, which in turn is a function of current timestep input x t , previous hidden state h t?1 and read value r t?1 from the memory. Let assume we are given these operators from recent MANNs such as NTM <ref type="bibr" target="#b10">(Graves et al., 2014)</ref> or DNC , represented as:</p><formula xml:id="formula_0">r t = f r (o t , M t?1 ) (1) M t = f w (o t , M t?1 )<label>(2)</label></formula><p>The controller output and hidden state are updated as follows:</p><formula xml:id="formula_1">o t = f o (h t?1 , r t?1 , x t ) (3) h t = f h (h t?1 , r t?1 , x t )<label>(4)</label></formula><p>Here, f o and f h are often implemented as RNNs while f r and f w are designed specifically for different memory types.</p><p>Current MANNs only support regular writing by applying Eq.</p><p>(2) every timestep. In effect, regular writing ignores the accumulated short-term memory stored in the controller hidden states which may well-capture the recent subsequence. We argue that the controller does not need to write to memory continuously as its hidden state also supports memorizing. Another problem of regular writing is time complexity. As the memory access is very expensive, reading/writing at every timestep makes MANNs much slower than RNNs. This motivates a irregular writing strategy to utilize the memorization capacity of the controller and and consequently, speed up the model. In the next sections, we first define a metric to measure the memorization performance of RNNs, as well as MANNs. Then, we solve the problem of finding the best irregular writing that optimizes the metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Memory analysis of RNNs</head><p>We first define the ability to "remember" of recurrent neural networks, which is closely related to the vanishing/exploding gradient problem <ref type="bibr" target="#b19">(Pascanu et al., 2013)</ref>. In RNNs, the state transition h t = ? (h t?1 , x t ) contains contributions from not only x t , but also previous timesteps x i&lt;t embedded in h t?1 . Thus, h t can be considered as a function of timestep inputs, i.e, h t = f (x 1 , x 2 , ..., x t ). One way to measure how much an input x i contributes to the value of h t is to calculate the norm of the gradient ?ht ?xi . If the norm equals zero, h t is constant w.r.t x i , that is, h t does not "remember" x i . As a bigger ?ht ?xi implies more influence of x i on h t , we propose using ?ht ?xi to measure the contribution of the i-th input attention <ref type="figure">Figure 1</ref>: Writing mechanism in Cached Uniform Writing. During non-writing intervals, the controller hidden states are pushed into the cache. When the writing time comes, the controller attends to the cache, chooses suitable states and accesses the memory. The cache is then emptied.</p><p>to the t-th hidden state. Let c i,t denotes this term, we can show that in the case of common RNNs, ? c c i,t ? c i?1,t with some ? c ? R + (see Appendix A -C for proof). This means further to the past, the contribution decays (when ? c &lt; 1) or grows (when ? c &gt; 1) with the rate of at least ? c .We can measure the average amount of contributions across T timesteps as follows (see Appendix D for proof):</p><p>Theorem 1. There exists ? ? R + such that the average contribution of a sequence of length T with respect to a RNN can be quantified as the following:</p><formula xml:id="formula_2">I ? = T t=1 c t,T T = c T,T T t=1 ? T ?t T (5) If ? &lt; 1, ? T ?t ? 0 as T ? t ? ?.</formula><p>This is closely related to vanishing gradient problem. LSTM is known to "remember" long sequences better than RNN by using extra memory gating mechanisms, which help ? to get closer to 1. If ? &gt; 1, the system may be unstable and suffer from the exploding gradient problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Memory analysis of MANNs</head><p>In slot-based MANNs, memory M is a set of D memory slots. A write at step t can be represented by the controller's hidden state h t , which accumulates inputs over several timesteps (i.e., x 1 , ...,x t ). If another write happens at step t+k, the state h t+k 's information containing timesteps x t+1 , ...,x t+k is stored in the memory (h t+k may involves timesteps further to the past, yet they are already stored in the previous write and can be ignored). During writing, overwriting may happen, replacing an old write with a new one. Thus after all, D memory slots associate with D chosen writes of the controller. From these observations, we can generalize Theorem 1 to the case of MANNs having D memory slots (see Appendix E for proof).</p><p>Theorem 2. With any D chosen writes at timesteps 1 ?K 1 &lt; K 2 &lt; ...&lt;K D &lt; T , there exist ?, C ? R + such that the lower bound on the average contribution of a sequence of length T with respect to a MANN having D memory slots can be quantified as the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Cached Uniform Writing</head><p>Require: a sequence x = {x t } T t=1 , a cache C sized L, a memory sized D. 1: for t = 1, T do 2:</p><formula xml:id="formula_3">C.append(h t?1 ) 3: if t mod L == 0 then 4:</formula><p>Use Eq.(10) to calculate a t 5:</p><formula xml:id="formula_4">Execute Eq.(3): o t = f o (a t , r t?1 , x t ) 6: Execute Eq.(4): h t = f h (a t , r t?1 , x t ) 7:</formula><p>Update the memory using Eq.(2) 8:</p><p>Read r t from the memory using Eq.(1) Update the controller using Eq.(4):</p><formula xml:id="formula_5">h t = f h (h t?1 , r t?1 , x t )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Assign r t = r t?1 13: end if 14: end for</p><formula xml:id="formula_6">I ? = C K1 t=1 ? K1?t + K2 t=K1+1 ? K2?t + ...+ K D t=K D?1 +1 ? K D ?t + T t=K D +1 ? T ?t T = C T D+1 i=1 li?1 j=0 ? j = C T D+1 i=1 f ? (l i ) (6) where l i = ? ? ? K 1 ; i = 1 K i ? K i?1 ; D ? i &gt; 1 T ? K D ; i = D + 1 , f ? (x) = 1?? x 1?? ? = 1 x ? = 1 , ?x ? R + .</formula><p>If ? ? 1, we want to maximize I ? to keep the information from vanishing. On the contrary, if ? &gt; 1, we may want to minimize I ? to prevent the information explosion. As both scenarios share the same solution (see Appendix F), thereafter we assume that ? ? 1 holds for other analyses. By taking average over T , we are making an assumption that all timesteps are equally important. This helps simplify the measurement as I ? is independent of the specific position of writing. Rather, it is a function of the interval lengths between the writes. This turns out to be an optimization problem whose solution is stated in the following theorem.</p><p>Theorem 3. Given D memory slots, a sequence with length T , a decay rate 0 &lt; ? ? 1,</p><formula xml:id="formula_7">then the optimal intervals {l i ? R + } D+1 i=1 satisfying T = D+1 i=1</formula><p>l i such that the lower bound on the average contribution</p><formula xml:id="formula_8">I ? = C T D+1 i=1 f ? (l i )</formula><p>is maximized are the following:</p><formula xml:id="formula_9">l 1 = l 2 = ... = l D+1 = T D + 1<label>(7)</label></formula><p>We name the optimal solution as Uniform Writing (UW) and refer to the term T D+1 and D+1 T as the optimal interval and the compression ratio, respectively. The proof is given in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Proposed Models</head><p>Uniform writing can apply to any MANNs that support writing operations. Since the writing intervals are discrete, i.e., l i ? N + , UW is implemented as the following:</p><formula xml:id="formula_10">M t = f w (o t , M t?1 ) if t = T D+1 k, k ? N + M t?1 otherwise (8)</formula><p>By following Eq. (8), the write intervals are close to the optimal interval defined in Theorem 3 and approximately maximize the average contribution. This writing policy works well if timesteps are equally important and the task is to remember all of them to produce outputs (i.e., in copy task). However, in reality, timesteps are not created equal and a good model may need to ignore unimportant or noisy timesteps. That is why overwriting in MANN can be necessary. In the next section, we propose a method that tries to balance between following the optimal strategy and employing overwriting mechanism as in current MANNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Local optimal design</head><p>To relax the assumptions of Theorem 3, we propose two improvements of the Uniform Writing (UW) strategy. First, the intervals between writes are equal with length L (1 ? L ? T D+1 ). If L = 1, the strategy becomes regular writing and if L = T D+1 , it becomes uniform writing. This ensures that after T L writes, all memory slots should be filled and the model has to learn to overwrite. Meanwhile, the average kept information is still locally maximized every L * D timesteps.</p><p>Second, we introduce a cache of size L to store the hidden states of the controller during a write interval. Instead of using the hidden state at the writing timestep to update the memory, we perform an attention over the cache to choose the best representative hidden state. The model will learn to assign attention weights to the elements in the cache. This mechanism helps the model consider the importance of each timestep input in the local interval and thus relax the equal contribution assumption of Theorem 3. We name the writing strategy that uses the two mentioned-above improvements as Cached Uniform Writing (CUW). An illustration of the writing mechanism is depicted in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Local memory-augmented attention unit</head><p>In this subsection, we provide details of the attention mechanism used in our CUW. To be specific, the best representative hidden state a t is computed as follows:</p><formula xml:id="formula_11">? tj = sof tmax v T tanh (W h t?1 + U d j + V r t?1 ) (9) a t = L j=1 ? tj d j<label>(10)</label></formula><p>where ? tj is the attention score between the t-th writing step and the j-th element in the cache; W , U , V and v are parameters; h and r are the hidden state of the controller and the read-out (Eq. (1)), respectively; d j is the cache element and can be implemented as the controller's hidden state</p><formula xml:id="formula_12">(d j = h t?1?L+j ).</formula><p>The vector a t will be used to replace the previous hidden state in updating the controller and memory. The whole process of performing CUW is summarized in Algo. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">An Ablation Study: Memory-augmented Neural Networks with and without Uniform Writing</head><p>In this section, we study the impact of uniform writing on MANNs under various circumstances (different controller types, memory types and number of memory slots). We restrict the memorization problem to the double task in which the models must reconstruct a sequence of integers sampled uniformly from range [1, 10] twice. We cast this problem to a sequence to sequence problem with 10 possible outputs per decoding step. The training stops after 10,000 iterations of batch size 64. We choose DNC 1 and NTM 2 as the two MANNs in the experiment. The recurrent controllers can be RNN or LSTM. With LSTM controller, the sequence length is set to 50. We choose sequence length of 30 to make it easier for the RNN controller to learn the task. The number of memory slots D is chosen from the set {2, 4, 9, 24} and {2, 4, 9, 14} for LSTM and RNN controllers, respectively. More memory slots will make UW equivalent to the regular writing scheme. For this experiment, we use Adam optimizer (Kingma &amp; Ba, 2014) with initial learning rate and gradient clipping of {0.001, 0.0001} and {1, 5, 10}, respectively. The metric used to measure the performance is the average accuracy across decoding steps. For each configuration of hyper-parameters, we run the experiment 5 times and report the mean accuracy with error bars.</p><p>Figs. 2(a) and (c) depict the performance of UW and regular writing under different configurations. In any case, UW boosts the prediction accuracy of MANNs. The performance gain can be seen clearly when the compression ratio is between 10 ? 40%. This is expected since when the compression ratio is too small or too big, UW converges to regular writing. Interestingly, increasing the memory size does not always improve the performance, as in the case of NTM with RNN controllers. Perhaps, learning to attend to many memory slots is tricky for some task given limited amount of training data. This supports the need to apply UW to MANN with moderate memory size. We also conduct experiments to verify the benefit of using UW for bigger memory. The results can be found in Appendix H.</p><p>We also measure the speed-up of training time when applying UW on DNC and NTM, which is illustrated in Figs. 2(b) and (d). The result shows that with UW, the training time can drop up to 60% for DNC and 28% for NTM, respectively. As DNC is more complicated than NTM, using UW to reduce memory access demonstrates clearer speed-up in training (similar behavior can be found for testing time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Synthetic Memorization</head><p>Here we address a broader range of baselines on two synthetic memorization tasks, which are the sequence copy and reverse. In these tasks, there is no discrimination amongst timesteps so the model's goal is to learn to compress the input efficiently for later retrieval. We experiment with different sequence lengths of 50 and 100 timesteps. Other details are the same as the previous double task except that we fix the learning rate and gradient clipping to 0.001 and 10, respectively. The standard baselines include LSTM, NTM and DNC. All memory-augmented models have the same memory size of 4 slots, corresponding to compression ratio of 10% and 5%, respectively. We aim at this range of compression ratio to match harsh practical requirements. UW and CUW (cache size L = 5) are built upon the DNC, which from our previous observations, works best for given compression ratios. We choose different dimensions N h for the hidden vector of the controllers to ensure the model sizes are approximately equivalent. To further verify that our UW is actually the optimal writing strategy, we design a new baseline, which is DNC with random irregular writing strategy (RW). The write is sampled from a binomial distribution with p = (D + 1) /T (equivalent to compression ratio). After sampling, we conduct the training for that policy. The final performances of RW are taken average from 3 different random policies' results.</p><p>The performance of the models is listed in <ref type="table">Table 1</ref>. As clearly seen, UW is the best performer for the pure memorization tests. This is expected from the theory as all timesteps are importantly equivalent. Local attention mechanism in CUW does not help much in this scenario and thus CUW finishes the task as the runner-up. Reverse seems to be easier than copy as the models tend to "remember" more the last-seen timesteps whose contributions ? T ?t remains significant. In both cases, other baselines including random irregular and regular writing underperform our proposed models by a huge margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Synthetic Reasoning</head><p>Tasks in the real world rarely involve just memorization. Rather, they require the ability to selectively remember the input data and synthesize intermediate computations.</p><p>To investigate whether our proposed writing schemes help the memory-augmented models handle these challenges, we conduct synthetic reasoning experiments which include add and max tasks. In these tasks, each number in the output sequence is the sum or the maximum of two numbers in the input sequence. The pairing is fixed as: y t = xt+x T ?t 2 , t = 1, T 2 for add task and y t = max (x 2t , x 2t+1 ) , t = 1, T 2 for max task, respectively. The length of the output sequence is thus half of the input sequence. A brief overview of input/output format for these tasks can be found in Appendix G. We deliberately use local (max) and distant (add) pairing rules to test the model under different reasoning strategies. The same experimental setting as in the previous section is applied except for the data sample range for the max task, which is [1, 50] 3 . LSTM and NTM are excluded from the baselines as they fail on these tasks. <ref type="table" target="#tab_2">Table 2</ref> shows the testing results for the reasoning tasks. Since the memory size is small compared to the number of events, regular writing or random irregular writing cannot compete with the uniform-based writing policies. Amongst all baselines, CUW demonstrates superior performance in both tasks thanks to its local attention mechanism. It should be noted that the timesteps should not be treated equally in these reasoning tasks. The model should weight a timestep differently based on either its content (max task) or location (add task) and maintain its memory for a long time by following uniform criteria. CUW is designed to balance the two approaches and thus it achieves better performance. Further insights into memory operations of these models are given in Appendix I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Synthetic Sinusoidal Regression</head><p>In real-world settings, sometimes a long sequence can be captured and fully reconstructed by memorizing some of its feature points. For examples, a periodic function such as sinusoid can be well-captured if we remember the peaks of the signal. By observing the peaks,   we can deduce the frequency, amplitude, phase and thus fully reconstructing the function.</p><p>To demonstrate that UW and CUW are useful for such scenarios, we design a sequential continuation task, in which the input is a sequence of sampling points across some sinusoid: y = 5+A sin(2?f x+?). Here, A ? U (1, 5), f ? U (10, 30) and ? ? U (0, 100). After reading the input y = {y t } T t=1 , the model have to generate a sequence of the following points in the sinusoid. To ensure the sequence y varies and covers at least one period of the sinusoid, we set x = {x t } T t=1 where x i = (t + 1 ) /1000, 1 ? U <ref type="figure">(?1, 1)</ref>. The sequence length for both input and output is fixed to T = 100. The experimental models are LSTM, DNC, UW and CUW (built upon DNC). For each model, optimal hyperparameters including learning rate and clipping size are tuned with 10,000 generated sinusoids. The memories have 4 slots and all baselines have similar parameter size. We also conduct the experiment with noisy inputs by adding a noise 2 ? U (?2, 2) to the input sequence y. This increases the difficulty of the task. The loss is the average of mean square error (MSE) over decoding timesteps.</p><p>We plot the mean learning curves with error bars over 5 runnings for sinusoidal regression task under clean and noisy condition in <ref type="figure" target="#fig_2">Figs. 3(a)</ref> and (b), respectively. Regular writing DNC learns fast at the beginning, yet soon saturates and approaches the performance of LSTM (M SE = 1.05 and 1.39 in clean and noisy condition, respectively). DNC performance does not improve much as we increase the memory size to 50, which implies the difficulty in learning with big memory. Although UW starts slower, it ends up with lower errors than DNC and perform slightly better than CUW in clean condition (M SE = 0.44 for UW and 0.61 for CUW). CUW demonstrates competitive performance against other baselines, approaching to better solution than UW for noisy task where the model should discriminate the timesteps (M SE = 0.98 for UW and 0.55 for CUW). More visualizations can be found in Appendix J.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Flatten Image Recognition</head><p>We want to compare our proposed models with DNC and other methods designed to help recurrent networks learn longer sequence. The chosen benchmark is a pixel-by-pixel image classification task on MNIST in which pixels of each image are fed into a recurrent model sequentially before a prediction is made. In this task, the sequence length is fixed to 768   <ref type="bibr" target="#b3">(Chang et al., 2017)</ref> .</p><p>with highly redundant timesteps (black pixels). The training, validation and testing sizes are 50,000, 10,000 and 10,000, respectively. We test our models on both versions of nonpermutation (MNIST) and permutation (pMNIST). More details on the task and data can be found in <ref type="bibr" target="#b16">(Le et al., 2015)</ref>. For DNC, we try with several memory slots from {15, 30, 60} and report the best results. For UW and CUW, memory size is fixed to 15 and cache size L is set to 10. The controllers are implemented as single layer GRU with 100-dimensional hidden vector. To optimize the models, we use RMSprop with initial learning rate of 0.0001. <ref type="table" target="#tab_4">Table 3</ref> shows that DNC underperforms r-LSTM, which indicates that regular DNC with big memory finds it hard to beat LSTM-based methods. After applying UW, the results get better and with CUW, it shows significant improvement over r-LSTM and demonstrates competitive performance against dilated-RNNs models. Notably, dilated-RNNs use 9 layers in their experiments compared to our singer layer controller. Furthermore, our models exhibit more consistent performance than dilated-RNNs. For completeness, we include comparisons between CUW and non-recurrent methods in Appendix K</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Document Classification</head><p>To verify our proposed models in real-world applications, we conduct experiments on document classification task. In the task, the input is a sequence of words and the output is the classification label. Following common practices in <ref type="bibr" target="#b29">(Yogatama et al., 2017;</ref><ref type="bibr" target="#b24">Seo et al., 2018)</ref>, each word in the document is embedded into a 300-dimensional vector using Glove embedding <ref type="bibr" target="#b20">(Pennington et al., 2014)</ref>. We use RMSprop for optimization, with initial learning rate of 0.0001. Early-stop training is applied if there is no improvement after 5 epochs in the validation set. Our UW and CUW are built upon DNC with single layer 512-dimensional LSTM controller and the memory size is chosen in accordance with the average length of the document, which ensures 10 ? 20% compression ratio. The cache size for CUW is fixed to 10. The datasets used in this experiment are common big datasets where the number of documents is between 120,000 and 1,400,000 with maximum of 4,392 words per document (see Appendix L for further details). The baselines are recent state-of-the-arts in the domain, some of which are based on recurrent networks such as D-LSTM <ref type="bibr" target="#b29">(Yogatama et al., 2017)</ref> and Skim-LSTM <ref type="bibr" target="#b24">(Seo et al., 2018)</ref>. We exclude DNC from the baselines as it is inefficient to train the model with big document datasets.</p><p>Our results are reported in <ref type="table" target="#tab_6">Table 4</ref>. On five datasets out of six, our models beat or match the best published results. For IMDb dataset, our methods outperform the best recurrent model (Skim-LSTM). The performance gain is competitive against that of the state-of-thearts. In most cases, CUW is better than UW, which emphasizes the importance of relaxing the timestep equality assumption in practical situations. Details results across different runs for our methods are listed in Appendix M.   <ref type="bibr" target="#b21">(Qui et al., 2018)</ref> . We use italics to denote the best published and bold the best records.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Traditional recurrent models such as RNN/LSTM <ref type="bibr" target="#b5">(Elman, 1990;</ref><ref type="bibr" target="#b12">Hochreiter &amp; Schmidhuber, 1997)</ref> exhibit some weakness that prevent them from learning really long sequences. The reason is mainly due to the vanishing gradient problem <ref type="bibr" target="#b19">(Pascanu et al., 2013)</ref> or to be more specific, the exponential decay of input value over time. One way to overcome this problem is enforcing the exponential decay factor close to one by putting a unitary constraint on the recurrent weight <ref type="bibr" target="#b0">(Arjovsky et al., 2016;</ref><ref type="bibr" target="#b28">Wisdom et al., 2016)</ref>. Although this approach is theoretically motivated, it restricts the space of learnt parameters.</p><p>More relevant to our work, the idea of using less or adaptive computation for good has been proposed in <ref type="bibr" target="#b9">(Graves, 2016;</ref><ref type="bibr" target="#b3">Yu et al., 2017;</ref><ref type="bibr" target="#b24">Seo et al., 2018)</ref>. Most of these works are based on the assumption that some of timesteps in a sequence are unimportant and thus can be ignored to reduce the cost of computation and increase the performance of recurrent networks. Different form our approach, these methods lack theoretical supports and do not directly aim to solve the problem of memorizing long-term dependencies.</p><p>Dilated RNN <ref type="bibr" target="#b3">(Chang et al., 2017)</ref> is another RNN-based proposal which improves long-term learning by stacking multiple dilated recurrent layers with hierarchical skip-connections. This theoretically guarantees the mean recurrent length and shares with our method the idea to construct a measurement on memorization capacity of the system and propose solutions to optimize it. The difference is that our system is memory-augmented neural networks while theirs is multi-layer RNNs, which leads to totally different optimization problems.</p><p>Recent researches recommend to replace traditional recurrent models by other neural architectures to overcome the vanishing gradient problem. The Transformer <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> attends to all timesteps at once, which ensures instant access to distant timestep yet requires quadratic computation and physical memory proportional to the sequence length. Memoryaugmented neural networks (MANNs), on the other hand, learn to establish a limited-size memory and attend to the memory only, which is scalable to any-length sequence. Compared to others, MANNs resemble both computer architecture design and human working memory <ref type="bibr" target="#b17">(Logie, 2014)</ref>. However, the current understanding of the underlying mechanisms and theoretical foundations for MANN are still limited.</p><p>Recent works on MANN rely almost on reasonable intuitions. Some introduce new addressing mechanisms such as location-based <ref type="bibr" target="#b10">(Graves et al., 2014)</ref>, least-used <ref type="bibr" target="#b23">(Santoro et al., 2016)</ref> and order-based . Others focus on the scalability of MANN by using sparse memory access to avoid attending to a large number of memory slots <ref type="bibr" target="#b22">(Rae et al., 2016)</ref>. These problems are different from ours which involves MANN memorization capacity optimization.</p><p>Our local optimal solution to this problem is related to some known neural caching <ref type="bibr" target="#b8">(Grave et al., 2017b;</ref><ref type="bibr">a;</ref><ref type="bibr">Yogatama et al., 2018)</ref> in terms of storing recent hidden states for later encoding uses. These methods either aim to create structural bias to ease the learning process <ref type="bibr">(Yogatama et al., 2018)</ref> or support large scale retrieval <ref type="bibr" target="#b7">(Grave et al., 2017a)</ref>. These are different from our caching purpose, which encourages overwriting and relaxes the equal contribution assumption of the optimal solution. Also, the details of implementation are different as ours uses local memory-augmented attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have introduced Uniform Writing (UW) and Cached Uniform Writing (CUW) as faster solutions for longer-term memorization in MANNs. With a comprehensive suite of synthetic and practical experiments, we provide strong evidences that our simple writing mechanisms are crucial to MANNs to reduce computation complexity and achieve competitive performance in sequence modeling tasks. In complement to the experimental results, we have proposed a meaningful measurement on MANN memory capacity and provided theoretical analysis showing the optimality of our methods. Further investigations to tighten the measurement bound will be the focus of our future work. </p><formula xml:id="formula_13">U t?i W x i + C</formula><p>where C is some constant with respect to x i . In this case, ?ht ?xi = U t?i W . By applying norm sub-multiplicativity 5 ,</p><formula xml:id="formula_14">c i?1,t = U t?i+1 W ? U U t?i W = U c i,t That is, ? c = U .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Derivation on the bound inequality in standard RNN</head><p>The standard RNN hidden state is described by the following recursive equation:</p><formula xml:id="formula_15">h t = tanh (W x t + U h t?1 + b) From ?ht ?xi = ?ht ?ht?1 ?ht?1 ?xi , by induction, ?h t ?x i = ? ? t j=i+1 ?h j ?h j?1 ? ? ?h i ?x i = ? ? t j=i diag tanh (a j ) ? ? U t?i W</formula><p>where a j = W x j + U h j?1 + b and diag (?) converts a vector into a diagonal matrix. As 0 ? tanh (x) = 1 ? tanh (x) 2 ? 1, diag tanh (X) is bounded by some value B. By applying norm sub-multiplicativity,</p><formula xml:id="formula_16">c i?1,t = ? ? t j=i diag tanh (a j ) ? ? diag tanh (a i?1 ) U t?i+1 W ? U t j=i diag tanh (a j ) U t?i W diag tanh (a i?1 ) = B U c i,t</formula><p>That is, ? c = B U .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Derivation on the bound inequality in LSTM</head><p>For the case of LSTM, the recursive equation reads:</p><formula xml:id="formula_17">c t = ? (U f x t + W f h t?1 + b f ) c t?1 + ? (U i x t + W i h t?1 + b i ) tanh (U z x t + W z h t?1 + b z ) h t = ? (U o x t + W o h t?1 + b o ) tanh (c t )</formula><p>Taking derivatives,</p><formula xml:id="formula_18">?h j ?h j?1 = ? (o j ) tanh (c j ) W o + ? (o j ) tanh (c j ) ? (f j ) c j?1 W f + ? (o j ) tanh (c j ) ? (i j ) tanh (z j ) W i + ? (o j ) tanh (c j ) ? (i j ) tanh (z j ) W z ?h j?1 ?x j?1 = ? (o j?1 ) tanh (c j?1 ) U o + ? (o j?1 ) tanh (c j?1 ) ? (f j?1 ) c j?2 U f + ? (o j?1 ) tanh (c j?1 ) ? (i j?1 ) tanh (z j?1 ) U i + ? (o j?1 ) tanh (c j?1 ) ? (i j?1 ) tanh (z j?1 ) U z ?h j ?x j = ? (o j ) tanh (c j ) U o + ? (o j ) tanh (c j ) ? (f j ) c j?1 U f + ? (o j ) tanh (c j ) ? (i j ) tanh (z j ) U i + ? (o j ) tanh (c j ) ? (i j ) tanh (z j ) U z</formula><p>where o j denotes the value in the output gate at j-th timestep (similar notations are used for input gate (i j ), forget gate (f j ) and cell value (z j )) and "non-matrix" terms actually represent diagonal matrices corresponding to these terms. Under the assumption that h 0 =0, we then make use of the results in <ref type="bibr" target="#b18">(Miller &amp; Hardt, 2018)</ref> stating that c t ? is bounded for all t. By applying l ? -norm sub-multiplicativity and triangle inequality, we can show that</p><formula xml:id="formula_19">?h j ?h j?1 ?h j?1 ?x j?1 = M ?h j ?x j + N with M ? ? 1/4 W o ? + 1/4 W f ? c j ? + 1/4 W i ? + W z ? = B m N ? ? 1/16 W o U i ? + 1/16 W o U f ? c j ? + c j?1 ? + 1/4 W o U z ? + 1/16 W i U o ? + 1/16 W i U f ? c j ? + c j?1 ? + 1/4 W i U z ? + 1/16 W f U o ? + 1/16 W f U i ? + 1/4 W f U z ? c j ? + c j?1 ? + 1/4 W z U o ? + 1/4 W z U f ? c j ? + c j?1 ? + 1/4 W z U i ? = B n</formula><p>By applying l ? -norm sub-multiplicativity and triangle inequality,</p><formula xml:id="formula_20">c i?1,t = t j=i+1 ?h j ?h j?1 ?h i ?h i?1 ?h i?1 ?x i?1 ? = t j=i+1 ?h j ?h j?1 ?h i ?x i m + n ? ? B m c i,t + B n t j=i+1 ?h j ?h j?1 ?</formula><p>As LSTM is ?-contractive with ? &lt; 1 in the l ? -norm (readers are recommended to refer to <ref type="bibr" target="#b18">(Miller &amp; Hardt, 2018)</ref> for proof), which implies</p><formula xml:id="formula_21">?hj ?hj?1 ? &lt; 1, B n t j=i+1 ?hj ?hj?1 ? ? 0 as t ? i ? ?.</formula><p>For t ? i &lt; ?, under the assumption that ?hj ?xj = 0, we can always find some value <ref type="figure">max (B m , B)</ref>.</p><formula xml:id="formula_22">B &lt; ? such that c i?1,t ? Bc i,t . For t?i ? ?, ? c ? B m . That is, ? c =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proof of theorem 1</head><p>Proof. Given that ? c c i,t ? c i?1,t with some ? c ? R + , we can use c t,t ? t?i c as the upper bound on c i,t with i = 1, t, respectively. Therefore,</p><formula xml:id="formula_23">f (0) ? T t=1 c t,T ? c T,T T t=1 ? T ?t c = f (? c ) where f (?) = c T,T T t=1 ? T ?t is continuous on R + . According to intermediate value theorem, there exists ? ? (0, ? c ] such that c T,T T t=1 ? T ?t = T t=1 c t,T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Proof of theorem 2</head><p>Proof. According to Theorem 1, there exists some ? i ? R + such that the summation of contribution stored between K i and K i+1 can be quantified as c Ki+1,Ki+1 </p><formula xml:id="formula_24">as I ? , where I ? = C K 1 t=1 ? K 1 ?t + K 2 t=K 1 +1 ? K 2 ?t +...+ K D t=K D?1 +1 ? K D ?t + T t=K D +1 ? T ?t T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Proof of theorem 3</head><p>Proof. The second-order derivative of f ? (x) reads:</p><formula xml:id="formula_25">f ? (x) = ? (ln ?) 2 1 ? ? ? x<label>(11)</label></formula><p>We have f ?, (x) ? 0 with ?x ? R + and 1 &gt; ? &gt; 0, so f ? (x) is a concave function. Thus, we can apply Jensen inequality as follows:</p><formula xml:id="formula_26">1 D + 1 D+1 i=1 f ? (l i ) ? f ? D+1 i=1 1 D + 1 l i = f ? T D + 1<label>(12)</label></formula><p>Equality holds if and only if l 1 = l 2 = ... = l D+1 = T D+1 . We refer to this as Uniform Writing strategy. By plugging the optimal values of l i , we can derive the maximized average contribution as follows:</p><formula xml:id="formula_27">I ? max ? g ? (T, D) = C (D + 1) T 1 ? ? T D+1 1 ? ?<label>(13)</label></formula><p>When ? = 1,</p><formula xml:id="formula_28">I ? = C T D+1 i=1</formula><p>l i = C. This is true for all writing strategies. Thus, Uniform</p><p>Writing is optimal for 0 &lt; ? ? 1.</p><p>We can show that this solution is also optimal for the case ? &gt; 1. As f ? (x) &gt; 0 with ?x ? R + ; ? &gt; 1, f ? (x) is a convex function and Eq. (12) flips the inequality sign. Thus, I ? reaches its minimum with Uniform Writing. For ? &gt; 1, minimizing I ? is desirable to prevent the system from diverging.</p><p>We can derive some properties of function g. Let <ref type="figure">D)</ref> is an increasing function if we fix T and let D vary. That explains why having more memory slots helps improve memorization capacity. If D = 0, g ? (T, 0) becomes E.q (5). In this case, MANNs memorization capacity converges to that of recurrent networks.</p><formula xml:id="formula_29">x = D+1 L , g ? (L, D) = g ? (x) = Cx( ? 1 x ?1 ??1 ). We have g ? (x) = C? 1 ? ? 1 x (x ? ln ?) &gt; 0 with 0 &lt; ? ? 1, ?x ? 0, so g ? (T,</formula><p>G Summary of synthetic discrete task format  <ref type="table">Table 6</ref>: Test accuracy (%) on synthetic copy task. MANNs have 50 memory slots. Both models are trained with 100,000 mini-batches of size 32.</p><formula xml:id="formula_30">Task Input Output Double x 1 x 2 ...x T x 1 x 2 ...x T x 1 x 2 ...x T Copy x 1 x 2 ...x T x 1 x 2 ...x T Reverse x 1 x 2 ...x T x T x T ?1 ...x 1 Add x 1 x 2 ...x T x1+x T ?1 2 x2+x T ?2 2 ... x T /2 +x T /2 2 Max x 1 x 2 ...x T max (x 1 , x 2 ) max (x 3 , x 4 ) ... max (x T ?1 , x T )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Memory operating behaviors on synthetic tasks</head><p>In this section, we pick three models (DNC, DNC+UW and DNC+CUW) to analyze their memory operating behaviors. <ref type="figure">Fig. 4</ref> visualizes the values of the write weights and read weights for the copy task during encoding input and decoding output sequence, respectively.</p><p>In the copy task, as the sequence length is 50 while the memory size is 4, one memory slot should contain the accumulation of multiple timesteps. This principle is reflected in the decoding process in three models, in which one memory slot is read repeatedly across several timesteps. Notably, the number of timesteps consecutively spent for one slot is close to 10the optimal interval, even for DNC ( <ref type="figure">Fig. 4(a)</ref>), which implies that the ultimate rule would be the uniform rule. As UW and CUW are equipped with uniform writing, their writing patterns follow the rule perfectly. Interestingly, UW chooses the first written location for the final write (corresponding to the &lt;eos&gt; token) while CUW picks the last written location.</p><p>As indicated in <ref type="figure">Figs. 4(b)</ref> and (c), both of them can learn the corresponding reading pattern for decoding process, which leads to good performances. On the other hand, regular DNC fails to learn a perfect writing strategy. Except for the timesteps at the end of the sequence, the timesteps are distributed to several memory slots while the reading phase attends to one memory slot repeatedly. This explains why regular DNC cannot compete with the other two proposed methods in this task.   Each row is a timestep and each column is a memory slot.</p><p>For the max task, <ref type="figure" target="#fig_5">Fig. 5</ref> displays similar visualization with an addition of write gate during encoding phase. The write gate indicates how much the model should write the input at some timestep to the memory. A zero write gate means there is no writing. For this task, a good model should discriminate between timesteps and prefer writing the greater ones. As clearly seen in <ref type="figure" target="#fig_5">Fig. 5(a)</ref>, DNC suffers the same problem as in copy task, unable to synchronize encoding writing with decoding reading. Also, DNC's write gate pattern does not show reasonable discrimination. For UW ( <ref type="figure" target="#fig_5">Fig. 5(b)</ref>), it tends to write every timestep and relies on uniform writing principle to achieve write/read accordance and thus better results than DNC. Amongst all, CUW is able to ignore irrelevant timesteps and follows uniform writing at the same time (see <ref type="figure" target="#fig_5">Fig. 5(c)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Visualizations of model performance on sinusoidal regression tasks</head><p>We pick randomly 3 input sequences and plot the output sequences produced by DNC, UW and CUW in Figs. 6 (clean) and 7 (noisy). In each plot, the first and last 100 timesteps correspond to the given input and generated output, respectively. The ground truth sequence is plotted in red while the predicted in blue. We also visualize the values of MANN write gates through time in the bottom of each plots. In irregular writing encoding phase, the write gate is computed even when there is no write as it reflects how much weight the controller puts on the timesteps. In decoding, we let MANNs write to memory at every timestep to allow instant update of memory during inference.</p><p>Under clean condition, all models seem to attend more to late timesteps during encoding, which makes sense as focusing on late periods of sine wave is enough for later reconstruction. However, this pattern is not clear in DNC and UW as in CUW. During decoding, the write gates tend to oscillate in the shape of sine wave, which is also a good strategy as this directly reflects the amplitude of generation target. In this case, both UW and CUW demonstrate this behavior clearer than DNC.</p><p>Under noisy condition, DNC and CUW try to follow sine-shape writing strategy. However, only CUW can learn the pattern and assign write values in accordance with the signal period, which helps CUW decoding achieve highest accuracy. On the other hand, UW choose to assign write value equally and relies only on its maximization of timestep contribution. Although it achieves better results than DNC, it underperforms CUW.</p><p>K Comparison with non-recurrent methods in flatten image classification task   <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> and <ref type="bibr" target="#b3">(Chang et al., 2017)</ref> .     <ref type="table">Table 9</ref>: Document classification accuracy (%) on several datasets reported for 3 different runs. Bold denotes the best records.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L Details on document classification datasets</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The accuracy (%) and computation time reduction (%) with different memory types and number of memory slots. The controllers/sequence lengths/memory sizes are chosen as LSTM/50/{2, 4, 9, 24} (a&amp;b)  andRNN/30/{2, 4, 9, 14} (c&amp;d), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Learning curves of models in clean (a) and noisy (b) sinusoid regression experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>contributions before K i -th timestep for simplicity). Let denote P (?) = Ki+1 t=Ki ? Ki+1?t , we have P (?) &gt; 0, ?? ? R + . Therefore, P (? i ) ? P min i (? i ) . Let C = min i (c i,i ) and ? = min i (? i ), the average contribution stored in a MANN has a lower bound quantified</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Memory operations on max task in DNC (a), DNC+UW (b) and DNC+CUW(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Sinusoidal generation with clean input sequence for DNC, UW and CUW in top-down order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Sinusoidal generation with noisy input sequence for DNC, UW and CUW in top-down order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy (%) on synthetic reasoning tasks. MANNs have 4 memory slots.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy (%) on MNIST, pMNIST. Previously reported results are from (Le et al., 2015) ? , (Arjovsky et al., 2016) ? , (Trinh et al., 2018) , and</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Document classification accuracy (%) on several datasets. Previously reported results are from<ref type="bibr" target="#b4">(Conneau et al., 2016)</ref> ? ,<ref type="bibr" target="#b29">(Yogatama et al., 2017)</ref> </figDesc><table /><note>* , (Seo et al., 2018) ? and</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, and Phil Blunsom. Memory architectures in recurrent neural network language models. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum?id=SkFqf0lAZ.</figDesc><table><row><cell>Adams Wei Yu, Hongrae Lee, and Quoc Le. Learning to skim text. In Proceedings of the</cell></row><row><cell>55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long</cell></row><row><cell>Papers), volume 1, pp. 1880-1890, 2017.</cell></row><row><cell>Keyi Yu, Yang Liu, Alexander G Schwing, and Jian Peng. Fast and accurate text classifi-</cell></row><row><cell>cation: Skimming, rereading and early stopping. ICLR Workshop Track, 2018.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Synthetic discrete task's input-output formats. T is the sequence length.</figDesc><table><row><cell cols="2">H UW performance on bigger memory</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">N h Copy (L=500)</cell></row><row><cell>DNC</cell><cell>128</cell><cell>24.19%</cell></row><row><cell cols="2">DNC+UW 128</cell><cell>81.45%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Test accuracy (%) on MNIST, pMNIST. Previously reported results are from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Statistics on several big document classification datasets</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Std 93.54?0.08 91.32?0.04 96.36?0.03 65.04?0.11 Std 93.73?0.08 91.25?0.04 96.36?0.04 65.16?0.24</figDesc><table><row><cell cols="4">M Document classification detailed records</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>AG</cell><cell>IMDb</cell><cell>Yelp P.</cell><cell>Yelp F.</cell></row><row><cell></cell><cell>1</cell><cell>93.42</cell><cell>91.39</cell><cell>96.39</cell><cell>64.89</cell></row><row><cell>UW</cell><cell>2 3</cell><cell>93.52 93.69</cell><cell>91.30 91.25</cell><cell>96.31 96.39</cell><cell>64.97 65.26</cell></row><row><cell cols="2">1 2 Mean/CUW 3</cell><cell>93.61 93.87 93.70</cell><cell>91.26 91.18 91.32</cell><cell>96.42 96.29 96.36</cell><cell>65.63 65.05 64.80</cell></row><row><cell cols="2">Mean/</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our reimplementation based on https://github.com/deepmind/dnc 2 https://github.com/MarkPKCollier/NeuralTuringMachine</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">With small range like[1, 10], there is no much difference in performance amongst models</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Methods that use semi-supervised training to achieve higher accuracy are not listed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">If not explicitly stated otherwise, norm refers to any consistent matrix norm which satisfies sub-multiplicativity.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A Derivation on the bound inequality in linear dynamic system The linear dynamic system hidden state is described by the following recursive equation:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust and scalable differentiable neural computer for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W18-2606" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Machine Reading for Question Answering</title>
		<meeting>the Workshop on Machine Reading for Question Answering</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="47" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unbounded cache model for online language modeling with open vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moustapha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6042" to="6052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwi?ska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">G?mez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Variational memory encoderdecoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thin</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1515" to="1525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dual memory neural computer for asynchronous two-view sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/3219819.3219981</idno>
		<ptr target="http://doi.acm.org/10.1145/3219819.3219981" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery; Data Mining, KDD &apos;18</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery; Data Mining, KDD &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Visuo-spatial working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Logie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">When recurrent models don&apos;t need to be recurrent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10369</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A new method of region embedding for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Qui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BkSDMA36Z" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scaling memory-augmented neural networks with sparse reads and writes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3621" to="3629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<title level="m">Neural Speed Reading via Skim-RNN. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning longer-term dependencies in rnns with auxiliary losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>PMLR 80</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35 th International Conference on Machine Learning</title>
		<meeting>the 35 th International Conference on Machine Learning<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fullcapacity unitary recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Les</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4880" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Generative and discriminative text classification with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01898</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
