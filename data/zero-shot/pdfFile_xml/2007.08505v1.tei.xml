<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FeatMatch: Feature-Based Augmentation for Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Kuo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Tech</orgName>
								<address>
									<country>? Virginia Tech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
							<email>cyma@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Tech</orgName>
								<address>
									<country>? Virginia Tech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
							<email>jbhuang@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Tech</orgName>
								<address>
									<country>? Virginia Tech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
							<email>zkira@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Tech</orgName>
								<address>
									<country>? Virginia Tech</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FeatMatch: Feature-Based Augmentation for Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>semi-supervised learning</term>
					<term>feature-based augmentation</term>
					<term>con- sistency regularization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent state-of-the-art semi-supervised learning (SSL) methods use a combination of image-based transformations and consistency regularization as core components. Such methods, however, are limited to simple transformations such as traditional data augmentation or convex combinations of two images. In this paper, we propose a novel learned feature-based refinement and augmentation method that produces a varied set of complex transformations. Importantly, these transformations also use information from both within-class and across-class prototypical representations that we extract through clustering. We use features already computed across iterations by storing them in a memory bank, obviating the need for significant extra computation. These transformations, combined with traditional image-based augmentation, are then used as part of the consistency-based regularization loss. We demonstrate that our method is comparable to current state of art for smaller datasets (CIFAR-10 and SVHN) while being able to scale up to larger datasets such as CIFAR-100 and mini-Imagenet where we achieve significant gains over the state of art (e.g., absolute 17.44% gain on mini-ImageNet). We further test our method on DomainNet, demonstrating better robustness to out-of-domain unlabeled data, and perform rigorous ablations and analysis to validate the method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Driven by large-scale datasets such as ImageNet as well as computing resources, deep neural networks have achieved strong performance on a wide variety of tasks. Training these deep neural networks, however, requires millions of labeled examples that are expensive to acquire and annotate. Consequently, numerous methods have been developed for semi-supervised learning (SSL), where a large number of unlabeled examples are available alongside a smaller set of labeled data. One branch of the most successful SSL methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref> uses image-based augmentation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b5">6]</ref> to generate different transf ormations of an input image, and consistency regularization to enforce invariant representations across these transformations. While these methods have achieved great success, the data augmentation methods for generating different transformations  <ref type="figure">Fig. 1</ref>: Consistency regularization methods are the most successful methods for semi-supervised learning. The main idea of these methods is to enforce consistency between the predictions of different transformations of an input image. (a) Image-based augmentation method generate different views of an input image via data augmentation, which are limited to operations in the image space as well as operations within a single instance or simple convex combination of two instances. (b) We propose an additional learned feature-based augmentation that operates in the abstract feature space. The learned feature refinement and augmentation module is capable of leveraging information from other instances, within or outside of the same class.</p><p>are limited to transformations in the image space and fail to leverage the knowledge of other instances in the dataset for diverse transformations.</p><p>In this paper, we propose novel feature-based refinement and augmentation that addresses the limitations of conventional image-based augmentation described above. Specifically, we propose a module that learns to refine and augment input image features via soft-attention toward a small set of representative prototypes extracted from the image features of other images in the dataset. The comparison between image-based augmentation and our proposed feature-based refinement and augmentation is shown in <ref type="figure">Fig. 1</ref>. Since the proposed module is learned and carried out in the feature space, diverse and abstract transformations of input images can be applied, which we validate in Sec. 4.4. Our approach only requires minimum computation via maintaining a memory bank and using k-means clustering to extract prototypes.</p><p>We demonstrate that adding our proposed feature-based augmentation along with conventional image-based augmentations, when used for consistency regularization, achieves significant gains. We test our method on standard SSL datasets such as SVHN and CIFAR-10, and show that our method, despite its simplicity, compares favorably against state-of-art methods in all cases. Further, through testing on CIFAR-100 and mini-ImageNet, we show that our method is scalable to larger datasets and outperformed the current best methods by significant margins. For example, we outperformed the closest state of the art by an absolute 17% on mini-ImageNet with 4k labels. We also propose another realistic setting on DomainNet <ref type="bibr" target="#b20">[21]</ref> to test the robustness of our proposed method under the case where the unlabeled samples are partially coming from shifted domains, in which we improved 23% over supervised baseline and 12% over semi-supervised baseline when 50% unlabeled samples are all coming from shifted domains. Finally, we conduct thorough ablations and thorough analysis to highlight that the method does, in fact, perform varied complex transformations in feature space (as evidenced by t-SNE and nearest neighbor image samples). To summarize, our key contributions include: -We develop a learned feature-based refinement and augmentation module to transform input image features in the abstract feature space by leveraging a small set of representative prototypes of all classes in the dataset. -We propose a memory bank mechanism to efficiently extract prototypes from images of the entire dataset with minimal extra computations. -We demonstrate thorough results across four standard SSL datasets and also propose a realistic setting where the unlabeled data partially come from domains shifted from the target labeled set. -We perform in-depth analysis of the prototype representations extracted and used for each instance, as well as what transformations the proposed featurebased refinement and augmentation module learns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Consistency Regularization Methods. Current state-of-the-art SSL methods mostly fall into this category. The key insight of this branch of methods is that the prediction of a deep model should be consistent across different semantic-preserving transformations of the same data. Consistency regularization methods regularize the model to be invariant to textural or geometric changes of an image. Specifically, given an input image x and a network composed of a feature encoder f x = Enc(x) and a classifier p x = Clf (f x ), we can generate the pseudo-label of the input image by p x = Clf (Enc(x)). Furthermore, given a data augmentation module AugD(?), we can generate an augmented copy of x byx = AugD(x). A consistency loss H, typically KL-Divergence loss, is then applied on the model predictions ofx to enforce consistent prediction:</p><formula xml:id="formula_0">L con = H(p, Clf (Enc(x))).</formula><p>Image-Based Augmentation. The core to consistency-based methods is how to generate diverse but reasonable transformations of the same data. A straightforward answer is to incorporate data augmentation, which has been widely used in the training of a deep model to increase data diversity and prevent overfitting. For example, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref> use traditional data augmentation to generate different transformations of semantically identical images. Data augmentation method <ref type="table">Table 1</ref>: Comparison to other SSL methods with consistency regularization.</p><p>ReMixMatch <ref type="bibr" target="#b2">[3]</ref> MixMatch <ref type="bibr" target="#b3">[4]</ref> Mean Teacher <ref type="bibr" target="#b26">[27]</ref> ICT <ref type="bibr" target="#b30">[31]</ref> PLCB <ref type="bibr" target="#b0">[1]</ref> FeatMatch (Ours)</p><formula xml:id="formula_1">Feature-Based Augmentation - - - - - " Image-Based Augmentation " " " " " " Temporal Ensembling " " " - Self-Supervised Loss " - - - - - Alignment of Class Distribution " - - - " -</formula><p>randomly perturbs an image in terms of its texture, eg. brightness, hue, sharpness, or its geometry, eg. rotation, translation, or affine transform. In addition to data augmentation, Miyato et al. <ref type="bibr" target="#b18">[19]</ref> and Yu et al. <ref type="bibr" target="#b32">[33]</ref> perturbed images along the adversarial direction, and Qiao et al. <ref type="bibr" target="#b21">[22]</ref> use multiple networks to generate different views (predictions) of the same data. Recently, several works propose data augmentation modules for supervised learning or semi-supervised learning, where the augmentation parameters can either be easily tuned <ref type="bibr" target="#b7">[8]</ref>, found by RL-training <ref type="bibr" target="#b6">[7]</ref>, or decided by the confidence of network prediction <ref type="bibr" target="#b2">[3]</ref>. Mixup <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13]</ref>, similar to data augmentation, is another effective way of increasing data diversity. It generates new training samples by a convex combination of two images and their corresponding labels. It has been shown that models trained with Mixup is robust toward out-of-distribution data <ref type="bibr" target="#b9">[10]</ref> and is beneficial for the uncertainty calibration of a network <ref type="bibr" target="#b27">[28]</ref>. Given two images x 1 and x 2 and their labels (or pseudo labels) y 1 and y 2 , they are mixed by a randomly sampled ratio r byx = r ? x 1 + (1 ? r) ? x 2 and? = r ? y 1 + (1 ? r) ? y 2 . This has been done in feature space as well <ref type="bibr" target="#b29">[30]</ref>. A standard classification loss H(?) is then applied on the prediction of the mixed samplex and the mixed label? by L mix = H(?, Clf (Enc((x))). Originally, Mixup methods were developed for supervised learning. ICT <ref type="bibr" target="#b30">[31]</ref> and MixMatch <ref type="bibr" target="#b3">[4]</ref> introduce Mixup into semi-supervised learning by using the pseudo-label of the unlabeled data. Furthermore, by controlling the mixing ratio r to be greater than 0.5 as proposed by <ref type="bibr" target="#b3">[4]</ref>, we can make sure that the mixed sample is closer to x 1 . Therefore, we can separate the mixed data into labeled mixed batchX if x 1 is labeled, and unlabeled mixed batch? if x 1 is unlabeled. Different loss weights can then be applied to modulate the strength of regularization from the unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature-Based Augmentation and Consistency</head><p>Image-based augmentation has been shown to be an effective approach to generate different views of an image for consistency-based SSL methods. However, conventional image-based augmentation has the following two limitations: (1) Operate in image space, which limits the possible transformations to textural or geometric within images, and (2) Operate within a single instance, which fails to transform data with the knowledge of other instances, either within or outside of the same class. Some recent works that utilize Mixup only partially address the second limitation of conventional data augmentation since mixup operates only between two instances. On the other hand, Manifold Mixup <ref type="bibr" target="#b29">[30]</ref> approaches the first limitation by performing Mixup in the feature space but is limited to a simple convex combination of two samples.</p><p>We instead propose to address these two limitations simultaneously. We proposed a novel method that refines and augments image features in the abstract feature space rather than image space. To efficiently leverage the knowledge of other classes, we condense the information of each class into a small set of prototypes by performing clustering in the feature space. The image features are then refined and augmented through information propagated from prototypes of all classes. We hypothesize that this feature refinement/augmentation can further improve the feature representations, and these refined features can produce better pseudo-labels than features without the refinement (See Sec. 4.4 for our analysis on this hypothesis). The feature refinement and augmentation are learned via a lightweight attention network for the representative prototypes and optimized end-to-end with other objectives such as classification loss. A consistency loss can naturally be applied between the prediction from the original features and the refined features to regularize the network as shown in <ref type="figure">Fig. 1b</ref>.</p><p>The final model seamlessly combines our novel feature-based augmentation with conventional image-based augmentation for consistency regularization, which is applied to data augmented from both sources. Despite the simplicity of the method, we find this achieves significant performance improvement. In summary, we compare our method with other highly relevant SSL works in <ref type="table">Table.</ref> 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Prototype Selection</head><p>In order to efficiently leverage the knowledge of other classes for feature refinement and augmentation, we propose to compactly represent the information of each class by clustering in the feature space. To select representative prototypes from the dataset, we propose to use K-Means clustering in the feature space to extract p k cluster means as prototypes for each class. However, there are two technical challenges: <ref type="bibr" target="#b0">(1)</ref> in an SSL setting, most images are unlabeled; (2) even if all the labels are available, it is still computationally expensive to extract features of all the images from the entire dataset before running K-Means.</p><p>To tackle these issues, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we collect features f xi and pseudolabels? i already generated by the network at every iteration of the training loop, i.e., no extra computation needed. In the recording loop, the pairs of pseudo label and features are detached from the computation graph and pushed into a memory bank for later usage. The prototypes are extracted by K-Means at every epoch when we go over the whole dataset. Finally, the feature refinement and augmentation module updates the prototypes with the newly extracted ones in the training loop. Even though the prototypes are extracted from the feature computed from the model a few iterations ago, as training progresses and the model gradually converges, the extracted prototypes fall on the correct cluster and are diverse enough to compactly represent the feature distribution per class. More analyses can be found in Sec. 4.4. Similar idea is concurrently explored in self-supervised learning by He et al. <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prototype Selection</head><formula xml:id="formula_2">( 1 , ? 1 ) ( 2 , ? 2 ) ( 3 , ? 3 ) ( , ? )</formula><p>??. The image features f xi as well as their pseudo labels? i already generated at each iteration of the training loop are collected and recorded in a memory bank as (f xi ,? i ) pairs. Once the training loop goes over the whole dataset, the recording loop will run K-Means to extract prototypes for each class, update the prototypes for feature-based augmentation, and clear the memory bank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learned Feature Augmentation</head><p>With a set of prototypes selected by the process described above, we propose a learned feature refinement and augmentation module via soft-attention <ref type="bibr" target="#b28">[29]</ref> toward the set of selected prototypes. The proposed module refines and augments input image features in the feature space by leveraging the knowledge of prototypes, either within or outside of the same class, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The lightweight feature refinement and augmentation module composed of three fully connected layers is jointly optimized with other objectives and hence learns a reasonable feature-based augmentation to aid classification. We provide further analysis and discussion in Sec. 4.4. Inspired by the attention mechanism <ref type="bibr" target="#b28">[29]</ref>, each input image feature attends to prototype features via attention weights computed by dot product similarity. The prototype features are then weighted summed by the attention weights and then fed back to the input image feature via residual connect for feature augmentation and refinement. Specifically, for an input image with extracted features f x and the i-th prototype features f p,i , we first project them into an embedding space by a learned function ? e as e x = ? e (f x ) and e p,i = ? e (f p,i ) respectively. We compute an attention weight w i between e x and e p,i as:</p><formula xml:id="formula_3">w i = softmax(e T x e p,i ),<label>(1)</label></formula><p>where softmax(?) normalizes the dot product similarity scores across all prototypes. The information aggregated from the prototypes and passed to the image features for feature refinement and augmentation can then be expressed as a sum of prototype features weighted by the attention weights:</p><formula xml:id="formula_4">f a = relu(? a ([e x , i w i e p,i ])),<label>(2)</label></formula><p>where ? a is a learnable function, and [?, ?] is a concatenation operation along the feature dimension. Finally, the input image features f x is refined via a residual connection as:</p><formula xml:id="formula_5">g x = relu(f x + ? r (f a )),<label>(3)</label></formula><p>where g x are the refined features of f x , and ? r is a learnable function. The attention mechanism described above can be trivially generalized to multi-head attention as in <ref type="bibr" target="#b28">[29]</ref>. In practice, we use multi-head attention, instead of single head for slightly better results. For simplicity, we define the feature refinement and augmentation process AugF (?) described above as g x = AugF (f x ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Consistency Regularization</head><p>The learned AugF module along with the selected prototypes provides an effective method for feature-based augmentation, which addresses the limitations of conventional data augmentation methods discussed previously. With the learned feature-based augmentation, we can naturally apply a consistency loss between the prediction of unaugmented features f x and augmented features g x .</p><p>However, given a classifier p = Clf (f ), which prediction should we use as pseudo-label, p g = Clf (g x ) or p f = Clf (f x )? We investigate this problem in Sec. 4.4 and find that AugF is able to refine the input features for better representation, thus generating better pseudo-labels. Therefore, we compute pseudolabel p g on the refined feature g x by p g = Clf (g x ). The feature-based consistency loss can be computed as: L con = H(p g , Clf (f x )). We can easily extend L con to work seamlessly with traditional augmentation methods, i.e., traditional data augmentation and Mixup. For simplicity, we will illustrate with only data augmentation, but Mixup can be easily adapted. Inspired by Berthelot et al. <ref type="bibr" target="#b2">[3]</ref>, we generate a weakly augmented image x and its strongly augmented cop? x. The pseudo-label is computed on the weakly augmented image x that undergoes feature-based augmentation and refinement for better pseudo-labels as p g = Clf (AugF (Enc(x))). We can then compute two consistency losses on the strongly augmented datax, one with AugF applied and the other without:</p><formula xml:id="formula_6">L con-g = H(p g , Clf (AugF (Enc(x)))<label>(4)</label></formula><p>L con-f = H(p g , Clf (Enc(x)))</p><p>Since the pseudo-label p g is computed on the image undergoing weak data augmentation and feature-based augmentation, the regularization signal of L con-g and L con-f comes from both image-based and feature-based augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Total Loss</head><p>Consistency regularization losses L con-g and L con-f in Eq. 4 and 5 are applied on unlabeled data. For labeled image x with label y, a regular classification loss can be applied:</p><formula xml:id="formula_8">L clf = H(y, Clf (AugF (Enc(x))))<label>(6)</label></formula><p>Therefore, the total loss can be written as: L clf + ? g L con-g + ? f L con-f . Where ? g and ? f are weights for L con-g and L con-f losses respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Standard SSL datasets. We conduct experiments on commonly used SSL datasets: SVHN <ref type="bibr" target="#b19">[20]</ref>, CIFAR-10 <ref type="bibr" target="#b14">[15]</ref>, CIFAR-100 <ref type="bibr" target="#b14">[15]</ref>, and mini-ImageNet <ref type="bibr" target="#b22">[23]</ref>. Following the standard approach in SSL, we randomly choose a certain number of labeled samples as a small labeled set and discard the labels for the remaining data to form a large unlabeled set. Our proposed method is tested under various amounts of labeled samples. SVHN is a dataset of 10 digits, which has about 70k training samples. CIFAR-10 and CIFAR-100 are natural image datasets with 10 and 100 classes respectively. Both dataset contains 50k training samples. For mini-ImageNet, we follow <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b0">1]</ref> to construct the mini-ImageNet training set. Specifically, given a predefined list of 100 classes <ref type="bibr" target="#b22">[23]</ref> from ILSVRC <ref type="bibr" target="#b23">[24]</ref>, 500 samples are selected randomly for each class, thus forming a training set of 50k samples. The samples are center-cropped and resized to 84x84 resolution. We then follow the same standard procedure and construct a small labeled set and a large unlabeled set from the 50k training samples. SSL under domain shift. In another realistic setting, we argue that the unlabeled data may come from a domain different from that of the target labeled data. For instance, given a small set of labeled natural images of animals, the large unlabeled set may also contain paintings of animals. To investigate the effect of domain shift in the unlabeled set, we proposed a new SSL task based on the DomainNet dataset <ref type="bibr" target="#b20">[21]</ref>, which contains 345 classes of images coming from six domains: Clipart, Infograph, Painting, Quickdraw, Real, and Sketch.</p><p>We use the Real domain as our target. Five percent of the data from the Real domain are kept as the target labeled set, and the rest are the target unlabeled set. We select Clipart, Painting, Sketch, and Quickdraw as shifted domains. To modulate the level of domain shift in the unlabeled data, we propose a parameter r u that controls the ratio of unlabeled data coming from the target Real domain or the shifted domains. Specifically, r u percent of target Real unlabeled set is replaced with data uniformly drawn from the shifted domains. By formulating the problem this way, the amount of unlabeled data remains constant. The only factor that affects the performance of the proposed method is the ratio between in-domain data and shifted domain data in the unlabeled set.</p><p>We randomly reserve 1% of data from the Real domain as the validation set. The final result is reported on the test set of the Real domain, with the model selected on the reserved validation set. The images are center-cropped and resized to 128x128 resolution, and the model we use is the standard ResNet-18 <ref type="bibr" target="#b11">[12]</ref>. There are around 120k training samples, which is more than twice larger than the standard SSL datasets such as CIFAR-10 and CIFAR-100. For a fair comparison, we fix all hyper-parameters across experiments of different r u to truly assess the robustness of proposed methods toward domain shift in the unlabeled data.</p><p>Hyper-parameters. We tune the hyper-parameters on CIFAR-10 with 250 labels with a validation set held-out from the training set. Our method is not sensitive to the hyper-parameters, which are kept fixed across all the datasets <ref type="table">Table 3</ref>: Comparison between the image-based baseline with our proposed feature-based augmentation method on DomainNet with 1) unlabeled data coming from the same domain as the labeled target (r u = 0%), and 2) half of unlabeled data coming from the same domain as the labeled target and the other half from shifted domains (r u = 50%). Numbers are error rates across 3 runs. and settings. Please see the supplementary for more implementation details and the values of hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We first show our results on CIFAR-100 and mini-ImageNet with 4k and 10k labels in <ref type="table" target="#tab_0">Table 2</ref>. Our method consistently improves over state of the arts by large margins, with about absolute 5% on CIFAR-100 with 4k labels and 17% on mini-ImageNet with 4k labels.</p><p>In <ref type="table">Table 3</ref>, we show our results on the larger dataset of DomainNet setting, which contains unlabeled data coming from other shifted domains. It can be clearly seen that in the setting of r u = 50%, where 50% of the unlabeled data are coming from other shifted domains, the performance drops by a large margin compared with the setting of r u = 0%, where all the unlabeled data are coming from the same domain as the target labeled set. Nevertheless, our proposed feature-based augmentation method improves over supervised baseline by absolute 36% error rate when r u = 0% and 23% when r u = 50%. When compared to the conventional image-based augmentation baseline, we improves by 12% when r u = 50% and 16% when r u = 0%.</p><p>In <ref type="table">Table 4</ref>, we show the comparison of our method with other SSL methods on standard CIFAR-10 and SVHN datasets. Our method achieves comparable results with the current state of the art, ReMixMatch, even though 1) we start from a lower baseline and 2) our method is much simpler (e.g., no class distribution alignment and no self-supervised loss), as compared in <ref type="table">Table 1</ref>. Our proposed feature-based augmentation method is complementary to image-based methods and can be easily integrated to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In the ablation study, we are interested in answering the following questions: 1) what is the effectiveness of the two proposed consistency losses -L con-f (Eq. 5) and L con-g (Eq. 4). 2) how much of the improvement is from the proposed featurebased augmentation method over the image-based augmentation baseline? For <ref type="table">Table 4</ref>: Comparison on CIFAR-10 and SVHN. Numbers represent error rate across three runs. The results reported in the first block with CNN-13 model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19]</ref> are from the original paper. The results reported in the second block with wide ResNet (WRN) are reproduced by <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>.  the image-based augmentation baseline, the AugF module is completely removed and thus the consistency regularization comes only from image-based augmentation. This is also the same image-based augmentation baseline that our final model with feature-based augmentation builds upon. The ablation study is conducted on CIFAR-10 with various amount of labeled samples ( <ref type="table" target="#tab_3">Table 5)</ref>. We can see from <ref type="table" target="#tab_3">Table 5</ref> that our image-based augmentation baseline achieves good results but only on cases where there are more labeled samples. We conjecture this is because the aggressive data augmentation applied to training images makes the training unstable. Nevertheless, our baseline performance is still competitive with respect to other image-based augmentation methods in <ref type="table">Table 4</ref> (though slightly worse than MixMatch). By adding our proposed AugF module(L con-f and L con-g ) for feature refinement and augmentation on top of the image-based augmentation baseline, the performance improves over baseline consistently, especially for 250 labels.</p><formula xml:id="formula_9">CIFAR</formula><p>We can also see that L con-f plays a more important role than L con-g , though our final model with both loss terms achieves the best result. In both L con-f and  L con-g , the pseudo-labels are computed from the features undergone featurebased augmentation. The only difference is which prediction we're driving to match the pseudo-label: 1) the prediction from the feature undergone both AugD and AugF (by L con-g loss), or 2) the prediction from the feature undergone only AugD (by L con-f loss)? As claimed in Sec. 3.3 and analyzed in Sec. 4.4, AugF is able to refine input image features for better representation and pseudo-labels of higher quality. Therefore, matching the slightly worse prediction from the feature undergone only AugD (by L con-f loss) induces a stronger consistency regularization. This explains why L con-f improves performance more crucially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>What augmentation does AugF learn? We compare the feature distribution via t-SNE 1) between input unaugmented image features and image-based augmented features in <ref type="figure" target="#fig_1">Fig. 4a, and 2</ref>) between input unaugmented image features and feature-based augmented features in <ref type="figure" target="#fig_4">Fig. 4b</ref>. In <ref type="figure" target="#fig_4">Fig. 4a</ref>, some local small clusters are captured by t-SNE and can be found in the zoomed sub-figure. This indicates that AugD can only perturb data locally, and fail to produce stronger augmentation for more effective consistency regularization in the fea-  ture space. In <ref type="figure" target="#fig_4">Fig. 4b</ref>, we can see AugF indeed learns to augment and refine features. Furthermore, the learned augmentation preserves semantic meaning as the augmented features still fall in the correct cluster. In the zoomed figure, we can see that the perturbed features distribute more uniformly and no local small clusters could be found. This indicates that AugF can produce stronger augmentation for more effective consistency regularization in the feature space.</p><p>To have a more concrete sense of the learned feature-based augmentation (AugF ), we show the augmented feature's nearest image neighbor in the feature space. Some sample results are shown in <ref type="figure" target="#fig_4">Fig. 4c</ref>, with the comparison to image-based augmentation (AugD) side by side. As shown in the figure, AugF is capable of transforming features in an abstract way, which goes beyond simple textural and geometric transformation as AugD does. For instance, it is able to augment data to different poses and backgrounds, which could be challenging for conventional image-based augmentation methods. What other reason does AugF improve model performance? We hypothesize that one other reason why our method can improve performance is that AugF module is capable of refining input image features for better representation by the extracted prototypes, and thus provides better pseudo-labels. The consistency regularization losses then drive the network's prediction to match the target pseudo-labels of higher quality, leading to overall improvement. With this hypothesis, we expect classification accuracy to be higher for features after feature refinement. To verify, we remove L con-f loss and retrain. The accuracy of pseudo-labeling from the features refined by AugF is on average 0.5 ? 1.0% higher. This confirms our hypothesis that L con-f drives the feature encoder to learn a better feature representation refined by AugF .</p><p>The reader may wonder: why doesn't AugF learn a shortcut solution of identity mapping to minimize L con-f and L con-g ? As can be seen from <ref type="figure" target="#fig_4">Fig. 4</ref>, AugF does not learn an identity mapping. Although learning an identity mapping may be a shortcut solution for minimizing L con-f and L con-g , it is not the case for the classification loss L clf (Eq. 6). This finding implicitly confirms our hypothesis that there is extra information from the prototypes that AugF can leverage to refine the feature representation for higher (pseudo-label) classification accuracy. What does Aug do internally? In <ref type="figure" target="#fig_6">Fig. 5a</ref> and 5c, we can see that even though our proposed prototype extraction method only uses simple K-Means to extract prototypes of each class based on potentially noisy pseudo-labels, and features recorded several iterations ago, our prototype selection method can still successfully extract a diverse set of prototypes per class. Moreover, in <ref type="figure" target="#fig_6">Fig. 5b</ref>, the attention mechanism inside AugF learns to attend to prototypes that belong to the same class with the input image feature. Note that there is no loss term specific for AugF , as it is simply jointly optimized with the standard classification and consistency regularization loss from semi-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce a method to jointly learn a classifier and feature-based refinement and augmentations which can be used within existing consistency-based SSL methods. Unlike traditional image-based transformations, our method can learn complex, feature-based transformations as well as incorporate information from class-specific prototypical representations extracted in an efficient manner (specifically using a memory bank). Using this method, we show comparable results as the current state of the art for smaller datasets such as CIFAR-10 and SVHN, and significant improvements on datasets with a large number of categories (e.g., 17.44% absolute improvement on mini-ImageNet). We also demonstrate increased robustness to out-of-domain unlabeled data, which is an important real-world problem, and perform ablations and analysis to demonstrate the learned feature transformation and extracted prototypical representations. As we build upon a weaker baseline and our method is much simpler, the performance of our method on the CIFAR-10 dataset is slightly worse in some settings. However, as we claimed in Section 4.2 of the main paper, our proposed feature-based augmentation method is complementary to conventional imagebased augmentation methods and can be easily integrated to further improve the performance. In <ref type="table" target="#tab_4">Table 6</ref>, we demonstrate that by incorporating (1) distribution alignment that aligns the marginal class distribution as described in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>, and (2) Cutout <ref type="bibr" target="#b8">[9]</ref>, an image-based augmentation method, our method indeed compares favorably against current state-of-the-art algorithms. Note that our method is still simpler when compared to state-of-the-art image-based method, e.g., ReMixMatch <ref type="bibr" target="#b2">[3]</ref>. For example, the ReMixMatch method also incorporates self-supervsied loss, temporal ensembling of model weights, and tailored data augmentation method (CTAugment <ref type="bibr" target="#b2">[3]</ref>), etc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Pseudo-Labeling Accuracy Before and After AugF</head><p>In Section 4.4 of the main paper, we analyze other reasons that AugF improves model performance. We conclude that our proposed AugF module also learns to refine input feature for a better representation by attending to the prototypes. This feature refinement process by AugF provides the training objectives of L con-g (Eq. 5) and L con-f (Eq. 6) with better pseudo-labels, which may be one of the reasons why our method can improve over image-based baseline by a larger margin. In <ref type="figure">Fig. 6</ref> below, we can see that the accuracy of pseudo-labels from the features refined by AugF is higher than those without refinement by AugF . <ref type="figure">Fig. 6</ref>: We monitor the accuracy of pseudo-labeling with feature-base refinement (red curve) and without feature-based refinement (blue curve) during training. We found that the pseudo-label from the refined feature (red) has on average 0.5 ? 1.0% higher accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Analysis on Prototypes</head><p>We test the sensitivity of our method for the hyper-parameter p k (number of prototypes per class) and I p (the interval at which a new set of prototypes is extracted). The analysis is conducted on a held-out validation set of the CIFAR-10 dataset with 250 labels. As shown in <ref type="table" target="#tab_5">Table 7</ref>, the final results are stable across different values of p k . We choose the number of prototypes per class p k = 20 in our method as it performs slightly better than others and has a slightly lower variance. In <ref type="table" target="#tab_6">Table 8</ref>, we can see that the final results are also stable across different I p . Therefore, for simplicity, we extract prototypes every epoch, which is approximately the same as I p = 400.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D More Results on the DomainNet Setting</head><p>In Section 4.1, we propose a practical setting where the unlabeled data may come from other domains. We show results with different r u , the ratio of unlabeled data coming from the target Real domain or the shifted domains, in Section 4.2. In this section, we show additional results of r u = 0.25 and r u = 0.75 on both our method and the image-based baseline in Tab. 9. The results show a similar trend is similar as the <ref type="table">Table 3</ref> in the main paper, where the accuracy goes down as r u goes up. Our method consistently improves over image-based semi-supervised baseline. Our method achieves comparable result even in the severe case of r u = 75% against the image-based baseline method with clean unlabeled data of r u = 0% <ref type="table">Table 9</ref>: Comparison between the image-based baseline with our proposed feature-based augmentation method on DomainNet with various r u , the ratio of unlabeled data coming from the shifted domains. For instance, r u = 25% means 25% of the unlabeled data are coming from the shifted domains and 75% are coming from the domain same as the labeled set. Numbers are error rates across 3 runs, meaning the lower the better. We train our model with Stochastic Gradient Descent and Nesterov momentum. As the AugF module heavily relies on the feature representation to compute attention weights, we pre-train the model without AugF for 4 epochs. We adapt the super convergence learning rate scheduler <ref type="bibr" target="#b25">[26]</ref> to reduce the total training iterations. Specifically, in the pre-training stage, the learning rate starts from 4e-4 and linearly increase to 4e-3 in I p iterations. After the pretraining stage, we add the AugF module and ramp up the learning rate linearly from 4e-3 to 4e-2 in I c iterations, and then ramp down back to 4e-3 in another I c iterations. In the meantime, the momentum ramps down from 0.95 to 0.85, and then ramps up back to 0.95. Finally, in the convergence stage, the learning rate ramps further down from 4e-3 to 4e-6 in I e iterations.</p><p>We follow the guidelines in <ref type="bibr" target="#b25">[26]</ref> to set these parameters without aggressive parameter tuning, and set I p = 3k, I c = 75k, and I e = 30k. As the DomainNet setting has more training samples, we increase these values I p = 4k, I c = 100k, and I e = 40k without tuning. We only tune the peak learning rate to be 4e-4 on a held-out validation set on CIFAR-10 with 250 labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Hyper-parameters</head><p>All the hyper-parameters are tuned on a held-out validation set on CIFAR-10 with 250 labels. These hyper-parameters are shared across all settings and experiments without further tuning. Since our method is built upon the imagebased baseline, we fix the hyper-parameters or select a reasonable value without tuning from the original papers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Data Augmentation Operations</head><p>We used the same sets of image transformations used in RandAugment <ref type="bibr" target="#b7">[8]</ref>. There are two parameters in RandAugment: (1) N -number of operations applied, and (2) M -maximal magnitude of the applied augmentation. We use N = 2 as in RandAugment, and set M to its max value without tuning. Note that the magnitude is randomly sampled from [?M, M ].</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Feature-Based Augmentation and Consistency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>A prototype recording loop that runs alongside the model training loop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Feature-Based Augmentation: The input image features are augmented by attention using extracted prototype features (Eq. 1), where the colors of represent the classes of prototypes. The prototype features are calcluated via a weighted sum using the attention weights, concatenated with the image features, and then undergo a fc layer ? a (Eq. 2) to produce attention features f a . Finally, we use the attention features to refine and augment the input image features with a residual connection (Eq. 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Augmented images of AugD vs. AugF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>(a) We jointly compute and plot t-SNE of input unaugmented image features (dimmer color) and image-based augmented features (brighter color). (b) We also jointly compute and plot t-SNE of input unaugmented image features (dimmer color) and feature-based augmented features (brighter color) with the exact same t-SNE parameters with (a). (c) To concretely visualize the augmented feature, we find their nearest image neighbor in the feature space and compare against the image-based augmentation method side by side.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Nearest image neighbors of prototypes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>(a) In the t-SNE plot, the extracted prototypes ( ) fall on the correct clusters and are able to compactly represent the cluster. (b) We visualize the learned attention weights from a batch of images toward prototypes. The images and prototypes are sorted by their classes for ease of illustration. As can be seen, images have higher attention weights to the prototypes with the same class. (c) We find the prototypes' nearest image neighbors in the feature space. The prototypes compactly represent a diverse sets of images in each class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>This work was funded by DARPA's Learning with Less Labels (LwLL) program under agreement HR0011-18-S-0044 and DARPAs Lifelong Learning Machines (L2M) program under Cooperative Agreement HR0011-18-2-0019.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Comparison on CIFAR-100 and mini-imageNet. Numbers represent error rate in three runs. For fair comparison, we use the same model as other methods: CNN-13 for CIFAR-100 and ResNet-18 for mini-ImageNet. ? 0.49 36.08 ? 0.51 72.51 ? 0.22 57.55 ? 1.11 Label Propagation [14] 43.73 ? 0.20 35.92 ? 0.47 70.29 ? 0.81 57.58 ? 1.47 PLCB [1] 37.55 ? 1.09 32.15 ? 0.50 56.49 ? 0.51 46.08 ? 0.11</figDesc><table><row><cell></cell><cell cols="2">CIFAR-100</cell><cell cols="2">mini-ImageNet</cell></row><row><cell></cell><cell cols="2"># Labeled samples</cell><cell cols="2"># Labeled samples</cell></row><row><cell>Method</cell><cell>4,000</cell><cell>10,000</cell><cell>4,000</cell><cell>10,000</cell></row><row><cell>?-model [25]</cell><cell>-</cell><cell>39.19 ? 0.36</cell><cell>-</cell><cell>-</cell></row><row><cell>SNTG [18]</cell><cell>-</cell><cell>37.97 ? 0.29</cell><cell>-</cell><cell>-</cell></row><row><cell>SSL with Memory [5]</cell><cell>-</cell><cell>34.51 ? 0.61</cell><cell>-</cell><cell>-</cell></row><row><cell>Deep Co-Training [22]</cell><cell>-</cell><cell>34.63 ? 0.14</cell><cell>-</cell><cell>-</cell></row><row><cell>Weight Averaging [2]</cell><cell>-</cell><cell>33.62 ? 0.54</cell><cell>-</cell><cell>-</cell></row><row><cell>Mean Teacher [27]</cell><cell>45.36</cell><cell></cell><cell></cell><cell></cell></row></table><note>FeatMatch (Ours) 31.06 ? 0.41 26.83 ? 0.04 39.05 ? 0.06 34.79 ? 0.22</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>? 2.05 31.53 ? 0.98 17.41 ? 0.37 17.65 ? 0.27 8.60 ? 0.18 5.57 ? 0.14 PseudoLabel [17] 49.98 ? 1.17 30.91 ? 1.73 16.21 ? 0.11 21.16 ? 0.88 10.19 ? 0.41 5.71 ? 0.07 Mixup [35] 47.43 ? 0.92 25.72 ? 0.66 13.15 ? 0.20 39.97 ? 1.89 16.79 ? 0.63 7.96 ? 0.14 VAT [19] 36.03 ? 2.82 18.68 ? 0.40 11.05 ? 0.31 8.41 ? 1.01 5.98 ? 0.21 4.20 ? 0.15 Mean Teacher [27] 47.32 ? 4.71 17.32 ? 4.00 10.36 ? 0.25 6.45 ? 2.43 3.75 ? 0.10 3.39 ? 0.11 MixMatch [4] 11.08 ? 0.87 7.75 ? 0.32 6.24 ? 0.06 3.78 ? 0.26 3.27 ? 0.31 2.89 ? 0.06 ReMixMatch [3] 6.27 ? 0.34 5.73 ? 0.16 5.14 ? 0.04 3.10 ? 0.50 2.83 ? 0.30 2.42 ? 0.09 FeatMatch (Ours) 7.50 ? 0.64 5.76 ? 0.07 4.91 ? 0.18 3.34 ? 0.19 3.10 ? 0.06 2.62 ? 0.08</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>-10</cell><cell></cell><cell></cell><cell>SVHN</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2"># Labeled samples</cell><cell></cell><cell># Labeled samples</cell><cell></cell></row><row><cell>Method</cell><cell>Model (param.)</cell><cell>250</cell><cell>1,000</cell><cell>4,000</cell><cell>250</cell><cell>1,000</cell><cell>4,000</cell></row><row><cell>SSL with Memory [5]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>11.91? 0.22</cell><cell>8.83</cell><cell>4.21</cell><cell>-</cell></row><row><cell>Deep Co-Training [22]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>8.35 ? 0.06</cell><cell>-</cell><cell>3.29 ? 0.03</cell><cell>-</cell></row><row><cell>Weight Averaging [2]</cell><cell></cell><cell>-</cell><cell cols="2">15.58 ? 0.12 9.05 ? 0.21</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ICT [31]</cell><cell>CNN-13 (3M)</cell><cell>-</cell><cell cols="4">15.48 ? 0.78 7.29 ? 0.02 4.78 ? 0.68 3.89 ? 0.04</cell><cell>-</cell></row><row><cell>Label Propagation [14]</cell><cell></cell><cell>-</cell><cell cols="2">16.93 ? 0.70 10.61 ? 0.28</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SNTG [18]</cell><cell></cell><cell>-</cell><cell cols="4">18.41 ? 0.52 9.89 ? 0.34 4.29 ? 0.23 3.86 ? 0.27</cell><cell>-</cell></row><row><cell>PLCB [1]</cell><cell></cell><cell>-</cell><cell cols="2">6.85 ? 0.15 5.97 ? 0.15</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>?-model [25]</cell><cell></cell><cell>53.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>WRN (1.5M)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on CIFAR-10 with various amount of labeled samples.</figDesc><table><row><cell>#Labeled samples</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Comparison to other state-of-the-art methods after incorporating some other modern SSL techniques (distribution alignment and Cutout). We show the results on the CIFAR-10 dataset with varying amounts of labeled samples. Numbers represent error rate across three runs. ? 0.34 5.73 ? 0.16 5.14 ? 0.04 FeatMatch (Ours in the main paper) 7.50 ? 0.64 5.76 ? 0.07 4.91 ? 0.18 FeatMatch (Ours with other SSL techniques) 6.00 ? 0.41 5.21 ? 0.08 4.64 ? 0.11</figDesc><table><row><cell>#Labeled samples</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Sensitivity analysis for p k . Numbers represent error rates in three runs.</figDesc><table><row><cell>pk = 1</cell><cell>pk = 5</cell><cell>pk = 10</cell><cell>pk = 20</cell></row><row><cell cols="4">8.14 ? 0.79 8.15 ? 0.19 8.01 ? 0.90 8.09 ? 0.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Sensitivity analysis for I p . Numbers represent error rates in three runs.</figDesc><table><row><cell>Ip = 200</cell><cell>Ip = 400</cell><cell>Ip = 600</cell><cell>Ip = 800</cell></row><row><cell cols="4">8.00 ? 0.81 7.99 ? 0.74 7.99 ? 0.79 8.38 ? 1.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>? 0.17 62.44 ? 0.67 % 65.82 ? 0.07 70.50 ? 0.51 FeatMatch (Ours) 40.66 ? 0.60 46.11 ? 1.15 54.01 ? 0.66 58.30 ? 0.93 Supervised baseline (5% labeled samples, lower bound) 77.25 ? 0.52 Supervised baseline (100% labeled samples, upper bound) 31.91 ? 0.15</figDesc><table><row><cell>Method (5% labeled samples)</cell><cell>ru = 0%</cell><cell>ru = 25%</cell><cell>ru = 50%</cell><cell>ru = 75%</cell></row><row><cell cols="2">(Semi-supervised) Baseline 56.63 E Implementation Details</cell><cell></cell><cell></cell><cell></cell></row><row><cell>E.1 Training</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc>Hyper-parameters and their meanings.</figDesc><table><row><cell cols="2">Hyper-parameter Description</cell><cell>Value</cell></row><row><cell>pk</cell><cell>Number of prototypes per class</cell><cell>20</cell></row><row><cell>Ip</cell><cell cols="2">The interval at which a new set of prototypes are extracted 1 epoch</cell></row><row><cell>ah</cell><cell>Number of attention heads in AugF</cell><cell>4</cell></row><row><cell>?g</cell><cell>Loss weight for Lcon-g</cell><cell>0.5</cell></row><row><cell>?f</cell><cell>Loss weight for Lcon-f</cell><cell>2.0</cell></row><row><cell>bl</cell><cell>Batch size for labeled data</cell><cell>64</cell></row><row><cell>bu</cell><cell>Batch size for unlabeled data</cell><cell>128</cell></row><row><cell>wd</cell><cell>Weight decay</cell><cell>2e-4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pseudolabeling and confirmation bias in deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02983</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improving consistencybased semi-supervised learning with weight averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05594</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>ICLR) (2020) 1, 4, 8, 11</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning with memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Autoaugment: Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mixup as locally linear out-of-manifold regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<title level="m">Momentum contrast for unsupervised visual representation learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">AugMix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Label propagation for deep semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR</title>
		<meeting>International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Smooth neighbors on teacher graphs for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Reading digits in natural images with unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep co-training for semisupervised image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Super-convergence: Very fast training of neural networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Topin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11006</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On mixup training: Improved calibration and predictive uncertainty for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michalak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Manifold mixup: Encouraging meaningful on-manifold interpolation as a regularizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03825</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tangent-normal adversarial regularization for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
