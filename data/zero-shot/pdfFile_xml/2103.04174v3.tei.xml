<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Wu</surname></persName>
							<email>bohanwu@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Nair</surname></persName>
							<email>surajn@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Mart?n-Mart?n</surname></persName>
							<email>robertom@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
							<email>feifeili@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
							<email>cbfinn@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A video prediction model that generalizes to diverse scenes would enable intelligent agents such as robots to perform a variety of tasks via planning with the model. However, while existing video prediction models have produced promising results on small datasets, they suffer from severe underfitting when trained on large and diverse datasets. To address this underfitting challenge, we first observe that the ability to train larger video prediction models is often bottlenecked by the memory constraints of GPUs or TPUs. In parallel, deep hierarchical latent variable models can produce higher quality predictions by capturing the multi-level stochasticity of future observations, but end-to-end optimization of such models is notably difficult. Our key insight is that greedy and modular optimization of hierarchical autoencoders can simultaneously address both the memory constraints and the optimization challenges of large-scale video prediction. We introduce Greedy Hierarchical Variational Autoencoders (GHVAEs), a method that learns highfidelity video predictions by greedily training each level of a hierarchical autoencoder. In comparison to stateof-the-art models, GHVAEs provide 17-55% gains in prediction performance on four video datasets, a 35-40% higher success rate on real robot tasks, and can improve performance monotonically by simply adding more modules. Visualization and more details are at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A core aspect of intelligence is the ability to predict the future. Indeed, if equipped with an accurate video prediction model, an intelligent agent such as a robot may be able to perform a variety of tasks using raw pixel inputs. For example, algorithms such as visual foresight <ref type="bibr" target="#b0">[1]</ref> can leverage an action-conditioned video ? Equal advising and ordered alphabetically.  <ref type="bibr">(GHVAEs)</ref>. Unlike traditional hierarchical variational autoencoders (VAEs), a GHVAE model trains each encoder-decoder module greedily using the frozen weights of the previously-trained modules. Greedy training circumvents fitting the entire model into memory and enables larger models to be trained within the same GPU or TPU memory. Further, greedy training improves the optimization stability of such a hierarchical model by breaking the bidirectional dependencies among individual latent variables. As a result, given the current image, xt, GHVAE predicts a more accurate next image,xt+1, than a hierarchical VAE. Each module is optimized sequentially, and all modules are used at test time.</p><p>prediction model to plan a sequence of actions that accomplish the desired task objective. Importantly, such video prediction models can in principle be trained with broad, unlabeled datasets, and building methods that can learn from large, diverse offline data is a recipe that has seen substantial success in visual <ref type="bibr" target="#b1">[2]</ref> and language <ref type="bibr" target="#b2">[3]</ref> understanding. However, learning an accurate video prediction model from large and diverse data remains a significant challenge. The future visual observations of the world are hierarchical <ref type="bibr" target="#b3">[4]</ref>, high-dimensional, and uncertain, demanding the model to accurately represent the multi-level stochasticity of future pixels, which can include both low-level features (e.g. the texture of a table as it becomes unoccluded by an object) and higher-level attributes (e.g. how an object will move when touched), such as the top images in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>To capture the stochasticity of the future, prior works have proposed a variety of stochastic latent variable models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. While these methods generated reasonable predictions for relatively small video prediction datasets such as the BAIR robot pushing dataset <ref type="bibr" target="#b7">[8]</ref>, they suffer from severe underfitting in larger datasets in the face of practical GPU or TPU memory constraints <ref type="bibr" target="#b8">[9]</ref>. On the other hand, while hierarchical variational autoencoders (VAEs) can in principle produce higher-quality predictions by capturing multiple levels of stochasticity, the bidirectional dependency between individual hierarchical latent variables (higher-level variables influence the lower level and vice versa) potentially creates an unsolved problem of optimization instability as the number of hierarchical latent variables in the network increases <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>The key insight of this work is that greedy and modular optimization of hierarchical autoencoders can simultaneously address both the memory constraints and the optimization challenges of learning accurate largescale video prediction. On one hand, by circumventing end-to-end training, greedy machine learning allows sequential training of sub-modules of the entire video prediction model, enabling much larger models to be learned within the same amount of GPU or TPU memory. On the other hand, optimizing hierarchical VAEs in a greedy and modular fashion breaks the bidirectional dependency among individual latent variables. As a result, these variables can remain stable throughout the entire training process, resolving the typical instability of training deep hierarchical VAEs.</p><p>With this key insight, this paper introduces Greedy Hierarchical VAEs ("GHVAEs" hereafter) ( <ref type="figure" target="#fig_0">Fig. 1</ref>)-a set of local latent VAE modules that can be sequentially stacked and trained in a greedy, module-by-module fashion, leading to a deep hierarchical variational video prediction model that in practice admits a stable optimization and in principle can scale to large video datasets. As evaluated in Section 4, GHVAEs outperform state-of-the-art video prediction models by 17-55% in FVD score <ref type="bibr" target="#b11">[12]</ref> on four different datasets, and by 35-40% success rate on two real robotic manipulation tasks when used for planning. In addition, our empirical and theoretical analyses find that GHVAE's performance can improve monotonically as the number of GHVAE modules in the network increases. In summary, the core contribution of this work is the use of greedy machine learning to improve both the optimization stability and the memory efficiency of hierarchical VAEs, leading to significant gains in both large-scale video prediction accuracy and real robotic task success rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The Underfitting Challenge of Large-Scale Video Prediction. Resolving the underfitting challenge of large-scale video prediction can lead to powerful generalization in visual foresight <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, which performs model-based robotic control <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> via action-conditioned video prediction <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Initially, video prediction <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b34">35]</ref> has been tackled by a deterministic model <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. VAEs were later adopted to model the stochasticity of future visual observations <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref>. Nevertheless, modeling the stochasticity of the real world using a trajectory-based latent variable model leads to blurry predictions inadvertently. This problem is then addressed by two lines of orthogonal work-VAE-GANs <ref type="bibr" target="#b5">[6]</ref> and timestep-based latent variable models <ref type="bibr" target="#b6">[7]</ref>. While these methods resolve blurry predictions in smallscale video datasets such as the BAIR robot pushing dataset <ref type="bibr" target="#b7">[8]</ref>, they suffer from severe underfitting in largescale, multi-domain, or multi-robot datasets, such as RoboNet <ref type="bibr" target="#b49">[50]</ref> and RoboTurk <ref type="bibr" target="#b50">[51]</ref>. In parallel, Villegas et al. <ref type="bibr" target="#b8">[9]</ref> validate that higher model capacity leads to greater prediction fidelity. This raises the question of how to learn larger models to meet the underfitting challenge of large-scale video prediction. On the other hand, Castrejon et al. <ref type="bibr" target="#b10">[11]</ref> apply dense connections to hierarchical VAEs to address the optimization challenge of fitting hierarchical variational video prediction models. While this work outperforms the state-of-theart in relatively small video datasets, it was unable to scale its hierarchical VAE up substantially due to deep optimization problems <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. Other works have also attempted to address the underfitting challenge of large-scale video prediction through other angles. For example, one line of work attempts to represent pixels as discrete as opposed to continuous distributions <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>. Other works predict forward alternative quantities such as object-centric representations <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref> and goal-centric representations <ref type="bibr" target="#b59">[60]</ref>. Unlike these approaches, our method scales to large real-world video datasets without requiring additional inductive biases. Greedy Machine Learning. Greedy machine learning <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66]</ref> was first introduced to pro- </p><formula xml:id="formula_0">(W 1 enc , W 1 dec , W 1 prior , W 1 post ) are trained end-to-end.</formula><p>In Stage 2, all weights from the first module are frozen and the second module is trained. In Stage 3, all first and second-module weights are frozen, and only the third module is trained, so on and so forth. The video prediction quality for xt+1 improves as more modules are added. The legends in the figure denote the four components in each GHVAE module (encoder, decoder, prior, and posterior) and whether each component is frozen (tilted red bars) or used only for training and not at test time (dashed as opposed to solid lines). To limit the number of spatial dimensions that requires prediction from the prior network, only the prior and posterior in the final, K th GHVAE module are used. The action at is included in action-conditioned video prediction and excluded in action-free video prediction. vide a good weight initialization for deep networks to escape bad local optima during end-to-end backpropagation. As originally proposed, each greedy module of a deep network is stacked on top of the preceding greedy module and trained locally based on the features extracted from the preceding module. Subsequently, greedy machine learning has been applied to pre-training good feature extractors and stacked autoencoders <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72]</ref> for downstream tasks in vision, sound, and language <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75]</ref>. Trained via self-supervised learning, these feature extractors and autoencoders excelled at capturing and preserving time-invariant information in sequential data such as videos. In contrast, we propose a video prediction method that uses a hierarchy of latent variables to explicitly model time-variant information about the future. Finally, greedy training of generative adversarial networks (GANs) is proposed to generate highquality, high-resolution single-images <ref type="bibr" target="#b75">[76]</ref>. Unlike these prior works, we propose a greedy approach to training large-scale video prediction models that simultaneously addresses the memory constraints and the optimization challenges of hierarchical VAEs.</p><p>Hierarchical Variational Autoencoders. Hierarchical <ref type="bibr" target="#b76">[77]</ref> and sequential VAEs <ref type="bibr" target="#b77">[78]</ref> were recently introduced to improve generative modeling in various vision tasks such as video prediction <ref type="bibr" target="#b10">[11]</ref> and image generation <ref type="bibr" target="#b78">[79]</ref>. They are known to have optimization challenges <ref type="bibr" target="#b9">[10]</ref>, mainly due to the bidirectional dependency among the individual latent variables. When optimized end-to-end, the hierarchical VAE needs to keep each latent variable useful for the video prediction task at hand throughout the entire training process, while preserving the dependent relationships among these variables simultaneously. To this end, previous works introduced a variety of inductive biases such as dense connections <ref type="bibr" target="#b10">[11]</ref>, ladder structures <ref type="bibr" target="#b79">[80]</ref>, bidirectional inference <ref type="bibr" target="#b80">[81]</ref>, progressive lossy compression <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b82">83]</ref>, and spectral regularization <ref type="bibr" target="#b78">[79]</ref> to alleviate such optimization difficulties specific to hierarchical VAEs. These approaches have largely been successful in the context of image generation, while we study the more difficult video prediction problem. Unlike these approaches, we propose a greedy training scheme that significantly alleviates the optimization challenges of conditional hierarchical VAEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Greedy Hierarchical VAEs (GHVAEs)</head><p>Overview. To develop an expressive yet stably optimized video prediction model, we introduce Greedy Hierarchical VAEs <ref type="figure" target="#fig_1">(Fig. 2)</ref>, which are locally optimized VAE modules that can be stacked together sequentially to incrementally add capacity to the model. To train a stack of modules without needing to fit the entire model into memory, each module is optimized locally using the frozen weights of the previously-trained modules. Concretely, a GHVAE model has multiple GHVAE modules. Each GHVAE module has four convolutional sub-networks: an encoder, a decoder, a prior network, and a posterior inference network. In the remainder of this section, we overview mathematical notation, describe each of these model components in detail, derive the training objective for each module as a variational lower bound, and theoretically analyze the implications of greedy training.</p><p>Notation. This paper uses K to denote the total number of GHVAE modules in the network, W k , k ? [1, K] to denote the k th GHVAE module, W k = {W k enc , W k dec , W k prior , W k post } to denote the k th module's encoder, decoder, prior network, and posterior inference network respectively,</p><formula xml:id="formula_1">x t ? X = R H 0 ?W 0 ?C 0 to represent the RGB image observation (height H 0 , width W 0 , channel C 0 = 3) at the current timestep t, h k t ? H k = R H k ?W k ?C k</formula><p>H to denote the hidden variable encoded by the k th module for the current timestep t, z k t+1 ? Z k = R H k ?W k ?C k Z to denote the k th stochastic latent variable used to explicitly model the stochasticity of the future observation at timestep t + 1, a t ? A to denote the agent's action at the current timestep t in the case of action-conditioned video prediction, and T to denote the model's roll-out horizon during training.</p><p>Encoder. Shown as grey downward arrows in <ref type="figure" target="#fig_1">Fig. 2</ref>, the K encoders in a GHVAE model incrementally map from x t to h K t and serve as part of both the VAE model and the posterior inference network. For the encoder design, it is important to recall that VAEs treat each dimension of a stochastic latent variable as independent (i.e. the mean-field approximation). However, convolutional embeddings of images contain significant spatial correlations due to the low frequency of natural images, violating this approximation. To mitigate this challenge, we design the encoder architecture to incrementally compress the spatial dimensions of the embeddings while simultaneously significantly expanding the channel dimensions of the embeddings. This allows the model, at its deepest layer, to store plenty of information (including spatial information) without strongly-correlated dimensions. Concretely, the k th encoder W k enc maps from h k?1 t to h k t (except for the first encoder W 0 enc , which maps x t to h 1 t ), and incrementally compresses the height and width,</p><formula xml:id="formula_2">H k &lt; H k?1 , W k &lt; W k?1 , while expanding the channels C k H &gt; C k?1 H .</formula><p>Decoder. Shown as blue arrows in <ref type="figure" target="#fig_1">Fig. 2</ref>, the K decoders in a GHVAE model incrementally map from the deepest stochastic latent variable z K t+1 back to x t+1 to predict the next image. Since encoding significant information in stochastic latent variables is difficult, we aim to allow the stochastic latent variables to only capture new information about the future that is absent from the past. In other words, any partial information of the future that exists in h k t does not need to be predicted and thus should not be contained in z k t+1 . Hence, the decoder in the deepest latent space, W K dec , takes as input both h K t and the posterior latent variable z K t+1 , so that the network can borrow information directly from the past. Similarly, each decoder W k dec ? {W 1 dec . . . W K?1 dec } takes as input both h k t and h k+1 t+1 and predicts h k t+1 (except for W 1 dec , which predicts x t+1 ). Mirroring the encoders, these decoders incrementally expand the height and width, while compressing the channels.</p><p>Prior Network. Shown as green arrows in <ref type="figure" target="#fig_1">Fig. 2</ref>, the prior network W k prior maps h k t and a t to the mean and variance of a diagonal Gaussian distribution for z k t+1 to model the stochasticity of future observations. The prior network is recurrent-convolutional and used both at train and test time. Empirically, using all K stochastic latent variables z 1 t+1 . . . z K t+1 leads to excessive stochasticity and degrades performance as the number of GHVAE modules increases. Therefore, one key design choice is that while a K-module GHVAE uses all K stochastic latent variables during training (i.e., z 1...K t+1 , one for each module) to sequentially learn the multi-level stochasticity of future observations, only the latent variable at the deepest level, z K t+1 , is used at test time and requires prediction from the prior network. This greedy training strategy allows each decoder to propagate uncertainty from the deepest layer to the shallower layers, and ultimately back to the pixel space. As a result, GHVAEs can implicitly model the multi-level stochasticity of future observations without explicitly using multiple stochastic latent variables at test time, and can maximally compress the latent space spatially module-by-module such that h K t and z K t+1 contain as few spatial dimensions as possible. Because the deepest encoder will have the fewest spatial dimensions, the only stochastic latent variable z K t+1 will have the least spatial correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Posterior Inference Network.</head><p>Although the encoder and decoder have minimized spatial dimensions in the deepest hidden layer h K , the encoding process has produced a high channel dimension C K H for h K . To improve the quality of prediction by the prior network, the channels in h K may need to be downsized to reduce the required output dimensions of the prior network. Hence, shown as brown arrows in <ref type="figure" target="#fig_1">Fig. 2</ref>, the posterior inference network maps the current module's hidden variable h k t+1 to the mean and variance of a diagonal Gaussian distribution over the stochastic latent variable z k t+1 . When modules are added, a new posterior inference network and a new prior network for the new latent space are trained based on the latest module's representation. z k t+1 is a posterior latent variable, since both h k t+1 and z k t+1 are encoded from the ground truth future observation x t+1 as opposed to the predicted next observation. For this reason, the recurrent-convolutional posterior network is only available at train time and not used for inference at test time.</p><p>Optimization. In this section, we use p k to denote the VAE model and q k to denote the variational distribution. The encoder, the decoder, and the prior network are all part of the model p k , and both the encoder and the posterior inference network are part of q k . The training process of a K-module GHVAE model is split into K training phases, and only the k th GHVAE module is trained during phase k, where k ? [1, K]. GHVAE's training objective for the k th module is:</p><formula xml:id="formula_3">max W k T ?1 t=0 L k greedy (x t+1 )<label>(1)</label></formula><p>where L k greedy (x t+1 ) is GHVAE's Evidence Lower-Bound (ELBO) with respect to the current module W k at timestep t + 1:</p><formula xml:id="formula_4">L k greedy (x t+1 ) = E q k (z k t+1 |xt+1) [log p k (x t+1 | x t , z k t+1 )] ? D KL q k (z k t+1 | x t+1 ) p k (z k t+1 |x t , a t )<label>(2)</label></formula><p>where</p><formula xml:id="formula_5">p k ? p W 1 * ...k?1 * ,k enc,dec,prior , q k ? q W 1 * ...k?1 * ,k enc,post</formula><p>, and W 1 * ...k?1 * are the frozen, greedily trained weights of all preceding GHVAE modules.</p><p>To improve training stability, we use a fixed standard deviation for the posterior latent variable distribution</p><formula xml:id="formula_6">q k (z k t+1 | x t+1 ) in the KL divergence term in Eq. 2.</formula><p>Theoretical Guarantees. GHVAE's ELBO manifests two theoretical guarantees. 1) ELBO Validity: Sequentially optimizing each GHVAE module in the network is equivalent to maximizing a lower-bound of the ELBO for training all GHVAE modules end-to-end. This suggests that GHVAE's ELBO is valid :</p><p>Theorem 1 (ELBO Validity) For any k ? Z + and any set of frozen, greedily or end-to-end trained weights</p><formula xml:id="formula_7">W 1 * ...k?1 * , log p(x t+1 ) ? max W 1...k?1,k L k e2e (x t+1 ) ? max W k L k greedy (x t+1 ) (3) where L k e2e (x t+1 ) is GHVAE's ELBO for timestep t + 1 when optimized end-to-end. More formally, L k e2e (x t+1 ) is L k greedy (x t+1 ) in Eq. 2, except that the VAE model p k ? p W 1...k?1,k enc,dec,prior and the variational distribution q k ? q W 1...k?1,k enc,post .</formula><p>2) Monotonic Improvement: Adding more modules can only raise (as opposed to lower) GHVAE's ELBO, which justifies and motivates maximizing the number of modules in a GHVAE model:</p><p>Theorem 2 (Monotonic Improvement) For any k ? Z + and any set of frozen, greedily or end-to-end trained weights</p><formula xml:id="formula_8">W 1 * ...k?1 * , log p(x t+1 ) ? L k greedy (x t+1 ; W 1 * ...k?1 * ) ? L k?1 (x t+1 ; W 1 * ...k?1 * )<label>(4)</label></formula><p>where L k?1 ? {L k?1 greedy , L k?1 e2e } and L k greedy is initialized with the weights W 1 * ...k?1 * . Further details of the GHVAE method and mathematical proofs for these two theorems are in Appendix A and C respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation and Analysis</head><p>We conduct video prediction and real robot experiments to answer six key questions about GHVAEs: 1) How do GHVAEs compare to state-of-the-art models in video prediction? 2) Can GHVAEs achieve monotonic improvement in video prediction accuracy by simply adding more modules, as Theorem 2 suggests? 3) Does training a GHVAE model end-to-end outperform training greedily per module, as Theorem 1 suggests? 4) Does the high expressivity of GHVAEs cause overfitting during training? 5) How important is the learned prior network to GHVAEs' performance? 6) Does the high expressivity of GHVAEs improve real robot performance? Visualizations and videos are at https://sites.google.com/view/ghvae, and more qualitative results are in Appendix B.</p><p>Video Prediction Performance. To answer the first question, this paper evaluates video prediction methods across five metrics: Fr?chet Video Distance (FVD) <ref type="bibr" target="#b11">[12]</ref>, Structural Similarity Index Measure (SSIM),   <ref type="bibr" target="#b10">[11]</ref> 0.264 <ref type="bibr" target="#b10">[11]</ref> Peak Signal-to-noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b83">[84]</ref>, and human preference. FVD and human preference both measure overall visual quality and temporal coherence without reference to the ground truth video. PSNR, SSIM, and LPIPS measure similarity to the ground-truth in different spaces, with LPIPS most accurately representing human perceptual similarity. To stress-test each method's ability to learn from large and diverse offline video datasets, we use four datasets: RoboNet <ref type="bibr" target="#b49">[50]</ref> to measure prediction of object interactions, KITTI <ref type="bibr" target="#b84">[85]</ref> and Cityscapes <ref type="bibr" target="#b85">[86]</ref> to evaluate the ability to handle partial observability, and Human3.6M [87] to assess prediction of structured motion. This paper compares GHVAEs to SVG' <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> and Hier-VRNN <ref type="bibr" target="#b10">[11]</ref>, which are two state-of-the-art prior methods that use nonhierarchical and hierarchical VAEs respectively. While SAVP <ref type="bibr" target="#b5">[6]</ref> is another prior method, we empirically found that SAVP underperforms SVG' on these datasets, and therefore omitted SAVP results for simplicity. All metrics are summarized via the mean and standard error over videos in the test set. For SVG' in particular, this paper compares to "SVG' (M=3, K=5)" <ref type="bibr" target="#b8">[9]</ref>, which is the largest and bestperforming SVG' model that Villegas et al. <ref type="bibr" target="#b8">[9]</ref> evaluate and the largest version of SVG' that can fit into a 24GB GPU with a batch size of 32. SVG' (M=3, K=5) has 3x larger convolutional LSTMs and 5x larger encoder and decoder convolutional networks compared to the original SVG <ref type="bibr" target="#b6">[7]</ref> and significantly outperforms the original SVG by 40-60% in FVD scores <ref type="bibr" target="#b8">[9]</ref>. Since Villegas et al. <ref type="bibr" target="#b8">[9]</ref> reported the FVD, SSIM, and PSNR performance of "SVG' (M=3, K=5)" on KITTI and Human3.6M, we directly compare to their results using the same evaluation methodology. For RoboNet and for evaluating LPIPS and human preference, we re-implement SVG' and report the corresponding performance. In <ref type="table" target="#tab_0">Table 1</ref>, the 6-module GHVAE model outperforms SVG' across all three datasets across all metrics. Most saliently, we see a 17-55% improvement in FVD score and a 13-45% improvement in LPIPS. Further, we see that humans prefer predictions from the GHVAE model more than 85% of the time.</p><p>To compare to Hier-VRNN <ref type="bibr" target="#b10">[11]</ref>, we use the Cityscapes driving dataset <ref type="bibr" target="#b85">[86]</ref>. Since Castrejon et al. <ref type="bibr" target="#b10">[11]</ref> already report FVD, SSIM, and LPIPS performance on Cityscapes, we directly compare against these results using the same evaluation setting. <ref type="table" target="#tab_1">Table 2</ref> indicates that GHVAEs outperform Hier-VRNN by 26% in FVD, 18% in SSIM, and 27% in LPIPS for Cityscapes when the number of modules reaches six.</p><p>These results indicate that GHVAEs significantly outperform state-of-the-art video prediction models, including hierarchical and non-hierarchical models. The strong performance of GHVAEs mainly originates from the capacity to learn larger models with a stable optimization within the same amount of GPU or TPU memory. For example, even though both GHVAE and SVG' consume 24GB of memory during training, GH-VAE contains 599 million parameters while SVG' has 298 million. Next, we perform several ablations to better understand the good performance of GHVAEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation 1:</head><p>Monotonic Improvement and Scalability of GHVAEs. Given that GHVAEs can be stacked sequentially, it becomes important to determine whether GHVAEs can achieve mono-  <ref type="table" target="#tab_2">Table 3</ref> that increasing the number of GHVAE modules from 2, to 4, to eventually 6 improves performance across all metrics. These results validate Theorem 2 and suggest that greedily adding more modules increases performance monotonically in practice and enables GHVAEs to scale to large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation 2: Greedy vs.</head><p>End-to-End Optimization of GHVAEs. End-to-end learning is conventionally preferred over greedy training when GPU or TPU memory constraints are loose. To examine whether this pattern also holds for GHVAEs, we trained a 6-module GHVAE model end-to-end using two 48GB GPUs (since the end-to-end model does not fit in 24GB GPUs) across five separate trials. In addition, we conducted a second experiment in which we fine-tune the greedily trained GHVAE model end-to-end using two 48GB GPUs. We found in <ref type="table" target="#tab_3">Table 4</ref> that the model was unable to converge to any good performance in any single run compared to the greedy setting. Qualitatively, when optimized end-to-end, GHVAE models need to update each module to improve video prediction quality while preserving the interdependency among individual hidden variables simultaneously, which can lead to optimization difficulties <ref type="bibr" target="#b9">[10]</ref>. Even if GHVAEs can be optimized end-to-end, limited GPU or TPU memory capacity will still make it infeasible to train as the number of modules grows beyond six. However, end-to-end fine-tuning does lead to minor performance gains as indicated by row "GHVAEs (End-to-End Fine-Tuning, Abl. 2)". These two experiments imply that greedy training of GHVAEs leads to higher optimization stability than end-to-end training from scratch. They also indicate that end-to-end training of GHVAE can outperform greedy training as suggested by Theorem 1, so long as the GHVAE model is first pre-trained greedily.</p><p>Ablation 3: Train-Test Comparison for GH-VAEs. Since GHVAEs aim to tackle the underfitting challenge of large-scale video prediction, we now study whether GHVAEs have started to overfit to the training data. We observe in <ref type="table" target="#tab_4">Table 5</ref> that for  RoboNet, a 6-module GHVAE's training performance is similar to its test performance across all four metrics, implying little overfitting. For KITTI, Human3.6M, and Cityscapes, we observe that train performance is better than test performance across most metrics, indicating some overfitting. We hypothesize that this is due to the smaller sizes of these three datasets compared to RoboNet, and, for Human3.6M, because the test set corresponds to two unseen human subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation 4:</head><p>Performance Contribution of Learned Prior. One of GHVAEs' insights is to predict forward the stochastic latent variable only at the deepest layer. Therefore, it may be important to quantify the contribution of the learned prior network to the overall performance. We observe in <ref type="table" target="#tab_5">Table 6</ref> that using a learned prior significantly outperforms using a uniform diagonal Gaussian prior particularly for action-conditioned datasets. We hypothesize that this is because a learned prior contains information about the action while a uniform prior does not.</p><p>Real Robot Performance. Finally, we evaluate whether improved video prediction performance translates to greater success on downstream tasks. We consider two manipulation tasks: Pick&amp;Wipe and Pick&amp;Sweep on a Franka Emika Panda robot arm. Concretely, each method is given a small, autonomously collected training dataset of 5000 videos of random robot interactions with diverse objects such as those in the  Franka robot is equipped with a 45 ? black RGB camera. We pre-train each model on RoboNet and fine-tune on an autonomously collected dataset of 5000 videos of the robot's random interactions with objects in the bin <ref type="figure" target="#fig_5">(Fig. 4a</ref>). Using the trained GHVAE video prediction model, the Franka robot is tested across two tasks: Pick&amp;Wipe (top and bottom left of bin in <ref type="figure" target="#fig_5">Fig. 4b)</ref> and Pick&amp;Sweep (top and bottom right of bin in <ref type="figure" target="#fig_5">Fig. 4b</ref>). All tasks are evaluated on objects, tools, and containers never seen during training.</p><p>dark-grey tabletop bin in <ref type="figure" target="#fig_5">Fig. 4a</ref>. At test time, to measure generalization, all objects, tools, and containers used are never seen during training. Empirically, training directly on this small 5000-video dataset leads to poor generalization to novel objects at test time for all methods. Thus, to enable better generalization, all networks are first pretrained on RoboNet <ref type="bibr" target="#b49">[50]</ref> and subsequently fine-tuned on this 5000-video dataset. In both tasks, the robot is given a single 64 ? 64 RGB goal image to indicate the task goal, with no hand-designed rewards provided. The model rollout horizon for each video prediction method is 10, with two prior context frames and a sequence of 10 future actions provided as input. All real-robot results are evaluated across 20 trials. For planning, we perform random shooting (details in Appendix B) with a 4-dimensional action space, which contains three scalars for the [x, y, z] endeffector translation and one binary scalar for opening vs. closing its parallel-jaw gripper.</p><p>In the first Pick&amp;Wipe task, the robot needs to pick a wiping tool (e.g. sponge, table cloth, etc.) up and wipe all objects off the plate using the wiping tool. The task is successful if the robot picks the wiping tool up and wipe all objects off the plate using the wiping tool within 50 timesteps. In the second Pick&amp;Sweep task, the robot is required to pick a sweeping tool (e.g. dustpan sweeper, table cloth, or sponge, etc.) up and sweep an object into the dustpan. The task is successful if the target object is swept into the dustpan within 50 timesteps. At the beginning of each task, the wiping or sweeping tool is not yet in the robot's gripper, which makes the tasks more difficult. <ref type="table" target="#tab_6">Table 7</ref> reveals that a 6-Module GHVAE model outperforms SVG' by 40% and 35% in success rate for Pick&amp;Wipe and Pick&amp;Sweep respectively. For Pick&amp;Wipe, SVG' produces blurry predictions especially when the robot and the plate overlap in the image. This reduces SVG's ability to predict the best action sequence for wiping objects off the plate. In contrast, GHVAE empirically produces accurate predictions of the robot's motion and the position of the wiping tool and the objects. For Pick&amp;Sweep, SVG' has difficulty predicting the movement of the object during the robot's sweeping motion, leading to more frequent task failures. In contrast, GHVAE predicts plausible robot sweep motions and object movements, reaching an 85% success rate. These results indicate that GHVAEs not only lead to better video prediction performance but that they lead to better downstream performance on real robotic manipulation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper introduces Greedy Hierarchical VAEs (GHVAEs), which are local VAE modules that can be stacked sequentially and optimized greedily to construct an expressive yet stably optimized hierarchical variational video prediction model. This method significantly outperforms state-of-the-art hierarchical and non-hierarchical video prediction methods by 17-55% in FVD score across four video datasets and by 35-40% in real-robot task success rate. Furthermore, GHVAE achieves monotonic improvement by simply stacking more modules. By addressing the underfitting challenge of large-scale video prediction, this work makes it possible for intelligent agents such as robots to learn from large-scale offline video datasets and generalize across a wide range of complex visuomotor tasks through accurate visual foresight.</p><p>While GHVAEs exhibit monotonic improvement, experimenting with GHVAEs beyond six modules is an important direction for future work to better understand the full potential of this method. On the other hand, leveraging this method to enable robotic agents to learn much harder and longer-horizon manipulation and navigation tasks is also an important future direction. Finally, it would be interesting to explore the use of GHVAEs for other generative modeling problems.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contents</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D . Failure Case Analysis 22</head><p>A. Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Memory Efficiency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 GHVAE</head><p>Because GHVAEs optimize each module with regard to image reconstruction, we must include in memory both the current module and some of the prior modules. Here, we briefly describe the memory savings of GHVAEs. GHVAEs save GPU or TPU memory allocation by avoiding the need to store gradient information in previous modules during back-propagation. Specifically, for the encoder, intermediate activations and all gradients from the frozen modules no longer need to be stored in memory. For the decoder, the gradients of the activations will still need to be stored for backpropagation into the currently trained module.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Other Methods</head><p>While GHVAEs alleviate the challenge of training large-scale video prediction models in the face of GPU or TPU memory constraints, there are other ways of addressing this challenge, such as increasing the number of GPUs or TPUs (as opposed to increasing the memory capacity per GPU or TPU), having different examples on different GPUs, and allocating model weights across more than one GPUs. Our method is orthogonal and complementary to such directions. Also, while increasing the number of GPUs or TPUs can increase the training batch size, our method can still allow larger models to be trained even after batch size per GPU lowers to 1.</p><p>It is also important to note that greedy training leads to higher optimization stability for GHVAEs in particular, as revealed in Ablation 2 of <ref type="table" target="#tab_3">Table 4</ref> in the main paper. Ablation 2 indicates that when GHVAEs are trained end-to-end from scratch, the model was unable to converge to any good performance in any single run compared to the greedy setting. GPU or TPU memory saving is only one of the benefits of performing greedy training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Intuition</head><p>In this section, we elaborate on the main paper's intuition on why it is important to capture the multi-level stochasticity of future observations in video prediction. Shown in <ref type="figure" target="#fig_5">Fig. 4</ref> is an example of a current and next image observation from RoboNet. In action-conditioned video prediction for RoboNet, the video prediction model is given a four-dimensional vector [dx, dy, dz, gripper], in which dx, dy, dz denote the future end-effector translation from the current position, and gripper is a binary integer for opening (gripper = 0) or closing (gripper = 1) the gripper. To accurately predict the next image observation, the video prediction model needs to precisely capture the end-effector position from the current monocular image, so that given the expected end-effector translation, the model can predict the new end-effector position and reconstruct all pixels that belong to the robot in the next image accordingly. The current end-effector position is considered a high-level visual feature that has inherent stochasticity because it is difficult to measure how long an inch is in this monocular image and therefore challenging to predict the precise pixel location of the robot in the next timestep. In addition, as the robot moves to a new position, the pixels currently occluded by the robot's arm will be revealed, and yet it is highly uncertain what is behind the robot's arm, let alone to predict these pixels for the next timestep. Concretely, there could be one or more objects behind the robot arm or zero objects. In the case where there are one or more objects, the ground truth texture and orientation of these objects are almost entirely occluded and unknown. These are the uncertainties around the low-level features in the image. In summary, multiple levels of uncertainty exist in the current image (from the high-level representation of end-effector position to the lower-level texture of the occluded objects and table background), therefore demanding the video prediction model to accurately model such multi-level stochasticity of future observations with hierarchical architectures.</p><p>As a side note, in the main paper, we posit that "VAEs treat each dimension of a stochastic latent variable as independent". Here, this statement refers to the case where the VAE uses a diagonal multivariate Gaussian distribution to model the latent variable distribution, which applies to GHVAEs as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Video Prediction</head><p>In this section, we visualize qualitative results and discuss how we calculate each performance metric for video prediction, how we perform human evaluation using Amazon Mechanical Turk, and additional ablation studies. <ref type="figure" target="#fig_6">Figure 5</ref>, 6, 7, 8 exhibits example rollouts from video prediction methods reported in the main paper. <ref type="figure" target="#fig_11">Figure 9</ref> and 10 are the example rollouts from real-robot experiments: Pick&amp;Sweep and Pick&amp;Wipe tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1 Visualizations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 Performance Evaluation</head><p>Methodologies for calculating performance metrics are available at <ref type="table">Table 9</ref>. Note that these methodologies match those reported in prior works so that experiments conducted in this paper provide fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Action  <ref type="table">Table 9</ref>: GPU Memory Usage for All Experiments in <ref type="table" target="#tab_0">Table 1 and Table 2</ref>. All Convolutional Layers in the 6-Module GHVAE model for CityScapes are Downsized by 40% to fit into 16GB GPU Memory for Fair Comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.3 Human Evaluation</head><p>For human evaluation, we provide 300 videos from both GHVAE and SVG' to Amazon Mechanical Turk workers in the form of 300 tasks. In each task, the workers are presented with three videos: a video generated by GHVAE, a video generated by SVG', and the ground truth video. The worker does not know which video is generated by GHVAE or SVG', but do know which one is the ground truth video. In each task, the workers are asked to select the video that is more realistically similar to the ground truth video. These selections count as preferences. We then average all preferences and report results in <ref type="table" target="#tab_0">Table 1</ref> and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.4 Ablation for Encoder-Decoder Architectures</head><p>In the early stages of our research, we have experimented with an alternative encoder-decoder architecture that expands or keeps the spatial dimension constant while reducing the channel dimension instead. The empirical performance of doing so significantly underperforms the current GHVAE architecture, which reduces spatial dimensions iteratively and compensates this dimensionality reduction by expanding the channel dimension. As mentioned in the paper, we hypothesize that reducing the spatial dimensions allows GHVAEs to perform better mean-field approximation in the deepest latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.5 Ablation for Single vs. Multiple Latents</head><p>In this section, we provide further intuition for the tradeoff between using single vs. multiple latent variables in a K-module GHVAE. Using multiple latent variables for GHVAE is an obvious option that we have empirically experimented with without satisfying results. Experimentally, when the GHVAE model uses all K latent variables, the earlier latent variables provide suboptimal information and undesirably noisy signals to the overall network because of their inability to perform high-fidelity mean-field approximation when the spatial dimensions are large. This empirical phenomenon motivated us to only use the deepest latent variable in a GHVAE model. It is however important to note that using a single latent variable does not prevent GHVAEs from learning to accurately represent     the multi-level stochasticity of future pixels. One can model such multi-level stochasticity using a single latent variable, provided that the decoders learn to appropriately project stochasticity from a succeeding layer to a preceding layer via non-linear transformation. In summary, we designed the GHVAE model to contain a single level GHVAE SVG' Here, GHVAE exhibits performance advantage over SVG'. Note that due to our random shooting planning strategy, the rollout length of each method is variable and different in every trial. Kindly see Appendix B.2.5 for more details. of stochastic prediction, which is propagated through earlier deterministic layers to model multi-level stochasticity of future observations.</p><formula xml:id="formula_9">t = 1 t = 2 t = 3 t = 4 t = 5 t = 6 t = 7 t = 8 t = 9 t = 10 Ground Truth (Sawyer) GHVAE SVG' (M=3, K=5) Ground Truth (Wid- owX) GHVAE SVG' (M=3, K=5) Ground Truth (Franka) GHVAE SVG' (M=3, K=5) Ground Truth (Baxter) GHVAE SVG' (M=3, K=5)</formula><formula xml:id="formula_10">t = 1 t = 2 t = 3 t = 4 t = 5 t = 6 t = 7 t = 8 t = 9 t = 10</formula><formula xml:id="formula_11">GHVAE SVG' t = 1 t = 2 t = 3 t = 4 t = 5 t = 6 t = 7 t = 8 t = 9 t = 10</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Real Robot</head><p>In this section, we elaborate on real-robot experimental setup and training data, visualizations of real-robot task execution and environments, and the random shooting planner we use to control the Franka robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 Setup</head><p>In the first Pick&amp;Wipe task, the robot needs to pick a wiping tool (e.g. sponge, table cloth, etc.) up and wipe all objects off the plate for cleaning using the wiping tool. Each of the 20 trials contains different plates, objects, and wiping tools all unseen during training, and there could be at most two objects on the plate. The initial physical locations of the plate, the objects on the plate, and the robot itself are all randomized except that the robot is above the wiping tool. At the beginning of each trial, the wiping tool is not yet in the robot's gripper, which makes the task more difficult. The task is considered successful if the robot picks the wiping tool up successfully and all objects are entirely wiped off the plate using the wiping tool within 50 timesteps.</p><p>In the second Pick&amp;Sweep task, the robot is required to pick a sweeping tool (e.g. dustpan sweeper, table cloth, or dry sponge, etc.) up and sweep an object into the dustpan that is randomly placed in the bin. At the beginning of each trial, the sweeping tool is not yet in the robot's gripper, which makes the task difficult. When a sweeping tool is not present in the scene, the robot then needs to sweep the object into the dustpan using its gripper. Each of the 20 trials contains different dustpans, objects, and sweeping tools all unseen during training. The physical location of the dustpan is uniformly random, and the object and the robot are also arbitrarily placed except that the robot is above the sweeping tool. The task is determined successful if the target object is swept into the dustpan within 50 timesteps. When a sweeping tool is indeed present, pushing the object into the dustpan using the robot's gripper will be considered a failure. Only pushing the object using the sweeping tool will be considered successful. This requires the video prediction methods to detect whether a tool was used for sweeping in the goal image and act accordingly in the physical task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2 Training Data</head><p>The video prediction models used for the real-robot experiments in this paper are not trained using the RoboNet dataset directly, but instead first pre-trained on RoboNet and then fine-tuned on a self-collected dataset of 5000 videos using the target Franka robot. Yet, this paper is about fitting video prediction models to large-scale datasets and this training scheme might seem to be contradicting with the main message. While the models can be trained directly on RoboNet, without fine-tuning on the 5000-video Franka dataset, the empirical task success rate is much lower for both GHVAE and SVG' on the target Franka environment due to unseen lighting conditions and camera viewpoint. On the other hand, if the models are only trained on the 5000-video dataset, the models easily overfit and fail to generalize to novel objects and tools. The purpose of large-scale video prediction is not to overfit a large dataset, but to learn powerful generalization such that the model can perform few-shot learning on the target environment using a small amount of data. Such a training scheme works in favor of learning large-scale video prediction, as opposed to defeating its purpose. Example environments for self-supervised training data collection are available at <ref type="figure" target="#fig_0">Fig. 14.</ref> The collection of training data is entirely self-supervised. Concretely, the robot randomly interacts with the training objects in the bin for 2-3 minutes in episodes of 20 timesteps, before pushing the objects from the corners to the center of the bin, so that object interaction remains frequent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.3 Task Execution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.4 Task Diversity</head><p>In <ref type="figure" target="#fig_0">Figure 13</ref>, we visualize more environments and tools used for real-robot tasks to reveal the diversity of the evaluation tasks. All objects used for evaluation are unseen during training.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.5 Planning</head><p>For simplicity, all real-robot experiments in this paper use a random shooting planner to optimize actions in visual foresight. Concretely, given a video prediction model and a goal image, we randomly sample a batch of 140 trajectories from the model and select the action sub-sequence for which the predicted images lead to the lowest L1 loss to the provided goal image. The robot replans after each execution of action sequences until the horizon of 50 timesteps is reached.</p><p>Concretely, the action space for the Franka robot has a dimension of 4 (A = R 4 ), which contains three scalars for the [x, y, z] end-effector translation and one binary scalar for opening vs. closing its parallel-jaw gripper. Given the current image x t , a goal image g, a sequence of t context images x 1:t and a sampled action sequence a t:t+T ?1 , the sequence of frames predicted by the video prediction model f is:</p><formula xml:id="formula_12">x t +1 = f (x t , a t , x 1:t ) (5) where t ? [t, t + T ? 1],x t = x t .</formula><p>In practice, T = 10 for the Franka robot, and we sample a batch of 140 action sequences {a 1 t:t+T ?1 , . . . , a 140 t:t+T ?1 } and predicted frames {x 1 t+1:t+T , . . . ,x 140 t+1:t+T }. Next, we calculate the optimal length of action sequence T * ? [1, T ], and the best action sequence index b * ? [1, 140] using the following equation:</p><formula xml:id="formula_13">b * , T * = arg min b?[1,140],T ?[1,T ] |x b t+T ? g|<label>(6)</label></formula><p>Finally, the best action sequence is then calculated as: a 1:T * = a b * 1:T * . The robot then executes this T * -timestep action sequence and repeats this planning procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Mathematical Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Proof of Theorem 1</head><p>Theorem 1 (ELBO Validity) For any k ? Z + and any set of frozen, greedily or end-to-end trained weights Proof. Suppose W k * is the optimal parameters of the last module of a k-module GHVAE model:</p><formula xml:id="formula_14">W 1 * ...k?1 * , log p(x t+1 ) ? max W 1...k?1,k L k e2e (x t+1 ) ? max W k L k greedy (x t+1 )<label>(3)</label></formula><formula xml:id="formula_15">W k * = arg max W k L k greedy (x t+1 )<label>(7)</label></formula><p>In other words:</p><formula xml:id="formula_16">max W k L k greedy (x t+1 ) = L k greedy (x t+1 ; W k * )<label>(8)</label></formula><p>Therefore:</p><formula xml:id="formula_17">log p(x t+1 ) ? max W 1...k L k e2e (x t+1 ) ? L k greedy (x t+1 ; W k * ) = max W k L k greedy (x t+1 )<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Proof of Theorem 2</head><p>Recall that:</p><formula xml:id="formula_18">L k greedy (x t+1 ) = E q k (z k t+1 |xt+1) log p k (x t+1 | x t , z k t+1 ) ? D KL q k (z k t+1 | x t+1 ) p k (z k t+1 |x t )<label>(10)</label></formula><p>Steps in the following derivation that don't change from the previous step are in gray, while annotations are in blue.</p><p>log p(x t+1 ) ? L k greedy (x t+1 ) [Variation Lower-Bound] (11)</p><formula xml:id="formula_19">= E q k (z k t+1 |xt+1) log p k (x t+1 | x t , z k t+1 ) ? D KL q k (z k t+1 | x t+1 ) p k (z k t+1 | x t ) [Eq. 10] (12) = E q k (z k t+1 |xt+1) log z k?1 t+1 p k (x t+1 | z k?1 t+1 , x t , z k t+1 )p k (z k?1 t+1 | x t , z k t+1 ) q k?1 (z k?1 t+1 | x t+1 ) q k?1 (z k?1 t+1 | x t+1 ) ?D KL q k (z k t+1 | x t+1 ) p k (z k t+1 | x t ) [Algebra] (13) = E q k (z k t+1 |xt+1) log E q k?1 (z k?1 t+1 |xt+1) p k (x t+1 | z k?1 t+1 , x t , z k t+1 )p k (z k?1 t+1 | x t , z k t+1 ) q k?1 (z k?1 t+1 | x t+1 ) ?D KL q k (z k t+1 | x t+1 ) p k (z k t+1 | x t ) [Algebra] (14) ? E q k (z k t+1 |xt+1) E q k?1 (z k?1 t+1 |xt+1) log p k (x t+1 | z k?1 t+1 , x t , z k t+1 ) + log p k (z k t+1 | x t , z k?1 t+1 )p k (z k?1 t+1 | x t ) p k (z k t+1 | x t ) ? log q k?1 (z k?1 t+1 | x t+1 ) ?D KL q k (z k t+1 | x t+1 ) p k (z k t+1 | x t ) [Jensen's Inequality, Bayes' Rule] (15) = E q k (z k t+1 |xt+1) E q k?1 (z k?1 t+1 |xt+1) log p k (x t+1 | z k?1 t+1 , x t )q k (z k t+1 | x t+1 , z k?1 t+1 , x t ) p k (z k t+1 | z k?1 t+1 , x t ) + log p k (z k t+1 | x t , z k?1 t+1 )p k (z k?1 t+1 | x t ) p k (z k t+1 | x t ) ? log q k?1 (z k?1 t+1 | x t+1 ) ? D KL q k (z k t+1 | x t+1 ) p k (z k t+1 | x t ) [Bayes' Rule] (16) = E q k (z k t+1 |xt+1) E q k?1 (z k?1 t+1 |xt+1) log p k (x t+1 | z k?1 t+1 , x t )q k (z k t+1 | x t+1 , z k?1 t+1 , x t )p k (z k?1 t+1 | x t ) p k (z k t+1 | x t ) ? log q k?1 (z k?1 t+1 | x t+1 ) ?D KL q k (z k t+1 | x t+1 ) p k (z k t+1 | x t )</formula><p>[Algebra] <ref type="bibr" target="#b16">(17)</ref> = E q k (z k t+1 |xt+1) E q k?1 (z k?1 t+1 |xt+1) log p k (x t+1 | z k?1 t+1 , x t ) + log p k (z k?1 t+1 | x t ) ? log q k?1 (z k?1 t+1 | x t+1 )</p><formula xml:id="formula_20">+ log q k (z k t+1 | x t+1 , z k?1 t+1 , x t ) ? log p k (z k t+1 | x t ) ?D KL q k (z k t+1 | x t+1 ) p k (z k t+1 | x t ) [Algebra] (18) = E q k (z k t+1 |xt+1) E q k?1 (z k?1 t+1 |xt+1) log p k?1 (x t+1 | z k?1 t+1 , x t ) + log p k?1 (z k?1 t+1 | x t ) ? log q k?1 (z k?1 t+1 | x t+1 ) + log q k (z k t+1 | x t+1 , z k?1 t+1 , x t )? log p k (z k t+1 | x t ) ?D KL q k (z k t+1 | x t+1 ) p k (z k t+1 | x t ) [z k?1 t+1 is independent of p k given p k?1 ] (19) = E q k (z k t+1 |xt+1) L k?1 (x t+1 ) + E q k?1 (z k?1 t+1 |xt+1) log q k (z k t+1 | x t+1 , z k?1 t+1 , x t )? log p k (z k t+1 | x t ) ?D KL q k (z k t+1 | x t+1 ) p k (z k t+1 | x t )</formula><p>[Eq. 10] (20)</p><p>= L k?1 (x t+1 ) + E q k (z k t+1 |xt+1) log q k (z k t+1 | x t+1 )? log p k (z k t+1 | x t ) ?D KL q k (z k t+1 | x t+1 ) p k (z k t+1 | x t ) [Remove conditionally independent variables, Algebra] (21)</p><formula xml:id="formula_21">= L k?1 (x t+1 )</formula><p>[Algebra] <ref type="bibr" target="#b21">(22)</ref> Ground Truth GHVAE <ref type="figure" target="#fig_0">Figure 15</ref>: Failure case for a 6-module GHVAE model on RoboNet. In this case, the GHVAE model failed to accurately track the movement of the blue bowl. This indicates that the GHVAE model is still slightly underfitting on RoboNet. We hypothesize that training an 8-module, 10-module or 12-module GHVAE model will resolve such failure case.</p><p>where L k?1 ? {L k?1 greedy , L k?1 e2e } and L k greedy is initialized with the weights W 1 * ...k?1 * . Notice that the proof above assumes action-free video prediction. The proof for action-conditioned video prediction is the same with every conditional variable x t in the proof above expanding into two joint conditional variables x t and a t . For example, the term p k (x t+1 | x t , z k t+1 ) would be p k (x t+1 | x t , a t , z k t+1 ) instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Clarification for Equation 2</head><p>Note that while Eq. 2 in the paper is an accurate mathematical form of GHVAE's ELBO, we have omitted a t in the term log p k (x t+1 | x t , z k t+1 ) in this equation since GHVAE in practice only uses a t in the prior network. In other words, a more general form for Eq. 2 is the following:</p><formula xml:id="formula_22">L k greedy (x t+1 ) = E q k (z k t+1 |xt+1) log p k (x t+1 | x t , a t , z k t+1 ) ? D KL q k (z k t+1 | x t+1 ) p k (z k t+1 |x t , a t )<label>(23)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Failure Case Analysis</head><p>While a 6-Module GHVAE outperforms SVG' and Hier-VRNN, the model is still slightly underfitting RoboNet. We provide visualizations of failure examples in <ref type="figure" target="#fig_0">Figure 15</ref>. In this figure, the GHVAE model failed to accurately track the movement of the blue bowl. This indicates that the GHVAE model is still slightly underfitting on RoboNet. Given that such failure to track graspable object does not occur frequently for RoboNet, we hypothesize that this failure case is due to underfitting, and that training an 8-module, 10-module or 12-module GHVAE model can potentially tackle such failure case.</p><p>In addition, we hypothesize that a monocular image can cause partial observability to the video prediction problem. In <ref type="figure" target="#fig_0">Figure 15</ref> for example, without visually capturing the precise 3D locations of the robot and the blue bowl, it is difficult to tell whether the robot has successfully grasped the blue bowl and to predict the future motions of the blue bowl accordingly. Therefore, adding an [x, y, z] state end-effector position vector or a second camera image from a different viewpoint (both are readily available information) to the GHVAE model can potentially resolve such a failure case.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Greedy Hierarchical Variational Autoencoders</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Training procedure and architecture for a three-module GHVAE. In Stage 1, all first-module weights</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Real Robot Experimental Setup. The</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>20 C. 1 .</head><label>201</label><figDesc>Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 C.2. Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 C.3. Clarification for Equation 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Example pair of current and next image in robotic manipulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>RoboNet Video Prediction. Specifically, we provide examples for various physical robots in RoboNet: Sawyer, WidowX, Franka, and Baxter. Both GHVAE and SVG' (M=3, K=5) are given the same two context images. Here, a 6-module GHVAE model exhibits visible performance superiority over SVG' (M=3, K=5) on generating realistic object (Sawyer) and robot movements (WidowX, Franka, Baxter). The red boxes highlight the differences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>KITTI Driving Video Prediction. Both GHVAE and SVG' (M=3, K=5) are given the same five context images. Here, a 6-module GHVAE model exhibits performance advantage over SVG' (M=3, K=5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Human3.6M Video Prediction. Both GHVAE and SVG' (M=3, K=5) are given the same five context images. Here, a 6-module GHVAE model exhibits performance advantage over SVG' (M=3, K=5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Cityscapes Driving Video Prediction. Both GHVAE and Hier-VRNN are given the same two context images. Here, a 6-module GHVAE model exhibits performance advantage over Hier-VRNN. Note that this paper directly compares to Hier-VRNN results reported in Castrejon et al.<ref type="bibr" target="#b10">[11]</ref> and does not re-implement the Hier-VRNN algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Video Prediction in Real-Robot Pick&amp;Sweep Tasks. Both GHVAE and SVG' are given the same two context images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Video Prediction in Real-Robot Pick&amp;Wipe Tasks. Both GHVAE and SVG' are given the same two context images. Here, GHVAE exhibits performance advantage over SVG'. Note that due to our random shooting planning strategy, the rollout length of each method is variable and different in every trial. Kindly see Appendix B.2.5 for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Real-Robot Task Execution in Pick&amp;Sweep Experiments. Here, a 6-module GHVAE model exhibits more frequent successes than SVG'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11</head><label>11</label><figDesc>and 12 exhibit example Pick&amp;Sweep and Pick&amp;Wipe trials of real-robot task execution using the GHVAE and SVG' methods. Real-robot execution videos are at https://sites.google.com/view/ghvae.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 12 :</head><label>12</label><figDesc>Real-Robot Task Execution in Pick&amp;Wipe Experiments. Here, a 6-module GHVAE model exhibits more frequent successes than SVG'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 13 :</head><label>13</label><figDesc>Sample Real-Robot Evaluation TasksSample Training Environment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 14 :</head><label>14</label><figDesc>Representative Real-Robot Training Environment. Note that all objects used during training are excluded from evaluation. The 5000-video training data for both the Pick&amp;Sweep and the Pick&amp;Wipe tasks are the same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>GHVAE vs. SVG' video prediction test performance (mean ? standard error). GHVAE outperforms SVG' on all datasets across all metrics. "Human" denotes human preferences between the two methods. 2?2.6 24.7?0.2 89.1?0.4 0.036?0.001 92.0% SVG' [9] 123.2?2.6 23.9?0.1 87.8?0.3 0.060?0.008 8.0% KITTI GHVAEs 552.9?21.2 15.8?0.1 51.2?2.4 0.286?0.015 93.3%</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>FVD ?</cell><cell cols="2">Video Prediction Test Performance PSNR ? SSIM ? LPIPS ?</cell><cell>Human</cell></row><row><cell>RoboNet</cell><cell cols="2">GHVAEs 95.SVG' [9] 1217.3 [9]</cell><cell>15.0 [9]</cell><cell cols="2">41.9 [9] 0.327?0.003 6.7%</cell></row><row><cell>Human3.6M</cell><cell cols="5">GHVAEs 355.2?2.9 26.7?0.2 94.6?0.5 0.018?0.002 86.6% SVG' [9] 429.9 [9] 23.8 [9] 88.9 [9] 0.028?0.006 13.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>GHVAE vs. Hier-VRNN test performance onCityScapes (mean ? standard error). All convolutional layers in the 6-module GHVAE are downsized by 40% to fit into 16GB GPU memory for fair comparison.</figDesc><table><row><cell>Method</cell><cell>FVD ?</cell><cell>SSIM ?</cell><cell>LPIPS ?</cell></row><row><cell>GHVAEs</cell><cell cols="3">418.0?5.0 74.0?0.4 0.193?0.014</cell></row><row><cell cols="3">Hier-VRNN [11] 567.5 [11] 62.8</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation 1: GHVAEs improve monotonically from 2, to 4, and to 6 modules when greedily optimized.</figDesc><table><row><cell># of</cell><cell cols="3">RoboNet Video Prediction Test Performance</cell></row><row><cell cols="2">Modules FVD ?</cell><cell>PSNR ? SSIM ?</cell><cell>LPIPS ?</cell></row><row><cell>6</cell><cell cols="3">95.2?2.6 24.7?0.2 89.1?0.4 0.036?0.001</cell></row><row><cell>4</cell><cell cols="3">151.2?2.3 24.2?0.1 87.5?0.4 0.059?0.006</cell></row><row><cell>2</cell><cell cols="3">292.4?11.1 23.5?0.2 86.4?0.2 0.106?0.010</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation 2: On RoboNet, GHVAEs perform better when optimized greedily than when trained end-to-end. RoboNet Video Prediction Test Performance FVD ? PSNR ? SSIM ? LPIPS ? End-to-end Training 509.9?6.2 21.2?0.3 83.5?1.0 0.148?0.004 Greedy Training 95.2?2.6 24.7?0.2 89.1?0.4 0.036?0.001 Greedy Training + 91.1?3.1 25.0?0.2 89.5?0.5 0.032?0.003 End-to-End Fine-tuning tonic improvement by simply adding more GHVAE modules, as suggested by Theorem 2. We observe in</figDesc><table><row><cell>Optimization</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation 3: Train vs. test performance for a 6module GHVAE. We observe slight overfitting in all datasets except RoboNet. RoboNet Train 94.4?3.9 24.9?0.3 89.3?0.7 0.036?0.002 Test 95.2?2.6 24.7?0.2 89.1?0.4 0.036?0.001 KITTI Train 453.5?12.5 19.4?0.2 61.4?1.6 0.209?0.006 Test 552.9?21.2 15.8?0.1 51.2?2.4 0.286?0.015 Human Train 258.9?6.8 28.6?0.3 96.4?0.1 0.015?0.002 3.6M Test 355.2?2.9 26.7?0.2 94.6?0.5 0.018?0.002 Cityscapes Train 401.8?5.4 25.2?0.1 74.9?0.1 0.194?0.006 Test 418.0?5.0 25.0?0.1 74.0?0.4 0.193?0.014</figDesc><table><row><cell>Dataset</cell><cell>Train / Test</cell><cell>Video Prediction Performance FVD ? PSNR ? SSIM ? LPIPS ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation 4: Using a learned prior in GHVAEs substantially outperforms a uniform prior particularly in action-conditioned video prediction.</figDesc><table><row><cell>Dataset</cell><cell>Learned / Uniform</cell><cell>Video Prediction Test Performance FVD ? PSNR ? SSIM ? LPIPS ?</cell></row></table><note>RoboNet Learned 95.2?2.6 24.7?0.2 89.1?0.4 0.036?0.001 Uniform 281.4?1.6 22.1?0.3 85.0?0.4 0.58?0.007 KITTI Learned 552.9?21.2 15.8?0.1 51.2?2.4 0.286?0.015 Uniform 823.3?12.0 13.0?0.2 46.9?0.3 0.291?0.005 Human Learned 355.2?2.9 26.7?0.2 94.6?0.5 0.018?0.002 3.6M Uniform 391.6?11.1 26.3?0.3 93.0?0.3 0.021?0.002 Cityscapes Learned 418.0?5.0 25.0?0.1 74.0?0.4 0.193?0.014 Uniform 495.2?1.8 24.7?0.1 69.1?0.4 0.220?0.005</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>GHVAE vs. SVG' real robot performance</figDesc><table><row><cell>Method</cell><cell cols="2">Test Task Success Rate Pick&amp;Wipe Tasks Pick&amp;Sweep Tasks</cell></row><row><cell>GHVAEs</cell><cell>90.0%</cell><cell>85.0%</cell></row><row><cell>SVG'</cell><cell>50.0%</cell><cell>50.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Memory Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 A.1.1 GHVAE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 A.1.2 Other Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . 13 A.2. Intuition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . 13 Video Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 B.1.1 Visualizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 B.1.2 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 B.1.3 Human Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 B.1.4 Ablation for Encoder-Decoder Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 B.1.5 Ablation for Single vs. Multiple Latents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 B.2. Real Robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.2.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.2.2 Training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B.2.3 Task Execution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B.2.4 Task Diversity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B.2.5 Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19</figDesc><table><row><cell>1. Introduction</cell><cell>1</cell></row><row><cell>2. Related Work</cell><cell>2</cell></row><row><cell>3. Greedy Hierarchical VAEs (GHVAEs)</cell><cell>3</cell></row><row><cell>4. Experimental Evaluation and Analysis</cell><cell>5</cell></row><row><cell>5. Conclusion</cell><cell>8</cell></row><row><cell>A . Method</cell><cell>12</cell></row><row><cell>A.1. B . Experiments</cell><cell>14</cell></row><row><cell>B.1.</cell><cell></cell></row></table><note>C . Mathematical Proofs</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 Table 8 :</head><label>88</label><figDesc>quantifies the amount of GPU or TPU memory saved for 1 to 6-module GHVAE models. This table indicates that the memory savings of a GHVAE model increases as the number of modules increases. End Training Memory Usage (GB) 3.44 4.63 5.79 13.57 19.99 28.34 Greedy Training Memory Usage (GB) 3.44 3.46 4.23 9.20 13.60 17.05 Memory Saved (% Greedy Training Memory) 0% 33.8% 36.9% 47.5% 47.0% 66.2% GPU or TPU Memory Usage of GHVAE Models. All numbers are computed on a batch size of 1 per GPU, a rollout horizon of 10, two context frames, and 64 ? 64 ? 3 image observations.</figDesc><table><row><cell>Model Parameter</cell><cell></cell><cell></cell><cell></cell><cell>Value</cell><cell></cell><cell></cell></row><row><cell>Number of Modules K</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell>End-to-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>where L k e2e (x t+1 ) is GHVAE's ELBO for timestep t + 1 when optimized end-to-end. More formally, L k e2e (x t+1 ) is L k greedy (x t+1 ) in Eq. 2, except that the VAE model p k ? p W 1...k?1,k</figDesc><table><row><cell></cell><cell></cell><cell>and the variational distribution q k ?</cell></row><row><cell>enc,post q W 1...k?1,k</cell><cell>.</cell><cell>enc,dec,prior</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">[87] C.Ionescu, D. Papava, V. Olaru, and SminchisescuCristian, "Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 7, pp. 1325-1339, 2014. 6</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by ONR grant N00014-20-1-2675. SN was supported by an NSF graduate research fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Visual foresight: Model-based deep reinforcement learning for vision-based robotic control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00568</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Language models are few-shot learners,&quot; 2020. 1</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical structure in perceptual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="441" to="474" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Stochastic adversarial video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01523</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Stochastic video generation with a learned prior,&quot; ser. Proceedings of Machine Learning Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<editor>J. Dy and A. Krause</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robustness via retrying: Closed-loop robotic manipulation with self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High fidelity video prediction with large stochastic recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How to train deep variational autoencoders and probabilistic ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved conditional vrnns for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Towards accurate generative models of video: A new metric &amp; challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01717</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning predictive models of a depth camera &amp; manipulator from raw execution traces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Byravan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4021" to="4028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep visual foresight for planning robot motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2786" to="2793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video pixel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1771" to="1779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised visual planning with temporal skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improvisation through physical understanding: Using novel objects as tools with visual foresight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual robot task planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Barnoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Katyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8832" to="8838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical foresight: Self-supervised learning of long-horizon tasks via visual subgoal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Trass: Time reversal as self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="115" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Survey of model-based reinforcement learning: Applications on robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Polydoros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nalpantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent &amp; Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="173" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Model-based and model-free reinforcement learning for visual servoing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farahmand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shademan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesv?ri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2917" to="2924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Solar: Deep structured representations for modelbased reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7444" to="7453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Actionconditional video prediction using deep networks in atari games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2863" to="2871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vunet: Dynamic scene view synthesis for traversability estimation using an rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2062" to="2069" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep visual mpc-policy learning for navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3184" to="3191" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Action-conditioned benchmarking of robotic video prediction models: a comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santos-Victor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8316" to="8322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dual motion gan for future-flow embedded video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1744" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video prediction via selective sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1705" to="1715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to decompose and disentangle representations for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sdc-net: Video prediction using spatially-displaced convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="718" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structure preserving video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1460" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Compositional video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="page" from="10" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dense optical flow prediction from a static image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2443" to="2451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Se3-nets: Learning rigid body motion using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Byravan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generating the future with adversarial transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1020" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Transformation-based models of video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08435</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4463" to="4471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video imagination from a single image with transformation generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the on Thematic Workshops of ACM Multimedia</title>
		<meeting>the on Thematic Workshops of ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="358" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Flexible spatiotemporal networks for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6523" to="6531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stochastic video prediction with conditional density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brofos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Action and Anticipation for Visual Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hierarchical long-term video prediction without supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wichers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Stochastic latent residual video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Delasalles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09219</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robonet: Largescale multi-robot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schmeckpeper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRL</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Roboturk: A crowdsourcing platform for robotic skill learning through imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mandlekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Booher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Orbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pixelcnn++: A pixelcnn implementation with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Reasoning about physical interactions with object-oriented prediction and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multiobject representation learning with iterative variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2424" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Cobra: Data-efficient model-based rl through unsupervised object discovery and curiosity-driven exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09275</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Entity abstraction in visual model-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Veerapaneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1439" to="1456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Contrastive learning of structured world models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Der Pol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Genesis: Generative scene inference and sampling with object-centric latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Goal-aware prediction: Learning to model what matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Efficient greedy learning of gaussian mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kr?se</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="469" to="485" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Latent space policies for hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02808</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Greedy layerwise learning can scale to imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Belilovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eickenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sideways: Depth-parallel training of video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Swirszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Stacked convolutional auto-encoders for hierarchical feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cire?an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Coarse-to-fine auto-encoder networks (cfan) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Static hand gesture recognition using stacked denoising sparse autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Nandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 Seventh International Conference on Contemporary Computing (IC3)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="99" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Classification of human actions using posebased features and stacked auto encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Ijjina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="268" to="277" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal representation for detection of road accidents using stacked autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Mohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="879" to="887" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Robust feature learning by stacked autoencoder with maximum correntropy criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6716" to="6720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Putting an end to end-to-end: Gradient-isolated learning of representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Veeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3039" to="3051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Greedy infomax for self-supervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Veeling</surname></persName>
		</author>
		<idno>2019. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Ladder variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3738" to="3746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Towards deeper understanding of variational autoencoding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08658</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Nvae: A deep hierarchical variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03898,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Learning hierarchical features from generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Biva: A very deep hierarchy of latent variables for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Li?vin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6551" to="6562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>arxiv:2006.11239, 2020. 3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
