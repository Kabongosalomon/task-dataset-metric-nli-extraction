<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Complex Systems Modeling and Simulation</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Complex Systems Modeling and Simulation</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yu</surname></persName>
							<email>yuting@hdu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Complex Systems Modeling and Simulation</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
							<email>yzhuang@zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au</email>
							<affiliation key="aff2">
								<orgName type="department">SIT, FEIT</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent developments in modeling language and vision have been successfully applied to image question answering. It is both crucial and natural to extend this research direction to the video domain for video question answering (VideoQA). Compared to the image domain where large scale and fully annotated benchmark datasets exists, VideoQA datasets are limited to small scale and are automatically generated, etc. These limitations restrict their applicability in practice. Here we introduce ActivityNet-QA, a fully annotated and large scale VideoQA dataset. The dataset consists of 58,000 QA pairs on 5,800 complex web videos derived from the popular ActivityNet dataset. We present a statistical analysis of our ActivityNet-QA dataset and conduct extensive experiments on it by comparing existing VideoQA baselines. Moreover, we explore various video representation strategies to improve VideoQA performance, especially for long videos. The dataset is available at https://github.com</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recent developments in deep neural networks have significantly accelerated the performance of many computer vision and natural language processing tasks. These advances stimulated research into bridging the semantic connections between vision and language, such as in visual captioning <ref type="bibr" target="#b1">(Donahue et al. 2015;</ref><ref type="bibr" target="#b13">Xu et al. 2015)</ref>, visual grounding <ref type="bibr" target="#b1">Chen, Kovvuri, and Nevatia 2017;</ref>) and visual question answering <ref type="bibr" target="#b8">(Malinowski, Rohrbach, and Fritz 2015;</ref><ref type="bibr">Fukui et al. 2016)</ref>.</p><p>Visual question answering (VQA) aims to generate natural language answers to free-form questions about a visual object (e.g., an image or a video). Compared to visual captioning, VQA is interactive and provides fine-grained visual understanding. Image question answering (ImageQA) in particular has shown recent success, with many approaches proposed to investigate the key components of this task, e.g., discriminative feature representation <ref type="bibr" target="#b0">(Anderson et al. 2018)</ref>, multi-modal fusion <ref type="bibr" target="#b15">Yu et al. 2018a</ref>) and visual reasoning <ref type="bibr" target="#b9">(Nam, Ha, and Kim 2016;</ref><ref type="bibr" target="#b5">Johnson et al. 2017b</ref>). This success has <ref type="figure">Figure 1</ref>: A VideoQA example. To answer the question correctly, one should fully understand the fine-grained semantics of the questions (i.e., the underlined keywords) and perform spatio-temporal reasoning on the visual contents of the video (i.e., frames in red border and objects in blue box).</p><p>been facilitated by large scale and well annotated training datasets, such as Visual Genome <ref type="bibr" target="#b5">(Krishna et al. 2016</ref>) and VQA <ref type="bibr" target="#b1">(Antol et al. 2015;</ref><ref type="bibr" target="#b2">Goyal et al. 2017)</ref>.</p><p>Video question answering (VideoQA) can be seen as a natural but more challenging extension of ImageQA, due to the additional complexity of understanding of image sequences and more diverse types of questions asked. <ref type="figure">Figure  1</ref> shows an example of VideoQA. To accurately answer the question, a VideoQA model requires simultaneous finegrained video content understanding and spatio-temporal reasoning. Existing approaches mainly focus on the temporal attention mechanism <ref type="bibr" target="#b4">(Jang et al. 2017;</ref><ref type="bibr" target="#b13">Xu et al. 2017)</ref> or memory mechanism <ref type="bibr" target="#b9">(Na et al. 2017;</ref><ref type="bibr" target="#b5">Kim et al. 2017b;</ref>. Na et al. introduced a read-write memory network to fuse multi-modal features and store temporal information using a multi-stage convolutional neural networks model <ref type="bibr" target="#b9">(Na et al. 2017</ref>  segmentation, and devised a reinforced decoder to generate the answer for long videos ).</p><p>As noted above, high-quality datasets are of considerable value for VQA research. Several VideoQA datasets have been compiled for different scenarios, such as MovieQA <ref type="bibr" target="#b12">(Tapaswi et al. 2016)</ref>, TGIF-QA <ref type="bibr" target="#b4">(Jang et al. 2017)</ref>, MSVD-QA, <ref type="bibr">MSRVTT-QA (Xu et al. 2017)</ref>, and Video-QA <ref type="bibr" target="#b16">(Zeng et al. 2017</ref>). Most of these VideoQA datasets exploit video source data from other datasets and then add questionanswer pairs to them. The detailed statistics of these datasets are listed in <ref type="table" target="#tab_1">Table 1</ref>. We can see that these existing datasets are imperfect and have at least one of the following limitations:</p><p>? The datasets are small scale. Without sufficient training samples, the obtained model suffers from under-fitting. Without sufficient testing samples, the evaluated results are unreliable. ? The questions and answers are automatically generated by algorithms (e.g., obtained from the captioning results or narrative descriptions using off-the-shelf algorithms) rather than human annotation. Automatically generated question-answer pairs lack diversity, making the learned model easy to over-fit. ? The videos are short. The length of a video is closely related to the complexity of video content. Questions on short videos (e.g., less than 10 seconds) are usually too easy to answer making it difficult distinguish the performance of different VideoQA approaches on the dataset. ? The videos represent a small number of activities. This severely restricts the generalizability of the VideoQA models trained on these datasets and poorly reflects model performance in real-world use.</p><p>In this paper, we construct a new benchmark dataset ActivityNet-QA for evaluating VideoQA performance. Our dataset exploits 5,800 videos from the ActivityNet dataset, which contains about 20,000 untrimmed web videos representing 200 action classes (Fabian Caba Heilbron and Niebles 2015). We annotate each video with ten questionanswer pairs using crowdsourcing to finally obtain 58,000 question-answer pairs. Compared with other VideoQA datasets, ActivityNet-QA is of large scale, fully annotated by humans, and with very long videos. To better understand the properties of ActivityNet-QA, we present statistical and visualization analyses. We further conduct experiments on ActivityNet-QA and compare results produced by existing VideoQA baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ActivityNet-QA Dataset</head><p>We first introduce the ActivityNet-QA dataset from three perspectives, namely video collection, QA generation, and statistical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Collection</head><p>We first collect videos for the dataset. Due to time limitation, we are unable to annotate every video in ActivityNet. Instead, we sample 5,800 videos from the 20,000 videos in Ac-tivityNet dataset. Specifically, we sample 3,200/1,800/800 videos from the original train/val/test splits of ActivityNet respectively. Moreover, we take class diversity and balance into consideration. Since the videos in the train and val splits of ActivityNet possess class labels, we use this information as a prior to guide sampling and force a uniform distribution of class labels in the sampled videos. The class information is not available for the test split, so we adopt a simple random sampling strategy instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QA Generation</head><p>As the videos are collected, we generate QA pairs for each video. To reduce the labor costs, some VideoQA datasets exploit the narrative descriptions or captions of videos <ref type="bibr" target="#b4">(Jang et al. 2017;</ref><ref type="bibr" target="#b13">Xu et al. 2017)</ref>, to automatically generate QA pairs using off-the-shelf algorithms <ref type="bibr" target="#b10">(Ren, Kiros, and Zemel 2015)</ref>. However, since the textual descriptions contains relatively little information about the videos, these generated QA pairs lack diversity and are often redundant. Therefore, we generate QA pairs by human crowdsourcing.</p><p>To control the generated questions, we define three template question types and ask the annotators to cover all the three question types for every video. Beyond these three questions, annotators are free to ask arbitrary questions about the videos. The three question types are as follows:</p><p>Motion. This type of question interrogates coarse temporal action understanding. Compared with traditional action recognition, this task is more challenging in this setting. To correctly answer the question with respect to a long video, a VideoQA model needs to correctly localize the action referred to by the question. Spatial Relationship. This type of question tests spatial reasoning on one static frame. In contrast to the spatial reasoning in ImageQA , this task additionally examine the temporal attention ability to find to the frame from the whole video first. Temporal Relationship. As a companion to spatial relationship, this type of questions examines the ability of reasoning temporal relationships of objects from a sequence of frames.</p><p>As a prerequisite, one should find the related frames from the whole video first. For the free type questions, it is hard to classify them into non-overlapped types even for humans. Referring to the taxonomy in existing VQA datasets <ref type="bibr" target="#b10">(Ren, Kiros, and Zemel 2015;</ref><ref type="bibr" target="#b1">Antol et al. 2015)</ref>, we manually categorize the samples into the following six classes by their answer types: Yes/No, Number, Color, Object, Location and Other.</p><p>To control the quality of generated questions, the following practical principles are applied:</p><p>? Questions and answers that are too long are probably caused by improper representation. Therefore, we empirically restricted the maximum question length to 20 words and maximum answer length to 5 words. ? For each QA pair, the question annotator and answer annotator are separate. If the answer annotator regards the generated question unanswerable, this question is doublechecked and may have been further regenerated. Employing this strategy effectively improve question objectivity, which is important for obtaining high-quality annotations. ? A portion of questions are randomly selected and sent to multiple annotators. The multiple answers to one question are merged by majority voting. Employing this strategy reduced the probability of erroneous answers and evaluated annotator reliability.</p><p>The initial QA pairs are in Chinese, since our crowdsourcing platform is located in China. As the lengths of questions and answers are well controlled, the state-of-theart machine translation algorithms can easily translate them into English. To further improve the quality of the translated results, we use a novel strategy to automatically detect potential mistakes in the translated results. For each sentence in Chinese, we use the APIs from the four commercial translation engines of Google, Baidu, Sogou and Youdao to obtain four translated versions in English. We evaluate the average similarities of each two natural language sentences using CIDEr score (Vedantam, Lawrence Zitnick, and Parikh 2015), setting an empirical threshold to the average CIDEr score, and manually checking samples that did not reach the threshold. For the remaining samples that did reach the threshold, we use the results obtained from the Baidu translation engine.</p><p>To better understand the different question types and the quality of translations, some examples from our dataset are shown in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Analysis</head><p>Here we present the detailed statistics of our ActivityNet-QA dataset. The distributions of question and answer lengths are shown in <ref type="figure" target="#fig_0">Figure 2</ref>(a). As noted above, the maximum question length is 20 and the maximum answer length is 5 respectively (in English). The average QA lengths for all the question types are reported in <ref type="figure" target="#fig_0">Figure 2(b)</ref>. Regardless of question type, the average question length is 8.67 and average answer length is 1.85. Similar to <ref type="bibr" target="#b1">(Antol et al. 2015</ref>), the answer lengths in our dataset are relatively short. Short answers are easier to process, and one can simply treat the answering problem as multi-class classification.</p><p>We also investigate the distribution of the questions (  ure 2(c)). For each video, we generate exactly ten QA pairs, including one motion type question, one spatial relationship type question, and one temporal relationship type question, respectively. The remaining seven questions are classified as free type as they are generated without constraints. To eliminate the effect of the answer prior and improve the role of video understanding for Yes/No type questions, we balance the ratio of yes and no samples to make it close to one. To better understand the organization of the free type questions, the answer distribution is shown in <ref type="figure" target="#fig_0">Figure 2(d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>In this section, we explore the difficulty of the ActivityNet-QA dataset using several baseline models based on different types of video features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Feature Representation</head><p>Existing VideoQA approaches usually extract two-stream features from the motion channel and the appearance channel of video, respectively <ref type="bibr" target="#b13">(Xu et al. 2017</ref>).</p><p>Since the untrimmed videos are very long, we split the videos into small units, each unit containing 16 consecutive frames without overlap between any two units. The average number of units counted on all the videos is 270, which is still too large for existing VideoQA models. Besides, the number of units varies for different videos, further complicating model training. Therefore, we propose two alternative sampling strategies to sample a fixed number of units T for all videos: Fixed Stepsize (FS). For a video, assume it contains N units and we expect to output T units. This strategy evenly sample T units from the N units with a fixed stepsize N T . Dynamic Stepsize (DS). An untrimmed video may contain many worthless frames with little information. To make the sampled video units more discriminative, we propose a sampling strategy with dynamic stepsize such that the selected units have a high probability of containing meaningful actions. To achieve this, we first introduce an external temporal action proposal model (Fabian Caba Heilbron and Ghanem 2016) to generate a set of action proposals:</p><formula xml:id="formula_0">P = {(t start i , t end i , c i )}<label>(1)</label></formula><p>where each proposal p i ? P contains a start index t start i ? R, an end index t end i ? R and a confidence score c i ? R. For each video unit, we regard it as a bin in a histogram. If the duration of a proposal p i covers the video unit, its confidence score c i is added to this unit. After traversing the candidate set P , we obtain a histogram w.r.t the video units. By normalizing the histogram using the softmax function, we obtain an action score distribution over the video units indicating the probability that one unit has valid actions. Finally, we sample the units w.r.t the score distribution to obtain T units with dynamic stepsize.</p><p>As we have obtained the sampled video unit set, for each unit u i , we used the VGG-16 network pre-trained on the ImageNet dataset (Simonyan and Zisserman 2014) to extract the appearance feature (the fc7 feature x i ? R 4096 ) given the central frame of the unit, and the C3D network pretrained on the Sport-1M dataset <ref type="bibr" target="#b13">(Tran et al. 2015)</ref> to extract the motion features (the fc7 feature y i ? R 4096 ) given the whole 16 consecutive frames of the unit.</p><p>To fuse the two-stream features, we use three fusion strategies: Mean Pooling, Concat and Eltwise Product to obtain the 4096-D, 8192-D, and 4096-D fused visual features for each video unit, respectively. We then perform L 2 normalization on each fused visual feature.</p><p>The overall flowchart for video feature representation is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VideoQA Baselines</head><p>Based on the extracted visual features, we implement the following VideoQA baselines. Note that the focus of this paper is the constructed ActivityNet-QA dataset and a discussion of what influences the performance on the dataset. Therefore, we do not perform comparison with complex VideoQA models, such as <ref type="bibr" target="#b2">(Gao et al. 2018;</ref><ref type="bibr" target="#b13">Xu et al. 2017</ref>).</p><p>E-VQA is the extension of an ImageQA baseline <ref type="bibr" target="#b1">(Antol et al. 2015)</ref>, where one long-short term memory (LSTM) network <ref type="bibr" target="#b3">(Hochreiter and Schmidhuber 1997</ref>) is used to encode all words in the question and another different LSTM network is used to encode the frames in the video. The features of the question and videos are then fused into the joint feature representation with element-wise multiplication for answer prediction. E-MN is the extension of the end-to-end memory networks model <ref type="bibr" target="#b11">(Sukhbaatar et al. 2015)</ref> for ImageQA, where the bidirectional LSTM networks are used to update the frame representations of the video. The updated representations are mapped into the memory and the question representation is used to perform multiple inference steps to predict the answer.</p><p>E-SA is the extension of the soft attention model <ref type="bibr" target="#b14">(Yao et al. 2015)</ref> for ImageQA, where the question are fist encoded using a LSTM network. The encoded question feature is used to attend on features of video features. Finally, the question feature and weighted video feature are fused to predict the answer.</p><p>All the above baselines are trained in an end-to-end manner. Since they are decoupled from the video features, they can be flexibly combined with the video features obtained by different strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We evaluate the aforementioned VideoQA models on our ActivityNet-QA dataset. We use 3,200 videos and 32,000 corresponding QA pairs in the train split to train the models, and 1,800 videos and 18,000 corresponding QA pairs in the val split to tune hyper-parameters. We report the predicted results on 800 videos and 8,000 QA pairs in the test split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>We formulate the VideoQA problem as a multi-class classification problem with each class corresponding to an answer.</p><p>To generate the answer vocabulary, we choose the top 1,000 most frequent answers in the train split as the answer vocabulary, which covers 84.7% / 86.2% / 85.6% of the train/val/test answers, respectively. To generate the question vocabulary, we select the top 8,000 most frequent words from the questions in the train split. We take the token unk for out-of-vocabulary words. Since there are several video feature representation strategies, we use the FS sampling with T = 20 and the mean-pooling fusion strategies as the default options in the experiments unless otherwise stated.</p><p>We implement all the methods and train the models using TensorFlow. For all models, we use the Adam solver with a base learning rate ? = 0.001, ? 1 = 0.9, and ? 2 = 0.99 and train the models to up to 100 epochs with a batch size of 100. The early stopping strategy is used if the accuracy on the validation set does not improve for 10 epochs. All models use the pre-trained 300-dimensional GloVe embedding <ref type="bibr" target="#b9">(Pennington, Socher, and Manning 2014)</ref> to initialize the question embedding layer. For the models using LSTM networks, the number of LSTM hidden units is set to 300, and the common space dimension is set to 256 as suggested by <ref type="bibr" target="#b13">(Xu et al. 2017</ref>). The number of memory units for E-MN is set to 500 as suggested by <ref type="bibr" target="#b16">(Zeng et al. 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Criteria</head><p>We evaluate the performance using two common evaluation criteria for VideoQA, i.e., accuracy (Xu et al. 2017) and WUPS <ref type="bibr" target="#b7">(Malinowski and Fritz 2014)</ref>. For the QA pairs in the test set with size N , given any testing question q i ? Q and its corresponding ground-truth answer y i ? Y , we denote the predicted answer from the VideoQA model by a i . Note that a i or y i corresponds to a sentence which can be seen as a set of words. Based on the definition above, the two evaluation criteria are:</p><p>Accuracy is a criterion that is used to commonly used to measure the performance of classification tasks.</p><formula xml:id="formula_1">Accuracy = 1 N N i=1 1[a i = y i ]<label>(2)</label></formula><p>where 1[?] is an indicator function that accuracy of the sample is 1 only if a i and y i are identical, and 0 otherwise. WUPS is a generalization of the accuracy measure that accounts for word-level ambiguities in the answer words. The WUPS score with the threshold ? is given by</p><formula xml:id="formula_2">WUPS = 1 N N i=1 min{ w?a i max v?y i ??(w, v), v?y i max w?a i ??(w, v)}<label>(3)</label></formula><p>where w and v are the words in the each predicted answer and ground-truth answer respectively. ? ? (w, v) is given by</p><formula xml:id="formula_3">? ? (w, v) = WUP(w, v) if WUP(w, v) ? ? 0.1 ? WUP(w, v)</formula><p>otherwise (4) Following the setting in <ref type="bibr" target="#b8">(Malinowski, Rohrbach, and Fritz 2015)</ref>, we choose two thresholds ? = 0.0 and ? = 0.9 for calculating the WUPS score and denote them by WUPS@0.0 and WUPS@0.9, respectively.   <ref type="table" target="#tab_4">Table 3</ref> shows the performance of the baselines on all question types based on the two evaluation criteria. From these results, we can make the following observations: 1) all baselines significantly outperform the Q-type prior baseline with respect to the overall accuracy and WUPS scores, indicating that without understanding the visual content of videos, one cannot achieve good performance on our dataset; 2) the accuracy of the temporal relationship type is lower than the others. This can be explained by the fact that temporal reasoning over long videos is still not well solved by baselines, and there remains significant room for further improvement; 3) E-SA slightly outperforms E-VQA and E-MN both in terms of accuracy and WUPS, respectively. However, the overall performance is still far from satisfactory, reflecting the difficulty of the dataset. <ref type="table" target="#tab_5">Table 4</ref> provides the detailed accuracies for the free type questions. The accuracy for the Object class is lower than the others due to the diversity of possible answers. E-SA still outperforms the other two methods steadily, indicating its effectiveness in modeling temporal attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>We next investigate the effect of using different video feature representation strategies. Sampling strategies. The effect of using different sampling strategies for different methods is shown in <ref type="figure" target="#fig_2">Figure 4(a)</ref>. The results show that the performance of all baselines improved by at least 1% when the FS sampling is replaced with the DS sampling. This verifies that the action distribution is an important prior when extracting video features, especially when the videos are long. The sampled frames by the DS sampling can be seen as the key-frames, which better reflect fine-grained video semantics. To better understand the differences between the two sampling strategies, we visualize the video units (represented by their central frames) in <ref type="figure" target="#fig_3">Figure 5</ref>. It can be seen that the DS sampling obtains more representative and diverse video units compared to the FS sampling. Sampling frequencies. <ref type="figure" target="#fig_2">Figure 4(b)</ref> shows the effect of T ={20, 40, 60} for E-SA. As the sampling strategy is correlated with the sampling frequency, we report the accuracies with respect to different sampling strategies. The results show that as T increases, the performance gap between the fixed sampling and dynamic sampling narrows. This can be interpreted as denser sampling better preserve the detailed information in videos. Moreover, using the video features generated with dense sampling frequency (e.g., T =60) greatly increase the complexity of VideoQA models, leading to degraded performance. <ref type="figure">Figure 6</ref>: Visualizations of the results obtained by different methods. For each video example, we show the questions (Q), ground-truth answers (GT) and the predictions of different methods, respectively. The left column shows the examples that at least one method give correct predictions, while the right column shows the examples that all methods give wrong predictions.</p><formula xml:id="formula_4">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</formula><p>Fusion strategies. In <ref type="figure" target="#fig_2">Figure 4</ref>(c), we explore the effect of using different fusion strategies with (T ={20, 40, 60}) and FS sampling for E-SA. It can be seen that Mean Pooling achieves the best performance compared to other two fusion strategies in terms of accuracy and robustness.</p><p>For qualitative analysis, we present some successful and failed cases obtained by different methods in <ref type="figure">Figure 6</ref>. These methods show a greater probability of correctly answering questions that focused on static frame, but fail to answer the questions involving temporal reasoning. These observations are useful for guiding further improvements for VideoQA models in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>In this paper, we present a new large scale dataset ActivityNet-QA for understanding complex web videos by question answering. Compared with existing VideoQA datasets, our dataset is unique in that: 1) the videos originate from ActivityNet, a large-scale video understanding dataset with long web videos; 2) the QA pairs are fully annotated by crowdsourcing. To guarantee the quality of our dataset, we conduct significant pre-and post-processing by both algorithmic and human efforts. Based on the constructed dataset, we apply several baselines to analyze the difficulty of our dataset and also investigate the strategies to learn better video feature representation; and 3) the QA pairs of our dataset are bilingual with alignment. This property may inspire multi-lingual VideoQA studies.</p><p>Since the models studied here represent the baseline, there remains significant room for improvement. For example, by introducing a more advanced video feature representation model that can learn better discriminative visual features or introducing a more powerful VideoQA model that can perform accurate spatio-temporal reasoning. Furthermore, auxiliary information on ActivityNet, e.g., dense captions <ref type="bibr" target="#b5">(Krishna et al. 2017)</ref> can be utilized to help better understanding the fine-grained semantics of videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>distribution (d) Answer distribution for Free type The statistics of our ActivityNet-QA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The flowchart of video feature representation procedures, including video units generation (left), video units sampling (middle) and unit feature extraction (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Overall accuracies of different strategies in video feature representations. (a) sampling strategies w.r.t. different VideoQA methods; (b) sampling frequencies w.r.t. different sampling strategies for E-SA; (c) sampling frequencies w.r.t. different fusion strategies for E-SA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualizations of three video examples with two sampling strategies (DS sampling on the left and FS sampling on the right). Each row shows the sampled video units (represented by their central frames) for a video with sampling frequency T =5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). Xu et al. represented a video as appearance and motion stream features and introduced a gradually refined attention model to fuse the two-stream features together. (Xu et al. 2017). Gao et al. proposed a co-memory network to jointly model and interact with the motion and appearance information (Gao et al. 2018). Zhao et al. introduced an adaptive hierarchical encoder to learn the segment-level video representation with adaptive video</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of existing VideoQA datasets with ours (OE: open-ended, and MC: multiple-choice).</figDesc><table><row><cell>Datasets</cell><cell>Video source</cell><cell>QA pairs generation</cell><cell>QA tasks</cell><cell cols="2"># Videos # QA pairs</cell><cell>Average video length</cell></row><row><cell>MSVD-QA (Xu et al. 2017)</cell><cell>MSVD</cell><cell>Automatic</cell><cell>OE</cell><cell>1,970</cell><cell>50,505</cell><cell>10s</cell></row><row><cell>MSRVTT-QA (Xu et al. 2017)</cell><cell>MSRVTT</cell><cell>Automatic</cell><cell>OE</cell><cell>10,000</cell><cell>243,680</cell><cell>15s</cell></row><row><cell>TGIF-QA (Jang et al. 2017)</cell><cell>TGIF</cell><cell cols="2">Automatic &amp; Human OE &amp; MC</cell><cell>56,720</cell><cell>103,919</cell><cell>3s</cell></row><row><cell>MovieQA (Tapaswi et al. 2016)</cell><cell>Movies</cell><cell>Human</cell><cell>MC</cell><cell>6,771</cell><cell>6,462</cell><cell>200s</cell></row><row><cell>Video-QA (Zeng et al. 2017)</cell><cell>Jukinmedia</cell><cell>Automatic</cell><cell>OE</cell><cell>18,100</cell><cell>174,775</cell><cell>45s</cell></row><row><cell>ActivityNet-QA (Ours)</cell><cell>ActivityNet</cell><cell>Human</cell><cell>OE</cell><cell>5,800</cell><cell>58,000</cell><cell>180s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Examples of questions in different types.</figDesc><table><row><cell>Types</cell><cell>Questions</cell></row><row><cell></cell><cell>What are the person wearing earphones doing?</cell></row><row><cell>Motion</cell><cell>What are people doing at the beginning of the video?</cell></row><row><cell></cell><cell>What is person wearing red t-shirt doing?</cell></row><row><cell></cell><cell>What is on the left of the lawn?</cell></row><row><cell>Spat. Rel.</cell><cell>What is on the left side of the man on his knees?</cell></row><row><cell></cell><cell>What is behind of the person sitting in the video?</cell></row><row><cell></cell><cell>What happened to the person in black before falling down?</cell></row><row><cell>Temp. Rel.</cell><cell>What happened to the woman before drying her hair?</cell></row><row><cell></cell><cell>What happened to the person before playing violin?</cell></row><row><cell></cell><cell>How many people are there in the video? [Number]</cell></row><row><cell></cell><cell>Is the athlete in the room? [Yes/No]</cell></row><row><cell>Free</cell><cell>What are the animals that appear in the video? [Object]</cell></row><row><cell></cell><cell>What is the color of the person's pants? [Color]</cell></row><row><cell></cell><cell>Where is the person in a black coat? [Location]</cell></row><row><cell></cell><cell>What is the gender of the athlete? [Other]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The accuracies of the methods in different question types. Q-type prior denotes a simple baseline using the most popular answer per question type as the prediction.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Accuracy (%)</cell><cell></cell><cell></cell><cell cols="2">WUPS (%)</cell></row><row><cell>Methods</cell><cell cols="4">Motion Spat. Rel. Temp. Rel. Free</cell><cell>All</cell><cell cols="2">WUPS@0.9 WUPS@0.0</cell></row><row><cell>Q-type prior</cell><cell>2.9</cell><cell>5.8</cell><cell>1.4</cell><cell>19.7</cell><cell>14.8</cell><cell>16.4</cell><cell>35.1</cell></row><row><cell>E-VQA</cell><cell>2.5</cell><cell>6.6</cell><cell>1.4</cell><cell>34.4</cell><cell>25.1</cell><cell>29.3</cell><cell>53.5</cell></row><row><cell>E-MN</cell><cell>3.0</cell><cell>8.1</cell><cell>1.6</cell><cell>36.9</cell><cell>27.1</cell><cell>31.5</cell><cell>55.9</cell></row><row><cell>E-SA</cell><cell>12.5</cell><cell>14.4</cell><cell>2.5</cell><cell>41.2</cell><cell>31.8</cell><cell>34.9</cell><cell>56.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The detailed accuracies of the Free type questions.</figDesc><table><row><cell></cell><cell cols="5">Y/N Color Obj. Loc. Num. Other</cell></row><row><cell cols="2">E-VQA 52.7 27.3</cell><cell>7.9</cell><cell>8.8</cell><cell>44.2</cell><cell>20.6</cell></row><row><cell>E-MN</cell><cell cols="4">55.1 28.0 12.0 12.2 44.4</cell><cell>24.2</cell></row><row><cell>E-SA</cell><cell cols="4">59.4 29.8 14.2 25.9 44.6</cell><cell>28.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<editor>CVPR. [Fabian Caba Heilbron and Niebles 2015] Fabian Caba Heilbron, Victor Escorcia, B. G., and Niebles, J. C.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
	<note>CVPR. Fukui et al. 2016] Fukui, A.; Park, D. H.; Yang, D.; Rohrbach, A.; Darrell, T.; and Rohrbach, M.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
	</analytic>
	<monogr>
		<title level="m">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1988" to="1997" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepstory: Video story qa by deep embedded memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07332</idno>
	</analytic>
	<monogr>
		<title level="m">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<editor>ICLR. [Kim et al. 2017b</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1682" to="1690" />
		</imprint>
	</monogr>
	<note>Malinowski and Fritz</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohrbach</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz ; Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A read-write memory network for movie story understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00471</idno>
	</analytic>
	<monogr>
		<title level="m">Dual attention networks for multimodal reasoning and matching</title>
		<imprint>
			<publisher>Pennington, Socher, and Manning</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiros</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2953" to="2961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
	</analytic>
	<monogr>
		<title level="m">Very deep convolutional networks for largescale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Movieqa: Understanding stories in movies through question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tapaswi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4631" to="4640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
		<idno>Xu et al. 2015</idno>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1645" to="1653" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4507" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering. ICCV 1821-1830</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2018.2817340</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering. Yu et al. 2018b. Rethinking diversified and discriminative proposal generation for visual grounding. IJCAI 1114-1120</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Leveraging video descriptions to learn video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4334" to="4340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-turn video question answering via multi-stream hierarchical attention context network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3690" to="3696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Open-ended longform video question answering via adaptive hierarchical reinforced networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3683" to="3689" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
