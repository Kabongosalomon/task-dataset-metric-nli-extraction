<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">END-TO-END AUDIO-VISUAL SPEECH RECOGNITION WITH CONFORMERS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">END-TO-END AUDIO-VISUAL SPEECH RECOGNITION WITH CONFORMERS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-audio-visual speech recognition</term>
					<term>end-to-end training</term>
					<term>convolution-augmented transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present a hybrid CTC/Attention model based on a ResNet-18 and Convolution-augmented transformer (Conformer), that can be trained in an end-to-end manner. In particular, the audio and visual encoders learn to extract features directly from raw pixels and audio waveforms, respectively, which are then fed to conformers and then fusion takes place via a Multi-Layer Perceptron (MLP). The model learns to recognise characters using a combination of CTC and an attention mechanism. We show that end-to-end training, instead of using pre-computed visual features which is common in the literature, the use of a conformer, instead of a recurrent network, and the use of a transformer-based language model, significantly improve the performance of our model. We present results on the largest publicly available datasets for sentence-level speech recognition, Lip Reading Sentences 2 (LRS2) and Lip Reading Sentences 3 (LRS3), respectively. The results show that our proposed models raise the state-of-the-art performance by a large margin in audio-only, visual-only, and audio-visual experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Audio-Visual Speech Recognition (AVSR) is the task of transcribing text from audio and visual streams, which has recently attracted a lot of research attention due to its robustness against noise. Since the visual stream is not affected by the presence of noise, an audiovisual model can lead to improved performance over an audio-only model as the level of noise increases.</p><p>Traditional audio-visual speech recognition methods follow a two-step approach, feature extraction and recognition <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>. Several End-to-End (E2E) approaches have been recently presented by combining feature extraction and recognition inside a deep neural network, and this has led to a significant improvement in Visual Speech Recognition (VSR) and Automatic Speech Recognition (ASR), respectively. In VSR, Assael et al. <ref type="bibr" target="#b3">[4]</ref> developed the first end-to-end network based on 3D convolution with Gated Recurrent Units (GRUs) for recognising visual speech on GRID <ref type="bibr" target="#b5">[6]</ref>. Shillingford et al. <ref type="bibr" target="#b26">[27]</ref> proposed an improved version of the model called Vision to Phoneme (V2P) which predicts phoneme distributions, instead of characters, from video clips. Chung and Zisserman <ref type="bibr" target="#b4">[5]</ref> developed an attention-based sequence-to-sequence model for VSR in-the-wild. Zhang et al. <ref type="bibr" target="#b35">[36]</ref> proposed a Temporal Focal block to capture temporal dynamics locally in a convolution-based sequence-to-sequence model. In ASR, <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref> have been recently shown to achieve better recognition performance by replacing the hand-crafted features such as log-Mel filter-bank features with deep representations from networks.</p><p>Several audio-visual approaches have been recently presented where pre-computed visual or audio features are used <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref>. Afouras et al. developed a transformer-based sequence-tosequence model by using pre-computed visual features and log-Mel filter-bank features as inputs. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref> focus on using video clips and log-Mel filter-bank features as inputs to train an audio-visual speech recognition model in an end-to-end manner. Few audiovisual studies are truly E2E, in the sense that they are trained with raw pixels and audio waveforms <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref>. In particular, <ref type="bibr" target="#b23">[24]</ref> was applied only to word classification while <ref type="bibr" target="#b16">[17]</ref> was tested on a constrained environment.</p><p>In this work, we extend our previous audio-visual model presented in <ref type="bibr" target="#b24">[25]</ref> to an end-to-end model, which extracts features directly from raw pixels and audio waveform, and introduce a few changes which significantly improve the performance. In particular, we integrate the feature extraction stage with the hybrid CTC/attention back-end and train the model jointly. This results in a significant improvement in performance. We also replace the recurrent networks with conformers, which further push the state-ofthe-art performance. Finally, we replace the RNN-based Language Model (RNN-LM) with a transformer-based LM which enhances the performance even more. We also perform a comparison between audio-only models trained with log-Mel filter-bank features and raw waveforms. Although in clean conditions they both perform similarly, the raw audio model performs slightly better in noisy conditions. We evaluate the proposed architecture on the largest inthe-wild audio-visual speech datasets, LRS2 and LRS3. The stateof-the-art performance is raised by a large margin for audio-only, visual-only and audio-visual experiments on both datasets, even outperforming methods trained on much larger external datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DATASETS</head><p>For the purpose of this study, we use two large-scale publicy available audio-visual datasets, LRS2 <ref type="bibr" target="#b4">[5]</ref> and LRS3 <ref type="bibr" target="#b2">[3]</ref>. Both datasets are very challenging as there are large variations in head pose and illumination. LRS2 <ref type="bibr" target="#b4">[5]</ref> consists of 224.1 hours with 144 482 video clips from BBC programs. In particular, there are 96 318 utterances for pre-training (195 hours), 45 839 for training (28 hours), 1 082 for validation (0.6 hours), and 1 243 for testing (0.5 hours).</p><p>LRS3 <ref type="bibr" target="#b2">[3]</ref> collected from TED and TEDx talks is twice as large as the LRS2 dataset. LRS3 contains 151 819 utterances (438.9 hours). Specifically, there are 118 516 utterances in the pretraining set (408 hours), 31 982 utterances in the training-validation set (30 hours) and 1 321 utterances in the test set (0.9 hours).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ARCHITECTURE</head><p>The proposed architecture for audio-visual speech recognition is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The encoder of the audio-visual model is comprised of three components, the front-end, the back-end, and the fusion modules, as explained below. Front-end The acoustic and visual front-ends architectures are shown in <ref type="table" target="#tab_0">Table 1</ref>. For the visual stream, we use a modified ResNet-18 <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref> in which the first convolutional layer is replaced by a 3D convolutional layer with a kernel size 5 ? 7 ? 7. The visual features at the end of the residual block are squeezed along the spatial dimension by a global average pooling layer. For the audio stream, we use a ResNet-18 based on 1D convolutional layers, where the filter size at the first convolutional layer is set to 80 (5ms). To downsample the time-scale, the stride is set to 2 at every block. The only exception is the first block, where we set the stride to 4. At the end of the front-end module, acoustic features are down-sampled to 25 frames per second so the match the frame rate of the visual features. Back-end We use the recently proposed conformer encoder <ref type="bibr" target="#b9">[10]</ref> as the back-end for temporal modeling. It is comprised of an embedding module, followed by a set of conformer blocks. In the embedding module, a linear layer projects the features from ResNet-18 to a d k -dimensional space. The projected features are encoded with relative position information <ref type="bibr" target="#b6">[7]</ref>. In each conformer block, a feed-forward module, a Multi-Head Self-Attention (MHSA) module, a convolutional module, and a feed-forward module are stacked in order. In particular, the feed-forward module is composed of a d ff -dimensional linear layer, followed by Rectified Linear Units (ReLU), a dropout layer, and a second linear layer with an output size of d k . The MHSA module receives queries Q, keys K, and val-</p><formula xml:id="formula_0">ues V as inputs, where Q ? R T ?d k , K ? R T ?d k , and V ? R T ?dv ,</formula><p>T denotes the sequence length and d k and dv are the dimensions for queries/keys and values, respectively. Suppose Q = K = V in the encoder and W Q i , W K i and W V i are denoted as the weights of linear transformation for Q, K and V, respectively, the matrix of outputs at i-th head self-attention is computed through Scaled Dot-Product Attention <ref type="bibr" target="#b30">[31]</ref>:</p><formula xml:id="formula_1">fi(Q i , K i , V i ) = softmax(Q i K T i )/d 0.5 k V i , where Q i = QW Q i , K i = KW K i , V i = V W V i .</formula><p>The convolutional module contains a point-wise convolutional layer with an expansion factor of 2, followed by Gated Linear Units (GLU) <ref type="bibr" target="#b7">[8]</ref>, a temporal depth-wise convolutional layer, a batch normalisation layer, a swish activation layer, a point-wise convolutional layer, and a layer normalisation layer. This combination has been shown to improve ASR performance compared to the transformer architecture as it better captures temporal information locally and globally <ref type="bibr" target="#b9">[10]</ref>. Fusion Layers The acoustic and visual features from the backend modules are then concatenated and projected to d k -dimensional space by an MLP. The MLP is composed of a linear layer with an output size of 4?d k followed by a batch normalization layer, ReLU, and a final linear layer with an output dimension d k .</p><p>Decoder We use the transformer decoder proposed in <ref type="bibr" target="#b30">[31]</ref>, which is composed of an embedding module, followed by a set of multihead self-attention blocks. In the embedding module, a sequence of the prefixes from index 1 to l ? 1 is projected to embedding vectors, where l is the target length index. The absolute positional encoding <ref type="bibr" target="#b30">[31]</ref> is also added to the embedding. A self-attention block is comprised of two attention modules and a feed-forward module. Specifically, the first self-attention module uses Q = K = V as input and future positions at its attention matrix are masked out. The second attention module uses the features from the previous self-attention module as Q and the representations from the encoder as K and V (K = V ). The component in the feed-forward module is the same as in the encoder. Loss functions Let x = [x1, ..., xT ] and y = [y1, ..., yL] be the input sequence and target symbols, respectively, with T and L representing the input and target lengths, respectively. Recent works in audio-visual speech recognition rely mostly on CTC <ref type="bibr" target="#b16">[17]</ref> or attention-based models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> for audio-visual recognition. CTC loss assumes conditional independence between each output prediction and has a form of pCTC(y|x) ? T t=1 p(yt|x). An attention-based model gets rid of this assumption by directly estimating the posterior on the basis of the chain rule, which has a form of pCE(y|x) = L l=1 p(y l |y &lt;l , x). In this work, we adopt a hybrid CTC/Attention architecture <ref type="bibr" target="#b31">[32]</ref> to force monotonic alignments and at the same time get rid of the conditional independence assumption. The objective function is computed as follows: L = ?logpCTC(y|x)+(1??)logpCE(y|x) (1) where ? controls the relative weight in CTC and attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>Pre-processing In each video, 68 facial landmarks are detected and tracked using dlib <ref type="bibr" target="#b13">[14]</ref>. To remove differences related to rotation and scale, the faces are aligned to a neural reference frame using a similarity transformation. A bounding box of 96 ? 96 is used to crop the mouth ROIs. The cropped patch is further converted to gray-scale and normalised with respect to the overall mean and variance on the training set. Each raw audio waveform is normalised by removing its mean and dividing by its standard deviation. Data augmentation Following <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28]</ref>, random cropping with a size of 88 ? 88 and horizontal flipping with a probability of 0.5 are performed for each image sequence. For each audio waveform, additive noise, time masking, and band reject filtering are performed in the time domain. Babble noise from the NOISEX corpus <ref type="bibr" target="#b29">[30]</ref> is added to the original audio clip with an SNR level from [-5 dB, 0 dB, 5 dB, 10 dB, 15 dB, 20 dB]. The selection of one of the noise levels or the use of a clean waveform is done using a uniform distribution. Similarly to <ref type="bibr" target="#b12">[13]</ref>, 2 sets of consecutive audio samples with a maximum length of 0.4 seconds are set to zero and 2 sets of consecutive frequency bands with a maximum width of 150 Hz are rejected. In audio-only experiments, we add speed perturbation by setting the speed between 0.9 and 1.1. Experimental settings The network is initialised randomly, with the exception of the front-end modules in the encoder part, which in some experiments are initialised based on the publicly available pre-trained models on LRW [18] 1 . The back-end modules use a set of hyper-parameters (e = 12, d ff = 2048, d k = 256, d v = 256), where e denotes the number of conformer blocks. The number of heads n head is set to 4 in visual-only models and 8 in audioonly/audio-visual models, respectively. Kernel size is set to 31 in each depth-wise convolutional layer. The transformer decoder uses 6 self-attention blocks, where the hyper-parameters settings in feedforward and self-attention modules are the same as in the encoder. The Adam optimizer <ref type="bibr" target="#b14">[15]</ref> with ?1 = 0.9, ?2 = 0.98 and = 10 ?9 is used for end-to-end training with a mini-batch size of 8. Following <ref type="bibr" target="#b30">[31]</ref>, the learning rate increases linearly with the first 25 000 steps, yielding a peak learning rate of 0.0004 and thereafter decreases proportionally to the inverse square root of the step number. The whole network is trained for 50 epochs. Note that the utterances with more than 600 frames in the pre-training set are excluded during training. Language Model We train a transformer-based language model <ref type="bibr" target="#b11">[12]</ref> for 10 epochs. The language model is trained by combining the training transcriptions of LibriSpeech (960 h) <ref type="bibr" target="#b20">[21]</ref>, pre-training and training sets of LRS2 <ref type="bibr" target="#b4">[5]</ref> and LRS3 <ref type="bibr" target="#b2">[3]</ref>, with a total of 16.2 million words. The weighted prior score from the language model is incorporated through a shallow fusion, which is described in Eq. 2.</p><formula xml:id="formula_2">y = argmax y?? {?logpCTC(y|x)+(1??)logpCE(y|x) +?logpLM(y)} (2)</formula><p>where? is a set of predictions of target symbols. ? is a relative CTC weight at the decoding phase, and ? is the relative weight for the language model. In our work, we set ? to 0.1 and ? to 0.6, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>In this section, we investigate the impact of each change on the baseline hybrid CTC/Attention model <ref type="bibr" target="#b24">[25]</ref>. Results on LRS2 are shown in <ref type="table">Table 2</ref>. We first train a model from scratch in an end-to-end manner, resulting in an absolute improvement of 12.6 % over the two stage approach, where visual features are first extracted and then fed to the back-end. We initialise the visual front-end with a model pre-trained on LRW and a further absolute improvement of <ref type="bibr" target="#b0">1</ref>   <ref type="table">Table 2</ref>. Ablation study on visual speech recognition performance on LRS2. 4.7 % is observed. Then, we replace the LSTM encoders and decoders with a conformer encoder and a transformer decoder, respectively, which results in an absolute improvement of 3.8 %. We also replace the RNN-based language model with a transformer-based language model and achieve a WER of 37.9 %. This leads to an absolute improvement of 4.5 %.</p><p>Results on LRS2 Results on LRS2 are reported in <ref type="table">Table 3</ref>. The proposed visual-only model reduces the WER from 48.3 % to 39.1 %, while using 6 ? fewer training data <ref type="bibr" target="#b0">[1]</ref>. In case, we use the pre-trained LRW model for initialisation the WER drops further to 37.9 %. The E2E audio-only model using audio waveforms for training achieves a WER of 4.3 %, resulting in an absolute improvement of 2.4 %. over the current state-of-the-art. For comparison purposes, we also run an experiment using 80-dimension log-Mel filter-bank features following <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref>. Similarly to the WavAugment <ref type="bibr" target="#b12">[13]</ref>, we augment the log-Mel filter-bank features via SpecAugment <ref type="bibr" target="#b22">[23]</ref>. By replacing the raw audio features with the log-Mel filter-bank features, we observe the same performance, WER 4.3 %, which indicates deep acoustic speech representations based on the proposed temporal network can be directly learnt from audio waveforms. To better investigate their differences, we conduct noisy experiments varying different levels of babble noise. The results are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. It is interesting to observe that the performance of the raw audio model slightly outperforms the log-Mel filter-bank based over varying levels of babble noise with a maximum absolute margin of 7.5 % at -5 dB. This indicates deep speech representations are more robust to noise than the log-Mel filter-bank features. In case, we initialise the audio encoder with a model pretrained on LRW then the WER drops to 3.9 %. It is evident that the audio-visual model which directly learns from audio waveforms and raw pixels leads to a small improvement over the audio-only models. We also run audio-only, visual-only, and audio-visual experiments varying the SNR levels of babble noise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this work, we present an encoder-decoder attention-based architecture for audio-visual speech recognition, which can be trained in an end-to-end fashion and leads to state-of-the-art results on LRS2 and LRS3. Additionally, the audio-visual experiments show that the audio-visual model significantly outperforms the audio-only model especially at high levels of noise. It would also be interesting to investigate in future work an adaptive fusion mechanism that learns to weigh each modality based on the noise levels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>End-to-end audio-visual speech recognition architecture. The inputs are pixels and raw audio waveforms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Word Error Rate (WER) as a function of the noise level. A: End-to-End audio model. V: End-to-End visual model, AV: End-to-End audio-visual model. log-Mel filter-bank: A conformer model trained with log-Mel filter-bank features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>? 7 2 , 64, stride 1 ? 2 2 The architecture of acoustic and visual Front-end. The dimensions of kernels are denoted by {temporal size ? spatial size 2 , channels}. The acoustic model and visual backbones have 3.85 M and 11.18 M parameters, respectively. Ta and Tv denote the number of input samples and frames, respectively.</figDesc><table><row><cell>stage</cell><cell cols="2">Input audio waveform</cell><cell cols="2">Input image sequence</cell></row><row><cell></cell><cell>(Ta ? 1)</cell><cell></cell><cell cols="2">(Tv ? W ? H)</cell></row><row><cell>conv1</cell><cell cols="2">conv1d, 80, 64, stride 4</cell><cell cols="2">conv3d, 5 maxpool, 1 ? 3 2</cell></row><row><cell>res2</cell><cell>conv1d, 3, 64 conv1d, 3, 64</cell><cell>? 2</cell><cell>conv2d, 3 2 , 64 conv2d, 3 2 , 64</cell><cell>? 2</cell></row><row><cell>res3</cell><cell>conv1d, 3, 128 conv1d, 3, 128</cell><cell>? 2</cell><cell>conv2d, 3 2 , 128 conv2d, 3 2 , 128</cell><cell>? 2</cell></row><row><cell>res4</cell><cell>conv1d, 3, 256 conv1d, 3, 256</cell><cell>? 2</cell><cell>conv2d, 3 2 , 256 conv2d, 3 2 , 256</cell><cell>? 2</cell></row><row><cell>res5</cell><cell>conv1d, 3, 512 conv1d, 3, 512</cell><cell>? 2</cell><cell>conv2d, 3 2 , 512 conv2d, 3 2 , 512</cell><cell>? 2</cell></row><row><cell cols="3">pool 6 average pooling, stride 20</cell><cell cols="2">global average pooling</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Word Error Rate (WER) of the audio-only, visual-only and audio-visual models on LRS2. VC2 clean denotes the filtered version of VoxCeleb2. LRS2&amp;3 consists of LRS2 and LRS3. LRS3 v0.4 is the updated version of LRS3 with speaker-independent settings.The results are shown inFig 2.Note that both audio-only and audiovisual models are augmented with noise injection. It is clear that the audio-visual model achieves better performance than the audio-only model. The gap between raw audio-only and audio-visual models becomes larger by the presence of high level of noise. This demonstrates that the audio-visual model is particularly beneficial when the audio modality is heavily corrupted by background noise. Results on LRS3 Results on LRS3 v0.4 are reported inTable 4. The best visual-only model has a WER of 43.3 %. We observe that our visual-only model outperforms other methods by a large margin while using fewer training data. For the audio-only and audio-visual experiments, our model pushes the state-of-the-art performance to 2.3 % and 2.3 %, respectively, outperforming [19] by 2.5 % and 2.2 %, respectively. It is worth pointing out that our model is trained on a dataset which is 52? smaller than [19], 595 vs 31000 hours. We should note that some works use the old version of LRS3 (denoted as v0.0), where some speakers appear both in the training and test sets. For fair comparisons, we also report the performance of audio-only, visual-only, and audio-visual model on this version of LRS3 as well. Specifically, the audio-only model achieves a WER Word Error Rate (WER) of the audio-only, visual-only and audio-visual models on LRS3. VC2 clean denotes the filtered version of VoxCeleb2. LRS2&amp;3 consists of LRS2 and LRS3. LRS3 v0.4 is the updated version of LRS3 with speaker-independent settings. to 1.3 %. The visual-only model reduces the WER to 30.4 %. The audio-visual model reduces the WER to 1.2 % which is the new state-of-the-art performance for this set. These significant improvements over LRS3 v0.4 are mainly due to the fact that in LRS3 v0.0 overlapped identities appear in both pre-training and test sets.</figDesc><table><row><cell></cell><cell>Training Data (Hours)</cell><cell>WER</cell><cell>Method</cell><cell>Training Data (Hours)</cell><cell>WER</cell></row><row><cell>Visual-only (?)</cell><cell></cell><cell></cell><cell>Visual-only (?)</cell><cell></cell><cell></cell></row><row><cell>MV-WAS [5]</cell><cell>LRS2 (224)</cell><cell>70.4</cell><cell>Conv-seq2seq [36]</cell><cell>LRW (157) + LRS2&amp;3 v0.0 (698)</cell><cell>60.1</cell></row><row><cell>LIBS [37]</cell><cell>MVLRS (730) + LRS2 (224)</cell><cell>65.3</cell><cell>KD + CTC [2]</cell><cell>VC2 clean (334) + LRS3 v0.4 (438)</cell><cell>59.8</cell></row><row><cell>CTC/Attention [25]</cell><cell>LRW (157) + LRS2 (224)</cell><cell>63.5</cell><cell>TM-seq2seq [1]</cell><cell cols="2">MVLRS (730) + LRS2&amp;3 v0.4 (632) 58.9</cell></row><row><cell>Conv-seq2seq [36]</cell><cell>LRW (157) + LRS2&amp;3 v0.0 (698)</cell><cell>51.7</cell><cell>EG-seq2seq [33]</cell><cell>LRW (157) + LRS3 v0.0 (474)</cell><cell>57.8</cell></row><row><cell>KD + CTC [2]</cell><cell cols="2">VC2 clean (334) + LRS2&amp;3 v0.4 (632) 51.3</cell><cell>V2P [27]</cell><cell>YT (3 886)</cell><cell>55.1</cell></row><row><cell>TDNN [34]</cell><cell>LRS2 (224)</cell><cell>48.9</cell><cell>RNN-T [19]</cell><cell>YT (31 000)</cell><cell>33.6</cell></row><row><cell>TM-seq2seq [1]</cell><cell cols="2">MVLRS (730) + LRS2&amp;3 v0.4 (632) 48.3</cell><cell>Ours (V)</cell><cell>LRS3 v0.4 (438)</cell><cell>46.9</cell></row><row><cell>Ours (V)</cell><cell>LRS2 (224)</cell><cell>39.1</cell><cell>Ours (V)</cell><cell>LRW (157) + LRS3 v0.4 (438)</cell><cell>43.3</cell></row><row><cell>Ours (V)</cell><cell>LRW (157) + LRS2 (224)</cell><cell>37.9</cell><cell>Ours (V)</cell><cell>LRW (157) + LRS3 v0.0 (474)</cell><cell>30.4</cell></row><row><cell>Audio-only (?)</cell><cell></cell><cell></cell><cell>Audio-only (?)</cell><cell></cell><cell></cell></row><row><cell>TM-seq2seq [1]</cell><cell cols="2">MVLRS (730) + LRS2&amp;3 v0.4 (632) 9.7</cell><cell>TM-seq2seq [1]</cell><cell cols="2">MVLRS (730) + LRS2&amp;3 v0.4 (632) 8.3</cell></row><row><cell>CTC/Attention [25] CTC/Attention [16] TDNN [34] Ours (filter-bank) Ours (raw A)</cell><cell>LRS2 (224) LibriSpeech (960) + LRS2 (224) LRS2 (224) LRS2 (224) LRS2 (224)</cell><cell>8.3 8.2 6.7 4.3 4.3</cell><cell>EG-seq2seq [33] RNN-T [19] Ours (filter-bank) Ours (raw A)</cell><cell>LRS3 v0.0 (474) YT (31 000) LRS3 v0.4 (438) LRS3 v0.4 (438)</cell><cell>7.2 4.8 2.3 2.3</cell></row><row><cell>Ours (raw A)</cell><cell>LRW (157) + LRS2 (224)</cell><cell>3.9</cell><cell>Ours (raw A)</cell><cell>LRW (157) +LRS3 v0.4 (438)</cell><cell>2.3</cell></row><row><cell>Audio-visual (?)</cell><cell></cell><cell></cell><cell>Ours (raw A)</cell><cell>LRW (157) +LRS3 v0.0 (474)</cell><cell>1.3</cell></row><row><cell>TM-seq2seq [1]</cell><cell cols="2">MVLRS (730) + LRS2&amp;3 v0.4 (632) 8.5</cell><cell>Audio-visual (?)</cell><cell></cell><cell></cell></row><row><cell>CTC/Attention [25]</cell><cell>LRW (157) + LRS2 (224)</cell><cell>7.0</cell><cell>TM-seq2seq [1]</cell><cell cols="2">MVLRS (730) + LRS2&amp;3 v0.4 (632) 7.2</cell></row><row><cell>TDNN [34]</cell><cell>LRS2 (224)</cell><cell>5.9</cell><cell>EG-seq2seq [33]</cell><cell>LRW (157) + LRS3 v0.0 (474)</cell><cell>6.8</cell></row><row><cell>Ours (raw A + V)</cell><cell>LRS2 (224)</cell><cell>4.2</cell><cell>RNN-T [19]</cell><cell>YT (31 000)</cell><cell>4.5</cell></row><row><cell>Ours (raw A + V)</cell><cell>LRW (157) + LRS2 (224)</cell><cell>3.7</cell><cell>Ours (raw A + V)</cell><cell>LRW (157) + LRS3 v0.4 (438)</cell><cell>2.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ours (raw A + V)</cell><cell>LRW (157) + LRS3 v0.0 (474)</cell><cell>1.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We would like to thank Dr. Jie Shen for his help with face tracking. The work of Pingchuan Ma has been partially supported by Honda and "AWS Cloud Credits for Research".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ASR is all you need: Cross-modal distillation for lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="2143" to="2147" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">LRS3-TED: A large-scale dataset for visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00496</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Lipnet: End-to-end sentence-level lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N. De</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01599</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="2421" to="2424" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="2978" to="2988" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Audio-visual speech modeling for continuous speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="5036" to="5040" />
		</imprint>
	</monogr>
	<note>Interspeech, 2020</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Language modeling with deep transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3905" to="3909" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data augmenting contrastive learning of speech representations in the time domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mazar?</surname></persName>
		</author>
		<idno>abs/2007.00991</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Listen, watch and understand at the cocktail party: Audio-visual-contextual speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1426" to="1430" />
		</imprint>
	</monogr>
	<note>in Interspeech, 2020</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Investigating the lombard effect influence on end-to-end audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4090" to="4094" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards practical lipreading with distilled and efficient models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mart?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno>abs/2007.06504</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Recurrent neural network transducer for audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Garcia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ASRU</publisher>
			<biblScope unit="page" from="905" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lipreading using temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6319" to="6323" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5206" to="5210" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">E2e-sincnet: Toward fully end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7714" to="7718" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6548" to="6552" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Audio-visual speech recognition with a hybrid CTC/attention architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in SLT</title>
		<imprint>
			<biblScope unit="page" from="513" to="520" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recent advances in the automatic recognition of audiovisual speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1306" to="1326" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Large-scale visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hughes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4135" to="4139" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Combining residual networks with LSTMs for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="3652" to="3656" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How to teach DNNs to pay attention to the visual modality in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sterpu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Saam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1052" to="1064" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Steeneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="251" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hybrid CTC/attention architecture for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1240" to="1253" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discriminative multimodality speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="433" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Audiovisual recognition of overlapped speech for the LRS2 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6984" to="6988" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-end speech recognition from the raw waveform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<biblScope unit="page" from="781" to="785" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatio-temporal fusion based convolutional sequence learning for lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="713" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hearing lips: Improving lip reading by distilling speech recognizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6917" to="6924" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
