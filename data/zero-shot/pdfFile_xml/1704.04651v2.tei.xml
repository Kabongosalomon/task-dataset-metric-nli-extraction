<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">THE REACTOR: A FAST AND SAMPLE-EFFICIENT ACTOR-CRITIC AGENT FOR REINFORCEMENT LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audr?nas</forename><surname>Gruslys</surname></persName>
							<email>audrunas@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
							<email>wdabney@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
							<email>mazar@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
							<email>bellemare@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<country>DeepMind</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<country>DeepMind</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">THE REACTOR: A FAST AND SAMPLE-EFFICIENT ACTOR-CRITIC AGENT FOR REINFORCEMENT LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2017)  and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C <ref type="bibr" target="#b14">(Mnih et al., 2016)</ref>. Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms, designed for expected value evaluation, into distributional algorithms. Next, we introduce the ?-leave-one-out policy gradient algorithm, which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.</p><p>Published as a conference paper at ICLR 2018 Data-efficiency and off-policy learning are essential for many real-world domains where interactions with the environment are expensive. Similarly, wall-clock time (time-efficiency) directly impacts an algorithm's applicability through resource costs. The focus of this work is to produce an agent that is sample-and time-efficient. To this end, we introduce a new reinforcement learning agent, called Reactor (Retrace-Actor), which takes a principled approach to combining the sample-efficiency of off-policy experience replay with the time-efficiency of asynchronous algorithms. We combine recent advances in both categories of agents with novel contributions to produce an agent that inherits the benefits of both and reaches state-of-the-art performance over 57 Atari 2600 games.</p><p>Our primary contributions are (1) a novel policy gradient algorithm, ?-LOO, which makes better use of action-value estimates to improve the policy gradient; (2) the first multi-step off-policy distributional reinforcement learning algorithm, distributional Retrace(?); (3) a novel prioritized replay for off-policy sequences of transitions; and (4) an optimized network and parallel training architecture.</p><p>We begin by reviewing background material, including relevant improvements to both value-function agents and actor-critic agents. In Section 3 we introduce each of our primary contributions and present the Reactor agent. Finally, in Section 4, we present experimental results on the 57 Atari 2600 games from the Arcade Learning Environment (ALE) <ref type="bibr" target="#b1">(Bellemare et al., 2013)</ref>, as well as a series of ablation studies for the various components of Reactor.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Model-free deep reinforcement learning has achieved several remarkable successes in domains ranging from super-human-level control in video games <ref type="bibr" target="#b13">(Mnih et al., 2015)</ref> and the game of Go , to continuous motor control tasks <ref type="bibr" target="#b10">(Lillicrap et al., 2015;</ref><ref type="bibr" target="#b23">Schulman et al., 2015)</ref>.</p><p>Much of the recent work can be divided into two categories. First, those of which that, often building on the DQN framework, act -greedily according to an action-value function and train using minibatches of transitions sampled from an experience replay buffer <ref type="bibr" target="#b28">(Van Hasselt et al., 2016;</ref><ref type="bibr" target="#b31">Wang et al., 2015;</ref><ref type="bibr" target="#b5">He et al., 2017;</ref><ref type="bibr" target="#b0">Anschel et al., 2017)</ref>. These value-function agents benefit from improved sample complexity, but tend to suffer from long runtimes (e.g. DQN requires approximately a week to train on Atari). The second category are the actor-critic agents, which includes the asynchronous advantage actor-critic (A3C) algorithm, introduced by <ref type="bibr" target="#b14">Mnih et al. (2016)</ref>. These agents train on transitions collected by multiple actors running, and often training, in parallel <ref type="bibr" target="#b24">(Schulman et al., 2017;</ref><ref type="bibr" target="#b30">Vezhnevets et al., 2017)</ref>. The deep actor-critic agents train on each trajectory only once, and thus tend to have worse sample complexity. However, their distributed nature allows significantly faster training in terms of wall-clock time. Still, not all existing algorithms can be put in the above two categories and various hybrid approaches do exist <ref type="bibr" target="#b35">(Zhao et al., 2016;</ref><ref type="bibr" target="#b17">O'Donoghue et al., 2017;</ref><ref type="bibr" target="#b4">Gu et al., 2017;</ref><ref type="bibr" target="#b32">Wang et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>We consider a Markov decision process (MDP) with state space X and finite action space A. A (stochastic) policy ?(?|x) is a mapping from states x ? X to a probability distribution over actions. We consider a ?-discounted infinite-horizon criterion, with ? ? [0, 1) the discount factor, and define for policy ? the action-value of a state-action pair (x, a) as</p><formula xml:id="formula_0">Q ? (x, a) def = E t?0 ? t r t |x 0 = x, a 0 = a, ? ,</formula><p>where ({x t } t?0 ) is a trajectory generated by choosing a in x and following ? thereafter, i.e., a t ? ?(?|x t ) (for t ? 1), and r t is the reward signal. The objective in reinforcement learning is to find an optimal policy ? * , which maximises Q ? (x, a). The optimal action-values are given by Q * (x, a) = max ? Q ? (x, a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">VALUE-BASED ALGORITHMS</head><p>The Deep Q-Network (DQN) framework, introduced by <ref type="bibr" target="#b13">Mnih et al. (2015)</ref>, popularised the current line of research into deep reinforcement learning by reaching human-level, and beyond, performance across 57 Atari 2600 games in the ALE. While DQN includes many specific components, the essence of the framework, much of which is shared by Neural Fitted Q-Learning <ref type="bibr" target="#b20">(Riedmiller, 2005)</ref>, is to use of a deep convolutional neural network to approximate an action-value function, training this approximate action-value function using the Q-Learning algorithm <ref type="bibr" target="#b33">(Watkins &amp; Dayan, 1992)</ref> and mini-batches of one-step transitions (x t , a t , r t , x t+1 , ? t ) drawn randomly from an experience replay buffer <ref type="bibr" target="#b11">(Lin, 1992)</ref>. Additionally, the next-state action-values are taken from a target network, which is updated to match the current network periodically. Thus, the temporal difference (TD) error for transition t used by these algorithms is given by</p><formula xml:id="formula_1">? t = r t + ? t max a ?A Q(x t+1 , a ;?) ? Q(x t , a t ; ?),<label>(1)</label></formula><p>where ? denotes the parameters of the network and? are the parameters of the target network.</p><p>Since this seminal work, we have seen numerous extensions and improvements that all share the same underlying framework. Double <ref type="bibr">DQN (Van Hasselt et al., 2016)</ref>, attempts to correct for the over-estimation bias inherent in Q-Learning by changing the second term of (1) to Q(x t+1 , arg max a ?A Q(x t+1 , a ; ?);?). The dueling architecture <ref type="bibr" target="#b31">(Wang et al., 2015)</ref>, changes the network to estimate action-values using separate network heads V (x; ?) and A(x, a; ?) with</p><formula xml:id="formula_2">Q(x, a; ?) = V (x; ?) + A(x, a; ?) ? 1 |A| a A(x, a ; ?).</formula><p>Recently, <ref type="bibr" target="#b6">Hessel et al. (2017)</ref> introduced Rainbow, a value-based reinforcement learning agent combining many of these improvements into a single agent and demonstrating that they are largely complementary. Rainbow significantly out performs previous methods, but also inherits the poorer time-efficiency of the DQN framework. We include a detailed comparison between Reactor and Rainbow in the Appendix. In the remainder of the section we will describe in more depth other recent improvements to DQN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">PRIORITIZED EXPERIENCE REPLAY</head><p>The experience replay buffer was first introduced by Lin (1992) and later used in DQN <ref type="bibr" target="#b13">(Mnih et al., 2015)</ref>. Typically, the replay buffer is essentially a first-in-first-out queue with new transitions gradually replacing older transitions. The agent would then sample a mini-batch uniformly at random from the replay buffer. Drawing inspiration from prioritized sweeping (Moore &amp; Atkeson, 1993), prioritized experience replay replaces the uniform sampling with prioritized sampling proportional to the absolute TD error <ref type="bibr" target="#b22">(Schaul et al., 2016)</ref>.</p><p>Specifically, for a replay buffer of size N , prioritized experience replay samples transition t with probability P (t), and applies weighted importance-sampling with w t to correct for the prioritization bias, where</p><formula xml:id="formula_3">P (t) = p ? t k p ? k , w t = 1 N ? 1 P (t) ? , p t = |? t | + , ?, ?, &gt; 0.</formula><p>(2)</p><p>Prioritized DQN significantly increases both the sample-efficiency and final performance over DQN on the Atari 2600 benchmarks .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">RETRACE(?)</head><p>Retrace(?) is a convergent off-policy multi-step algorithm extending the DQN agent <ref type="bibr" target="#b16">(Munos et al., 2016)</ref>. Assume that some trajectory {x 0 , a 0 , r 0 , x 1 , a 1 , r 1 , . . . , x t , a t , r t , . . . , } has been generated according to behaviour policy ?, i.e., a t ? ?(?|x t ). Now, we aim to evaluate the value of a different target policy ?, i.e. we want to estimate Q ? . The Retrace algorithm will update our current estimate Q of Q ? in the direction of</p><formula xml:id="formula_4">?Q(x t , a t ) def = s?t ? s?t (c t+1 . . . c s )? ? s Q,<label>(3)</label></formula><p>where ? ? s Q def = r s + ?E ? [Q(x s+1 , ?)] ? Q(x s , a s ) is the temporal difference at time s under ?, and c s = ? min 1, ? s , ? s = ?(a s |x s ) ?(a s |x s ) .</p><p>The Retrace algorithm comes with the theoretical guarantee that in finite state and action spaces, repeatedly updating our current estimate Q according to (3) produces a sequence of Q functions which converges to Q ? for a fixed ? or to Q * if we consider a sequence of policies ? which become increasingly greedy w.r.t. the Q estimates <ref type="bibr" target="#b16">(Munos et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">DISTRIBUTIONAL RL</head><p>Distributional reinforcement learning refers to a class of algorithms that directly estimate the distribution over returns, whose expectation gives the traditional value function <ref type="bibr" target="#b2">(Bellemare et al., 2017)</ref>. Such approaches can be made tractable with a distributional Bellman equation, and the recently proposed algorithm C51 showed state-of-the-art performance in the Atari 2600 benchmarks. C51 parameterizes the distribution over returns with a mixture over Diracs centered on a uniform grid,</p><formula xml:id="formula_6">Q(x, a; ?) = N ?1 i=0 q i (x, a; ?)z i , q i = e ?i(x,a) N ?1 j=0 e ?j (x,a) , z i = v min + i v max ? v min N ? 1 ,<label>(5)</label></formula><p>with hyperparameters v min , v max that bound the distribution support of size N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ACTOR-CRITIC ALGORITHMS</head><p>In this section we review the actor-critic framework for reinforcement learning algorithms and then discuss recent advances in actor-critic algorithms along with their various trade-offs. The asynchronous advantage actor-critic (A3C) algorithm <ref type="bibr" target="#b14">(Mnih et al., 2016)</ref>, maintains a parameterized policy ?(a|x; ?) and value function V (x; ? v ), which are updated with</p><formula xml:id="formula_7">? = ? ? log ?(a t |x t ; ?)A(x t , a t ; ? v ), ? v = A(x t , a t ; ? v )? ?v V (x t ),<label>(6)</label></formula><p>where,</p><formula xml:id="formula_8">A(x t , a t ; ? v ) = n?1 k ? k r t+k + ? n V (x t+n ) ? V (x t ).<label>(7)</label></formula><p>A3C uses M = 16 parallel CPU workers, each acting independently in the environment and applying the above updates asynchronously to a shared set of parameters. In contrast to the previously discussed value-based methods, A3C is an on-policy algorithm, and does not use a GPU nor a replay buffer.</p><p>Proximal Policy Optimization (PPO) is a closely related actor-critic algorithm <ref type="bibr" target="#b24">(Schulman et al., 2017)</ref>, which replaces the advantage (7) with,</p><formula xml:id="formula_9">min(? t A(x t , a t ; ? v ), clip(? t , 1 ? , 1 + )A(x t , a t ; ? v )), &gt; 0,</formula><p>where ? t is as defined in Section 2.1.2. Although both PPO and A3C run M parallel workers collecting trajectories independently in the environment, PPO collects these experiences to perform a single, synchronous, update in contrast with the asynchronous updates of A3C.</p><p>Actor-Critic Experience Replay (ACER) extends the A3C framework with an experience replay buffer, Retrace algorithm for off-policy corrections, and the Truncated Importance Sampling Likelihood Ratio (TISLR) algorithm used for off-policy policy optimization <ref type="bibr" target="#b32">(Wang et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE REACTOR</head><p>The Reactor is a combination of four novel contributions on top of recent improvements to both deep value-based RL and policy-gradient algorithms. Each contribution moves Reactor towards our goal of achieving both sample and time efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">?-LOO</head><p>The Reactor architecture represents both a policy ?(a|x) and action-value function Q(x, a). We use a policy gradient algorithm to train the actor ? which makes use of our current estimate Q(x, a) of Q ? (x, a). Let V ? (x 0 ) be the value function at some initial state x 0 , the policy gradient theorem says that ?V ? (x 0 ) = E t ? t a Q ? (x t , a)??(a|x t ) , where ? refers to the gradient w.r.t. policy parameters . We now consider several possible ways to estimate this gradient.</p><p>To simplify notation, we drop the dependence on the state x for now and consider the problem of estimating the quantity G = a Q ? (a)??(a).</p><p>In the off-policy case, we consider estimating G using a single action? drawn from a (possibly different from ?) behaviour distribution? ? ?. Let us assume that for the chosen action? we have access to an unbiased estimate R(?) of Q ? (?). Then, we can use likelihood ratio (LR) method combined with an importance sampling (IS) ratio (which we call ISLR) to build an unbiased estimate of G:?</p><formula xml:id="formula_11">ISLR = ?(?) ?(?) (R(?) ? V )? log ?(?),</formula><p>where V is a baseline that depends on the state but not on the chosen action. However this estimate suffers from high variance. A possible way for reducing variance is to estimate G directly from (8) by using the return R(?) for the chosen action? and our current estimate Q of Q ? for the other actions, which leads to the so-called leave-one-out (LOO) policy-gradient estimate:  This estimate has low variance but may be biased if the estimated Q values differ from Q ? . A better bias-variance tradeoff may be obtained by the more general ?-LOO policy-gradient estimate:</p><formula xml:id="formula_12">G LOO = R(?)??(?) + a =? Q(a)??(a).<label>(9</label></formula><formula xml:id="formula_13">G ?-LOO = ?(R(?) ? Q(?))??(?) + a Q(a)??(a),<label>(10)</label></formula><p>where ? = ?(?, ?,?) can be a function of both policies, ? and ?, and the selected action?. Notice that when ? = 1, (10) reduces to (9), and when ? = 1/?(?), then <ref type="formula" target="#formula_1">(10)</ref> i?</p><formula xml:id="formula_14">G 1 ? -LOO = ?(?) ?(?) (R(?) ? Q(?))? log ?(?) + a Q(a)??(a).<label>(11)</label></formula><p>This estimate is unbiased and can be seen as a generalization of? ISLR where instead of using a state-only dependent baseline, we use a state-and-action-dependent baseline (our current estimate Q) and add the correction term a ??(a)Q(a) to cancel the bias. Proposition 1 gives our analysis of the bias of G ?-LOO , with a proof left to the Appendix.</p><formula xml:id="formula_15">Proposition 1. Assume? ? ? and that E[R(?)] = Q ? (?). Then, the bias of G ?-LOO is a (1 ? ?(a)?(a))??(a)[Q(a) ? Q ? (a)] .</formula><p>Thus the bias is small when ?(a) is close to 1/?(a), or when the Q-estimates are close to the true Q ? values, and unbiased regardless of the estimates if ?(a) = 1/?(a). The variance is low when ? is small, therefore, in order to improve the bias-variance tradeoff we recommend using the ?-LOO estimate with ? defined as: ?(?) = min c, 1 ?(?) , for some constant c ? 1. This truncated 1/? coefficient shares similarities with the truncated IS gradient estimate introduced in <ref type="bibr" target="#b32">(Wang et al., 2017)</ref> (which we call TISLR for truncated-ISLR):</p><formula xml:id="formula_16">G TISLR = min c, ?(?) ?(?) (R(?) ? V )? log ?(?)+ a ?(a) ?(a) ? c + ?(a)(Q ? (a) ? V )? log ?(a).</formula><p>The differences are: (i) we truncate 1/?(?) = ?(?)/?(?) ? 1/?(?) instead of truncating ?(?)/?(?), which provides an additional variance reduction due to the variance of the LR ? log ?(?) = ??(?) ?(?) (since this LR may be large when a low probability action is chosen), and (ii) we use our Q-baseline instead of a V baseline, reducing further the variance of the LR estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DISTRIBUTIONAL RETRACE</head><p>In off-policy learning it is very difficult to produce an unbiased sample R(?) of Q ? (?) when following another policy ?. This would require using full importance sampling correction along the trajectory. Instead, we use the off-policy corrected return computed by the Retrace algorithm, which produces a (biased) estimate of Q ? (?) but whose bias vanishes asymptotically <ref type="bibr" target="#b16">(Munos et al., 2016)</ref>.</p><p>In Reactor, we consider predicting an approximation of the return distribution function from any state-action pair (x, a) in a similar way as in <ref type="bibr" target="#b2">Bellemare et al. (2017)</ref>. The original algorithm C51 described in that paper considered single-step Bellman updates only. Here we need to extend this idea to multi-step updates and handle the off-policy correction performed by the Retrace algorithm, as defined in <ref type="formula" target="#formula_4">(3)</ref>. Next, we describe these two extensions.</p><p>Multi-step distributional Bellman operator: First, we extend C51 to multi-step Bellman backups. We consider return-distributions from (x, a) of the form i q i (x, a)? zi (where ? z denotes a Dirac in z)</p><p>which are supported on a finite uniform grid</p><formula xml:id="formula_17">{z i } ? [v min , v max ], z i &lt; z i+1 , z 1 = v min , z m = v max .</formula><p>The coefficients q i (x, a) (discrete distribution) corresponds to the probabilities assigned to each atom z i of the grid. From an observed n-step sequence {x t , a t , r t , x t+1 , . . . , x t+n }, generated by behavior policy ? (i.e, a s ? ?(?|x s ) for t ? s &lt; t + n), we build the n-step backed-up return-distribution from (x t , a t ). The n-step distributional Bellman target, whose expectation is</p><formula xml:id="formula_18">t+n?1 s=t ? s?t r s + ? n Q(x t+n , a), is given by: i q i (x t+n , a)? z n i , with z n i = t+n?1 s=t ? s?t r s + ? n z i .</formula><p>Since this distribution is supported on the set of atoms {z n i }, which is not necessarily aligned with the grid {z i }, we do a projection step and minimize the KL-loss between the projected target and the current estimate, just as with C51 except with a different target distribution <ref type="bibr" target="#b2">(Bellemare et al., 2017)</ref>.</p><p>Distributional Retrace: Now, the Retrace algorithm defined in <ref type="formula" target="#formula_4">(3)</ref> involves an off-policy correction which is not handled by the previous n-step distributional Bellman backup. The key to extending this distributional back-up to off-policy learning is to rewrite the Retrace algorithm as a linear combination of n-step Bellman backups, weighted by some coefficients ? n,a . Indeed, notice that (3) rewrites as</p><formula xml:id="formula_19">?Q(x t , a t ) = n?1 a?A ? n,a t+n?1 s=t ? s?t r s + ? n Q(x t+n , a) n-step Bellman backup ? Q(x t , a t ),</formula><p>where ? n,a = c t+1 . . . c t+n?1 ?(a|x t+n ) ? I{a = a t+n }c t+n . These coefficients depend on the degree of off-policy-ness (between ? and ?) along the trajectory. We have that n?1 a ? n,a = n?1 c t+1 . . . c t+n?1 (1 ? c t+n ) = 1, but notice some coefficients may be negative. However, in expectation (over the behavior policy) they are non-negative. Indeed,</p><formula xml:id="formula_20">E ? [? n,a ] = E c t+1 . . . c t+n?1 E at+n??(?|xt+n) ?(a|x t+n ) ? I{a = a t+n }c t+n |x t+n = E c t+1 . . . c t+n?1 ?(a|x t+n ) ? ?(a|x t+n )? min 1, ?(a|x t+n ) ?(a|x t+n ) ? 0,</formula><p>by definition of the c s coefficients (4). Thus in expectation (over the behavior policy), the Retrace update can be seen as a convex combination of n-step Bellman updates.</p><p>Then, the distributional Retrace algorithm can be defined as backing up a mixture of n-step distributions. More precisely, we define the Retrace target distribution as:</p><formula xml:id="formula_21">i=1 q * i (x t , a t )? zi , with q * i (x t , a t ) = n?1 a ? n,a j q j (x t+n , a t+n )h zi (z n j ),</formula><p>where h zi (x) is a linear interpolation kernel, projecting onto the support {z i }:</p><formula xml:id="formula_22">h zi (x) = ? ? ? ? ? (x ? z i?1 )/(z i ? z i?1 ), if z i?1 ? x ? z i (z i+1 ? x)/(z i+1 ? z i ), if z i ? x ? z i+1 0, if x ? z i?1 or x ? z i+1 1, if (x ? v min and z i = v min ) or (x ? v max and z i = v max ) ? ? ? ? ?</formula><p>We update the current probabilities q(x t , a t ) by performing a gradient step on the KL-loss</p><formula xml:id="formula_23">?KL(q * (x t , a t ), q(x t , a t )) = ? i=1 q * i (x t , a t )? log q i (x t , a t ).<label>(12)</label></formula><p>Again, notice that some target "probabilities" q * i (x t , a t ) may be negative for some sample trajectory, but in expectation they will be non-negative. Since the gradient of a KL-loss is linear w.r.t. its first argument, our update rule (12) provides an unbiased estimate of the gradient of the KL between the expected (over the behavior policy) Retrace target distribution and the current predicted distribution. 1 Remark: The same method can be applied to other algorithms (such as TB(?) <ref type="bibr" target="#b18">(Precup et al., 2000)</ref> and importance sampling <ref type="bibr" target="#b19">(Precup et al., 2001)</ref>) in order to derive distributional versions of other off-policy multi-step RL algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PRIORITIZED SEQUENCE REPLAY</head><p>Prioritized experience replay has been shown to boost both statistical efficiency and final performance of deep RL agents <ref type="bibr" target="#b22">(Schaul et al., 2016)</ref>. However, as originally defined prioritized replay does not handle sequences of transitions and weights all unsampled transitions identically. In this section we present an alternative initialization strategy, called lazy initialization, and argue that it better encodes prior information about temporal difference errors. We then briefly describe our computationally efficient prioritized sequence sampling algorithm, with full details left to the appendix.</p><p>It is widely recognized that TD errors tend to be temporally correlated, indeed the need to break this temporal correlation has been one of the primary justifications for the use of experience replay <ref type="bibr" target="#b13">(Mnih et al., 2015)</ref>. Our proposed algorithm begins with this fundamental assumption. Assumption 1. Temporal differences are temporally correlated, with correlation decaying on average with the time-difference between two transitions.</p><p>Prioritized experience replay adds new transitions to the replay buffer with a constant priority, but given the above assumption we can devise a better method. Specifically, we propose to add experience to the buffer with no priority, inserting a priority only after the transition has been sampled and used for training. Also, instead of sampling transitions, we assign priorities to all (overlapping) sequences of length n. When sampling, sequences with an assigned priority are sampled proportionally to that priority. Sequences with no assigned priority are sampled proportionally to the average priority of assigned priority sequences within some local neighbourhood. Averages are weighted to compensate for sampling biases (i.e. more samples are made in areas of high estimated priorities, and in the absence of weighting this would lead to overestimation of unassigned priorities).</p><p>The lazy initialization scheme starts with priorities p t corresponding to the sequences {x t , a t , . . . , x t+n } for which a priority was already assigned. Then it extrapolates a priority of all other sequences in the following way. Let us define a partition (I i ) i of the states ordered by increasing time such that each cell I i contains exactly one state s i with already assigned priority. We define the estimated priorityp t to all other sequences asp t = si?J(t)</p><formula xml:id="formula_24">wi i ?J(t) w i p(s i ), where J(t)</formula><p>is a collection of contiguous cells (I i ) containing time t, and w i = |I i | is the length of the cell I i containing s i . For already defined priorities denotep t = p t . Cell sizes work as estimates of inverse local density and are used as importance weights for priority estimation. 2 For the algorithm to be unbiased, partition (I i ) i must not be a function of the assigned priorities. So far we have defined a class of algorithms all free to choose the partition (I i ) and the collection of cells I(t), as long that they satisfy the above constraints. <ref type="figure">Figure 4</ref> in the Appendix illustrates the above description. Now, with probability we sample uniformly at random, and with probability 1 ? we sample proportionally top t . We implemented an algorithm satisfying the above constraints and called it Contextual Priority Tree (CPT). It is based on AVL trees <ref type="bibr" target="#b29">(Velskii &amp; Landis, 1976)</ref> and can execute sampling, insertion, deletion and density evaluation in O(ln(n)) time. We describe CPT in detail in the Appendix in Section 6.3.</p><p>We treated prioritization as purely a variance reduction technique. Importance-sampling weights were evaluated as in prioritized experience replay, with fixed ? = 1 in (2). We used simple gradient magnitude estimates as priorities, corresponding to a mean absolute TD error along a sequence for Retrace, as defined in (3) for the classical RL case, and total variation in the distributional Retrace case. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">AGENT ARCHITECTURE</head><p>In order to improve CPU utilization we decoupled acting from learning. This is an important aspect of our architecture: an acting thread receives observations, submits actions to the environment, and stores transitions in memory, while a learning thread re-samples sequences of experiences from memory and trains on them <ref type="figure">(Figure 2, left)</ref>. We typically execute 4-6 acting steps per each learning step. We sample sequences of length n = 33 in batches of 4. A moving network is unrolled over frames 1-32 while the target network is unrolled over frames 2-33.</p><p>We allow the agent to be distributed over multiple machines each containing action-learner pairs. Each worker downloads the newest network parameters before each learning step and sends delta-updates at the end of it. Both the network and target network are stored on a shared parameter server while each machine contains its own local replay memory. Training is done by downloading a shared network, evaluating local gradients and sending them to be applied on the shared network. While the agent can also be trained on a single machine, in this work we present results of training obtained with either 10 or 20 actor-learner workers and one parameter server. In <ref type="figure">Figure 2</ref> (right) we compare resources and runtimes of Reactor with related algorithms. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">NETWORK ARCHITECTURE</head><p>In some domains, such as Atari, it is useful to base decisions on a short history of past observations. The two techniques generally used to achieve this are frame stacking and recurrent network architectures. We chose the latter over the former for reasons of implementation simplicity and computational efficiency. As the Retrace algorithm requires evaluating action-values over contiguous sequences of trajectories, using a recurrent architecture allowed each frame to be processed by the convolutional network only once, as opposed to n times times if n frame concatenations were used.</p><p>The Reactor architecture uses a recurrent neural network which takes an observation x t as input and produces two outputs: categorical action-value distributions q i (x t , a) (i here is a bin identifier), and policy probabilities ?(a|x t ). We use an architecture inspired by the duelling network architecture <ref type="bibr" target="#b31">(Wang et al., 2015)</ref>. We split action-value -distribution logits into state-value logits and advantage logits, which in turn are connected to the same LSTM network <ref type="bibr" target="#b7">(Hochreiter &amp; Schmidhuber, 1997)</ref>.</p><p>Final action-value logits are produced by summing state-and action-specific logits, as in <ref type="bibr" target="#b31">Wang et al. (2015)</ref>. Finally, a softmax layer on top for each action produces the distributions over discounted future returns.</p><p>The policy head uses a softmax layer mixed with a fixed uniform distribution over actions, where this mixing ratio is a hyperparameter (Wiering, 1999, Section 5.1.3). Policy and Q-networks have separate LSTMs. Both LSTMs are connected to a shared linear layer which is connected to a shared convolutional neural network <ref type="bibr" target="#b9">(Krizhevsky et al., 2012</ref>). The precise network specification is given in <ref type="table" target="#tab_5">Table 3</ref> in the Appendix.</p><p>Gradients coming from the policy LSTM are blocked and only gradients originating from the Qnetwork LSTM are allowed to back-propagate into the convolutional neural network. We block gradients from the policy head for increased stability, as this avoids positive feedback loops between ? and q i caused by shared representations. We used the Adam optimiser <ref type="bibr" target="#b8">(Kingma &amp; Ba, 2014)</ref>, with a learning rate of 5 ? 10 ?5 and zero momentum because asynchronous updates induce implicit momentum <ref type="bibr" target="#b12">(Mitliagkas et al., 2016)</ref>. Further discussion of hyperparameters and their optimization can be found in Appendix 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>We trained and evaluated Reactor on 57 Atari games <ref type="bibr" target="#b1">(Bellemare et al., 2013)</ref>. <ref type="figure" target="#fig_1">Figure 3 compares</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">COMPARING TO PRIOR WORK</head><p>We evaluated Reactor with target update frequency T update = 1000, ? = 1.0 and ?-LOO with ? = 1 on 57 Atari games trained on 10 machines in parallel. We averaged scores over 200 episodes using 30 random human starts and noop starts (Tables 4 and 5 in the Appendix). We calculated mean and median human normalised scores across all games. We also ranked all algorithms (including random and human scores) for each game and evaluated mean rank of each algorithm across all 57 Atari games. We also evaluated mean Rank and Elo scores for each algorithm for both human and noop start settings. Please refer to Section 6.2 in the Appendix for more details.</p><p>Tables 1 &amp; 2 compare versions of our algorithm, 5 with several other state-of-art algorithms across 57 Atari games for a fixed random seed across all games <ref type="bibr" target="#b1">(Bellemare et al., 2013)</ref>. We compare Reactor against are: DQN <ref type="bibr" target="#b13">(Mnih et al., 2015)</ref>, Double <ref type="bibr">DQN (Van Hasselt et al., 2016)</ref>, DQN with prioritised experience replay , dueling architecture and prioritised dueling <ref type="bibr" target="#b31">(Wang et al., 2015)</ref>, ACER <ref type="bibr" target="#b32">(Wang et al., 2017)</ref>, A3C <ref type="bibr" target="#b14">(Mnih et al., 2016)</ref>, and Rainbow <ref type="bibr" target="#b6">(Hessel et al., 2017)</ref>. Each algorithm was exposed to 200 million frames of experience, or 500 million frames when followed by 500M, and the same pre-processing pipeline including 4 action repeats was used as in the original DQN paper <ref type="bibr" target="#b13">(Mnih et al., 2015)</ref>.</p><p>In <ref type="table" target="#tab_3">Table 1</ref>, we see that Reactor exceeds the performance of all algorithms across all metrics, despite requiring under two days of training. With 500 million frames and four days training we see Reactor's performance continue to improve significantly. The difference in time-efficiency is especially apparent when comparing Reactor and Rainbow (see <ref type="figure" target="#fig_1">Figure 3</ref>, right). Additionally, unlike Rainbow, Reactor does not use Noisy Networks <ref type="bibr" target="#b3">(Fortunato et al., 2017)</ref>, which was reported to have contributed to the performance gains. When evaluating under the no-op starts regime <ref type="table" target="#tab_4">(Table 2)</ref>, Reactor out performs all methods except for Rainbow. This suggests that Rainbow is more sample-efficient when training and evaluation regimes match exactly, but may be overfitting to particular trajectories due to the significant drop in performance when evaluated on the random human starts.</p><p>Regarding ACER, another Retrace-based actor-critic architecture, both classical and distributional versions of Reactor <ref type="figure" target="#fig_1">(Figure 3</ref>) exceeded the best reported median human normalized score of 1.9 with noop starts achieved in 500 million steps. 6 6 APPENDIX Proposition 1. Assume? ? ? and that E[R(?)] = Q ? (?). Then, the bias of G ?-LOO is a (1 ? ?(a)?(a))??(a)[Q(a) ? Q ? (a)] .</p><p>Proof. The bias of? ?-LOO is</p><formula xml:id="formula_25">E[? ?-LOO ] ? G = a ?(a)[?(a)(E[R(a)] ? Q(a))]??(a) + a Q(a)??(a) ? G = a (1 ? ?(a)?(a))[Q(a) ? Q ? (a)]??(a)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">HYPERPARAMETER OPTIMIZATION</head><p>As we believe that algorithms should be robust with respect to the choice of hyperparameters, we spent little effort on parameter optimization. In total, we explored three distinct values of learning rates and two values of ADAM momentum (the default and zero) and two values of T update on a subset of 7 Atari games without prioritization using non-distributional version of Reactor. We later used those values for all experiments. We did not optimize for batch sizes and sequence length or any prioritization hyperparamters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">RANK AND ELO EVALUATION</head><p>Commonly used mean and median human normalized scores have several disadvantages. A mean human normalized score implicitly puts more weight on games that computers are good and humans are bad at. Comparing algorithm by a mean human normalized score across 57 Atari games is almost equivalent to comparing algorithms on a small subset of games close to the median and thus dominating the signal. Typically a set of ten most score-generous games, namely Assault, Asterix, Breakout, Demon Attack, Double Dunk, Gopher, Pheonix, Stargunner, Up'n Down and Video Pinball can explain more than half of inter-algorithm variance. A median human normalized score has the opposite disadvantage by effectively discarding very easy and very hard games from the comparison. As typical median human normalized scores are within the range of 1-2.5, an algorithm which scores zero points on Montezuma's Revenge is evaluated equal to the one which scores 2500 points, as both performance levels are still below human performance making incremental improvements on hard games not being reflected in the overall evaluation. In order to address both problem, we also evaluated mean rank and Elo metrics for inter-algorithm comparison. Those metrics implicitly assign the same weight to each game, and as a result is more sensitive of relative performance on very hard and easy games: swapping scores of two algorithms on any game would result in the change of both mean rank and Elo metrics.</p><p>We calculated separate mean rank and Elo scores for each algorithm using results of test evaluations with 30 random noop-starts and 30 random human starts <ref type="table">(Tables 5 and 4</ref>). All algorithms were ranked across each game separately, and a mean rank was evaluated across 57 Atari games. For Elo score evaluation algorithm, A was considered to win over algorithm B if it obtained more scores on a given Atari. We produced an empirical win-probability matrix by summing wins across all games and used this matrix to evaluate Elo scores. A ranking difference of 400 corresponds to the odds of winning of 10:1 under the Gaussian assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">CONTEXTUAL PRIORITY TREE</head><p>Contextual priority tree is one possible implementation of lazy prioritization <ref type="figure">(Figure 4)</ref>. All sequence keys are put into a balanced binary search tree which maintains a temporal order. An AVL tree <ref type="bibr" target="#b29">(Velskii &amp; Landis (1976)</ref>) was chosen due to the ease of implementation and because it is on average more evenly balanced than a Red-Black Tree.</p><p>Each tree node has up to two children (left and right) and contains currently stored key and a priority of the key which is either set or is unknown. Some trees may only have a single child subtree while <ref type="figure">Figure 4</ref>: Illustration of Lazy prioritization, where sequences with no explicitly assigned priorities get priorities estimated by a linear combination of nearby assigned priorities. Exact boundaries of blue and red intervals are arbitrary (as long as all conditions described in Section 3.3 are satisfied) thus leading to many possible algorithms. Each square represents an individual sequence of size 32 (sequences overlap). Inverse sizes of blue regions work as local density estimates allowing to produce unbiased priority estimates. <ref type="figure">Figure 5</ref>: Rules used to evaluate summary statistics on each node of a binary search tree where all sequence keys are kept sorted by temporal order. c l and c r are total number of nodes within left and right subtrees. m l and m l are estimated mean priorities per node within the subtree. A central square node corresponds to a single key stored within the parent node with its corresponding priority of p (if set) or ? if not set. Red subtrees do not have any singe child with a set priority, and a result do not have priority estimates. A red square shows that priority of the key stored within the parent node is not known. Unknown mean priorities is marked by a question mark. Empty child nodes simply behave as if c = 0 with p =?. Rules a-f illustrate how mean values are propagated down from children to parents when priorities are only partially known (rules d and e also apply symmetrically). Sampling is done by going from the root node up the tree by selecting one of the children (or the current key) stochastically proportional to orange proportions. Sampling terminates once the current (square) key is chosen. <ref type="figure">Figure 6</ref>: Example of a balanced priority tree. Dark blue nodes contain keys with known priorities, light blue nodes have at least one child with at least a single known priority, while ping nodes do not have any priority estimates. Nodes 1, 2 and 3 will obtain priority estimates equal to 2/3 of the priority of key 5 and 1/3 of the priority of node 4. This implies that estimated priorities of keys 1, 2 and 3 are implicitly defined by keys 4 and 6. Nodes 8, 9 and 11 are estimated to have the same priority as node 10. some may have none. In addition to this information, we were tracking other summary statistics at each node which was re-evaluated after each tree rotation. The summary statistics was evaluated by consuming previously evaluated summary statistics of both children and a priority of the key stored within the current node. In particular, we were tracking a total number of nodes within each subtree and mean-priority estimates updated according to rules shown in <ref type="figure">Figure 5</ref>. The total number of nodes within each subtree was always known (c in <ref type="figure">Figure 5</ref>), while mean priority estimates per key (m in <ref type="figure">Figure 5</ref>) could either be known or unknown.</p><p>If a mean priority of either one child subtree or a key stored within the current node is unknown then it can be estimated to by exploiting information coming from another sibling subtree or a priority stored within the parent node.</p><p>Sampling was done by traversing the tree from the root node up while sampling either one of the children subtrees or the currently held key proportionally to the total estimated priority masses contained within. The rules used to evaluate proportions are shown in orange in <ref type="figure">Figure 5</ref>. Similarly, probabilities of arbitrary keys can be queried by traversing the tree from the root node towards the child node of an interest while maintaining a product of probabilities at each branching point. Insertion, deletion, sampling and probability query operations can be done in O(ln(n)) time.</p><p>The suggested algorithm has the desired property that it becomes a simple proportional sampling algorithm once all the priorities are known. While some key priorities are unknown, they are estimated by using nearby known key priorities ( <ref type="figure">Figure 6)</ref>.</p><p>Each time when a new sequence key is added to the tree, it was set to have an unknown priority. Any priority was assigned only after the key got first sampled and the corresponding sequence got passed through the learner. When a priority of a key is set or updated, the key node is deliberately removed from and placed back to the tree in order to become a leaf-node. This helped to set priorities of nodes in the immediate vicinity more accurately by using the freshest information available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">NETWORK ARCHITECTURE</head><p>The value of = 0.01 is the minimum probability of choosing a random action and it is hard-coded into the policy network. <ref type="figure" target="#fig_2">Figure 7</ref> shows the overall network topology while <ref type="table" target="#tab_5">Table 3</ref> specifies network layer sizes.   <ref type="bibr" target="#b6">(Hessel et al., 2017)</ref>. While ACER is the most closely related algorithmically, Rainbow is most closely related in terms of performance and thus a deeper understanding of the trade-offs between Rainbow and Reactor may benefit interested readers. There are many architectural and algorithmic differences between Rainbow and Reactor. We will therefore begin by highlighting where they agree. Both use a categorical action-value distribution critic <ref type="bibr" target="#b2">(Bellemare et al., 2017)</ref>, factored into state and state-action logits <ref type="bibr" target="#b31">(Wang et al., 2015)</ref>,</p><formula xml:id="formula_26">q i (x, a) = l i (x, a) j l j (x, a) , l i (x, a) = l i (x) + l i (x, a) ? 1 |A| b?A l i (x, b).</formula><p>Both use prioritized replay, and finally, both perform n-step Bellman updates.</p><p>Despite these similarities, Reactor and Rainbow are fundamentally different algorithms and are based upon different lines of research. While Rainbow uses Q-Learning and is based upon DQN <ref type="bibr" target="#b13">(Mnih et al., 2015)</ref>, Reactor is an actor-critic algorithm most closely based upon A3C <ref type="bibr" target="#b14">(Mnih et al., 2016)</ref>. Each inherits some design choices from their predecessors, and we have not performed an extensive ablation comparing these various differences. Instead, we will discuss four of the differences we believe are important but less obvious.</p><p>First, the network structures are substantially different. Rainbow uses noisy linear layers and ReLU activations throughout the network, whereas Reactor uses standard linear layers and concatenated ReLU activations throughout. To overcome partial observability, Rainbow, inheriting this choice from DQN, uses frame stacking. On the other hand, Reactor, inheriting its choice from A3C, uses LSTMs after the convolutional layers of the network. It is also difficult to directly compare the number of parameters in each network because the use of noisy linear layers doubles the number of parameters, although half of these are used to control noise, while the LSTM units in Reactor require more parameters than a corresponding linear layer would.</p><p>Second, both algorithms perform n-step updates, however, the Rainbow n-step update does not use any form of off-policy correction. Because of this, Rainbow is restricted to using only small values of n (e.g. n = 3) because larger values would make sequences more off-policy and hurt performance. By comparison, Reactor uses our proposed distributional Retrace algorithm for off-policy correction of n-step updates. This allows the use of larger values of n (e.g. n = 33) without loss of performance.</p><p>Third, while both agents use prioritized replay buffers <ref type="bibr" target="#b22">(Schaul et al., 2016)</ref>, they each store different information and prioritize using different algorithms. Rainbow stores a tuple containing the state x t?1 , action a t?1 , sum of n discounted rewards n?1 k=0 r t+k k?1 m=0 ? t+m , product of n discount factors n?1 k=0 ? t+k , and next-state n steps away x t+n?1 . Tuples are prioritized based upon the last observed TD error, and inserted into replay with a maximum priority. Reactor stores length n sequences of tuples (x t?1 , a t?1 , r t , ? t ) and also prioritizes based upon the observed TD error. However, when inserted into the buffer the priority is instead inferred based upon the known priorities of neighboring sequences. This priority inference was made efficient using the previously introduced contextual priority tree, and anecdotally we have seen it improve performance over a simple maximum priority approach.</p><p>Finally, the two algorithms have different approaches to exploration. Rainbow, unlike DQN, does not use -greedy exploration, but instead replaces all linear layers with noisy linear layers which induce randomness throughout the network. This method, called Noisy Networks <ref type="bibr" target="#b3">(Fortunato et al., 2017)</ref>, creates an adaptive exploration integrated into the agent's network. Reactor does not use noisy networks, but instead uses the same entropy cost method used by A3C and many others <ref type="bibr" target="#b14">(Mnih et al., 2016)</ref>, which penalizes deterministic policies thus encouraging indifference between similarly valued actions. Because Rainbow can essentially learn not to explore, it may learn to become entirely greedy in the early parts of the episode, while still exploring in states not as frequently seen. In some sense, this is precisely what we want from an exploration technique, but it may also lead to highly deterministic trajectories in the early part of the episode and an increase in overfitting to those trajectories. We hypothesize that this may be the explanation for the significant difference in Rainbow's performance between evaluation under no-op and random human starts, and why Reactor does not show such a large difference. <ref type="table">Table 4</ref>: Scores for each game evaluated with 30 random human starts. Reactor was evaluated by averaging scores over 200 episodes. All scores (except for Reactor) were taken from <ref type="bibr" target="#b31">Wang et al. (2015)</ref>, <ref type="bibr" target="#b14">Mnih et al. (2016)</ref> and <ref type="bibr" target="#b6">Hessel et al. (2017)</ref>. <ref type="table">Table 5</ref>: Scores for each game evaluated with 30 random noop starts. Reactor was evaluated by averaging scores over 200 episodes. All scores (except for Reactor) were taken from <ref type="bibr" target="#b31">Wang et al. (2015)</ref> and <ref type="bibr" target="#b6">Hessel et al. (2017)</ref>. <ref type="bibr">GAME</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">ATARI RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Single-step (left)  and multi-step (right) distribution bootstrapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(Left) Reactor performance as various components are removed. (Right) Performance comparison as a function of training time in hours. Rainbow learning curve provided by Hessel et al. (2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>the performance of Reactor with different versions of Reactor each time leaving one of the algorithmic improvements out. We can see that each of the algorithmic improvements (Distributional retrace, beta-LOO and prioritized replay) contributed to the final results. While prioritization was arguably the most important component, Beta-LOO clearly outperformed TISLR algorithm. Although distributional and non-distributional versions performed similarly in terms of median human normalized scores, distributional version of the algorithm generalized better when tested with random human starts(Table 1).</figDesc><table><row><cell>ALGORITHM</cell><cell cols="2">NORMALIZED MEAN</cell><cell>ELO</cell></row><row><cell></cell><cell>SCORES</cell><cell>RANK</cell><cell></cell></row><row><cell>RANDOM</cell><cell>0.00</cell><cell>11.65</cell><cell>-563</cell></row><row><cell>HUMAN</cell><cell>1.00</cell><cell>6.82</cell><cell>0</cell></row><row><cell>DQN</cell><cell>0.69</cell><cell>9.05</cell><cell>-172</cell></row><row><cell>DDQN</cell><cell>1.11</cell><cell>7.63</cell><cell>-58</cell></row><row><cell>DUEL</cell><cell>1.17</cell><cell>6.35</cell><cell>32</cell></row><row><cell>PRIOR</cell><cell>1.13</cell><cell>6.63</cell><cell>13</cell></row><row><cell>PRIOR. DUEL.</cell><cell>1.15</cell><cell>6.25</cell><cell>40</cell></row><row><cell>A3C LSTM</cell><cell>1.13</cell><cell>6.30</cell><cell>37</cell></row><row><cell>RAINBOW</cell><cell>1.53</cell><cell>4.18</cell><cell>186</cell></row><row><cell>REACTOR ND 5</cell><cell>1.51</cell><cell>4.98</cell><cell>126</cell></row><row><cell>REACTOR</cell><cell>1.65</cell><cell>4.58</cell><cell>156</cell></row><row><cell>REACTOR 500M</cell><cell>1.82</cell><cell>3.65</cell><cell>227</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>ALGORITHM</cell><cell cols="2">NORMALIZED MEAN</cell><cell>ELO</cell></row><row><cell></cell><cell>SCORES</cell><cell>RANK</cell><cell></cell></row><row><cell>RANDOM</cell><cell>0.00</cell><cell>10.93</cell><cell>-673</cell></row><row><cell>HUMAN</cell><cell>1.00</cell><cell>6.89</cell><cell>0</cell></row><row><cell>DQN</cell><cell>0.79</cell><cell>8.65</cell><cell>-167</cell></row><row><cell>DDQN</cell><cell>1.18</cell><cell>7.28</cell><cell>-27</cell></row><row><cell>DUEL</cell><cell>1.51</cell><cell>5.19</cell><cell>143</cell></row><row><cell>PRIOR</cell><cell>1.24</cell><cell>6.11</cell><cell>70</cell></row><row><cell>PRIOR. DUEL.</cell><cell>1.72</cell><cell>5.44</cell><cell>126</cell></row><row><cell>ACER 6 500M</cell><cell>1.9</cell><cell>-</cell><cell>-</cell></row><row><cell>RAINBOW</cell><cell>2.31</cell><cell>3.63</cell><cell>270</cell></row><row><cell>REACTOR ND 5</cell><cell>1.80</cell><cell>4.53</cell><cell>195</cell></row><row><cell>REACTOR</cell><cell>1.87</cell><cell>4.46</cell><cell>196</cell></row><row><cell>REACTOR 500M</cell><cell>2.30</cell><cell>3.47</cell><cell>280</cell></row><row><cell>: Random human starts</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>30 random no-op starts.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Specification of the neural network used (illustrated in Figure 7) COMPARISONS WITH RAINBOW In this section we compare Reactor with the recently published Rainbow agent</figDesc><table><row><cell>LAYER</cell><cell>INPUT</cell><cell></cell><cell>PARAMETERS</cell><cell></cell></row><row><cell></cell><cell>SIZE</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CONVOLUTIONAL</cell><cell></cell><cell>KERNEL</cell><cell>OUTPUT</cell><cell>STRIDES</cell></row><row><cell></cell><cell></cell><cell>WIDTH</cell><cell>CHANNELS</cell><cell></cell></row><row><cell>CONV 1</cell><cell>[84, 84, 1]</cell><cell>[8, 8]</cell><cell>16</cell><cell>4</cell></row><row><cell>CONCATRELU</cell><cell>[20, 20, 16]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CONV 2</cell><cell>[20, 20, 32]</cell><cell>[4, 4]</cell><cell>32</cell><cell>2</cell></row><row><cell>CONCATRELU</cell><cell>[9, 9, 32]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CONV 3</cell><cell>[9, 9, 64]</cell><cell>[3, 3]</cell><cell>32</cell><cell>1</cell></row><row><cell>CONCATRELU</cell><cell>[7, 7, 32]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FULLY CONNECTED</cell><cell></cell><cell></cell><cell>OUTPUT SIZE</cell><cell></cell></row><row><cell>LINEAR</cell><cell>[7, 7, 64]</cell><cell></cell><cell>128</cell><cell></cell></row><row><cell>CONCATRELU</cell><cell>[128]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RECURRENT</cell><cell></cell><cell></cell><cell>OUTPUT SIZE</cell><cell></cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSTM</cell><cell>[256]</cell><cell></cell><cell>128</cell><cell></cell></row><row><cell>LINEAR</cell><cell>[128]</cell><cell></cell><cell>32</cell><cell></cell></row><row><cell>CONCATRELU</cell><cell>[32]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LINEAR</cell><cell>[64]</cell><cell></cell><cell>#ACTIONS</cell><cell></cell></row><row><cell>SOFTMAX</cell><cell>[#ACTIONS]</cell><cell></cell><cell>#ACTIONS</cell><cell></cell></row><row><cell>X(1-)+ /#ACTIONS</cell><cell>[#ACTIONS]</cell><cell></cell><cell>#ACTIONS</cell><cell></cell></row><row><cell>RECURRENT Q</cell><cell></cell><cell></cell><cell>OUTPUT SIZE</cell><cell></cell></row><row><cell>LSTM</cell><cell>[256]</cell><cell></cell><cell>128</cell><cell></cell></row><row><cell>VALUE LOGIT HEAD</cell><cell></cell><cell></cell><cell>OUTPUT SIZE</cell><cell></cell></row><row><cell>LINEAR</cell><cell>[128]</cell><cell></cell><cell>32</cell><cell></cell></row><row><cell>CONCATRELU</cell><cell>[32]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LINEAR</cell><cell>[64]</cell><cell></cell><cell>#BINS</cell><cell></cell></row><row><cell>ADVANTAGE LOGIT HEAD</cell><cell></cell><cell cols="3">#ACTIONS ? #BINS</cell></row><row><cell>LINEAR</cell><cell>[128]</cell><cell></cell><cell>32</cell><cell></cell></row><row><cell>CONCATRELU</cell><cell>[32]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We store past action probabilities ? together with actions taken in the replay memory.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Not to be confused with importance weights of produced samples. 3 Sum of absolute discrete probability differences.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">All results are reported with respect to the combined total number of observations obtained over all worker machines.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">CONCLUSIONIn this work we presented a new off-policy agent based on Retrace actor-critic architecture and show that it achieves similar performance as the current state-of-the-art while giving significant real-time performance gains. We demonstrate the benefits of each of the suggested algorithmic improvements, including Distributional Retrace, beta-LOO policy gradient and contextual priority tree.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">'ND' stands for a non-distributional (i.e. classical) version of Reactor using Retrace<ref type="bibr" target="#b16">(Munos et al., 2016)</ref>. 6 Score for ACER inTable 2was obtained from(Figure 1in<ref type="bibr" target="#b32">Wang et al. (2017)</ref>), but is not directly comparable due to the authors' use of a cumulative maximization along each learning curve before taking the median.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oron</forename><surname>Anschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Baram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nahum</forename><surname>Shimkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="176" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yavar</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06887</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.10295</idno>
		<title level="m">Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Q-prop: Sample-efficient policy gradient with an off-policy critic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to play in a day: Faster deep reinforcement learning by optimality tightening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Frank S He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02298</idno>
		<title level="m">Rainbow: Combining improvements in deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-improving reactive agents based on reinforcement learning, planning and teaching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-H</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="69" to="97" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Asynchrony begets momentum, with an application to deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hadjis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communication, Control, and Computing (Allerton), 2016 54th Annual Allerton Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="997" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><forename type="middle">Puigdomenech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Prioritized sweeping: Reinforcement learning with less data and less time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher G Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="130" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Safe and efficient off-policy reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Bellemare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1046" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combining policy gradient and q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Brendan O&amp;apos;donoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Eligibility traces for off-policy policy evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning</title>
		<meeting>the Seventeenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Off-policy temporal-difference learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Laerning</title>
		<meeting>the 18th International Conference on Machine Laerning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural fitted q iteration-first experiences with a data efficient neural reinforcement learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">3720</biblScope>
			<biblScope unit="page" from="317" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05952</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Prioritized experience replay. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prioritized experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">George van den Driessche, Thore Graepel, and Demis Hassabis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature24270</idno>
		<ptr target="http://dx.doi.org/10.1038/nature24270" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Mastering the game of go without human knowledge</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double qlearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2094" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An algorithm for the organisation of information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Adel&amp;apos;son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Velskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Landis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dokl. Akad. Nauk SSSR</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="263" to="266" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Sasha</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01161</idno>
		<title level="m">Feudal networks for hierarchical reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Freitas</surname></persName>
		</author>
		<title level="m">Dueling network architectures for deep reinforcement learning. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sample efficient actor-critic with experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="272" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Explorations in efficient reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiering</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of Amsterdam</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with experience replay based on sarsa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanheng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence (SSCI), 2016 IEEE Symposium Series on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
