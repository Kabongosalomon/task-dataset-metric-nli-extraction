<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-task Attention Mechanism for Dense Multi-task Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Lopes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Valeo.ai</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoul</forename><surname>De Charette</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-task Attention Mechanism for Dense Multi-task Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-task Learning</term>
					<term>Cross-task Attention Mechanism</term>
					<term>Self-Attention</term>
					<term>Autonomous Driving</term>
					<term>Unsupervised Domain Adaptation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-task learning has recently become a promising solution for comprehensive understanding of complex scenes. Not only being memory-efficient, multi-task models with an appropriate design can favor exchange of complementary signals across tasks. In this work, we jointly address 2D semantic segmentation, and two geometry-related tasks, namely dense depth, surface normal estimation as well as edge estimation showing their benefit on indoor and outdoor datasets. We propose a novel multi-task learning architecture that exploits pairwise cross-task exchange through correlation-guided attention and self-attention to enhance the average representation learning for all tasks. We conduct extensive experiments considering three multi-task setups, showing the benefit of our proposal in comparison to competitive baselines in both synthetic and real benchmarks. We also extend our method to the novel multi-task unsupervised domain adaptation setting. Our code is available at https://github.com/cv-rits/DenseMTL</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>: Overview of our MTL framework. The three tasks semantic Segmentation, Depth regression and Normal estimation share an encoder E. The task-specific decoders S, D and N exchange information in the "multi-Task Exchange Block" (mTEB) via an attention-based mechanism, resulting in refined features to produce final predictions. literature focuses on pushing performance of single-tasks, being either semantic tasks like segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref> and detection <ref type="bibr" target="#b31">[32]</ref>, or geometrical tasks like depth/normal estimation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9]</ref>. Very few have paid attention to a more comprehensive objective of joint semantics and geometry understanding, while in practice, this is desirable in critical applications such as robotics and autonomous driving. In those systems, we would expect to have cooperative synergies among all tasks, i.e. tasks should be processed together in one unified system rather than separately. Arguably, promoting such synergies could also bring mutual benefit to all tasks involved. For example, having disruptive changes in depth maps may signal semantic boundaries in segmentation maps; while pixels of some semantic classes, like "road", may share similar surface normals.</p><p>To this end, multi-task learning (MTL) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref> has become a promising solution as it seeks to learn a unified model that excels in average on all tasks. A common MTL design is to have a large amount of parameters shared among tasks while keeping certain private parameters for individual ones; information is exchanged via shared parameters, making possible the synergy flow. Some recent works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33]</ref> focus on new multimodal modules that further boost tasks interactions. One important advantage of such MTL models is memory-efficiency thanks to the shared parts. In this work, we adopt the same principle and design our model with shared encoder and dedicated task decoders.</p><p>Recent MTL approaches come with different solutions to improve the information exchanged or distilled across tasks, often known as multi-modal distillation. PAD-Net <ref type="bibr" target="#b33">[34]</ref> and MTI-Net <ref type="bibr" target="#b25">[26]</ref> have proven the effectiveness of self-attention <ref type="bibr" target="#b27">[28]</ref> for MTL, i.e. a mechanism to self-discover most attentive signals of each task-specific feature for the other tasks. Differently, ATRC <ref type="bibr" target="#b1">[2]</ref> advocates to better look at pair-wise task similarity to guide multi-modal distillation. We partially follow the same direction as in ATRC with our correlation-guided attention, in which we use correlations between task-specific features to guide the construction of exchanged messages. Further, we propose an unified mechanism that combines two different attentions, correlation-guided attention and self-attention, via a learnable channel-wise weighting scheme.</p><p>In summary, we propose a novel multi-modal distillation design for MTL, relying on pair-wise cross-task attentions mechanisms (coined xTAM, Sec. 3.1) combined into a multi-task exchange block (coined mTEB, Sec. 3.2). We address three critical tasks for outdoor scene understanding: semantic segmentation, dense depth estimation and surface normal estimation. In extensive experiments on multiple benchmarks, our proposed framework outperforms competitive MTL baselines (Sec. <ref type="bibr">4.2)</ref>. With our new multi-task module, we also report improvement in semantic segmentation on Cityscapes where self-supervised depth plays as the auxiliary task <ref type="bibr" target="#b11">[12]</ref>. Empirically, we showcase the merit of our proposed multi-task exchange in a new setting of MTL unsupervised domain adaptation (MTL-UDA, Sec. 4.5), outperforming all baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-task learning. In the early days of neural networks, Caruana <ref type="bibr" target="#b2">[3]</ref> introduces the idea of multi-task learning with hard parameter sharing, i.e. some parameters are shared between all tasks, some are dedicated to separate tasks. In the same spirit, UberNet <ref type="bibr" target="#b15">[16]</ref> features a deep architecture to jointly address a large number of low-, mid-and high-level tasks. Zamir et al. <ref type="bibr" target="#b36">[37]</ref> conduct a large-scale study with 26 tasks on four million images of indoor scenes, studying the dependencies and transferabilities across tasks. While those seminal works show great potential of multi-task learning, they do note a few challenges, most notably the negative transfer phenomenon that degrades performance of certain tasks when learned jointly <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14]</ref>. Some works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5]</ref> reason that negative transfer is due to the imbalance of multi-task losses and introduce mechanisms to subsequently weight these individual loss terms. <ref type="bibr">Kendall et al. [14]</ref> propose to weight multiple losses by estimating the homoscedastic uncertainty of each task. Chen et al. <ref type="bibr" target="#b4">[5]</ref> introduce GradNorm, an algorithm that helps dynamically tuning gradient magnitudes such that the learning pace of different tasks are balanced. Differently, Sener and Koltun <ref type="bibr" target="#b22">[23]</ref> cast multi-task learning as multi-objective optimization where the goal is to find Pareto-optimal solutions. Cross-task mechanisms. Closer to our work are methods focusing on improving exchange or distillation across tasks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b1">2]</ref>; the main idea is that each task could benefit from different yet complementary signals from the others. Inspired from the success of visual attention in perception tasks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b6">7]</ref>, PAD-Net <ref type="bibr" target="#b32">[33]</ref> uses an attention mechanism to distill information across multi-modal features. In MTI-Net <ref type="bibr" target="#b25">[26]</ref>, Vandenhende et al. extend PAD-Net with a multi-scale solution to better distill multimodal information. Zhang et al. <ref type="bibr" target="#b37">[38]</ref> propose to aggregate affinity maps of all tasks, which is then "diffused" to refine task-specific features. PSD <ref type="bibr" target="#b38">[39]</ref> mines and propagates patch-wise affinities via graphlets, instead we model the interactions with attention mechanisms. Bruggemann et al. <ref type="bibr" target="#b1">[2]</ref> introduce ATRC to enhance multi-modal distillation with four relational context types based on different types and levels of attention. Similar to ATRC, our method also exploits pairwise task similarity to refine task-specific features. Differently though, we advocate to combine the pairwise similarity with cross-task self-attention via a learnable weighting scheme to learn the refined residuals that complement the original task-specific features.</p><p>Recent efforts seek for label-efficient learning paradigms to train models for urban scene understanding. Tosi et al. <ref type="bibr" target="#b24">[25]</ref> introduces real-time multi-task network based on knowledge distillation and self-supervision training. Hoyer et al. <ref type="bibr" target="#b11">[12]</ref> propose a novel architecture and different strategies to improve semantic segmentation with selfsupervised monocular depth estimation. One interesting finding in <ref type="bibr" target="#b11">[12]</ref> is that multi-task learning module of PAD-Net <ref type="bibr" target="#b32">[33]</ref> complements other self-supervised strategies and further improve segmentation performance. On this regard, we study the effect brought by our proposed module in this particular setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In multi-task learning, the aim is to optimize a set of n tasks: i P tT 1 , ..., T n u while seeking a general good performance -as opposed to favoring a single task. Our model takes an input image and makes n predictions. Generally, this is achieved with a shared encoder and separate task-specific decoders. Here, we introduce a novel mechanism that enhances cross-task talks via features exchange, building on the intuition that each decoder discovers unique but complementary features due to its separate supervision <ref type="bibr" target="#b36">[37]</ref>. Our bidirectional xTAM module enables information between task pairs (i, j) via the discovery of directional features, f j i and f i j . For clarity, only xTAM j i is detailed here. It relies on two attention mechanisms. Fist, a correlation-guided attention (green, upper part) to discover xtask j i , the features from task j contributing to task i. Second, a self-attention (purple, lower part) to discover complementary signal self j i from j. Gray blocks are 1x1 projection layers for compatibility, and ? s is a s-times downscale for memory efficiency (reversely, ? s for upscale). Best viewed in color.</p><p>We formulate in Sec. 3.1 a bidirectional cross-Task Attention Mechanism module, coined xTAM, taking as input a pair of task-specific features and returning two directional features. We then present in Sec. 3.2 our complete MTL framework for scene understanding that encompasses our multi-Task Exchange Block (mTEB, see <ref type="figure">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">xTAM: Bidirectional Cross-Task Attention Mechanism</head><p>Building upon recent works demonstrating task interactions <ref type="bibr" target="#b23">[24]</ref>, our module seeks to capture the shared pair-wise task knowledge while preserving their exclusive (nonshared) knowledge. Our intuition is that we can exploit features from pairs of tasks denoted pi, jq and self-discover the interactions from their correlation matrices for either directional interaction: i j and j i. <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates our xTAM component which helps knowledge distillation between tasks, taking as input the task features pf i , f j q and returning the corresponding directional features for the other task, i.e. pf j i , f i j q.</p><p>For ease of speech we describe the directional xTAM j i where task j helps task i, but emphasize that xTAM, being bidirectional, is made of both xTAM j i and xTAM i j .</p><p>Directional cross-task attention (xTAM j i ). Considering here i as our primary task, and j as the secondary, we seek to estimate the features from task j that can contribute to i. We leverage two cross-task attentions: (i) a correlation-guided attention which goal is to exploit cross-task spatial correlations to guide the extraction of contributing features of the secondary task to the primary task, and (ii) a self-attention on the secondary task to self-discover complementary signals which are beneficial for the primary task. Visualizations of the two attentions are colored in green and purple in <ref type="figure" target="#fig_0">Fig. 2</ref>, respectively. Importantly, note that each attention contributes differently to the primary task i relying either on identification of shared j and i knowledge or on exclusive j knowledge. We use 1x1 convolutional layers (gray blocks in <ref type="figure" target="#fig_0">Fig. 2</ref>) for dimension compatibility.</p><p>Correlation-guided attention. To guide feature inclusion, we rely on the spatial correlation of task features (green blocks in <ref type="figure" target="#fig_0">Fig. 2</ref>). In practice, because computing such correlation is memory-greedy, we reduce complexity by projecting the f i and f j downscaled features onto d-dimensional sub-spaces as in <ref type="bibr" target="#b27">[28]</ref> such that K " P Q p? s f i q, Q " P K p? s 2 f j q, having ? s the downscale operator with s the scale factor, and P Q , P K separate 1x1 convolutions. The spatial-correlation matrix C j i is then obtained by applying a softmax on the matrix multiplication K T?Q normalized by ? d:</p><formula xml:id="formula_0">C j i " softmax?K T?Q ? d?.<label>(1)</label></formula><p>d is the feature size <ref type="bibr" target="#b27">[28]</ref>. Intuitively, C j i has high values where features from i and j are highly correlated and low values otherwise, which we use to weight features from f j . Subsequently, we obtain our correlation-guided attention features xtask j i by multiplying the correlation matrix C j i with the projected j features:</p><formula xml:id="formula_1">xtask j i "? s pV?C j i q ,<label>(2)</label></formula><p>with V " P V p? s 2 f j q, P V the 1x1 projection, and ? s the upsample operator.</p><p>Self-attention. We additionally employ a spatial attention <ref type="bibr" target="#b32">[33]</ref> which we denote as 'self-attention' to contrast with the above which takes pairs of differing tasks. Instead the following mechanism (purple blocks in <ref type="figure" target="#fig_0">Fig. 2</ref>) takes features from j alone and aims to extract private information from f j that are relevant for predicting task i:</p><formula xml:id="formula_2">self j i " F f pf j q d ?pF m pf j qq ,<label>(3)</label></formula><p>being our self-attention features. Where d is the element-wise multiplication, and ? the sigmoid function. Both F f and F m are convolutional layers which are supervised by the target task i to learn to extract relevant information from features f j . The self-attention features self j i is defined as the point-wise multiplication between the features coming from F f and the dynamic mask provided by F m .</p><p>Directional feature. To construct the final directional features f j i for the j i interaction, the two attention based feature maps are combined as:</p><formula xml:id="formula_3">f j i " rdiagp? 1 , ..., ? c q?xtask j i , self j i s,<label>(4)</label></formula><p>where r., .s is the channel-wise concatenation operation, and ? 1..c are learnable scalars used to weight the c channels of xtask j i . All ? 1..c are initialized with 0; learning will adaptively adjust the per-channel weighting. Intuitively, cross-task exchange first starts with self-attention only, then gradually adjusts the ? 1..c values to include some contribution from the correlation-guided attention. This initialization strategy is important to stabilize training, especially here where we combine different types of attention.</p><p>Overall, the bidirectional xTAM block is made up of two directional blocks, xTAM j i and xTAM i j ; it outputs both f j i and f i j for each pi, jq task pair.</p><p>Discussion. Our xTAM design is different from existing multi-modal distillation modules <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b1">2]</ref>. While PAD-Net <ref type="bibr" target="#b32">[33]</ref> and MTI-Net <ref type="bibr" target="#b25">[26]</ref> only consider cross-task selfattention to distill multi-modal information, ATRC [2] models pair-wise task similarity, which shares a similar spirit to our correlation-guided attention. Different to those, our xTAM learns to adaptively combine cross-task self-attention and correlation-guided attention using a learnable weighting scheme. Of note, adopting the multi-scale strategy of MTI-Net <ref type="bibr" target="#b25">[26]</ref> or having other pair-wise modules of ATRC <ref type="bibr" target="#b1">[2]</ref> is orthogonal to our xTAM. There might be potential improvements by systematically combining those strategies with xTAM. However, we focus here on giving an extensive study of xTAM design and applications; we leave such combinations for future investigation.</p><p>3.2 Multi-task Learning Framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3:</head><p>Our cross-task MTL framework. We jointly predict semantic segmentation (S, blue), depth (D, orange) and surface normal (N , green). All tasks share encoder (E) and have dedicated decoders (shown as trapezoids). Our cross-task attention module is inserted within a "multi-Task Exchange Block" (mTEB) (Sec. 3.2) that takes taskspecific features as inputs and returns refined features of the same kinds. Here, three xTAM modules are used to extract pairs of directional features, where each represents the complementary signal from one task to another, e.g. f D S encapsulates depth information (D) to extracted to improve the segmentation task (S). Multi-modal directional features are combined in the square blocks, resulting in multi-modal residuals for refinement. Task specific losses (shown in red) are applied before and after our mTEB.</p><p>We focus on semantic and geometrical estimation tasks as they are critical for scene understanding, especially outdoor environments. <ref type="figure">Fig. 3</ref> overviews our MTL framework having a unique encoder E and illustrates the interactions taking place between the three different task decoders (S, D and N ) in a three-task setup: T " tSegmentation, Depth, Normalu. We first describe our "multi-Task Exchange Block" (mTEB) that encapsulates our cross-task exchanges, and then go over our MTL training setup. multi-Task Exchange Block (mTEB). Our block is a plugin that can be inserted at any stage in the decoder architecture, taking task-specific features as inputs, here pf S , f D , f N q, and outputting refined features, here pf S ,f D ,f N q. The block consists of one bidirectional xTAM module (Sec. 3.1) per task pair, each returning two directional features which are fused to the input task features.</p><p>Considering i to be the primary task, we concatenate the n?1 directional features f j i | jPT ztiu , contributing towards task i, along the channel dimension. The concatenated feature map is then processed with F i (depicted as square blocks in <ref type="figure">Fig. 3</ref>): a 1x1 convolution followed by a batch-normalization <ref type="bibr" target="#b12">[13]</ref> and ReLU activation. Projected features are then fused with the main task base features, f i , via an element-wise addition denoted ? , leading to the final refined featuresf i :</p><formula xml:id="formula_4">f i " f i ? F i`r f j i | jPT ztiu s?,<label>(5)</label></formula><p>where r...s is the channel-wise concatenation operation. Intuitively, the xTAM directional features are combined in F i to constitute multi-modal residuals that complement original features f i . By design, the refined features have same dimensions than the input task-specific features.</p><p>Although, it is possible to insert a mTEB at several levels in the decoder. We show experimentally that the overall best choice is to have a single block before the last layer.</p><p>Training. To train our framework, we use a multi-scale multi-task objective defined as a linear combination of the task losses applied at two or more stages of the decoder: before any mTEB, and on the full resolution output. Although we do not explicitly introduce losses to encourage cross-task distillation, this is made possible implicitly by the design of xTAM and mTEB. More details are provided in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>This section presents all results and ablation studies of our proposed MTL framework. We describe in Sec. 4.1 our experimental setups including the four leveraged datasets, five baselines, three set of tasks; we show as well the training details and evaluation metrics. As tasks, we consider the four dense prediction tasks of semantic segmentation, depth estimation, surface normal estimation, and semantic edge estimation, that we refer to as S, D, N , and E respectively. Our source code is publicly released.</p><p>Sec. 4.2 puts forward main experiments for fully supervised multi-task learning, including: two sets 'S-D' (segmentation + depth) and 'S-D-N ' (+ normals) on different urban datasets, in Tab. 1. As well as performance on an indoor dataset with the additional task of edge estimation in a third task set 'S-D-N -E' (+ edges), in Tab. 2.</p><p>Sec. 4.3 presents three ablation studies on (i) the position of mTEB in the decoders, (ii) different types of fusions used in mTEB, and (iii) alternative designs for the correlation-guided attention in xTAM.</p><p>We further demonstrate the benefit of our multi-task exchange framework, for semantic segmentation in Sec. 4.4 and for Unsupervised Domain Adaptation in Sec. 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Common setup.</p><p>Datasets. We leverage two synthetic datasets and two real datasets: VKITTI2 <ref type="bibr" target="#b7">[8]</ref>, Synthia <ref type="bibr" target="#b20">[21]</ref> and Cityscapes <ref type="bibr" target="#b5">[6]</ref>, NYUDv2 <ref type="bibr" target="#b18">[19]</ref> respectively. In Cityscapes, we use stereo depth from semi-global matching as labels. Surface normals labels are estimated from depth maps following <ref type="bibr" target="#b34">[35]</ref>. Lastly, NYUDv2 provides ground-truth semantic edge maps which we use for supervision. We train semantic segmentation on 14 classes on VKITTI2, 40 classes on NYUDv2, and 16 classes on Synthia and Cityscapes. In VKITTI2 and Synthia, we perform a random 80/20 split. For Cityscapes and NYUDv2, we use official splits and load the images at 1024?512 and 576?448 respectively. Whereas they are scaled to 1024?320 for VKITTI2 and 1024?512 for Synthia.</p><p>Baselines. Following <ref type="bibr" target="#b26">[27]</ref> we compare our performance against single task learning networks (STL), where each task is predicted separately by a dedicated networks, and a naive multi-task learning baseline (MTL) using a shared encoder and task-specific decoders. PAD-Net <ref type="bibr" target="#b32">[33]</ref> is a more competitive baseline than MTL. We address two variants of PAD-Net: (i) the original model used in <ref type="bibr" target="#b32">[33]</ref> and (ii) a stronger model introduced in <ref type="bibr" target="#b11">[12]</ref> ('3-ways PAD-Net '). Details on all models are in Appendix B.</p><p>Architecture. All networks have a shared ResNet-101 <ref type="bibr" target="#b10">[11]</ref> encoder and a suitable amount of task decoders depending on the task set. Decoders of the STL, MTL and PAD-Net baselines are similar as described in <ref type="bibr" target="#b26">[27]</ref>. For 3-ways PAD-Net and our model, we stick to <ref type="bibr" target="#b11">[12]</ref> and use an Atrous Spatial Pyramid Pooling (ASPP) <ref type="bibr" target="#b3">[4]</ref> followed by four upsampling blocks with skip connections <ref type="bibr" target="#b19">[20]</ref>.</p><p>Training details. We train all models for 40k iterations, set gradient norm clipping to 10 and reduce the learning rates by 0.1 at 30k steps as in <ref type="bibr" target="#b11">[12]</ref>. We train our MTL models with the Adam optimizer <ref type="bibr" target="#b14">[15]</ref> with ? 1 " 0.9 and ? 2 " 0.98; learning rate is set to 2.0e?4 for the backbone, and 3.0e?4 for all decoders. In other models, we use the SGD optimizer, setting the learning rates of the backbone and decoders to 1.0e?3 and 1.0e?2, respectively as in <ref type="bibr" target="#b11">[12]</ref>. We use momentum of 0.9 and weight decay of 5.0e?4. We always report the average and standard deviation over 3 runs.</p><p>Supervision. For training, we use the cross-entropy for semantic segmentation L S and edge estimation L E <ref type="bibr" target="#b1">[2]</ref>. The berHu loss <ref type="bibr" target="#b16">[17]</ref> is applied on inverse normalized values for depth regression as L D pdq " berHupd f ar {d, d f ar {dq. To supervise surface normals training, we use the approach of <ref type="bibr" target="#b9">[10]</ref>: L N pnq " 1?cospn, nq, which computes the cosine between the estimatedn and ground-truth n normal vectors as cospn, nq " n?n{?n??n?. Losses are weighted using our task balancing strategy.</p><p>Single-task metrics. We report a standard metric for each task: the mean intersection over union (mIoU) for semantic segmentation, the root mean square error in meters (RMSE) for depth regression, the mean error in degrees (mErr) for surface normals estimation, and the F1-score (F1) for semantic edge estimation.</p><p>Multi-task setup.</p><p>Task balancing. We observe that tasks loss weighting heavily impacts the performance of multi-task models similarly to <ref type="bibr" target="#b26">[27]</ref>, though we also notice that weights vary primarily with the loss functions and their interaction, and much less on the data or model. This is fairly reasonable knowing gradients scale with the loss term magnitude. Hence, as in <ref type="bibr" target="#b26">[27]</ref> we applied grid-search for each unique task set, and use the same optimal set of weight for all methods. For T "tS, Du we obtain tw S "50, w D "1u, for T "tS, D, Nu we get tw S "100, w D "1, w N "100u, and choose for T "tS, D, N, Eu, tw S "100, w D "1, w N "100, w E "50u. Importantly, this search is done on the MTL baseline ensuring we do not favor our method in any way.</p><p>MTL metrics. Again, our goal is to improve the overall performance of the model over a set of tasks T , which we measure using the delta metric of <ref type="bibr" target="#b26">[27]</ref> written ? T . The latter measures the relative performance of a given multi-task model compared to the performance of dedicated Single Task Learning (STL) models: ? T pfq " 1{n ? iPT p?1q gi pm i?bi q{b i , where m i and b i are the metrics on task i of the model f and STL model respectively. While g i is the metric direction of improvement, i.e. g i "0 when higher is better, and g i "1 when lower is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main results</head><p>Tab. 1 reports our main experimental results on three datasets, and two multi-task setups: 'S-D' and 'S-D-N '. On synthetic data with perfect segmentation and depth labels, our model outperforms all baselines by large margins of up to`5.69? SD in 'S-D' and up to`5.75? SDN in 'S-D-N '. In Cityscapes, having stereo depth pseudo labels, our model is outperformed by 3-ways PAD-Net , only in the 'S-D' setup, and outperforms 3-ways PAD-Net in 'S-D-N '. Quantitative results prove the effectiveness of our proposed cross-task attention mechanism.</p><p>The improvements of PAD-Net over MTL baselines confirm the benefit of selfattention mechanism <ref type="bibr" target="#b32">[33]</ref>. Revisiting the "3-ways" architecture introduced in <ref type="bibr" target="#b11">[12]</ref> in a different context of multi-task learning, we observe a significant leap in performance of 3-ways PAD-Net over PAD-Net; the two models are only different in decoder design. Such result emphasizes the importance of decoder in MTL.</p><p>An important remark here is that the best overall performance (i.e. delta metrics) coincides with the best individual task performances, which shows the benefit of handling multiple tasks together. It is striking to look at the 'S-D-N ' setup, where we also account for the rarely studied normal estimation task. In this three-tasks setup, in addition to the ? SDN pf SDN q metric, we report a partial delta measurement ? SD pf SDN q. This allows to compare f SD to f SDN , respectively the 'S-D' and 'S-D-N ' models considering normal estimation as an auxiliary task. We record a gap of up to +3.23 points between ? SD pf SDN q and ? SD pf SD q, showing the benefit of injecting additional geometrical cues in the form of surface normals to help the other two tasks.</p><p>We provide additional results in Tab. 2 by evaluating our method on the indoor dataset NYUDv2 and introducing a third task 'E'. Not only do we show the effectiveness of our method, we also demonstrate the benefit of multi-task learning. The partial  <ref type="table">Table 1</ref>: MTL performance on two sets. We report individual task metrics but seek best overall tasks performance measured by ? SD and ? SDN (cf. text), computed w.r.t Single Task Learning (STL). Except for 'S-D' on Cityscapes where we are second, we outperform significantly the baselines on all delta metrics. We also report ? SD in 'S-D-N ' for direct comparison with 'S-D'. Notice ? SD pf SD q ? ? SD pf SDN q on all methods highlighting the importance of surface normals estimation. We highlight best and 2nd best.  <ref type="table">Table 2</ref>: Results on NYUDv2 evaluated on three sets of tasks. We report a delta metric for each set as in Tab. 1 and provide partial metrics ? SD pf SDN q and ? SDN pf SDNE q to compare between sets and highlight the benefit of inserting additional tasks in the framework. We underline the fact that ? SDN pf SDN q ? ? SDN pf SDNE q validating the benefit of edge estimation. Ours outperforms all other methods across the three sets. delta metrics ? SDE pf SD q and ? SDN pf SDNE q underline the advantage of normals and edge estimation respectively. They are shown to provide additional valuable cues which help further understand the structure of the scene. <ref type="figure">Figure 4</ref> shows qualitative results on the Cityscapes dataset. This is noticeable when looking at thin elements (e.g. pedestrians, bicycles, etc.) and object contours. The visuals are in fair adequacy with the quantitative analysis. More results in Appendix C.</p><formula xml:id="formula_5">Methods 'S-D' 'S-D-N ' Semseg ? Depth ? Delta ? Semseg ? Depth ? Delta ? Normals ? Delta ? mIoU % RMSE m ? SD % mIoU % RMSE m ? SD</formula><formula xml:id="formula_6">'S-D' 'S-D-N ' 'S-D-N -E' Methods Semseg ? Depth ? Delta ? Semseg ? Depth ? Delta ? Normals ? Delta ? Semseg ? Depth ? Normals ? Delta ? Edges ? Delta ? mIoU % RMSE m ?SD % mIoU % RMSE m ?SD</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation studies</head><p>We report ablations of our cross-task distillation module on the VKITTI2 dataset on both the 'S-D' and 'S-D-N ' setups. We evaluate our method using different combinations of our module, varying the position and number of exchange blocks (Tab. 3), changing the type of fusion used (Tab. 4a), or the type of attention used (Tab. 4b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmentation</head><p>Depth Normal</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PAD-Net [33]</head><p>3-ways PAD-Net <ref type="bibr" target="#b11">[12]</ref> Ours <ref type="figure">Fig. 4</ref>: Qualitative MTL results on Cityscapes in the 'S-D-N ' setup. The first row shows the input image and corresponding segmentation, depth and normals groundtruths. Segmentation results of our models are better overall, especially in boundary areas -see "bicycle", "rider" and "pedestrian". Surface normals predictions of PAD-Net is blurrier as compared to 3-ways PAD-Net and Ours.</p><p>Multi-task exchange. In Tab. 3 we report variations of our MTL framework with mTEB (cf. Sec. 3.2) positioned at different scales in the decoders, where scale s is the number of downsampling from input resolution. We observe that with a single block (upper part, Tab. 3), the ? SD and ? SDN metrics are lower if the block is placed early in the decoder (i.e. scale 4) as opposed to being later in the decoder (i.e. scale 1). This resonates with the intuition that late features are more task-specific, and thus better appropriate for cross-task distillation. Using several blocks (lower part, Tab. 3) was shown marginally beneficial, and at a cost of significantly more parameters ('Param.' column).</p><p>Multi-task features fusion. Considering a single mTEB at scale 1, we replace the original add fusion operator in Eq. (5) with either a concatenation or a product and report results in Tab. 4a. We show this choice impacts the overall delta metric. Experimentally, we notice that the gap differs on other datasets though 'add' and 'prod' remain best.</p><p>Correlation-guided attention. While our original xTAM design (cf. Sec. 3.1) uses spatial attention with Eq. (1) for cross-task correlation-guided exchange, other practices exist in the literature <ref type="bibr" target="#b25">[26]</ref>. Hence, in Tab. 4b we replace our choice of attention in Eq. (1) using either spatial attention, channel attention, or both. Results show that channel attentions or both are less efficient here, and come at higher implementation complexity.</p><p>In Appendix A.2 we show it is better preferred to combine correlation-guided attention and self-attention in our xTAM in order to reach the best performance.  <ref type="table">Table 3</ref>: Performance on VKITTI2 when varying the mTEB positions. The upper part shows our block is more efficient when located later in the decoders (i.e. scale 1) which we attribute to more task-specific features. Using more blocks leads to a small boost at the cost of more parameters ('Param.' column represents the size of distillation block).  <ref type="table">Table 4</ref>: Ablation of our fusion and cross-task attention mechanism on VKITTI2.</p><formula xml:id="formula_7">'S-D-N ' Fusion Semseg ? Depth ? Normals ? Delta ? mIoU % RMSE</formula><p>(a) Changing our addition operation in Eq. (5) with a product or a concatenation shows slight advantage for our addition strategy. (b) We replace spatial attention in our xTAM (Sec. 3.1) with either channel only, or both spatial and channel attentions. Overall, spatial performs better with lower complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">MTL for segmentation</head><p>We now evaluate the effectiveness of our multi-task strategy focusing only on semantic segmentation performance -often seen as a core task for scene analysis. In Tab. 5, we report performance training with two tasks (semantics+depth) on 'Cityscapes SDE' where unlike prior results we use Self-supervised monocular Depth Estimation (SDE) with Monodepth2 <ref type="bibr" target="#b8">[9]</ref>, as in "3-ways" <ref type="bibr" target="#b11">[12]</ref>. In the latter, a proposed strategy is to use the multi-task module of PAD-Net ("3-ways PAD-Net " in Tab. 5). We proceed similarly but replacing the above module with our mTEB (hereafter, "3-ways mTEB "). We refer to <ref type="bibr" target="#b11">[12]</ref> and Appendix B.2 for in-depth technical details. Note that neither "3-ways PAD-Net " nor "3-ways mTEB "use the strategies of "3-ways" like data augmentation and selection for annotation. Still, Tab. 5 shows that using our module, i.e. 3-ways mTEB , achieves best results (`3.20 mIoU). This shows the increased benefit of our multi-task module. * We only report mIoU for '3-ways' because <ref type="bibr" target="#b11">[12]</ref> does not release their weights on the full train set. <ref type="table">Table 5</ref>: Multi-task learning with 'S-D' for semantic segmentation on Cityscapes SDE. Using the architecture of "3-ways" <ref type="bibr" target="#b11">[12]</ref> we either insert PAD-Net ("3-ways PAD-Net ") or our mTEB ("3-ways mTEB ") as distillation block and compare the performance on semantics only, showing our multi-task exchange is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">MTL for Unsupervised Domain Adaptation (MTL-UDA)</head><p>Unsupervised domain adaptation is the line of research dealing with distribution shifts between the source domain, in which we have labeled data, and the target domain, in which only unlabeled data is available for training. Here, we extend our experiments to the novel MTL Unsupervised Domain Adaptation setup (MTL-UDA) where the goal is to perform well in average on all tasks in the target domain. We argue task exchange is beneficial for MTL-UDA since semantic-and geometry-related tasks exhibit different behaviors and sensitivities to the shift of domain, but were shown complementary in above experiments. We leverage typical synthetic to real scenarios, namely Synthia? ?Cityscapes and VKITTI2? ?Cityscapes, reporting results on the 'S-D' multi-task setup. Typically, while semantic segmentation degrades due to the shifts in color and texture, depth estimation is more affected by change in the scene composition and the optics sensor.</p><p>Architecture adjustments. We adopt a simple multi-task Domain Adaptation (DA) solution based on output-level DA adversarial training <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22]</ref>. As output-level techniques do not alter the base MTL architecture, it allows direct understanding whether our MTL design is favorable or not for adaptation. In detail, for each task, a small discriminator network is jointly trained with the main MTL model, being two sides in an adversarial game. While discriminators try to tell from which domain the input data comes from, the MTL model tries to fool all discriminators by making outputs of source and target domain indistinguishable, which eventually helps align source/target. The complete DA architecture and training are detailed in Appendix B.3.</p><p>Baselines adjustments. We use the same baselines as in Sec. 4.2 and introduce patch discriminators to align the processed output prediction maps (see Appendix B.3). We use the STL-UDA models -consisting of a single task decoder and one output typeas baseline for measuring the delta metric. We train all methods on source and target in mix batches containing two instances from either domains.</p><p>Results. In Tab. 6 results show that the use of our multi-task exchange significantly improves performance in all scenarios and metrics. Along with MTL results, we re-port STL source, trained only on source, and STL oracle, trained on labeled target. In Synthia? ?Cityscapes (Tab. 6a) our method very significantly outperforms the naive MTL-UDA baseline by`38.1? SD , and by`8.34? SD in VKITTI2? ?Cityscapes (Tab. 6b), also coinciding with the best individual task metrics. An interesting note is that STL-UDA is preferable over straightforward multi-task learning in UDA (MTL-UDA), which we attribute to the competing objectives in MTL-UDA.  <ref type="table">Table 6</ref>: Multi-Task Learning Unsupervised Domain Adaptation considering a two tasks 'S-D' setup. We report ? SD metric w.r.t single-task learning unsupervised domain adaptation (STL-UDA). Overall, it is interesting to note that naive multi-task adaptation strategy (MTL-UDA) is lowering performance w.r.t STL-UDA, while our method outperforms all other strategies in both scenarios. To best match the source classes definition, we either use 16 or 8 semantic classes; hence the difference in mIoU for STL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We address semantic and geometry scene understanding with a multi-task learning framework on the four tasks: semantic segmentation, dense depth regression, surface normal estimation, and edge estimation. We propose a novel multi-modal distillation module, namely multi-Task Exchange Block, built upon xTAM, a bidirectional crosstask attention mechanism that combines two types of cross-task attentions to refine and enhance all task-specific features. Extensive experiments in various datasets demonstrate the effectiveness of our proposed framework over competitive MTL baselines. We extend the application of our module in two practical scenarios: (i) to improve semantic segmentation of a recent self-supervised framework <ref type="bibr" target="#b11">[12]</ref> in Cityscapes and (ii) to improve performance of multi-task synthetic-2-real unsupervised domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Method details</head><p>Our method enables task exchanges to boost the general tasks performance with our multi-task exchange block (mTEB) introduced in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Training</head><p>Considering T " tT 1 , . . . , T n u the set of n tasks to be jointly optimized, our general MTL training loss is a weighted combination of individual tasks losses and writes:</p><formula xml:id="formula_8">L tasks " 1 |S| ? sPS ? tPT ? t L s t`? tPT ? t L final t ,<label>(6)</label></formula><p>where ? t is the task-balancing weight, L t is the task-specific supervision loss at intermediate scale s (i.e. L s t ) or at the final prediction stage (i.e. L final t ). Additionally, S defines the intermediate scales of supervision considering scale s to be the task intermediate output at resolution 1 2 s w.r.t input resolution. In practice, to enforce cross-task exchange without direct mTEB supervision, we set S equal to the scales at which the mTEB are inserted. In our method, we keep a single mTEB and set S " t1u.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Ablations</head><p>We study the overall benefit of our multi-task exchange block (mTEB) on VKITTI2 with our complete architecture. To further demonstrate the effect of attention mechanisms in our method, we remove self-attention from the directional features of xTAM, so that Eq. (4) now writes: f j i " rdiagp? 1 , ..., ? c q?xtask j i s . We report the performance of 'S-D-N ' with spatial cross-task attention. Without self-attention we get 97. <ref type="bibr" target="#b39">40</ref>  <ref type="bibr" target="#b39">[40]</ref> is applied since the models used are not scale-aware. ? Normals estimation, we measure the absolute angle error in degrees between the label n and predicted mapn: m N " degpn, nq. For all datasets we retrieve labels from the depth map <ref type="bibr" target="#b34">[35]</ref>: we unproject the pixels using the camera intrinsics and depth values, then compute the cross-product using neighboring points (from 2D perspective) <ref type="bibr" target="#b9">[10]</ref> and average over four pairs of neighbors <ref type="bibr" target="#b35">[36]</ref>. Cityscapes <ref type="bibr" target="#b5">[6]</ref> provides disparity maps which we use to compute noisy surface normals labels. ? Edge estimation, we apply the F1-score between the predicted and ground-truth maps: m E " F1p?, eq. NYUDv2 <ref type="bibr" target="#b18">[19]</ref> provides ground-truth semantic edge maps.  <ref type="table">Table 7</ref>: Performance of the 'MTL' baseline model (cf. <ref type="figure">Fig. 5</ref>) for different sets of multi-task weights on VKITTI2. There are important remarks. First, uniform weighting is far from optimal. Second, best ? T does not always equate to optimality on individual metrics as shown by the results in bold. Ultimately, to avoid favoring a single task, we use the set of weights with highest ? T metric for all models, as highlighted in red .</p><p>Task balancing. Tab. 7 reports a subset of our grid-search to select an optimal set of weights for both the 'S-D' and 'S-D-N ' sets. To avoid favoring a specific task or model, the evaluation is conducted on the 'MTL' baseline model and we select the set of weights from best ? T metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Main results</head><p>General architectures (Tabs. 1 to 4). In <ref type="figure">Fig. 5</ref> we show the general architectures (i.e. , considering depth supervision), including the STL, MTL and PAD-Net models <ref type="bibr" target="#b11">[12]</ref>, 3-ways PAD-Net <ref type="bibr" target="#b11">[12]</ref> and Ours. Based on the same encoder taken from a pretrained ResNet-101 backbone <ref type="bibr" target="#b10">[11]</ref>, those multi-task networks differ only in decoder design. We observe improvements in all tasks using both Atrous Spatial Pyramid Pooling (ASPP) and UNetlike connections as done in <ref type="bibr" target="#b11">[12]</ref> (cf. 3-ways PAD-Net vs. PAD-Net in Tab. 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SDE architectures (Tab. 5).</head><p>To allow monocular depth estimation in 'MTL for segmentation' (Sec. 4.4), we adopt the setup of <ref type="bibr" target="#b11">[12]</ref> where intermediate depth estimation from pair of consecutive frames is supervised by a photometric reconstruction loss <ref type="bibr" target="#b8">[9]</ref>. <ref type="figure">Fig. 6</ref> shows the architecture used for all 3-ways variants of Tab. 5. Variants consist of swapping the yellow 'Exchange block' with either 'PAD-Net block' (3-ways PAD-Net ) or our 'mTEB' (3-ways mTEB ).</p><p>To train, we use 1.0e?5, 5.0e?5, and 1.0e?6 as learning rates for the encoder, decoder and pose estimation network respectively. The training strategy is similar to our other MTL setups, only this time we initialize all models with weights from a singlebranch model trained on self-supervised depth estimation (cf. <ref type="bibr" target="#b11">[12]</ref>). Since the depth loss STL <ref type="bibr" target="#b26">[27]</ref> MTL <ref type="bibr" target="#b26">[27]</ref> PAD-Net <ref type="bibr" target="#b32">[33]</ref> 3-ways PAD-Net <ref type="bibr" target="#b11">[12]</ref> Ours <ref type="figure">Fig. 5</ref>: General architectures. For clarity, we only visualize two tasks in the multi-task networks. While the encoder is identical, models differ in their decoder architecture, with PAD-Net, 3-ways PAD-Net and Ours using dedicated tasks exchange blocks.</p><p>differs from the supervised one used in Sec. 4.1 we do not apply the weighting found for 'S-D' but instead resort to uniform weighting for direct comparison to <ref type="bibr" target="#b11">[12]</ref>. <ref type="figure">Fig. 6</ref>: Architecture for Self-supervised Depth Estimation (SDE). To accommodate monocular depth on 'Cityscapes SDE', we follow the setup of <ref type="bibr" target="#b11">[12]</ref> with added intermediate depth supervision (D 1:x ). For the two variants in Tab. 5, we use the above architecture, replacing the 'exchange block' with the desired one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 MTL for Unsupervised Domain Adaptation</head><p>Architecture and training. <ref type="figure">Fig. 7</ref> illustrates our adversarial learning scheme with source/target data flows for multi-task UDA. We consider the two-task 'S-D' setup. As explained in main table Sec. 4.5, domain alignment is made possible with output-level DA adversarial training. In our work, alignment is done at both intermediate and final output-levels. We follow the strategies introduced in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref>. Discriminators D are train on the source dataset X src and target dataset X trg by minimizing the binary classification loss:</p><formula xml:id="formula_9">L D " 1 |X src | ? xsrcPXsrc L BCE pDpQ xsrc q, 1q`1 |X trg | ? xtrgPXtrg L BCE pDpQ xtrg q, 0q,<label>(7)</label></formula><p>where L BCE is the Binary Cross-Entropy loss, and Q x stands for either segmentation output Q S x or depth output Q D x of the network. To compete with the discriminators, the main MTL network is additionally trained with the adversarial losses L adv , written as:</p><formula xml:id="formula_10">L adv " 1 |X trg | ? xtrgPXtrg L BCE pDpQ xtrg q, 1q.<label>(8)</label></formula><p>The final MTL-UDA loss becomes:</p><formula xml:id="formula_11">L MTL-UDA " 1 |S| ? sPS ? tPT p? t L s t`?adv L s advt q`? tPT p? t L final t`?adv L final advt q,<label>(9)</label></formula><p>where ? adv is used to weight the adversarial losses and is set to 5.0e?3.</p><p>For segmentation alignment, we use "weighted self-information" map <ref type="bibr" target="#b28">[29]</ref> computed from the softmax segmentation output P x with the formula:</p><formula xml:id="formula_12">Q S x "?P x d logpP x q.<label>(10)</label></formula><p>For depth alignment, we normalize the depth-map outputs using the source's min and max depth values, and directly align the continuous normalized maps Q D x <ref type="bibr" target="#b30">[31]</ref>.</p><p>Class mapping. To allow compatible semantics in the VKITTI2? ?Cityscapes DA scenario, we apply the class mapping in Tab. 8.  3-ways PAD-Net <ref type="bibr" target="#b11">[12]</ref> Ours PAD-Net <ref type="bibr" target="#b32">[33]</ref> 3-ways PAD-Net <ref type="bibr" target="#b11">[12]</ref> Ours <ref type="figure">Fig</ref> 3-ways PAD-Net <ref type="bibr" target="#b11">[12]</ref> Ours PAD-Net <ref type="bibr" target="#b32">[33]</ref> 3-ways PAD-Net <ref type="bibr" target="#b11">[12]</ref> Ours <ref type="figure">Fig. 9</ref>: Qualitative results on VKITTI2. Overall, Ours produces better and sharper. Comparing against 3-ways PAD-Net is harder visually due to high scores (cf. Tab. 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional results</head><p>'S-D' 'S-D-N '</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmentation</head><p>Depth Segmentation Depth Normals</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PAD-Net [33]</head><p>3-ways PAD-Net <ref type="bibr" target="#b11">[12]</ref> Ours PAD-Net <ref type="bibr" target="#b32">[33]</ref> 3-ways PAD-Net <ref type="bibr" target="#b11">[12]</ref> Ours <ref type="figure">Fig. 10</ref>: Qualitative results on Cityscapes. Overall, Ours produces better and sharper. Comparing against 3-ways PAD-Net is harder visually due to high scores (cf. Tab. 1).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Bidirectional Cross-task Attention (xTAM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>. 8 :</head><label>8</label><figDesc>Qualitative results on Synthia. Overall, Ours produces better and sharper. Comparing against 3-ways PAD-Net is harder visually due to high scores (cf. Tab. 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>83?0.25 5.166?0.063 +03.76?0.77 71.27?0.21 5.108?0.076 +05.37?0.83 18.51?0.10 +05.45?0.72 PAD-Net [33] 70.87?0.15 4.917?0.014 +06.85?0.24 72.27?0.25 4.949?0.072 +07.58?0.56 19.28?0.09 +05.62?0.43 3-ways PAD-Net [12] 77.50?0.17 4.289?0.028 +17.60?0.13 79.93?0.5 4.218?0.082 +20.06?0.92 15.54?0.14 +20.29?0.84 Ours 80.53?0.43 4.161?0.022 +21.04?0.52 82.99?0.38 4.056?0.076 +23.83?0.98 14.30?0.15 +24.92?0.87 73?0.12 5.720?0.029 +01.89?0.21 87.83?0.21 5.714?0.033 +02.00?0.27 22.30?0.68 +02.54?0.80 PAD-Net [33] 88.43?0.12 5.571?0.058 +03.63?0.45 88.67?0.15 5.543?0.043 +04.09?0.29 22.16?0.70 +04.09?0.83 3-ways PAD-Net [12] 96.13?0.15 4.013?0.051 +21.78?0.54 96.87?0.06 3.756?0.013 +24.46?0.14 15.54?0.56 +27.25?0.90 Ours 97.00?0.10 3.423?0.025 +27.47?0.16 97.53?0.06 3.089?0.006 +30.70?0.05 14.44?0.52 +33.00?0.73 23?0.25 6.777?0.010 +00.52?0.27 70.67?0.06 6.755?0.018 +01.00?0.17 43.52?0.00 +01.12?0.11 3-ways PAD-Net [12] 75.00?0.10 6.528?0.063 +05.91?0.44 75.50?0.10 6.491?0.081 +06.56?0.61 41.84?0.05 +06.09?0.37 Ours 74.95?0.10 6.649?0.003 +04.96?0.08 76.08?0.14 6.407?0.013 +07.61?0.04 40.05?0.33 +08.15?0.22</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>%</cell><cell cols="2">mErr.?? SDN %</cell></row><row><cell>STL</cell><cell>[27] 67.43?0.15 5.379?0.055</cell><cell>?</cell><cell>idem</cell><cell>idem</cell><cell>?</cell><cell>19.61?0.12</cell><cell>?</cell></row><row><cell cols="8">Synthia [27] 69.VKITTI2 MTL STL [27] 84.53?0.06 5.720?0.027 MTL STL [27] 67.93?0.06 6.622?0.020 MTL [27] 70.43?0.12 6.797?0.520 +00.52?0.32 70.93?0.15 6.736?0.023 +01.34?0.28 43.60?0.01 +01.30?0.18 ? idem idem ? 23.14?0.68 ? ? idem idem ? 44.10?0.01 ? [27] 87.Cityscapes PAD-Net [33] 70.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>44?0.34 0.638?0.004 +1.63?0.37 39.90?0.41 0.642?0.003 +1.89?0.67 36.07?0.09 +1.76?0.53 39.70?0.35 0.636?0.001 36.10?0.12 +1.88?0.33 55.11?0.15 +1.50?0.20 PAD-Net [33] 35.30?0.84 0.659?0.004 -5.36?0.83 36.14?0.30 0.660?0.006 -4.32?0.68 36.72?0.08 -2.97?0.43 36.19?0.24 0.662?0.005 36.58?0.06 -2.92?0.37 54.79?0.07 -2.24?0.26 3-ways PAD-Net [12] 39.47?0.16 0.622?0.001 +2.90?0.23 40.28?0.30 0.619?0.004 +4.16?0.50 35.35?0.09 +3.93?0.27 40.16?0.28 0.614?0.010 35.25?0.09 +4.14?0.65 59.66?0.16 +5.27?0.49 Ours 38.93?0.35 0.604?0.004 +3.54?0.21 40.28?0.41 0.598?0.002 +5.80?0.65 33.72?0.14 +6.49?0.50 40.84?0.37 0.593?0.004 33.38?0.19 +7.52?0.27 61.12?0.24 +8.47?0.12</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>%</cell><cell cols="2">mErr.??SDN %</cell><cell>mIoU %</cell><cell>RMSE m</cell><cell cols="2">mErr.??SDN %</cell><cell>F1 %</cell><cell>?SDNE %</cell></row><row><cell>STL</cell><cell>[27] 38.70?0.10 0.635?0.013</cell><cell>?</cell><cell>idem</cell><cell>idem</cell><cell>?</cell><cell>36.90?0.26</cell><cell>?</cell><cell>idem</cell><cell>idem</cell><cell>idem</cell><cell>?</cell><cell>54.90?0.00</cell><cell>?</cell></row></table><note>MTL [27] 39.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Scales Param. ? Semseg ? Depth ? Delta ? Param. ? Semseg ? Depth ? Normals ? Delta ? 88?0.30 3.604?0.020 25.81?0.33 0.00 97.38?0.02 3.491?0.041 14.50?0.57 30.51?0.98 ? 3.09 97.32?0.06 3.556?0.029 26.50?0.29 2.32 97.43?0.04 3.559?0.024 14.51?0.50 30.12?0.79 ? 3.09 97.24?0.03 3.476?0.018 27.15?0.16 2.32 97.49?0.08 3.353?0.025 14.45?0.48 31.43?0.76 ? 0.77 97.07?0.06 3.468?0.016 27.11?0.12 9.26 97.47?0.06 3.244?0.035 14.57?0.51 31.89?0.92 ? 0.77 97.00?0.10 3.423?0.025 27.47?0.16 9.26 97.53?0.06 3.089?0.006 14.44?0.52 33.00?0.73 ? ? 3.86 97.09?0.03 3.369?0.022 27.99?0.18 11.58 97.53?0.02 3.080?0.025 14.47?0.57 33.02?0.90 ? ? ? 4.63 97.01?0.02 3.377?0.008 27.88?0.06 13.89 97.39?0.02 3.136?0.046 14.81?0.77 32.13?1.39 ? ? ? ? 7.72 97.05?0.03 3.369?0.010 27.97?0.08 23.15 96.82?0.23 3.307?0.066 15.39?0.65 30.08?1.39</figDesc><table><row><cell>mTEB</cell><cell></cell><cell>'S-D'</cell><cell></cell><cell></cell><cell></cell><cell>'S-D-N '</cell><cell></cell></row><row><cell>4 3 2 1 #M added</cell><cell>mIoU %</cell><cell>RMSE m</cell><cell>? SD %</cell><cell>#M added</cell><cell>mIoU %</cell><cell>RMSE m</cell><cell>mErr.?? SDN %</cell></row><row><cell cols="2">0.00 96.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>m mErr.?? SDN % concat 97.43?0.06 3.315?0.099 14.83?0.43 31.08?0.61 prod 97.63?0.06 3.139?0.024 14.34?0.49 32.90?0.81 add 97.53?0.06 3.089?0.006 14.44?0.52 33.00?0.73 43?0.06 3.315?0.099 14.83?0.43 31.08?0.61 channel 97.08?0.24 3.585?0.047 15.03?0.51 28.70?0.92 both 97.33?0.11 3.352?0.068 14.85?0.48 30.42?0.51</figDesc><table><row><cell></cell><cell></cell><cell cols="2">'S-D-N '</cell></row><row><cell>Type</cell><cell cols="3">Semseg ? Depth ? Normals ? Delta ?</cell></row><row><cell></cell><cell>mIoU %</cell><cell>RMSE m</cell><cell>mErr.?? SDN %</cell></row><row><cell cols="2">spatial 97.</cell><cell></cell></row><row><cell>(a) Fusion operation</cell><cell></cell><cell></cell></row></table><note>(b) Type of attention</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Net [12] 97.21 79.38 90.50 47.68 49.68 51.17 49.41 64.65 91.40 93.85 72.41 46.92 92.66 80.17 42.43 66.39 69.74 3-ways mTEB ours 97.62 82.29 92.44 46.52 54.76 59.82 60.94 73.13 92.22 94.55 76.40 58.49 94.26 85.14 49.41 71.70 74.36</figDesc><table><row><cell>Methods</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>veg</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>mbike</cell><cell>bike</cell><cell>mIoU %</cell></row><row><cell>3-ways</cell><cell>[12] *</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>*</cell><cell>71.16</cell></row><row><cell>3-ways PAD-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>/3.556/14.39}30.30 for mIoU/RMSE/mErr}? SDN , and 97.43/3.315/14.83}31.08 with self-attention. This shows the two types of attention are best combined. We detail the four task-specific metrics below. ? Semantic segmentation uses mIoU as the average of the per-class Intersection over Union (%) between label s and predicted map? : m S " mIoUp?, sq. ? Depth regression uses the Root Mean Square Error computed between label d and predicted mapd: m D " RMSEpd, dq, reporting the RMSE in meters over the evaluated set of images. In the SDE (Sec. 4.4) and DA (Sec. 4.5) setups, a perimage median scaling</figDesc><table><row><cell>B Experimental details</cell></row><row><cell>B.1 Multi-task setup</cell></row><row><cell>Metrics.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>73?0.12 5.720?0.029 +1.89?0.21 100 1 88.00?0.20 5.754?0.030 +1.75?0.17 100 10 86.13?0.32 5.693?0.039 +1.18?0.45 200 1 88.13?0.12 5.790?0.055 +1.52?0.45 500 1 88.17?0.15 5.847?0.043 +1.04?0.30 (a) 'S-D' gridsearch weights Semseg ? Depth ? Normals ? Delta ? ?S ?D ?N mIoU % 00?0.20 5.717?0.048 22.40?0.71 +2.45?0.99 100 1 100 88.07?0.15 5.696?0.038 22.29?0.70 +2.75?1.04 10?0.20 5.738?0.039 22.41?0.70 +2.35?0.99 150 1 100 88.13?0.15 5.732?0.037 22.31?0.71 +2.54?0.94</figDesc><table><row><cell cols="2">weights Semseg ? Depth ? ?S ?D mIoU % RMSE m 1 1 83.83?0.15 5.713?0.060 -0.35?0.47 Delta ? ? SD % 1 10 79.87?0.21 5.708?0.036 -2.66?0.40 10 1 86.20?0.71 5.693?0.055 +1.30?0.22 50 1 87.RMSE m 1 1 1 83.50?0.20 5.707?0.058 23.03?0.70 -0.17?0.64 mErr.?? SDN % 10 1 1 86.53?0.21 5.694?0.032 22.98?0.68 +1.17?0.93 10 1 10 86.63?0.21 5.675?0.050 22.61?0.70 +1.85?0.80 50 1 1 87.73?0.21 5.706?0.051 22.90?0.71 +1.69?0.81 50 1 10 87.77?0.15 5.714?0.065 22.56?0.69 +2.15?0.86 50 1 50 87.73?0.21 5.701?0.062 22.37?0.70 +2.49?0.76 100 1 1 88.03?0.15 5.746?0.030 22.95?0.69 +1.49?0.92 100 1 10 87.97?0.15 5.714?0.048 22.59?0.69 +2.19?0.79 100 1 50 88.150 1 10 88.10?0.20 5.752?0.059 22.59?0.70 +2.01?0.86</cell></row><row><cell>150 1</cell><cell>50 88.(b) 'S-D-N ' gridsearch</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Figs.<ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10</ref> show additional qualitative results for Synthia, VKITTI2, and Cityscapes, respectively. Comparing Ours with PAD-Net show an evident segmentation improvement on thin elements such as poles or pedestrians in Figs 8,10, with significantly better and sharper results for depth and normals estimations in all setups of Figs.8,9,10.   Comparing Ours against 3-ways PAD-Net is not easy visually due to high scores of the two (cf. Tab. 1), but again Ours is better on thin elements, e.g. inFigs. 8,10.Fig. 7: Multi-task UDA with output-level adversarial learning. Arrows indicating data flows are drawn in either red (source), blue (target) or a mix (both). Additional discriminators (shown as yellow triangles) are jointly trained with our multi-task model.</figDesc><table><row><cell cols="4">VKITTI2 mapped Cityscapes mapped</cell></row><row><cell>terrain</cell><cell>ignore</cell><cell>road</cell><cell>road</cell></row><row><cell>sky</cell><cell>sky</cell><cell cols="2">sidewalk ignore</cell></row><row><cell>tree</cell><cell cols="3">vegetation building building</cell></row><row><cell cols="3">vegetation vegetation wall</cell><cell>vegetation</cell></row><row><cell cols="3">building building fence</cell><cell>ignore</cell></row><row><cell>road</cell><cell>road</cell><cell>pole</cell><cell>pole</cell></row><row><cell cols="2">guardrail ignore</cell><cell>light</cell><cell>light</cell></row><row><cell>sign</cell><cell>sign</cell><cell>sign</cell><cell>sign</cell></row><row><cell>light</cell><cell>light</cell><cell cols="2">vegetation vegetation</cell></row><row><cell>pole</cell><cell>pole</cell><cell>sky</cell><cell>sky</cell></row><row><cell>misc</cell><cell>ignore</cell><cell>person</cell><cell>ignore</cell></row><row><cell>truck</cell><cell>vehicle</cell><cell>rider</cell><cell>ignore</cell></row><row><cell>car</cell><cell>vehicle</cell><cell>car</cell><cell>vehicle</cell></row><row><cell>van</cell><cell>vehicle</cell><cell>bus</cell><cell>vehicle</cell></row><row><cell></cell><cell></cell><cell>mbike</cell><cell>ignore</cell></row><row><cell></cell><cell></cell><cell>bike</cell><cell>ignore</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Class mapping for VKITTI2? ?Cityscapes DA setup</figDesc><table><row><cell>'S-D'</cell><cell></cell><cell></cell><cell>'S-D-N '</cell><cell></cell></row><row><cell>Segmentation</cell><cell>Depth</cell><cell>Segmentation</cell><cell>Depth</cell><cell>Normal</cell></row><row><cell>PAD-Net [33]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was funded by the French Agence Nationale de la Recherche (ANR) with project SIGHT (ANR-20-CE23-0016). It was performed using HPC resources from GENCI-IDRIS (Grant 2021-AD011012808).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>COMPSTAT</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Exploring relational context for multi-task dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bruggemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kanakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Obukhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In: ICCV (2021) 2, 3, 6, 8</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geometric unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2021)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2016) 1</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Three ways to improve semantic segmentation with self-supervised depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>K?ring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2021) 2, 3, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning to relate depth and semantics for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Obukhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kanakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task learning as multi-objective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Standley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">Which tasks should be learned together in multi-task learning? In: ICML (2020)</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distilled semantics for comprehensive scene understanding from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Z</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<title level="m">Mti-net: Multi-scale task interaction networks for multi-task learning. In: ECCV (2020) 2, 3</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-task learning for dense prediction tasks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Dada: Depth-aware domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Semi-supervised multi-task learning for semantics and depth</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pad-net: Multi-tasks guided prediction-anddistillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unsupervised learning of geometry with edge-aware depth-normal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised learning of geometry from videos with edge-aware depth-normal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pattern-structure diffusion for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and egomotion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
