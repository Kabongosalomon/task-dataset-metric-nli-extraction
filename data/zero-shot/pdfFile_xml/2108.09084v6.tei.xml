<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fastformer: Additive Attention Can Be All You Need</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
							<email>wuchuhan15@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering &amp; BNRist</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
							<email>wufangzhao@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering &amp; BNRist</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
							<email>yfhuang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering &amp; BNRist</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
							<email>xingx@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fastformer: Additive Attention Can Be All You Need</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer is a powerful model for text understanding. However, it is inefficient due to its quadratic complexity to input sequence length. Although there are many methods on Transformer acceleration, they are still either inefficient on long sequences or not effective enough. In this paper, we propose Fastformer, which is an efficient Transformer model based on additive attention. In Fastformer, instead of modeling the pair-wise intractions between tokens, we first use additive attention mechanism to model global contexts, and then further transform each token representation based on its interaction with global context representations. In this way, Fastformer can achieve effective context modeling with linear complexity. Extensive experiments on five datasets show that Fastformer is much more efficient than many existing Transformer models and can meanwhile achieve comparable or even better long text modeling performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer <ref type="bibr" target="#b19">(Vaswani et al., 2017)</ref> and their variants have achieved great success in many fields. For example, Transformer is the backbone architecture of many state-of-the-art pre-trained language models in NLP, such as BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> and GPT . Transformer also shows great promises in vision-related tasks <ref type="bibr" target="#b8">(Dosovitskiy et al., 2021)</ref>. The core of a Transformer model is self-attention mechanism, which allows the Transformer to model the contexts within an input sequence <ref type="bibr" target="#b14">(Parikh et al., 2016)</ref>. However, since self-attention computes the dot-product between the input representations at each pair of positions, its complexity is quadratic to the input sequence length <ref type="bibr" target="#b19">(Vaswani et al., 2017)</ref>. Thus, it is difficult for standard Transformer models to efficiently handle long input sequences <ref type="bibr" target="#b18">(Tay et al., 2020)</ref>.</p><p>There are many methods to accelerate the Transformer model <ref type="bibr" target="#b1">(Beltagy et al., 2020;</ref><ref type="bibr">Zaheer et al., 2020;</ref><ref type="bibr" target="#b21">Wang et al., 2020b;</ref><ref type="bibr" target="#b12">Kitaev et al., 2020;</ref><ref type="bibr" target="#b17">Tay et al., 2021)</ref>. For example, BigBird <ref type="bibr">(Zaheer et al., 2020)</ref> computes sparse attention instead of a dense one. It uses a combination of local attention, global attention at certain positions and random attention between a certain number of tokens. However, sparse attention usually cannot fully model the global context <ref type="bibr" target="#b25">(Wu et al., 2021b)</ref>. Linformer <ref type="bibr" target="#b21">(Wang et al., 2020b)</ref> exploits the low-rank characteristic of the self-attention matrix by computing approximated ones. It projects attention key and value into low-dimensional matrices that are independent of the sequence length. However, the approximation is in fact context-agnostic, which may weaken the context modeling ability of Transformer. In addition, these methods are not efficient enough when the input sequence length is very long.</p><p>In this paper we propose Fastformer 1 , which is an efficient Transformer variant based on additive attention that can achieve effective context modeling in linear complexity. In Fastformer, we first use additive attention mechanism to summarize the input attention query matrix into a global query vector. Next, we model the interaction between attention key and the global query vector via element-wise product to learn global contextaware key matrix, and we further summarize it into a global key vector via additive attention. Then we use element-wise product to aggregate the global key and attention value, which are further processed by a linear transformation to compute the global context-aware attention value. Finally, we add together the original attention query and the global context-aware attention value to form the final output. We conduct extensive experiments on five benchmark datasets in various tasks, including sentiment classification, topic prediction, news recommendation and text summarization. The results demonstrate that Fastformer is much more efficient than many Transformer models and can achieve quite competitive results in long text modeling.</p><p>The contributions of this paper are summarized as follows:</p><p>? We propose an additive attention based Transformer named Fastformer. To the best of our knowledge, Fastformer is the most efficient Transformer architecture.</p><p>? We propose to model the interaction between global contexts and token representations via element-wise product, which can help fully model context information in an efficient way.</p><p>? Extensive experiments on five datasets show that Fastformer is much more efficient than many Transformer models and can achieve competitive performance.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformer and Self-Attention</head><p>The Transformer model is built upon multi-head self-attention, which can effectively model the contexts within a sequence by capturing the interactions between the inputs at each pair of positions <ref type="bibr" target="#b19">(Vaswani et al., 2017</ref>). An h-head selfattention mechanism can be formulated as follows:</p><formula xml:id="formula_0">MultiHead(Q, K, V) = Concat(head 1 , head 2 , ..., head h )W O ,<label>(1)</label></formula><p>where Q, K, V ? R N ?d are the input query, key and value matrices, N is the sequence length, d is the hidden dimension in each attention head, and W O ? R hd?d is a linear transformation parameter matrix. The representation learned by each attention head is formulated as follows:</p><formula xml:id="formula_1">head i = Attention(QW Q i , KW K i , VW V i ) = softmax( QW Q i (KW K i ) T ? d )VW V i , (2) where W Q i , W K i , W V i ? R d?d</formula><p>are learnable parameters. From this formula, we can see that the computational complexity is quadratic to the sequence length N . It has become a bottleneck for Transformers to handle long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Efficient Transformer</head><p>In recent years, there are many approaches to improving the efficiency of Transformer architecture <ref type="bibr" target="#b18">(Tay et al., 2020)</ref>. Some methods use sparse attention mechanisms to reduce the complexity of self-attention <ref type="bibr" target="#b1">Beltagy et al., 2020;</ref><ref type="bibr">Zaheer et al., 2020;</ref><ref type="bibr" target="#b28">Zhang et al., 2021)</ref>. For example, Longformer <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref> uses sliding window attention to attend local contexts and uses global attention on a few pre-selected input locations to capture global contexts. Big-Bird <ref type="bibr">(Zaheer et al., 2020)</ref> combines local attention, global attention at certain positions and random attention on several randomly selected token pairs. However, these methods may need a relative larger number of attended tokens to reduce the performance degradation on longer sequences, which usually leads to a limited speed-up ratio.</p><p>Another way is using hashing technique to accelerate self-attention computation. For example, Reformer <ref type="bibr" target="#b12">(Kitaev et al., 2020)</ref> uses a multiround hashing scheme to put similar representations into same buckets when computing selfattention, which can theoretically reduce the complexity to O(N log(N )). However, the computational complexity constant of Reformer is quite large, making it inefficient in processing common sequences with rather limited lengths.</p><p>There are also several methods that aim to reduce the computational cost by computing approximated self-attention <ref type="bibr" target="#b4">(Choromanski et al., 2020;</ref><ref type="bibr" target="#b21">Wang et al., 2020b;</ref><ref type="bibr" target="#b17">Tay et al., 2021)</ref>. For instance, Linformer <ref type="bibr" target="#b21">(Wang et al., 2020b)</ref> projects the attention key and value into low-rank matrices to approximate the self-attention mechanism. Linear Transformer <ref type="bibr">(Katharopoulos et al., 2020)</ref> uses a kernelbased formulation of self-attention and the associative property of matrix multiplication to approximate the dot-product attention. However, these methods approximate self-attention in a contextagnostic manner, which may not be optimal for text modeling. In addition, they still bring heavy computational cost when the sequence length is very long. Different from the aforementioned methods, Fastformer uses additive attention to model global contexts and uses element-wise product to model the interaction between each input representation and global contexts, which can greatly reduce the computational cost and meanwhile effectively capture contextual information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Fastformer</head><p>In this section, we introduce our Fastformer approach based on additive attention. The architecture of Fastformer is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. It first uses additive attention mechanism to summarize the query sequence into a global query vector, next models the interaction between the global query vector and attention keys with element-wise product and summarize keys into a global key vector via additive attention, then models the interactions between global key and attention values via elementwise product and uses a linear transformation to learn global context-aware attention values, and finally adds them with the attention query to form the final output. In this way, the computational complexity can be reduced to linearity, and the contextual information in the input sequence can be effectively captured. Next, we introduce the details of Fastformer in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>The Fastformer model first transforms the input embedding matrix into the query, key and value sequences. The input matrix is denoted as E ? R N ?d , where N is the sequence length and d is the hidden dimension. Its subordinate vectors are denoted as [e 1 , e 2 , ..., e N ]. Following the standard Transformer, in each attention head 2 we use three independent linear transformation layer to transform the input into the attention query, key and value matrices Q, K, V ? R N ?d , which are writ-</p><formula xml:id="formula_2">ten as Q = [q 1 , q 2 , ..., q N ], K = [k 1 , k 2 , ..., k N ] and V = [v 1 , v 2 , ..., v N ], respectively.</formula><p>Next, modeling the contextual information of the input sequence based on the interactions among attention query, key and value is a critical problem for Transformer-like architectures. In the vanilla Transformer, dot-product attention mechanism is used to fully model the interaction between query and key. Unfortunately, its quadratic complexity makes it inefficient in long sequence modeling. A potential way to reduce the computational complexity is to summarize the attention matrices (e.g., query) before modeling their interactions. Additive attention is a form of attention mechanism that can efficiently summarize important information within a sequence in linear complexity. Thus, we first use additive attention to summarize the query matrix into a global query vector q ? R d , which condenses the global contextual information in the attention query. More specifically, the attention weight ? i of the i-th query vector is computed as follows:</p><formula xml:id="formula_3">? i = exp(w T q q i / ? d) N j=1 exp(w T q q j / ? d) ,<label>(3)</label></formula><p>where w q ? R d is a learnable parameter vector. The global attention query vector is computed as follows:</p><formula xml:id="formula_4">q = N i=1 ? i q i .<label>(4)</label></formula><p>Then, a core problem in Fastformer is how to model the interaction between the summarized global query vector and the key matrix. There are several intuitive options, such as adding or concatenating the global query to each vector in the key matrix. However, they cannot differ the influence of the global query on different keys, which is not beneficial for context understanding. Element-wise product is an effective operation to model the nonlinear relations between two vectors <ref type="bibr" target="#b22">(Wang et al., 2017)</ref>. Thus, we use the element-wise product between the global query vector and each key vector to model their interactions and combine them into a global context-aware key matrix. We denote the i-th vector in this matrix as p i , which is formulated as p i = q * k i (the symbol * means element-wise product). In a similar way, we use additive attention mechanism to summarize the global context-aware key matrix due to efficiency reasons. The additive attention weight of its i-th vector is computed as follows:</p><formula xml:id="formula_5">? i = exp(w T k p i / ? d) N j=1 exp(w T k p j / ? d) ,<label>(5)</label></formula><p>where w k ? R d is the attention parameter vector. The global key vector k ? R d is further computed as follows:</p><formula xml:id="formula_6">k = N i=1 ? i p i .<label>(6)</label></formula><p>Finally, we model the interaction between attention value matrix and the global key vector for better context modeling. Similar with the query-key interaction modeling, we also perform elementwise product between the global key and each value vector to compute a key-value interaction vector u i , which is formulated as u i = k * v i . Motivated by the vanilla Transformer, we apply a linear transformation layer to each key-value interaction vector to learn its hidden representation. The output matrix from this layer is denoted as R = [r 1 , r 2 , ..., r N ] ? R N ?d . This matrix is further added together with the query matrix to form the final output of Fastformer. Note that the output matrix from each attention head is concatenated along the hidden dimension axis.</p><p>We can see that in Fastformer, each key and value vector can interact with the global query or key vector to learn contextualized representations. By stacking multiple Fastformer layers, we can fully model contextual information. Motivated by the weight sharing techniques used in <ref type="bibr" target="#b21">(Wang et al., 2020b)</ref>, we share the value and query transformation parameters to reduce the memory cost. In addition, we share the parameters across different Fastformer layers to further reduce the parameter size and mitigate the risk of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Complexity Analysis</head><p>In this section, we analyze the computational complexity of Fastformer. For the additive attention networks to learn global query and key vectors, their time and memory cost are both O(N ? d), and their total number of additional parameters is 2hd (h is the attention head number). In addition, the time cost and memory cost of element-wise product is also O(N ? d), the total complexity is O(N ? d), which is much more efficient than the standard Transformer with O(N 2 ? d) complexity. <ref type="bibr">3</ref> If the weight sharing technique is used, the total parameter size of Fastformer per layer is 3hd 2 + 2hd. Compared with Transformer with at least 4hd 2 parameters 4 , Fastformer also uses fewer parameters. These analysis results demonstrate the theoretical efficiency of Fastformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Experimental Settings</head><p>We conduct extensive experiments on five benchmark datasets for different tasks. Their details   are introduced as follows. The first one is Amazon <ref type="bibr" target="#b9">(He and McAuley, 2016</ref>) (we use the Electronics domain) 5 , which is a widely used dataset for review rating prediction. The second one is IMDB <ref type="bibr" target="#b7">(Diao et al., 2014)</ref>. <ref type="bibr">6</ref> It is a benchmark dataset for movie review rating prediction. The third one is MIND <ref type="bibr">(Wu et al., 2020) 7</ref> , which is a large-scale English dataset for news recommendation and intelligence. We perform two tasks on this dataset, i.e., the news topic classification task based on news body and personalized news recommendation task based on the relevance between candidate news and user interests inferred from historical clicked news. The fourth one is CNN/DailyMail dataset (Hermann et al., 2015) (denoted as CNN/DM), which is a widely used benchmark dataset for text summarization. The fifth one is PubMed <ref type="bibr" target="#b5">(Cohan et al., 2018)</ref>, which is another benchmark text summarization dataset with much longer documents lengths. The detailed statistical information of the datasets introduced above are shown in Tables 1, 2 and 3.</p><p>In our experiments, we use Glove <ref type="bibr" target="#b15">(Pennington et al., 2014)</ref> embeddings to initialize token embedding matrix. To obtain the embeddings in the classification and news recommendation tasks, we apply an additive attention network to convert the matrix output by Fastformer into an embedding. In addition, in the news recommendation task, following  we use Fastformer in a hierarchical way to first learn news embeddings from news titles and then learn user embeddings from the embeddings of historical clicked news. We use Adam <ref type="bibr" target="#b2">(Bengio and LeCun, 2015)</ref> for model optimization. More detailed experimental settings are included in Appendix. We run our experiments on an Nvidia Tesla V100 GPU with 32GB memory. We repeat each experiment 5 times and report the average performance as well as the standard deviations. On the classification tasks, we use accuracy and macro-F scores as performance metrics. On the news recommendation task, following <ref type="bibr" target="#b26">(Wu et al., 2020)</ref> we use AUC, MRR, nDCG@5 and nDCG@10 as the metrics. On the text summarization tasks, we use the ROUGE-1, ROUGE-2 and ROUGE-L metrics (denoted as R-1, R-2 and R-L) to evaluate the generated summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effectiveness Comparison</head><p>First, we compare the performance of Fastformer with many baseline methods, including: (1) Transformer <ref type="bibr" target="#b19">(Vaswani et al., 2017)</ref>, the vanilla Transformer; <ref type="formula">(2)</ref>   <ref type="bibr">(Katharopoulos et al., 2020)</ref>, another linear complexity Transformer using kernel functions to approximate self-attention mechanism; (6) Poolingformer <ref type="bibr" target="#b28">(Zhang et al., 2021)</ref>, a hierarchical architecture that first uses sliding window self-attention to capture short-range contexts and then uses pooling self-attention to capture long-range contexts.</p><p>The performance of these methods on the three classification datasets are compared in <ref type="table" target="#tab_4">Table 4</ref>. From the results, we find that efficient Transformer variants usually outperform the standard Transformer model. This is because the quadratic computational cost of vanilla Transformer limits the maximum sequence length can be handled, and many useful contexts are lost when truncating the input text sequence. Fastformer can achieve competitive or better performance than other efficient Transformer variants in both long and short text modeling. This is because Fastformer can effectively model global contexts and their relationship   <ref type="table">Table 5</ref>: Performance of different methods in the news recommendation task. *Ensemble of five independently trained models, which is the 1 st ranked result on the MIND leaderboard.</p><p>to different tokens, which can help understand context information accurately.</p><p>We also compare the performance of different methods in the news recommendation task. We add three recent news recommendation methods to the comparison, including: (1) NRMS , which uses multi-head self-attention networks to learn news and user representations; (2) FIM <ref type="figure" target="#fig_3">(Wang et al., 2020a)</ref>, a fine-grained interest matching method for personalized news recommendation; (3) PLM-NR <ref type="bibr" target="#b24">(Wu et al., 2021a)</ref>, empowering news recommendation with pre-trained language models. In PLM-NR, we use the best performed UniLM <ref type="bibr" target="#b0">(Bao et al., 2020)</ref> empowered model. In addition, we explore to replace the user encoder in PLM-NR with Fastformer. The results are shown in <ref type="table">Table 5</ref>. We can see that among different Transformer architectures, Fastformer achieves the best performance, and it also outperforms its basic NRMS model. In addition, Fastformer can further improve the performance of PLM-NR, and the ensemble model achieves the best results on  the MIND leaderboard 8 . These results show that Fastformer is not only effective in text modeling, but also effective in understanding user interest. We further conduct experiments on the text summarization tasks to verify the effectiveness of Fastformer in natural language generation. The results are shown in <ref type="table" target="#tab_6">Table 6</ref>. We find that on the CNN/DM dataset, many efficient Transformer variants (except Poolingformer and Fastformer) are inferior to the vanilla Transformer. This is because sparse attention based method such as Longformer and Big-Bird cannot fully model the document contexts, and approximated self-attention based methods such as Linformer and Linear Transformer cannot effectively consider context information in the approximation. Since the summaries in the CNN/DM dataset have short lengths, they may be less effective than vanilla Transformer when the same sequence length is used. Fastformer can achieve the best performance in most metrics, which shows the advantage of Fastformer in natural language generation.</p><formula xml:id="formula_7">Method CNN/DailyMail PubMed R-1 R-2 R-L R-1 R-2 R-L</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Efficiency Comparison</head><p>In this section, we evaluate the efficiency of different methods. We first compare the theoretical computational complexity of these methods in Ta-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Complexity <ref type="table">Table 7</ref>: Asymptotic computational complexity of different methods. N is the sequence length, K is the average number of tokens to be attended by per token, d is the hidden dimension, is the attention matrix approximation error in Linformer, and w is the window size in Poolingformer. ble 7. 9 The complexity of vanilla Transformer is O(N 2 ? d), while other compared methods have linear complexity with respect to the input sequence length. However, the complexity of Longformer and BigBird depends on the average number of tokens to be attended by each token, Linformer has a complexity depending on the square of the approximation error, the complexity of Linear Transformer has a quadratic term of hidden dimension, and the complexity of Poolingformer depends on its window size. Different from them, the complexity of Fastformer only depends on the sequence length and the hidden dimension, and it has the least complexity among compared methods. This result shows that Fastformer is efficient in theory. Then, we conduct experiments to measure the real training and inference cost of different meth-ods. 10 Motivated by prior works <ref type="bibr">(Katharopoulos et al., 2020;</ref><ref type="bibr" target="#b21">Wang et al., 2020b)</ref>, we vary the sequence length from 128 to 65535 and scale the batch size inversely with the sequence length. We generate pseudo samples with random tokens and fix the token embeddings to better measure the computational cost of different methods. The results are shown in <ref type="figure" target="#fig_3">Fig. 2</ref>. <ref type="bibr">11</ref> We find that Transformer is inefficient when the sequence length is relatively long (e.g., 512). In addition, we find that although Poolingformer has linear complexity in theory, it is inefficient in practice. This is because it uses a large window size (e.g., 256) to compute pooling weight in a convolution-like way, which leads to a very large constant term of the computational cost. Besides, Fastformer is much more efficient than other linear complexity Transformer variants in terms of both training and inference time. These results verify the efficiency of Fastformer.</p><formula xml:id="formula_8">Transformer O(N 2 ? d) Longformer O(N ? K ? d) BigBird O(N ? K ? d) Linformer O(N ? d/ 2 ) Linear Transformer O(N ? d 2 ) Poolingformer O(N ? d ? w) Fastformer O(N ? d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Influence of Interaction Function</head><p>Next, we study the influence of using different functions to model the interactions among query, key and value in Fastformer. We compare the performance of Fastformer and its variants using the add or concatenation functions to combine the global query/key vector with the vectors in the key/value matrix. The results are shown in <ref type="figure" target="#fig_5">Fig. 3</ref>. We find concatenating is not a good option for Fastformer. This is because simply concatenating two vectors cannot consider the interactions between them. In addition, we find adding is not optimal. This is  because the add function can only model the linear interactions between two vectors, which may also be insufficient to learn accurate context representations. Different from concatenation and add, element-wise product can model the non-linear interactions between two variables, which may help model the complex contexts in long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Influence of Parameter Sharing</head><p>Then, we study the influence of different parameter sharing techniques on the performance of Fastformer, including sharing query and value transformation matrices, sharing the parameters across different attention heads, and sharing parameters across different layers. <ref type="bibr">12</ref> The results are shown  in <ref type="figure" target="#fig_7">Fig. 4</ref>. We find that using query-value parameter sharing can achieve similar or slightly better performance than the Fastformer model without any parameter sharing techniques. Thus, it is favorable to reduce the parameter size by sharing the query and value transformation matrix. In addition, we find head-wise parameter sharing will lead to notable performance drops. This is because different attention heads are expected to capture different patterns of contexts, and sharing their parameters is not beneficial for context modeling. Moreover, we find incorporating layer-wise sharing method can further improve the model performance. This is because parameter sharing among different layers can mitigate the risk of overfitting <ref type="bibr" target="#b13">(Lan et al., 2020)</ref>. Thus, in Fastformer we incorporate both queryvalue sharing and layer-wise sharing strategies to improve the model performance and meanwhile reduce the model parameter size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Applications of Fastformer</head><p>We further apply Fastformer to a downstream Ad CVR prediction task. We conduct experiments on a large-scale Ad CVR prediction data collected from Bing Ads, which contains user behaviors such as search query, webpage browsing and the conversion of clicked Ads. The task is a binary classification task by predicting whether a clicked Ad leads to a conversion. The dataset contains 52.2m training samples, 387k validation samples and 611k test samples (logs in the last week are for test, and the rest are for training and validation). Similar to news recommendation, we first use a Transformer or Fastformer to learn ad/behavior embedding, and then use another Transformer or Fastformer to learn user embedding from behavior embeddings. The prediction AUC, training memory utilization and local inference latency of Transformer and Fastformer based methods are shown in  <ref type="table" target="#tab_7">Table 8</ref>: Accuracy, memory cost and inference speed comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we propose Fastformer, which is a Transformer variant based on additive attention that can handle long sequences efficiently with linear complexity. In Fastformer, we first use additive attention to summarize the query matrix into a global query vector. Next, we combine it with each key vector via element-wise product to learn global context-aware key matrix, and we further summarize it into a global key vector via additive attention. We then model the interactions between global context-aware key and the value to learn global context-aware attention value, which is further combined with the query to form the final output. Extensive experiments on five benchmark datasets show that Fastformer is much more efficient than many existing Transformer models and meanwhile can achieve competitive or even better performance in long text modeling. In our future work, we plan to pre-train Fastformer-based language models to better empower NLP tasks with long document modeling. In addition, we will explore applying Fastformer to other scenarios such as e-commerce recommendation and Ads CTR prediction to improve user modeling based on long user behavior sequences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of Fastformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Longformer (Beltagy et al., 2020), a Transformer variant with sparse attention. It combines sliding window attention and global attention to model local and global contexts; (3) BigBird (Zaheer et al., 2020), an extension of Longformer, which incorporates sparse random attention mechanism; (4) Linformer (Wang et al., 2020b), a Transformer variant with linear complexity, which use low-dimensional key and value matrices to compute approximated self-attention; (5) Linear Transformer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Training and inference speed of different methods. The y-axis (time) is in logarithmic scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Influence of different combination functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Influence of different parameter sharing strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Dataset #Train #Val #Test Avg. len. #Class</figDesc><table><row><cell cols="2">Amazon 40.0k</cell><cell>5.0k</cell><cell>5.0k</cell><cell>133.4</cell><cell>5</cell></row><row><cell>IMDB</cell><cell cols="3">108.5k 13.6k 13.6k</cell><cell>385.7</cell><cell>10</cell></row><row><cell>MIND</cell><cell cols="3">128.8k 16.1k 16.1k</cell><cell>505.4</cell><cell>18</cell></row><row><cell cols="6">Table 1: Statistics of sentiment and news topic classifi-</cell></row><row><cell cols="2">cation datasets.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>#News</cell><cell></cell><cell cols="2">161,013 #Users</cell><cell></cell><cell>1,000,000</cell></row><row><cell cols="5">#Train impression 2,232,748 #Val impression</cell><cell>376,471</cell></row><row><cell cols="2">#Test impression</cell><cell cols="3">2,370,727 Avg. click his. len.</cell><cell>37.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of MIND dataset for the news recommendation task.</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Train #Val #Test Avg. doc/summary len.</cell></row><row><cell cols="2">CNN/DailyMail 287.1k 13.4k 11.5k</cell><cell>781/56</cell></row><row><cell>PubMed</cell><cell>108.5k 13.6k 13.6k</cell><cell>3016/203</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the text summarization datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>32?0.35 42.31?0.33 52.04?0.50 42.69?0.47 80.90?0.20 60.02?0.21 Longformer 65.45?0.39 42.48?0.44 52.21?0.36 43.36?0.38 81.36?0.21 62.59?0.23 BigBird 66.14?0.42 42.96?0.40 53.23?0.46 44.03?0.44 81.93?0.24 63.58?0.26 Linformer 66.20?0.49 43.13?0.48 53.17?0.59 44.34?0.57 82.16?0.28 63.77?0.30 Linear Transformers 66.12?0.42 43.04?0.44 53.09?0.47 44.30?0.49 82.25?0.23 63.81?0.22 Poolingformer 66.05?0.44 43.00?0.45 53.78?0.51 44.52?0.50 82.46?0.24 64.10?0.26 Fastformer 66.13?0.29 43.23?0.30 54.10?0.42 44.65?0.44 82.34?0.19 63.89?0.20</figDesc><table><row><cell>Methods</cell><cell>Amazon Accuracy Macro-F</cell><cell>IMDB Accuracy Macro-F</cell><cell>MIND Accuracy Macro-F</cell></row><row><cell>Transformer</cell><cell>65.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The results of different methods in the sentiment and topic classification tasks. Best average scores are highlighted.</figDesc><table><row><cell>Methods</cell><cell cols="3">AUC MRR nDCG@5 nDCG@10</cell></row><row><cell>NRMS</cell><cell>68.18 33.29</cell><cell>36.31</cell><cell>42.20</cell></row><row><cell>FIM</cell><cell>68.31 33.42</cell><cell>36.47</cell><cell>42.35</cell></row><row><cell>PLM-NR</cell><cell>70.64 35.39</cell><cell>38.71</cell><cell>44.38</cell></row><row><cell>Transformer</cell><cell>68.22 33.32</cell><cell>36.35</cell><cell>42.23</cell></row><row><cell>Longformer</cell><cell>67.98 33.04</cell><cell>36.18</cell><cell>42.06</cell></row><row><cell>BigBird</cell><cell>68.14 33.28</cell><cell>36.30</cell><cell>42.18</cell></row><row><cell>Linformer</cell><cell>68.02 33.19</cell><cell>36.22</cell><cell>42.10</cell></row><row><cell>Linear Transformers</cell><cell>67.76 32.94</cell><cell>36.16</cell><cell>41.97</cell></row><row><cell>Poolingformer</cell><cell>68.54 33.60</cell><cell>36.69</cell><cell>42.60</cell></row><row><cell>Fastformer</cell><cell>69.11 34.25</cell><cell>37.26</cell><cell>43.38</cell></row><row><cell>Fastformer+PLM-NR</cell><cell>71.04 35.91</cell><cell>39.16</cell><cell>45.03</cell></row><row><cell cols="2">Fastformer+PLM-NR* 72.68 37.45</cell><cell>41.51</cell><cell>46.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance of different methods in the text summarization task. Best scores are highlighted.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>The results show the effectiveness and efficiency of Fastformer in Ad CVR prediction. 13</figDesc><table><row><cell></cell><cell>AUC</cell><cell>Memory</cell><cell>Latency</cell></row><row><cell cols="2">Transformer 0.7299</cell><cell>30GB</cell><cell>176ms</cell></row><row><cell>Fastformer</cell><cell cols="3">0.7394(+1.3%) 25GB(-16.7%) 163ms(-7.2%)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A pytorch version of Fastformer using the huggingface style is available at https://github.com/wuch15/Fastformer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Different attention heads use the same formulation but different parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that the complexity of linear transformations is not taken into account by many prior works, and we also do not consider their effects on computational costs.4  Including the query, key, value and output transformation matrices. The parameters in the bias term, feed-forward network and layer normalization are not counted.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://jmcauley.ucsd.edu/data/amazon/ 6 https://github.com/nihalb/JMARS 7 https://msnews.github.io/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://msnews.github.io/ 128/512 256/256 512/128 1024/64 2048/32 4096/16 8192/8 16384/4 32768/2 65536/1 Sequence Length/Batch Size</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">We exclude the complexity of the linear Transformation applied after the input and before the output.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">We use the model configurations in the news topic classification task.11  The maximum sequence length of Transformer is limited by the GPU memory.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">We do not observe significant differences in terms of time cost when using different parameter sharing methods, and we only compare the performance here.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">The latency improvement scale is relatively smaller than inFig. 2because the sequences are much shorter and there are multiple additional MLPs in the model.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Experimental Environment</head><p>Our experiments are conducted on a cloud Linux server with Ubuntu 16.04 operating system. The codes are written in Python 3.6 using the Keras library 2.2.4 with Tensorflow 1.12 backend. The GPU type is Nvidia Tesla V100 with 32GB GPU memory. Each experiment is run by a single thread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Preprocessing</head><p>In our experiments, we use the NLTK tool to preprocess the texts. We use the word tokenize and function to convert the input texts into token sequences. The word embeddings of out-ofvocabulary words are filled with random vectors that have the same mean and co-variation values as other words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Hyperparameter Settings</head><p>The detailed hyperparameter settings on each dataset used in this paper are listed in <ref type="table">Table 9</ref>.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unilmv2: Pseudomasked language models for unified language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="642" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Masked language modeling for proteins via linearly scalable long-context transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03555</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A discourse-aware attention model for abstractive summarization of long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokhwan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="615" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In KDD</title>
		<imprint>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<title level="m">Teaching machines to read and comprehend. NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nikolaos Pappas, and Fran?ois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Synthesizer: Rethinking self-attention for transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10183" to="10192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fine-grained interest matching for neural news recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="836" to="845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Linformer: Selfattention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Item silk road: Recommending items from information domains to social users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural news recommendation with multi-head self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyu</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6390" to="6395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Empowering news recommendation with pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1652" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hi-transformer: Hierarchical interactive transformer for efficient and effective long document modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mind: A large-scale dataset for news recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiun-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winnie</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3597" to="3606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Onta??n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Poolingformer: Long document modeling with pooling attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12437" to="12446" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
