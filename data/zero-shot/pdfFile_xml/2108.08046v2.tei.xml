<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Graph Normalized AutoEncoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-11-01">2021. November 1-5, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahn</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myoungho</forename><surname>Kim</surname></persName>
						</author>
						<title level="a" type="main">Variational Graph Normalized AutoEncoders</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM &apos;21)</title>
						<meeting>the 30th ACM International Conference on Information and Knowledge Management (CIKM &apos;21)						</meeting>
						<imprint>
							<date type="published" when="2021-11-01">2021. November 1-5, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3459637.3482215</idno>
					<note>Virtual Event, QLD, Australia. ACM, New York, NY, USA, 5 pages. https: //</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Neural networks</term>
					<term>Unsuper- vised learning</term>
					<term>Learning latent representations KEYWORDS Link Prediction, Graph Embedding, Graph Convolutional Networks, Normalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Link prediction is one of the key problems for graph-structured data. With the advancement of graph neural networks, graph autoencoders (GAEs) and variational graph autoencoders (VGAEs) have been proposed to learn graph embeddings in an unsupervised way. It has been shown that these methods are effective for link prediction tasks. However, they do not work well in link predictions when a node whose degree is zero (i.g., isolated node) is involved. We have found that GAEs/VGAEs make embeddings of isolated nodes close to zero regardless of their content features. In this paper, we propose a novel Variational Graph Normalized AutoEncoder (VGNAE) that utilize 2 -normalization to derive better embeddings for isolated nodes. We show that our VGNAEs outperform the existing state-of-the-art models for link prediction tasks. The code is available at https://github.com/SeongJinAhn/VGNAE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Link prediction in the graph is to determine whether there is an unknown relationship between nodes in the incomplete original graph. For example, it can be used to predict a link indicating if a specific substance in a biological network has a positive effect on the treatment of a specific disease, or if a specific literature or patent belongs to a certain author.</p><p>With the advancement of deep learning techniques, graph embedding models have been widely used in solving various problems Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM '21, November 1-5, 2021, Virtual Event, QLD, Australia ? 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8446-9/21/11. . . $15.00 https://doi.org/10. <ref type="bibr">1145/3459637.3482215</ref> in graphs. Graph embedding models generate low-dimensional vectors for nodes in the graph, called node embeddings, and learn these vector representations according to their purpose. Among graph embedding technologies, Graph Convolutional Networks (GCNs) are known to outperform other graph embedding models <ref type="bibr" target="#b1">[Bruna et al. 2013;</ref><ref type="bibr" target="#b2">Defferrard et al. 2016;</ref><ref type="bibr" target="#b7">Kipf and Welling 2016a]</ref>. GCNs utilize information of neighborhood to embed nodes by fusing features of the node itself and its neighbors. In link prediction tasks, a Graph Autoencoder (GAE) [Kipf and Welling 2016b] is used to learn node embeddings generated by the GCN-encoder via the reconstruction loss. Since then, numerous GAE/VGAE variants have been proposed to achieve improved performance of link prediction <ref type="bibr" target="#b3">[Di et al. 2020;</ref><ref type="bibr" target="#b13">Mavromatis and Karypis 2020;</ref><ref type="bibr" target="#b17">Pan et al. 2018;</ref><ref type="bibr" target="#b20">Salha et al. 2020</ref>].</p><p>Figure 1: (a) A graph with isolated nodes ( 7 and 8 ), (b) content features of nodes and (c) embedding space of latent vectors corresponding to node <ref type="bibr">(i=1,2,...,8)</ref> In many applications, there occur nodes in a graph that have no connection with other nodes. In this paper, we call such nodes, i.e., nodes with no connection isolated nodes. For example, consider the following scenario. There is a high school that maintains a social network G among members (e.g., students, professors, and staffs) where nodes are members and an edge between two members represents a "friendship" relation. Suppose the school has a number of freshmen. Here "Find out friends of students" can be considered a link prediction task where new nodes (freshmen) are involved. Such new nodes do not have any connection initially, and hence are isolated nodes in G when link prediction tasks are performed.</p><p>Since there are no connectivity information of this case, feature contents of isolated nodes (e.g. the circles or hobby of students) play a major role in link prediction.</p><p>We have found that existing GAEs do not properly handle feature contents of isolated nodes. GAEs learn to make a low similarity of embeddings between a pair of unconnected nodes. Consider a graph consisting of eight nodes in <ref type="figure">Figure 1</ref>  Relative positions of are determined based on the content information and the connectivity information in <ref type="figure">Figure 1</ref> (a). Now consider two isolated nodes 7 and 8 . Since 7 and 8 are not connected with other nodes, similarities between their embeddings and all other nodes should be low. We have found that GAEs tend to make the Euclidean norm of embedding vectors of isolated nodes small in order to reduce similarities between embeddings of isolated nodes and all the other nodes. As a result, the embeddings of isolated nodes go close to zero regardless of their content features.</p><p>In this paper, we propose a novel graph embedding technique, called Variational Graph Normalized AutoEncoder (VGNAE) for link prediction where the aforementioned problem of isolated nodes is properly handled. We propose a Graph Normalized Convolutional Network (GNCN) that effectively use 2 -normalization to prevent embeddings of isolated nodes from going near zero. Our VGNAE is a VGAE model where a GNCN is used to derive the mean and a GCN is used to derive the variance. We show through extensive experiments that our proposed VGNAE effectively handles the problem of isolated nodes, and outperforms other existing state-ofthe-art link prediction models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>A graph can be represented as = ( , , ) where is a set of vertices, ? ? is a set of edges, and is a feature matrix of . N(v) denotes a set of neighbors of ? , = | | denotes the number of vertices, and ? ? is an adjacency matrix of . Let ? be a vector that is an embedding of a node , and || ?|| denotes an euclidean norm ( 2 -norm) of vector ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Convolutional Networks</head><p>Graph Convolutional Networks (GCNs) generalize the convolution operations to the graph domain. The SpectralCNN <ref type="bibr" target="#b1">[Bruna et al. 2013]</ref> first proposes convolutional networks to the graph domains using the graph fourier transform. The ChebyConv <ref type="bibr" target="#b2">[Defferrard et al. 2016]</ref> parameterizes the graph convolution with chebyshev polynomials for efficient and localized filters. The GCN [Kipf and Welling 2016a] simplifies ChebyConv by using a normalization trick. Some unsupervised learning methods using GCNs have been proposed. <ref type="bibr" target="#b8">Kipf et al. [Kipf and Welling 2016b]</ref> propose two graph auto-encoders (GAEs and VGAEs) that reconstruct the adjacency matrix by node embeddings generated by GCNs.</p><p>LGAE <ref type="bibr" target="#b20">[Salha et al. 2020</ref>] is a simple and interpretable linear models leveraging one-hop linear encoding. ARGA and ARVGA <ref type="bibr" target="#b17">[Pan et al. 2018</ref>] are two variants of adversarial approaches to learn robust embeddings. GraphInfoClust <ref type="bibr">[Mavromatis and</ref> Karypis 2020] captures richer information and nodal interaction by maximizing the mutual information between nodes of a same cluster. sGraphite-VAE <ref type="bibr" target="#b3">[Di et al. 2020</ref>] extends the GNNs by exploring the aggregation using mutual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">2 -normalization</head><p>Certain properties about the norm of the embedding of the object (|| ? ||) have been addressed in several studies. In neural translation models, an infrequent word is prone to have a embedding with a low 2 -norm (|| ? ||) <ref type="bibr" target="#b0">[Arefyev et al. 2018;</ref><ref type="bibr" target="#b10">Kobayashi et al. 2020;</ref><ref type="bibr" target="#b15">Nguyen and Chiang 2017;</ref><ref type="bibr" target="#b16">Nguyen and Salazar 2019;</ref><ref type="bibr" target="#b21">Schakel and Wilson 2015]</ref>. In image recognition models, an embedding representing poor quality image has a low 2 -norm and vice versa <ref type="bibr" target="#b12">[Liu et al. 2017;</ref><ref type="bibr" target="#b25">Wang et al. 2018]</ref>. Also in image search, methods in <ref type="bibr" target="#b4">[Eghbali and Tahvildari 2019;</ref><ref type="bibr" target="#b27">Wu et al. 2017</ref>] normalizes the embedding to minimize the quantization error in high-resolution image search. They and their subsequent studies use 2 -normalization <ref type="bibr" target="#b19">[Ranjan et al. 2017;</ref><ref type="bibr" target="#b24">Wang et al. 2017;</ref>] to minimize errors caused by the imbalance between norms. In addition, some works <ref type="bibr" target="#b14">[Merrill et al. 2020;</ref><ref type="bibr" target="#b16">Nguyen and Salazar 2019]</ref> show that the magnitude of the parameter continues (norm) to increase during gradient descent. Zhang et al. <ref type="bibr" target="#b29">[Zhang et al. 2020</ref>] turns out that the imbalance between norms causes an unstable direction update and uses 2 -normalization to resolve the problem.</p><p>As far as we know, PairNorm <ref type="bibr" target="#b30">[Zhao and Akoglu 2019]</ref> and MSG-Norm ] are the only approach that use 2 -normalization in GCNs. However, they are proposed to solve the over-smoothing problem, not for the problem caused by isolated nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR APPROACH 3.1 Norm-zero tendency of isolated nodes</head><p>For node ? { 1 , 2 , ..., } in a graph, there are certain relationships between the norm of nodes || ? || from GAEs and degrees . <ref type="figure" target="#fig_1">Figure 2 (a)</ref> shows node embeddings from a GAE for the Cora and CiteSeer datasets in a 2-dimensional embedding space. <ref type="figure" target="#fig_1">Figure 2 (b)</ref> shows the norms of node embeddings || ? || from the GAE with respect to degrees of nodes for the Cora and CiteSeer datasets. As shown in <ref type="figure" target="#fig_1">Figure 2 (a)</ref>, embeddings of isolated nodes are around ? 0. The norm of those vectors will be close to zero. In <ref type="figure" target="#fig_1">Figure 2 (b)</ref>, we can find out the norms of an embedding vectors of isolated nodes tend to be close to zero. This also happens with the mean vector of VGAEs. We call this phenomenon "norm-zero tendency of isolated nodes", which is an extreme case of the imbalance between norms. This tendency makes embeddings of isolated nodes indistinguishable regardless of values of their content features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Normalized Convolutional Network</head><p>We propose a novel graph neural network called a Graph Normalized Convolutional Network (GNCN) that uses 2 -normalization before propagation. Consider a feature matrix = [ ? 1 , ? 2 , ..., ? ] where ? ? is a content feature vector of node and is the number of nodes. A GNCN first generates the feature transformed vectors ( ? ? ? ) with a learnable matrix W ? ? .</p><formula xml:id="formula_0">? ? = ?<label>(1)</label></formula><p>Let ? be a scaling constant that represents a norm of the hidden feature being propagated. Our proposed GNCN generates the normalized feature transformed vectors (? ? ) and propagates the normalized vector to generates node embeddings (? ? ). Now, for a feature matrix ? ? and an adjacency matrix , ( , , ) is defined as follows:</p><formula xml:id="formula_1">? = ? ? || ? ? || (2) ? = 1 + 1 ? + ?? ? ( ) 1 ? + 1 ?? + 1 ? where ? {1, 2, ..., }<label>(3)</label></formula><formula xml:id="formula_2">GNCN(X, A, s) =?? 1 2??? 1 2 ( ) (4) Here ( [ ? ? 1 , ? ? 2 , ..., ? ? ] ) = [ ? ? 1 | | ? ? 1 | | , ? ? 2 | | ? ? 2 | | , ..., ? ? | | ? ? | | ] , = [ ? 1 , ? 2</formula><p>, ..., ? ] ? R ? is a node embedding matrix, ? R ? is a trainable matrix,?= + ,?is a degree matrix of?, and is a scaling constant. We will show that our proposed GNCN properly handles the norm-zero tendency of isolated nodes through experiments in Sec 4.4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Variational Graph Normalized AutoEncoder</head><p>In this paper, we propose two variants of graph autoencoder called Graph Normalized AutoEncoder (GNAE) and Variational Graph Normalized AutoEncoder (VGNAE). For each node ? {1, 2, ..., }, GNAE encodes the local structure information and node feature information of its neighborhood to derive latent variables ? ? . To generate = [ ? 1 , ? 2 , ..., ? ] ? ? , GNAE uses a GNCN encoder that avoids the norm-zero tendency of isolated nodes. GNAE uses a inner-product decoder to create the reconstructed adjacent matrix?from as follows:</p><formula xml:id="formula_3">= ( ), with = ( , , )<label>(5)</label></formula><p>where is a sigmoid function. We also propose a Variational Graph Normalized AutoEncoder (VGNAE). Since mean vectors in VGAEs also have norm-zero tendency of isolated nodes, we derive mean vectors of VGNAE with a GNCN encoder. Our VNGAE takes a simple inference model by using the mean field approximation to define the variational family </p><formula xml:id="formula_4">( | , ) = =1 ( | , ) with ( | , ) = ( | ,<label>( 2 )</label></formula><p>)</p><formula xml:id="formula_5">(6) where = [ 1 , 2 , ..., ] = ( , , )</formula><p>is the matrix of mean vectors ; similarly = [ 1 , ..., ] = ( , ). Our generative model reconstructs graph structure by using a simple inner product decoder.</p><formula xml:id="formula_6">( | ) = =1 =1 ( | ? , ? ) with ( = 1| ) = ( ? ? )</formula><p>(7) For GNAE, we minimize the reconstruction error of?. For VG-NAE, optimization is made by maximizing a tractable variational lower bound (ELBO) as follows:</p><formula xml:id="formula_7">= ( | , ) [ ( | )] ? ( ( | , )|| ( )) (8)</formula><p>where ( || ) = ? ( ) is the Kullback-Leibler divergence between and . We use a Gaussian prior ( ) = =1 ( |0, ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We conduct various experiments to show performance improvements of our proposed VGNAEs in link prediction when isolated nodes are involved. First, we show the performance improvement of our GNCN by comparing the AUC scores of isolated nodes in various types of graph structured networks. Then, we evaluate our GAE/VGNAE employing the GNCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use various types of attributed graphs. First, we use three citation network datasets (Cora, Citeseer, and Pubmed). Second, we use a coauthor network Coauthor CS <ref type="bibr" target="#b22">[Shchur et al. 2018</ref>]. Third, we use a co-purchase graph Amazon Photo <ref type="bibr" target="#b22">[Shchur et al. 2018</ref>]. Statistics about datasets are described in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Setup</head><p>We implement all our models using Pytorch 1.4.0 <ref type="bibr" target="#b18">[Paszke et al. 2019</ref>]. We use the Adam optimizer [Kingma and Ba 2014] with a learning rate of 0.005. We train all models for a maximum of 300 epochs and early stopping with a window size of 50. In all the experiments, 64 dimensions are used for node embeddings. A scaling constant ( ) in GNCN is set as 1.8. For GCN [Kipf and Welling 2016a], we use a two-layer GCN with the dimension of hidden embeddings is set to 128. For GraphHeat <ref type="bibr" target="#b28">[Xu et al. 2020]</ref> and APPNP <ref type="bibr" target="#b9">[Klicpera et al. 2018]</ref>, the optimal hyper-parameters (e.g. scaling parameter and teleport rate ) are chosen through validation set. For GraphHeat <ref type="bibr" target="#b28">[Xu et al. 2020]</ref>, 0.4 is used for the coeffient . For APPNP <ref type="bibr" target="#b9">[Klicpera et al. 2018]</ref>, 0.15 is used for the teleport rate and 10 is used for the number of propagations. The link prediction models are evaluated by the area under the ROC curve (AUC) and average precision (AP) scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>4.3.1 GNCN : Power of normalization for isolated nodes. In this section, we present that performance of GAEs with existing GCN encoders degrades when isolated nodes are involved. We also present that GAEs using our proposed GNCN encoders effectively encode isolated nodes. We compare the performance of GAEs using various GCN-based encoders for isolated nodes and connected nodes on Cora, CiteSeer, CS and, Photo datasets. The compared GCNbased encoders are GCN [Kipf and Welling 2016a], GAT <ref type="bibr" target="#b23">[Veli?kovi? et al. 2017]</ref>, SGCN <ref type="bibr" target="#b26">[Wu et al. 2019]</ref>, SuperGAT <ref type="bibr" target="#b5">[Kim and Oh 2020]</ref>, APPNP <ref type="bibr" target="#b9">[Klicpera et al. 2018]</ref>, GraphHeat <ref type="bibr" target="#b28">[Xu et al. 2020]</ref>, PairNorm <ref type="bibr" target="#b30">[Zhao and Akoglu 2019]</ref>, and MSGNorm .</p><p>For every dataset, we use a training set 60%, a validation set 10%, and a test set 30% among all edges. We add the same number of randomly sampled negative edges to the valid set and test set. We measured the AUC score for isolated nodes and connected nodes in test sets. The results are shown in <ref type="figure" target="#fig_2">Figures 3. Figure 3</ref> shows that the AUC scores of GAEs with other GCN encoders of isolated nodes is significantly lower than that of connected nodes (10 ? 20%) for all types of graphs. Experimental results on graphs of various types show that the degrade of performance in isolated nodes occurs in general graphs. In addition, it can be seen that the accuracy at the isolated nodes for each GCN varies depending on the type of dataset. We confirmed that existing GCNs are not suitable encoders of GAEs when isolated nodes are involved. The methods using proposed GNCN ensured the highest performance of the isolated nodes for all graphs without compromising the performance of the connected nodes.  <ref type="bibr" target="#b13">[Mavromatis and Karypis 2020]</ref>, and sGraphite-VAE <ref type="bibr" target="#b3">[Di et al. 2020]</ref>. For all datasets, 20%, 40%, and 80% of edges are used for training sets. For the remaining edges, the ratio of 1 to 3 are used for valiatdation sets and test sets. We add the same number of randomly sampled negative edges for each valid and test set. For each dataset divided in this way, the AUC and AP scores are measured. The results of link prediction in the dataset are shown in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Performance Comparision of GNAE</head><p>As can be seen in <ref type="table" target="#tab_2">Table 2</ref>, our GNAE/VGNAE show superior results compared to other methods in all divisions. Also we can observe that the fewer observed edges (the smaller the ratio of the training rate), the better the performance of our proposed method compared to other SOTA models. This is because as the ratio of unobserved edges increase, the number of isolated nodes also increases.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We have presented that in GAE and VGAE embeddings of isolated nodes tends to go to zero regardless of their content features on various graph-structured networks. This tendency can significantly degrade accuracy of link prediction when many isolated nodes are involved. We have argued that the 2 -normalization is an effective technique to generate proper embeddings of isolated nodes in the link prediction task. We have shown through extensive experiments that our proposed VGNAE performs better than other existing methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a). Figure 1 (c) shows vectors in an embedding space of the graph in Figure 1 (a) where (i=1, 2, ..., 8) is a latent vector corresponding to node (i=1, 2, ..., 8). arXiv:2108.08046v2 [cs.LG] 12 Sep 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) A 2-dimensional embedding space of a GAE in the Cora and the CiteSeer dataset (b) a degree of nodes ( ) and their norms of node embeddings || ? || from a GAE in the Cora and the CiteSeer dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>AUC scores of isolated nodes and connected nodes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The statistics of the benchmark graph datasets</figDesc><table><row><cell cols="2">Dataset Type</cell><cell cols="3">#Nodes #Edges #Features</cell></row><row><cell>Cora</cell><cell>Citation</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell></row><row><cell cols="2">CiteSeer Citation</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell></row><row><cell cols="2">PubMed Citation</cell><cell>19,717</cell><cell>44,338</cell><cell>500</cell></row><row><cell>CS</cell><cell>Coauthor</cell><cell>18,333</cell><cell>81,894</cell><cell>6,805</cell></row><row><cell>Photo</cell><cell cols="2">Co-purchase 7,487</cell><cell cols="2">119,043 745</cell></row><row><cell>as follows:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>/VGNAE with state-of-theart methods. We conduct experiments on citation networks (Cora, CiteSeer, and PubMed) to compare the performance of GNAE/VGNAE with state-of-the-art link prediction models, i.e., LGAE<ref type="bibr" target="#b20">[Salha et al. 2020]</ref>, ARGA [Pan et al. 2018], ARGVA [Pan et al. 2018], Graph InfoClust (GIC)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experiment results for citation datasets on link prediction task</figDesc><table><row><cell>Data</cell><cell cols="3">Metric Train GAE LGAE ARGA GIC sGraph GNAE VGNAE</cell></row><row><cell></cell><cell></cell><cell>20% 0.782 0.866 0.795 0.880 0.845 0.887</cell><cell>0.890</cell></row><row><cell></cell><cell>AUC</cell><cell>40% 0.856 0.908 0.844 0.914 0.840 0.926</cell><cell>0.929</cell></row><row><cell>Cora</cell><cell></cell><cell cols="2">80% 0.922 0.938 0.919 0.933 0.885 0.956 20% 0.793 0.878 0.806 0.881 0.829 0.901 0.901 0.954</cell></row><row><cell></cell><cell>AP</cell><cell>40% 0.861 0.915 0.856 0.911 0.828 0.936</cell><cell>0.933</cell></row><row><cell></cell><cell></cell><cell>80% 0.930 0.945 0.927 0.929 0.867 0.957</cell><cell>0.958</cell></row><row><cell></cell><cell></cell><cell>20% 0.786 0.906 0.750 0.930 0.928 0.946</cell><cell>0.941</cell></row><row><cell></cell><cell>AUC</cell><cell>40% 0.836 0.925 0.832 0.936 0.936 0.956</cell><cell>0.961</cell></row><row><cell>CiteSeer</cell><cell></cell><cell>80% 0.894 0.955 0.904 0.962 0.963 0.965 20% 0.797 0.913 0.777 0.934 0.897 0.953</cell><cell>0.970 0.948</cell></row><row><cell></cell><cell>AP</cell><cell>40% 0.850 0.929 0.844 0.938 0.910 0.958</cell><cell>0.966</cell></row><row><cell></cell><cell></cell><cell>80% 0.903 0.959 0.915 0.966 0.943 0.970</cell><cell>0.971</cell></row><row><cell></cell><cell></cell><cell>20% 0.937 0.946 0.936 0.950 0.837 0.950</cell><cell>0.951</cell></row><row><cell></cell><cell>AUC</cell><cell>40% 0.959 0.962 0.955 0.958 0.876 0.963</cell><cell>0.964</cell></row><row><cell>PubMed</cell><cell></cell><cell>80% 0.967 0.974 0.973 0.960 0.896 0.975 20% 0.940 0.947 0.941 0.947 0.859 0.950</cell><cell>0.976 0.949</cell></row><row><cell></cell><cell>AP</cell><cell>40% 0.961 0.961 0.959 0.956 0.879 0.961</cell><cell>0.963</cell></row><row><cell></cell><cell></cell><cell>80% 0.967 0.975 0.976 0.965 0.902 0.975</cell><cell>0.976</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">How much does a word weigh? Weighting word embeddings for word sense induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Arefyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Ermolaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09209</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mutual Information Maximization in Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep spherical quantization for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepehr</forename><surname>Eghbali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladan</forename><surname>Tahvildari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11690" to="11699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How to find your friendly neighborhood: Graph attention design with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongkwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational graph auto-encoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Attention is not only a weight: Analyzing transformers with vector norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goro</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuki</forename><surname>Kuribayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Yokoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10102</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="212" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Graph InfoClust: Leveraging clusterlevel node information for unsupervised graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Costas</forename><surname>Mavromatis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06946</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Vivek Ramanujan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09697</idno>
		<title level="m">Parameter Norm Growth During Training of Transformers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving lexical choice in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Toan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01329</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Transformers without tears: Improving the normalization of self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Toan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05895</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Shirui Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04407</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">L2-constrained softmax loss for discriminative face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Simple and effective graph autoencoders with one-hop linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Salha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07614</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Adriaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.02297</idno>
		<title level="m">Measuring word significance using distributed representations of words</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of graph neural network evaluation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1041" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiscale quantization for fast similarity search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananda</forename><forename type="middle">Theertha</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Holtmann-Rice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Simcha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5745" to="5755" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Keting Cen, and Xueqi Cheng. 2020. Graph convolutional networks using heat kernel for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.16002</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Metric Learning with Spherical Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12223</idno>
		<title level="m">Pairnorm: Tackling oversmoothing in gnns</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ring loss: Convex feature normalization for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dipan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5089" to="5097" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
