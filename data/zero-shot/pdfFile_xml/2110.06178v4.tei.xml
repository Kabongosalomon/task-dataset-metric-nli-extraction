<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2022 TADA! TEMPORALLY-ADAPTIVE CONVOLUTIONS FOR VIDEO UNDERSTANDING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyuan</forename><surname>Huang</surname></persName>
							<email>ziyuan.huang@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Advanced Robotics Centre</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="institution">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Pan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Qing</surname></persName>
							<email>qingzhiwu.qzw@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="institution">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqian</forename><surname>Tang</surname></persName>
							<email>mingqian.tmq@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="institution">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
							<email>ziwei.liu@ntu.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><forename type="middle">H</forename><surname>Ang</surname><genName>Jr</genName></persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Robotics Centre</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2022 TADA! TEMPORALLY-ADAPTIVE CONVOLUTIONS FOR VIDEO UNDERSTANDING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spatial convolutions 1 are widely used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding 2 , which shows that adaptive weight calibration along the temporal dimension is an efficient way to facilitate modelling complex temporal dynamics in videos. Specifically, TAdaConv empowers the spatial convolutions with temporal modelling abilities by calibrating the convolution weights for each frame according to its local and global temporal context. Compared to previous temporal modelling operations, TAdaConv is more efficient as it operates over the convolution kernels instead of the features, whose dimension is an order of magnitude smaller than the spatial resolutions. Further, the kernel calibration brings an increased model capacity. We construct TAda2D and TAda-ConvNeXt networks by replacing the 2D convolutions in ResNet and ConvNeXt with TAdaConv, which leads to at least on par or better performance compared to state-of-the-art approaches on multiple video action recognition and localization benchmarks. We also demonstrate that as a readily plug-in operation with negligible computation overhead, TAdaConv can effectively improve many existing video models with a convincing margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Convolutions are an indispensable operation in modern deep vision models <ref type="bibr" target="#b17">(He et al., 2016;</ref>, whose different variants have driven the state-of-the-art performances of convolutional neural networks (CNNs) in many visual tasks <ref type="bibr" target="#b55">(Xie et al., 2017;</ref><ref type="bibr" target="#b7">Dai et al., 2017;</ref><ref type="bibr" target="#b62">Zhou et al., 2019)</ref> and application scenarios <ref type="bibr" target="#b19">(Howard et al., 2017;</ref>. In the video paradigm, compared to the 3D convolutions <ref type="bibr" target="#b47">(Tran et al., 2015)</ref>, the combination of 2D spatial convolutions and 1D temporal convolutions are more widely preferred owing to its efficiency <ref type="bibr" target="#b48">(Tran et al., 2018;</ref><ref type="bibr" target="#b42">Qiu et al., 2017)</ref>. Nevertheless, 1D temporal convolutions still introduce non-negligible computation overhead on top of the spatial convolutions. Therefore, we seek to directly equip the spatial convolutions with temporal modelling abilities. TAdaConv adaptively calibrates the kernel weights for each frame by its temporal context. 2021;  to leverage the existing pre-trained models such as ResNet. This is critical in video applications, since training video models from scratch is highly resource demanding <ref type="bibr" target="#b13">(Feichtenhofer et al., 2019;</ref><ref type="bibr" target="#b12">Feichtenhofer, 2020)</ref> and prone to over-fitting on small datasets. Second, for most dynamic filters, the weights are generated with respect to its spatial context <ref type="bibr" target="#b63">(Zhou et al., 2021;</ref><ref type="bibr" target="#b22">Jia et al., 2016)</ref> or the global descriptor <ref type="bibr" target="#b4">(Chen et al., 2020a;</ref>, which has difficulty in capturing the temporal variations between frames.</p><p>In this work, we present the Temporally-Adaptive Convolution (TAdaConv) for video understanding, where the convolution kernel weights are no longer fixed across different frames. Specifically, the convolution kernel for the t-th frame W t is factorized to the multiplication of the base weight and a calibration weight: W t = ? t ? W b , where the calibration weight ? t is adaptively generated from the input data for all channels in the base weight W b . For each frame, we generate the calibration weight based on the frame descriptors of its adjacent time steps as well as the global descriptor, which effectively encodes the local and global temporal dynamics in videos. The difference between TAdaConv and the spatial convolutions is visualized in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>The main advantages of this factorization are three-fold: (i) TAdaConv can be easily plugged into any existing models to enhance temporal modelling, and their pre-trained weights can still be exploited; (ii) the temporal modelling ability can be highly improved with the help of the temporallyadaptive weight; (iii) in comparison with temporal convolutions that often operate on the learned 2D feature maps, TAdaConv is more efficient by directly operating on the convolution kernels.</p><p>TAdaConv is proposed as a drop-in replacement for the spatial convolutions in existing models. It can both serve as a stand-alone temporal modelling module for 2D networks, or be inserted into existing convolutional video models to further enhance the ability to model temporal dynamics. For efficiency, we construct TAda2D by replacing the spatial convolutions in ResNet <ref type="bibr" target="#b17">(He et al., 2016)</ref>, which leads to at least on par or better performance than the state-of-the-arts. Used as an enhancement of existing video models, TAdaConv leads to notable improvements on multiple video datasets. The strong performance and the consistent improvements demonstrate that TAdaConv can be an important operation for modelling complex temporal dynamics in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>ConvNets for temporal modelling. A fundamental difference between videos and images lies in the temporal dimension, which makes temporal modeling an important research area for understanding videos. Recent deep CNNs for video understanding can be divided into two types. The first type jointly models spatio-temporal information by 3D convolutions <ref type="bibr" target="#b2">(Carreira &amp; Zisserman, 2017;</ref><ref type="bibr" target="#b47">Tran et al., 2015;</ref><ref type="bibr" target="#b12">Feichtenhofer, 2020;</ref><ref type="bibr" target="#b49">Tran et al., 2019)</ref>. The second type builds upon 2D networks, where most approaches employ 2D convolutions that share weights among all the frames for spatial modelling, and design additional operations for temporal modelling, such as temporal shift <ref type="bibr" target="#b31">(Lin et al., 2019a)</ref>, temporal difference <ref type="bibr" target="#b23">Jiang et al., 2019)</ref>, temporal convolution <ref type="bibr" target="#b48">(Tran et al., 2018;</ref><ref type="bibr" target="#b34">Liu et al., 2021b)</ref> and correlation operation <ref type="bibr" target="#b50">(Wang et al., 2020)</ref>, etc. Our work directly empowers the spatial convolutions with temporal modelling abilities, which can be further coupled with other temporal modelling operations for stronger video recognition performances.  </p><formula xml:id="formula_0">1 ? K ? K W 2D Conv X X C o ? C ? K 2 K ? K ? K W 3D Conv X X C o ? C ? K 3<label>(</label></formula><formula xml:id="formula_1">C ? T C ? T ? H ? W BN, ReLU C/r ? T 1D conv C ? T W b C o ? C ? K 2 T ? C o ? C ? K 2 TAdaConv ? W t Spat Pool 1 C ? 1 C ? T Corrected 1D conv C/r ? T C ? 1</formula><p>Global Modelling <ref type="figure">Figure 2</ref>: An instantiation of TAdaConv and the temporal feature aggregation used in TAda2D. Dynamic networks. Dynamic networks refer to networks with content-adaptive weights or modules, such as dynamic filters/convolutions <ref type="bibr" target="#b22">(Jia et al., 2016;</ref>, dynamic activations <ref type="bibr" target="#b29">(Li et al., 2020d;</ref><ref type="bibr" target="#b5">Chen et al., 2020b)</ref>, and dynamic routing <ref type="bibr" target="#b28">Li et al., 2020c)</ref>, etc. Dynamic networks have demonstrated their exceeding network capacity and thus performance compared to the static ones. In video understanding, dynamic filter <ref type="bibr" target="#b34">(Liu et al., 2021b)</ref> or temporal aggregation <ref type="bibr" target="#b37">(Meng et al., 2021)</ref> also demonstrate strong capability of temporal modelling. Recently, some convolutions applies spatially-adaptive weights <ref type="bibr" target="#b10">(Elsayed et al., 2020;</ref>, showing the benefit of relaxing the spatial invariance in modelling diverse visual contents. Similarly, our proposed TAdaConv enhances temporal modelling by relaxing the invariance along the temporal dimension. TAdaConv has two key differences from previous works: (i) the convolution weight in TAdaConv is factorized into a base weight and a calibration weight, which enables TAdaConv to exploit pre-trained weights; (ii) the calibration weights are generated according to the temporal contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TADACONV: TEMPORALLY-ADAPTIVE CONVOLUTIONS</head><p>In this work, we seek to empower the spatial convolutions with temporal modelling abilities. Inspired by the calibration process of temporal convolutions (Sec. 3.1), TAdaConv dynamically calibrates the convolution weights for each frame according to its temporal context (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">REVISITING TEMPORAL CONVOLUTIONS</head><p>We first revisit the temporal convolutions to showcase its underlying process and its relation to dynamic filters. We consider depth-wise temporal convolution for simplicity, which is more widely used because of its efficiency <ref type="bibr" target="#b34">(Liu et al., 2021b;</ref><ref type="bibr" target="#b23">Jiang et al., 2019)</ref>. Formally, for a 3?1?1 temporal convolution filter parameterized by ? = [? 1 , ? 2 , ? 3 ] and placed (ignoring normalizations) after the 2D convolution parameterized by W, the output featurex t of the t-th frame can be obtained by:</p><formula xml:id="formula_2">x t = ? 1 ? ?(W * x t?1 ) + ? 2 ? ?(W * x t ) + ? 3 ? ?(W * x t+1 ) ,<label>(1)</label></formula><p>where the ? indicates the element-wise multiplication, * denotes the convolution over the spatial dimension and ? denotes ReLU activation <ref type="bibr" target="#b38">(Nair &amp; Hinton, 2010)</ref>. It can be rewritten as follows:</p><formula xml:id="formula_3">x t = W t?1 * x t?1 + W t * x t + W t+1 * x t+1 ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">W i,j t?1 = M i,j t?1 ? ? 1 ? W, W i,j t = M i,j t ? ? 2 ? W and W i,j t+1 = M i,j t+1</formula><p>? ? 3 ? W are spatiotemporal location adaptive convolution weights. M t ? R C?H?W is a dynamic tensor, with its value dependent on the result of the spatial convolutions (see Appendix A for details). Hence, the temporal convolutions in the (2+1)D convolution essentially performs (i) weight calibration on the spatial convolutions and (ii) feature aggregation between adjacent frames. However, if the temporal modelling is achieved by coupling temporal convolutions to spatial convolutions, a non-negligible computation overhead is still introduced (see <ref type="table" target="#tab_2">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FORMULATION OF TADACONV</head><p>For efficiency, we set out to directly empower the spatial convolutions with temporal modelling abilities. Inspired by the recent finding that the relaxation of spatial invariance strengthens spatial modelling <ref type="bibr" target="#b63">(Zhou et al., 2021;</ref><ref type="bibr" target="#b10">Elsayed et al., 2020)</ref>, we hypothesize that temporally adaptive weights can also help temporal modelling. Therefore, the convolution weights in a TAdaConv layer are varied on a frame-by-frame basis. Since we observe that previous dynamic filters can hardly utilize the pretrained weights, we take inspiration from our observation in the temporal convolutions and factorize the weights for the t-th frame W t into the multiplication of a base weight W b shared for all frames, and a calibration weight ? t that are different for each time step:</p><formula xml:id="formula_5">x t = W t * x t = (? t ? W b ) * x t ,<label>(3)</label></formula><p>Calibration weight generation. To allow for the TAdaConv to model temporal dynamics, it is crucial that the calibration weight ? t for the t-th frame takes into account not only the current frame, but more importantly, its temporal context, i.e., ? t = G(..., x t?1 , x t , x t+1 , ...). Otherwise TAdaConv would degenerate to a set of unrelated spatial convolutions with different weights applied on different frames. We show an instantiation of the generation function G in <ref type="figure">Fig. 2(b)</ref>.</p><p>In our design, we aim for efficiency and the ability to capture inter-frame temporal dynamics. For efficiency, we operate on the frame description vectors v ? R T ?C obtained by the global average pooling over the spatial dimension GAP s for each frame, i.e., v t = GAP s (x t ). For temporal modelling, we apply stacked two-layer 1D convolutions F with a dimension reduction ratio of r on the local temporal context v adj</p><formula xml:id="formula_6">t = {v t?1 , v t , v t+1 } obtained from x adj t = {x t?1 , x t , x t+1 }: F(x adj t ) = Conv1D C/r?C (?(BN(Conv1D C?C/r (v adj t )))) .<label>(4)</label></formula><p>where ? and BN denote the ReLU <ref type="bibr" target="#b38">(Nair &amp; Hinton, 2010)</ref> and batchnorm <ref type="bibr" target="#b21">(Ioffe &amp; Szegedy, 2015)</ref>.</p><p>In order for a larger inter-frame field of view in complement to the local 1D convolution, we further incorporate global temporal information by adding a global descriptor g to the weight generation process F through a linear mapping function FC:</p><formula xml:id="formula_7">F(x adj t , g) = Conv1D C/r?C (?(BN(Conv1D C?C/r (v adj t + FC C?C (g)))) ,<label>(5)</label></formula><p>where g = GAP st (x) with GAP st being global average pooling on spatial and temporal dimensions.</p><p>Initialization. The TAdaConv is designed to be readily inserted into existing models by simply replacing the 2D convolutions. For an effective use of the pre-trained weights, TAdaConv is initialized to behave exactly the same as the standard convolution. This is achieved by zero-initializing the weight of the last convolution in F and adding a constant vector 1 to the formulation:</p><formula xml:id="formula_8">? t = G(x) = 1 + F(x adj t , g) .<label>(6)</label></formula><p>In this way, at initial state, W t = 1 ? W b = W b , where we load W b with the pre-trained weights.</p><p>Calibration dimension. The base weight W b ? R Cout?Cin?k 2 can be calibrated in different dimensions. We instantiate the calibration on the C in dimension (? t ? R 1?Cin?1 ), as the weight generation based on the input features yields a more precise estimation for the relation of the input channels than the output channels or spatial structures (empirical analysis in <ref type="table" target="#tab_8">Table 7</ref>). Comparison with other dynamic filters. Table 1 compares TAdaConv with existing dynamic filters. Mixtue-of-experts based dynamic filters such as CondConv dynamically aggregates multiple kernels to generate the weights that are shared for all locations. The weights in most other dynamic filters are completely generated from the input, such as DynamicFilter <ref type="bibr" target="#b22">(Jia et al., 2016)</ref> and DDF <ref type="bibr" target="#b63">(Zhou et al., 2021)</ref> in images and TAM <ref type="bibr" target="#b34">(Liu et al., 2021b)</ref> in videos. Compared to image based ones, TAdaConv achieves temporal modelling by generating weights from the local and global temporal context. Compared to TANet <ref type="bibr" target="#b34">(Liu et al., 2021b)</ref>, TAdaConv is better at temopral modellling because of temporally adaptive weights. Further, TAdaConv can effectively generate weights identical to the pre-trained ones, while it is difficult for previous approaches to exploit pre-trained models. More detailed comparisons of dynamic filters are included in Appendix J.  Comparison with temporal convolutions. <ref type="table" target="#tab_2">Table 2</ref> compares the TAdaConv with R(2+1)D in parameters and FLOPs, which shows most of our additional computation overhead on top of the spatial convolution is an order of magnitude less than the temporal convolution. For detailed computation analysis and comparison with other temporal modelling approaches, please refer to Appendix B.</p><formula xml:id="formula_9">(2+1)D Conv TAdaConv FLOPs Co ? Ci ? k 2 ? T HW Co ? Ci ? k 2 ? T HW + Ci ? (T HW + T ) +Co ? Ci ? k ? T HW +Ci ? Ci/r ? (2 ? k ? T + 1) + Co ? Ci ? k 2 ? T E.G.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TADA2D: TEMPORALLY ADAPTIVE 2D NETWORKS</head><p>We construct TAda2D networks by replacing the 2D convolutions in ResNet (R2D, see Appendix D) with our proposed TAdaConv. Additionally, based on strided average pooling, we propose a temporal feature aggregation module for the 2D networks, corresponding to the second essential step for the temporal convolutions. As illustrated in <ref type="figure">Fig. 2(c)</ref>, the aggregation module is placed after TAdaConv. Formally, given the output of TAdaConvx, the aggregated feature can be obtained as follows:</p><formula xml:id="formula_10">x aggr = ?(BN 1 (x) + BN 2 (TempAvgPool k (x))) ,<label>(7)</label></formula><p>where TempAvgPool k denotes strided temporal average pooling with kernel size of k. We use different batch normalization parameters for the features extracted by TAdaConvx and aggregated by strided average pooling TempAvgPool k (x), as their distributions are essentially different. During initialization, we load pre-trained weights to BN 1 , and initialize the parameters of BN 2 to zero. Coupled with the initialization of TAdaConv, the initial state of the TAda2D is exactly the same as the Temporal Segment Networks <ref type="bibr" target="#b51">(Wang et al., 2016)</ref>, while the calibration and the aggregation notably increases the model capacity with training (See Appendix I). In the experiments, we refer to this structure as the shortcut (Sc.) branch and the separate BN (SepBN.) branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS ON VIDEO CLASSIFICATION</head><p>To show the effectiveness and generality of the proposed approach, we present comprehensive evaluation of TAdaConv and TAda2D on two video understanding tasks using four large-scale datasets.</p><p>Datasets. For video classification, we use Kinetics-400 <ref type="bibr" target="#b24">(Kay et al., 2017)</ref>, Something-Something-V2 <ref type="bibr" target="#b14">(Goyal et al., 2017)</ref>, and Epic-Kitchens-100 <ref type="bibr" target="#b8">(Damen et al., 2020)</ref>. K400 is a widely used action classification dataset with 400 categories covered by ?300K videos. SSV2 includes 220K videos with challenging spatio-temporal interactions in 174 classes. EK100 includes 90K segments labelled by 97 verb and 300 noun classes with actions defined by the combination of nouns and verbs. For action localization, we use HACS <ref type="bibr" target="#b59">(Zhao et al., 2019)</ref> and Epic-Kitchens-100 <ref type="bibr" target="#b8">(Damen et al., 2020)</ref>.</p><p>Model. In our experiments, we mainly use ResNet (R2D) as our base model, and construct TAda2D by replacing the spatial convolutions with the TAda-structure in <ref type="figure">Fig. 2(c)</ref>. Alternatively, we also construct TAdaConvNeXt based on the recent ConvNeXt model <ref type="bibr" target="#b35">(Liu et al., 2022)</ref>. For TAdaCon-vNeXt, we use a tubelet embedding stem similar to <ref type="bibr" target="#b0">(Arnab et al., 2021)</ref> and only use TAdaConv to replace the depth-wise convolutions in the model. More details are included in Appendix D.</p><p>Training and evaluation. During training, 8, 16 or 32 frames are sampled with temporal jittering, following convention <ref type="bibr" target="#b31">(Lin et al., 2019a;</ref><ref type="bibr" target="#b34">Liu et al., 2021b;</ref><ref type="bibr" target="#b13">Feichtenhofer et al., 2019)</ref>. We include further training details in the appendix C. For evaluation, we use three spatial crops with 10 or 4 clips (K400&amp;EK100), or 2 clips (SSV2) uniformly sampled along the temporal dimension. Each crop has the size of 256?256, which is obtained from a video with its shorter side resized to 256.   </p><formula xml:id="formula_11">TAdaConv FA. Sc. SepBN. Top-1 ? - - - 32.0 - - - - 59.2 +27.2 Avg. - 47.9 +15.9 Avg. 49.0 +17.0 Avg. 57.0 +25.0 Avg. - 60.1 +28.1 Avg. 61.5 +29.5 Avg. 63.8 +31.8 Max.</formula><p>63.5 +31.5 Mix.</p><p>63.7 +31.7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">TADACONV ON EXISTING VIDEO BACKBONES</head><p>TAdaConv is designed as a plug-in substitution for the spatial convolutions in the video models. Hence, we first present plug-in evaluations in <ref type="table" target="#tab_4">Table 3</ref>. TAdaConv improves the classification performance with negligible computation overhead on a wide range of video models, including Slow-Fast <ref type="bibr" target="#b13">(Feichtenhofer et al., 2019)</ref>, R3D <ref type="bibr" target="#b15">(Hara et al., 2018)</ref> and R(2+1)D <ref type="bibr" target="#b48">(Tran et al., 2018)</ref>, by an average of 1.3% and 2.8% respectively on K400 and SSV2 at an extra computational cost of less than 0.02 GFlops. Further, not only can TAdaConv improve spatial convolutions, it also notably improve 3D and 1D convolutions. For fair comparison, all models are trained using the same training strategy. Further plug-in evaluations for action classification is presented in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ABLATION STUDIES</head><p>We present thorough ablation studies for the justification of our design choices and the effectiveness of our TAdaConv in modelling temporal dynamics. SSV2 is used as the evaluation benchmark, as it is widely acknowledged to have more complex spatio-temporal interactions. Dynamic vs. learnable calibration. We first compare different source of calibration weights (with our initialization strategy) in <ref type="table" target="#tab_7">Table 6</ref>. We compare our calibration with no calibration, calibration using learnable weights, and calibration using dynamic weights generated only from a global descriptor (C?1). Compared with the baseline (TSN <ref type="bibr" target="#b51">(Wang et al., 2016)</ref>) with no calibration, learnable calibration with shared weights has limited improvement, while temporally varying learnable calibration (different calibration weights for different temporal locations) performs much stronger. A larger improvement is observed when we use dynamic calibration, where temporally varying calibration further raises the accuracy. The results also validate our hypothesis that temporal modelling can benefit from temporally adaptive weights. Further, TAdaConv generates calibration weight from both local (C?T) and global (C?1) contexts and achieves the highest performance.  Calibration weight initialization. Next, we show that our initialization strategy for the calibration weight generation plays a critical role for dynamic weight calibration. As in <ref type="table" target="#tab_7">Table 6</ref>, randomly initializing learnable weights slightly degrades the performance, while randomly initializing dynamic calibration weights (by randomly initializing the last layer of the weight generation function) notably degenerates the performance. It is likely that randomly initialized dynamic calibration weights purturb the pre-trained weights more severely than the learnable weights since it is dependent on the input. Further comparisons on the initialization are shown in the <ref type="table" target="#tab_8">Table A7</ref> in the Appendix.</p><p>Calibration weight generation function. Having established that the temporally adaptive dynamic calibration with appropriate initialization can be an ideal strategy for temporal modelling, we further ablate different ways for generating the calibration weight in <ref type="table" target="#tab_5">Table 4</ref>. Linear weight generation function (Lin.) applies a single 1D convolution to generate the calibration weight, while non-linear one (Non-Lin.) uses two stacked 1D convolutions with batch normalizations and ReLU activation in between. When no temporal context is considered (K.=1 or (1,1)), TAdaConv can still improve the baseline but with a limited gap. Enlarging the kernel size to cover the temporal context (K.=3, (1,3), (3,1) or (3,3)) further yields a boost of over 20% on the accuracy, with K.=(3,3) having the strongest performance. This shows the importance of the local temporal context during calibration weight generation. Finally, for the scope of temporal context, introducing global context to frame descriptors performs similarly to only generating temporally adaptive calibration weights solely on the global context (in <ref type="table" target="#tab_7">Table 6</ref>). The combination of the global and temporal context yields a better performance for both variants. We further show in Appendix J that this function in our TAdaConv yields a better calibration on the base weight than other existing dynamic filters.</p><p>Feature aggregation. The feature aggregation module used in the TAda2D network is ablated in <ref type="table" target="#tab_6">Table 5</ref>. First, the performance is similar for plain aggregation x = Avg(x) and aggregation with a shortcut (Sc.) branch x = x + Avg(x), with Sc. being slightly better. Separating the batchnorm (Eq. 7) for the shortcut and the aggregation branch brings notable improvement. Strided max and mix (avg+max) pooling slightly underperform the average pooling variant. Overall, the combination of TAdaConv and our feature aggregation scheme has an advantage over the TSN baseline of 31.8%. Calibration dimension. Multiple dimensions can be calibrated in the base weight. <ref type="table" target="#tab_8">Table 7</ref> shows that calibrating the channel dimension more suitable than the spatial dimension, which means that the spatial structure of the original convolution kernel should be retained. Within channels, the calibration works better on C in than C out or both combined. This is probably because the calibration weight generated by the input feature can better adapt to itself.</p><p>Different stages employing TAdaConv. The solid lines in <ref type="figure" target="#fig_3">Fig 3 show</ref> the stage by stage replacement of the spatial convolutions in a ResNet model. It has a minimum improvement of 17.55%, when TAdaConv is employed in Res2. Compared to early stages, later stages contribute more to the final performance, as later stages provide more accurate calibration because of its high abstraction level. Overall, TAdaConv is used in all stages for the highest accuracy.  <ref type="bibr" target="#b14">(Goyal et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Backbone Frames?clips?crops</head><p>GFLOPs Top-1 Top-5 TDN  ResNet-50 (8f +32f )?1?1 47 64.0 88.8 TDN  ResNet-50 (16f +64f )?1?1 94 65.3 89.5 TDN  ResNet-50 (8f +32f +16f +64f )?1?1 141 67.0 90.3 TSM <ref type="bibr" target="#b31">(Lin et al., 2019a)</ref> ResNet-50 8f ?2?3 43 59.1 85.6 TSM <ref type="bibr" target="#b31">(Lin et al., 2019a)</ref> ResNet  Different proportion of channels calibrated by TAdaConv. Here, we calibrate only a proportion of channels using TAdaConv and leave the other channels uncalibrated. The results are presented as dotted lines in <ref type="figure" target="#fig_3">Fig 3.</ref> We find TAdaConv can improve the baseline by a large margin even if only 1/64 channels are calibrated, with larger proportion yielding further larger improvements.</p><p>Visualizations. We qualitatively evaluate our approach in comparison with the baseline approaches (TSN and R(2+1)D) by presenting the Grad-CAM (Selvaraju et al., 2017) visualizations of the last stage in <ref type="figure" target="#fig_4">Fig. 4</ref>. TAda2D can more completely spot the key information in videos, thanks to the temporal reasoning based on global spatial information and the global temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">MAIN RESULTS</head><p>SSV2. As shown in <ref type="table" target="#tab_9">Table 8</ref>, TAda2D outperforms previous approaches using the same number of frames. Compared to TDN that uses more frames, TAda2D performs competitively. Visualization in <ref type="figure" target="#fig_3">Fig. 3(b)</ref> also demonstrates the superiority of our performance/efficiency trade-off. An even stronger performance is achieved with a similar amount of computation by TAdaConvNeXt, which provides an accuracy of 67.1% with 94GFLOPs.</p><p>Epic-Kitchens-100. <ref type="table" target="#tab_11">Table 9</ref> lists our results on EK100 in comparison with the previous approaches 3 . We calculate the final action prediction following the strategies in <ref type="bibr" target="#b20">Huang et al. (2021)</ref>. For fair comparison, we reimplemented our baseline TSN using the same training and evaluation strategies. TAda2D improves this baseline by 11.46% on the action prediction. Over previous approaches, TAda2D achieves a higher accuracy with a notable margin.</p><p>Kinetics-400. Comparison with the state-of-the-art models on Kinetics-400 is presented in <ref type="table" target="#tab_1">Table 10</ref>, where we show TAda2D performs competitively in comparison with the models using the same  backbone and the same number of frames. Compared with models with more frames, e.g., TDN, TAda2D achieves a similar performance with less frames and computation. When compared to the more recent Transformer-based models, our TAdaConvNeXt-T provides competitive accuracy with similar or less computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS ON TEMPORAL ACTION LOCALIZATION</head><p>Dataset, pipeline, and evaluation. Action localization is an essential task for understanding untrimmed videos, whose current pipeline makes it heavily dependent on the quality of the video representations. We evaluate our TAda2D on two large-scale action localization datasets, HACS <ref type="bibr" target="#b59">(Zhao et al., 2019)</ref> and Epic-Kitchens-100 <ref type="bibr" target="#b8">(Damen et al., 2020)</ref>. The general pipeline follows <ref type="bibr" target="#b8">(Damen et al., 2020;</ref><ref type="bibr" target="#b39">Qing et al., 2021a;</ref><ref type="bibr">c)</ref>. For evaluation, we follow the standard protocol for the respective dataset. We include the details on the training pipeline and the evaluation protocal in the Appendix C.</p><p>Main results. <ref type="table" target="#tab_1">Table 11</ref> shows that, compared to the baseline, TAda2D provides a stronger feature for temporal action localization, with an average improvement of over 4% on the average mAP on both datasets. In Appendix H, we further demonstrate the TAdaConv can also improve action localization when used as a plug-in module for existing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>This work proposes Temproally-Adaptive Convolutions (TAdaConv) for video understanding, which dynamically calibrates the convolution weights for each frame based on its local and global temporal context in a video. TAdaConv shows superior temporal modelling abilities on both action classification and localization tasks, both as stand-alone and plug-in modules for existing models. We hope this work can facilitate further research in video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>In the appendix, we provide detailed analysis on the temporal convolutions (Appendix A), computational analysis (Appendix B), further implementation details (Appendix C) on the action classification and localization, model structures that we used for evaluation (Appendix D), per-category improvement analysis on Something-Something-V2 (Appendix E), further plug-in evaluations on Epic-Kitchens classification (Appendix G) plug-in evaluations on the temporal action localization task (Appendix H), the visualization of the training procedure of TSN and TAda2D (Appendix I), as well as detailed comparisons between TAdaConv and existing dynamic filters (Appendix J). Further, we show additional qualitative analysis in <ref type="figure" target="#fig_3">Fig. A3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DETAILED ANALYSIS ON TEMPORAL CONVOLUTIONS</head><p>Here, we provide detailed analysis to showcase the underlying process of temporal modelling by temporal convolutions. As in Sec. 3.1, we use depth-wise temporal convolutions for simplicity and its wide application. We first analyze the case where temporal convolutions are directly placed after spatial convolutions without non-linear activation in between, before activation functions is inserted in the second part of our analysis.</p><p>Without activation. We first consider a simple case with no non-linear activation functions between the temporal convolution and the spatial convolution. Given a 3?1?1 depth-wise temporal convolution parameterized by ? = [? 1 , ? 2 , ? 3 ], where ? 1 , ? 2 , ? 3 ? R Co , a spatial convolution parameterized by W ? R Co?Ci?k 2 , the output featurex t of the t-th frame can be obtained by:</p><formula xml:id="formula_12">x t = ? 1 ? (W * x t?1 ) + ? 2 ? (W * x t ) + ? 3 ? (W * x t+1 ) ,<label>(8)</label></formula><p>where ? denotes element-wise multiplication with broadcasting, and * denotes convolution over the spatial dimension. In this case, ? could be grouped with the spatial convolution weight W and the combination of temporal and spatial convolution can be rewritten as Eq. 2 in the manuscript:</p><formula xml:id="formula_13">x t = W t?1 * x t?1 + W t * x t + W t+1 * x t+1 ,<label>(2)</label></formula><p>where W t?1 = ? 1 ? W, W t = ? 2 ? W and W t+1 = ? 3 ? W. This equation share the same form with the Eq. 2 in the manucript. In this case, the combination of temporal convolution with spatial convolution can be certainly viewed as the temporal convolution is simply performing calibration on spatial convolutions before aggregation, with different weights assigned to different time steps for the calibration.</p><p>With activation. Next, we consider a case where activation is in between the temporal convolution and spatial convolution. The output featurex t are now obtained by Eq. 1 in the manuscript:</p><formula xml:id="formula_14">x t = ? 1 ? ?(W * x t?1 ) + ? 2 ? ?(W * x t ) + ? 3 ? ?(W * x t+1 ) .<label>(1)</label></formula><p>Next, we show that this can be still rewritten in the form of Eq. 2. Here, we consider the case where ReLU <ref type="bibr" target="#b38">(Nair &amp; Hinton, 2010)</ref> is used as the activation function, denoted as ?:</p><formula xml:id="formula_15">?(x) = x x &gt; 0 0 x ? 0 .<label>(9)</label></formula><p>Hence, the term ?(W * x t ) can be easily expressed as:</p><formula xml:id="formula_16">?(W * x t ) = M t ? W * x t ,<label>(10)</label></formula><p>where M t ? R C?H?W is a binary map sharing the same shape as x t , indicating whether the corresponding element in W * x t is greater than 0 or not. That is:</p><formula xml:id="formula_17">M (c,i,j) t = 1 if (W * x t ) (c,i,j) &gt; 0 0 if (W * x t ) (c,i,j) ? 0 ,<label>(11)</label></formula><p>where c, i, j are the location index in the tensor. Hence, Eq. 1 can be expressed as:</p><formula xml:id="formula_18">x t = ? 1 ? M t?1 ? W * x t?1 + ? 2 ? M t ? W * x t + ? 3 ? M t+1 ? W * x t+1 .<label>(1)</label></formula><p>In this case, we can set W (i,j)</p><formula xml:id="formula_19">t?1 = ? 1 ? M (i,j) t?1 ? W, W (i,j) t = ? 2 ? M (i,j) t ? W, and W (i,j) t+1 = ? 3 ? M (i,j)</formula><p>t+1 ? W, where (i, j) indicate the spatial location index. In this case, each filter for a specific time step t is composed of H ? W filters and Eq. 1 can be rewritten as Eq. 2. Interestingly, it can be observed that with ReLU activation function, the convolution weights are different for all spatiotemporal locations, since the binary map M depends on the results of the spatial convolutions. <ref type="table" target="#tab_1">Table A1</ref>: Comparison of different operations for spatial and temporal modeling <ref type="bibr" target="#b31">(Lin et al., 2019a;</ref><ref type="bibr" target="#b48">Tran et al., 2018;</ref><ref type="bibr" target="#b50">Wang et al., 2020;</ref><ref type="bibr" target="#b15">Hara et al., 2018)</ref>. 'T.' refers to the temporal modeling ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T. Operation</head><p>Parameters</p><formula xml:id="formula_20">FLOPs Spat. conv Co ? Ci ? k 2 Co ? Ci ? k 2 ? T HW Temp. conv Co ? Ci ? k Co ? Ci ? k ? T HW Temp. shift Co ? Ci ? k 2 Co ? Ci ? k 2 ? T HW (2+1)D conv Co ? Ci ? (k 2 + k) Co ? Ci ? (k 2 + k) ? T HW 3D conv Co ? Ci ? k 3 Co ? Ci ? k 3 ? T HW Correlation Ci ? T ? k 2 Ci ? k 2 ? T HW TAdaConv Co ? Ci ? k 2 + 2 ? Ci ? Ci/r ? k Co ? Ci ? k 2 ? T HW + Ci ? (T HW + T ) +Ci ? Ci/r ? (2 ? k ? T + 1) + Co ? Ci ? k 2 ? T B COMPUTATIONAL ANALYSIS</formula><p>Consider the input tensor with the shape of C i ? T ? H ? W , where C i denotes the number of input channels, TAdaConv essentially performs 2D convolution, with weights dynamically generated. Hence, a good proportion of the computation is carried out by the 2D convolutions:</p><formula xml:id="formula_21">FLOPs(Conv2D) = C o ? C i ? k 2 ? T ? H ? W Params(Conv2D) = C o ? C i ? k 2 ,</formula><p>where C o denote the number of output channels, and k denotes the kernel size of the 2D convolutions. For the weight generation, the features are first aggregated by average pooling over the spatial dimension and the temporal dimension, which contains no parameters:</p><formula xml:id="formula_22">FLOPs(GAP spatial ) = C i ? T ? H ? W FLOPs(GAP temporal ) = C i ? T .</formula><p>For the local information, a two layer 1D convolution with kernel size of k w are applied, with reduction ratio of r in between. For global information, a one layer 1D convolution with kernel size of 1 is applied.</p><formula xml:id="formula_23">FLOPs(Gen) = 2 ? C i ? C i /r ? k ? T + C i ? C i /r Params(Gen) = 2 ? C i ? C i /r ? k + C i ? C i /r ,</formula><p>Further, the calibration weight ? ? R Ci?T is multiplied to the kernel weight of 2D convolutions W ? R Co?Ci?k 2 : FLOPs(Calibration) = C o ? C i ? k 2 ? T , Hence, the overall computation and parameters are: FLOPs(TAdaConv) =FLOPs(Conv2D) + FLOPs(GAP) + FLOPs(Gen) + FLOPs(Calibration)</p><formula xml:id="formula_24">=C o ? C i ? k 2 ? T HW + C i ? T HW + C i ? T + 2 ? C i ? C i /r ? k ? T + C i ? C i /r + C o ? C i ? k 2 ? T Params(TAdaConv) =Params(Conv2D) + Params(Gen) =C o ? C i ? k 2 + 2 ? C i ? C i /r ? k + C i ? C i /r .</formula><p>For the FLOPs, despite the overwhelming number of terms, all the other terms are at least an order of magnitude smaller than the first term. This is in contrast to the (2+1)D convolutions for spatiotemporal modelling, where the FLOPs are C o ? C i ? (k 2 + k) ? T HW . The extra computation introduced by the temporal convolutions is only k times smaller than the 2D convolutions. <ref type="table" target="#tab_1">Table A1</ref> shows the comparison of computation and parameters with other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C FURTHER IMPLEMENTATION DETAILS</head><p>Here, we further describe the implementation details for the action classification and action localization experiments. For fair comparison, we keep all the training strategies the same for our baseline, the plug-in evaluations as well as our own models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 ACTION CLASSIFICATION</head><p>Our experiments on the action classification are conducted on three large-scale datasets. For all action classification models, we train them with synchronized SGD using 16 GPUs. The batch size for each GPU is 16 and 8 respectively for 8-frame and 16-frame models. The weights in TAda2D are initialized using ImageNet <ref type="bibr" target="#b9">(Deng et al., 2009)</ref> pre-trained weights <ref type="bibr" target="#b17">(He et al., 2016)</ref>, except for the calibration function G and the batchnorm statistics (BN 2 ) in the average pooling branch. In the calibration function, we randomly initialize the first convolution layer (for non-linear weight generation) following <ref type="bibr" target="#b16">He et al. (2015)</ref>, and fill zero to the weight of last convolution layer. The batchnorm statistics are initialized to be zero so that the initial state behaves the same as without the average pooling branch. For all models, we use a dropout ratio (Hinton et al., 2012) of 0.5 before the classification heads. Spatially, we randomly resize the short side of the video to <ref type="bibr">[256,</ref><ref type="bibr">320]</ref> and crop a region of 224?224 to the network in ablation studies, and set the scale to <ref type="bibr">[224,</ref><ref type="bibr">340]</ref> following TANet <ref type="bibr" target="#b34">(Liu et al., 2021b)</ref> for comparison against the state-of-the-arts. Temporally, we perform interval based sampling for Kinetics-400 and Epic-Kitchens-100, with interval of 8 for 8 frames, interval of 5 for 16 frames and interval of 2 for 32 frames. On Something-Something-V2, we perform segment based sampling.</p><p>On Kinetics-400, a half-period cosine schedule is applied for decaying the learning rate following <ref type="bibr" target="#b13">Feichtenhofer et al. (2019)</ref>, with the base learning rate set to 0.24 for ResNet-base models using SGD. For TAdaConvNeXt, the base learning rate is set to 0.0001 for the backbone and 0.001 for the head, using adamw <ref type="bibr" target="#b36">(Loshchilov &amp; Hutter, 2017)</ref> as the optimizer. The models are trained for 100 epochs. In the first 8 epochs, we adopt a linear warm-up strategy starting from a learning rate of 0.01. The weight decay is set to 1e-4. The frames are sampled based on a fixed interval, which is 8 for 8-frame models, 5 for 16-frame models and 2 for 32-frame models. Additionally for TAdaConvNeXt-T, a drop-path rate of 0.4 is employed.</p><p>On Epic-Kitchens-100, the models are initialized with weights pre-trained on Kinetics-400, and are further fine-tuned following a similar strategy as in kinetics. The training length is reduced to 50 epochs, with 10 epochs for warm-up. The base learning rate is 0.48. Following <ref type="bibr" target="#b8">Damen et al. (2020)</ref> and <ref type="bibr" target="#b20">Huang et al. (2021)</ref>, we connect two separate heads for predicting verbs and nouns. Action predictions are obtained according to the strategies in <ref type="bibr" target="#b20">Huang et al. (2021)</ref>, which is shown to have a higher accuracy over the original one in <ref type="bibr" target="#b8">Damen et al. (2020)</ref>. For fair comparison, we also trained and evaluated our baseline using the same strategy.</p><p>On Something-Something-V2, we initialize the model with ImageNet pretrained weights for ResNetbased models and Kinetics-400 pre-trained weights for TAdaConvNeXt. A segment-based sampling strategy is adopted, where for T -frame models, the video is divided into T segments before one frame is sampled from each segment randomly for training or uniformly for evaluation. The models are trained for 64 epochs, with the first 4 being the warm-up epochs. The base learning rate is set to 0.48 in training TAda2D with SGD, and 0.001/0.0001 respectively for the head and the backbone in training TAdaConvNeXt with adamw. Following <ref type="bibr" target="#b33">Liu et al. (2021a)</ref>, we use stronger augmentations such as mixup , cutmix <ref type="bibr" target="#b57">(Yun et al., 2019)</ref>, random erasing <ref type="bibr" target="#b60">(Zhong et al., 2020)</ref> and randaugment <ref type="bibr" target="#b6">(Cubuk et al., 2020)</ref> with the same parameters in <ref type="bibr" target="#b35">Liu et al. (2022)</ref>.</p><p>It is worth noting that, for SlowFast models <ref type="bibr" target="#b13">(Feichtenhofer et al., 2019)</ref> in the plug-in evaluations, we do not apply precise batch normalization statistics in our implementation as in its open-sourced codes, which is possibly the reason why our re-implemented performance is slightly lower than the original published numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 ACTION LOCALIZATION</head><p>We evaluate our model on the action localization task using two large-scale datasets. The overall pipeline for our action localization evaluation is divided into finetuning the classification models, obtaining action proposals and classifying the proposals.</p><p>Finetuning. On Epic-Kitchens, we simply use the evaluated action classification model. On HACS, following <ref type="bibr" target="#b41">(Qing et al., 2021c)</ref>, we initialize the model with Kinetics-400 pre-trained weights and train the model with adamW for 30 epochs (8 warmups) using 32 GPUs. The mini-batch size is 16 videos per GPU. The base learning rate is set to 0.0002, with cosine learning rate decay as in Kinetics. In our case, only the segments with action labels are used for training. Proposal generation. For the action proposals, a boundary matching network (BMN) <ref type="bibr" target="#b32">(Lin et al., 2019b)</ref> is trained over the extracted features on the two datasets. On Epic-Kitchens, we extract features with the videos uniformly decoded at 60 FPS. For each clip, we use 8 frames with an interval of 8 to be consistent with finetuning, which means a feature roughly covers a video clip of one seconds. The interval between each clip for feature extraction is 8 frames (i.e., 0.133 sec) as well.</p><p>The shorter side of the video is resized to 224 and we feed the whole spatial region into the backbone to retain as much information as possible. Following <ref type="bibr" target="#b39">Qing et al. (2021a)</ref>, we generate proposals using BMN based on sliding windows. The predictions on the overlapped region of different sliding windows are simply averaged. On HACS, the videos are decoded at 30 FPS, and extend the interval between clips to be 16 (i.e., 0.533 sec) because the actions in HACS last much longer than in Epic-Kitchens. The shorter side is resized to 128 for efficient processing. For the settings in generating proposals, we mainly follow <ref type="bibr" target="#b41">Qing et al. (2021c)</ref>, except that the temporal resolution is resized to 100 in our case instead of 200.</p><p>Classification. On Epic-Kitchens, we classify the proposals with the fine-tuned model using 6 clips. Spatially, to comply with the feature extraction process, we resize the shorter side to 224 and feed the whole spatial region to the model for classification. On HACS, considering the property of the dataset that only one action category can exist in a video, we obtain the video level classification results by classifying the video level features, following <ref type="bibr" target="#b41">Qing et al. (2021c)</ref>.</p><p>Evaluation. For evaluation, we follow the standard evaluation protocol used in the respective datasets, i.e., the average mean Average Precision (average mAP) at IoU threshold [0.5:0.05:0.95] for HACS <ref type="bibr" target="#b59">(Zhao et al., 2019)</ref> and [0.1:0.1:0.5] for Epic-Kitchens-100 <ref type="bibr" target="#b8">(Damen et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D MODEL STRUCTURES</head><p>The detailed model structures for R2D, R(2+1)D and R3D is specified in <ref type="table" target="#tab_2">Table A2</ref>. We highlight the convolutions that are replaced by TAdaConv by default or optionally. For all of our models, a small modification is made in that we remove the max pooling layer after the first convolution and set the spatial stride of the second stage to be 2, following <ref type="bibr" target="#b50">Wang et al. (2020)</ref>. Temporal resolution is kept unchanged following recent works <ref type="bibr" target="#b13">(Feichtenhofer et al., 2019;</ref><ref type="bibr" target="#b27">Li et al., 2020b;</ref><ref type="bibr" target="#b23">Jiang et al., 2019)</ref>. Our R3D is obtained by simply expanding the R2D baseline in the temporal dimension by a factor <ref type="table" target="#tab_1">Class Index   0  10  20  30  40  50  60  70  80  90  100  110  120  130  140  150  160  170  0   20   40   60   80   Accuracy   TSN</ref> Improvement of TAda2D <ref type="figure" target="#fig_0">Figure A1</ref>: Per-category performance comparison of TAda2D against the baseline TSN. We achieve an average per-category performance improvement of 30.35%. <ref type="table" target="#tab_4">Table A3</ref>: Comparison with the state-of-the-art approaches over action classification on Epic-Kitchens-100 <ref type="bibr" target="#b8">(Damen et al., 2020)</ref>. ? indicates the main evaluation metric for the dataset. For fair comparison, we implement all the baseline models using our own training strategies. of three. We initialize with weights reduced by 3 times, which means the original weight is evenly distributed in adjacent time steps. We construct the R(2+1)D by adding a temporal convolution operation after the spatial convolution. The temporal convolution can also be optionally replaced by TAdaConv, as shown in <ref type="table" target="#tab_4">Table 3 and Table A3</ref>. For its initialization, the temporal convolution weights are randomly initialized, while the others are initialized with the pre-trained weights on ImageNet. For SlowFast models, we keep all the model structures identical to the original work <ref type="bibr" target="#b13">(Feichtenhofer et al., 2019)</ref>.</p><p>For TAdaConvNeXt, we keep most of the model architectures as in ConvNeXt <ref type="bibr" target="#b35">(Liu et al., 2022)</ref>, except that we use a tubelet embedding similar to <ref type="bibr" target="#b0">(Arnab et al., 2021)</ref>, with a size of 3?4?4 and stride of 2?4?4. Center initialization is used as in <ref type="bibr" target="#b0">(Arnab et al., 2021)</ref>. Based on this, we simply replace the depth-wise convolutions with TAdaConv to construct TAdaConvNeXt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E PER-CATEGORY IMPROVEMENT ANALYSIS ON SSV2</head><p>This section provides a per-category improvement analysis on the Something-Something-V2 dataset in <ref type="figure" target="#fig_0">Fig.A1</ref>. As shown in . Most of these categories contain large movements across the whole video, whose improvement benefits from temporal reasoning over the global spatial context. For class 30, most of its actions lasts a long time (as it needs to be determined whether the end of something is let down or not). The improvements over the baseline mostly benefits from the global temporal context that are included in the weight generation process.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F FURTHER ABLATION STUDIES</head><p>Here we provide further ablation studies on the kernel size in the calibration weight generation. As shown in <ref type="table" target="#tab_5">Table A4a</ref> and <ref type="table" target="#tab_5">Table A4b</ref>, kernel size does not affect the classification much, as long as the temporal context is considered. Further, <ref type="table" target="#tab_5">Table A4c</ref> shows the sensitivity analysis on the reduction ratio, which demonstrate the robustness of our approach against different set of hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G FURTHER PLUG-IN EVALUATION FOR TADACONV ON CLASSIFICATION</head><p>In complement to <ref type="table" target="#tab_4">Table 3</ref>, we further show in <ref type="table" target="#tab_4">Table A3</ref> the plug-in evaluation on the action classification task on the Epic-Kitchens-100 dataset. As in the plug-in evaluation on Kinetics and Something-Something-V2, we compare performances with and without TAdaConv over three baseline models, SlowFast <ref type="bibr" target="#b13">(Feichtenhofer et al., 2019)</ref>, R(2+1)D <ref type="bibr" target="#b48">(Tran et al., 2018)</ref> and R3D <ref type="bibr" target="#b15">(Hara et al., 2018)</ref> respectively representing three kinds of temporal modeling techniques. The results are in line with our observation in <ref type="table" target="#tab_4">Table 3</ref>. Over all three kinds of temporal modelling strategies, adding TAdaConv further improves the recognition accuracy of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H PLUG-IN EVALUATION FOR TADACONV ON ACTION LOCALIZATION</head><p>Here, we show the plug-in evaluation on the temporal action localization task. Specifically, we use SlowFast as our baseline, as it is shown to be superior in the localization performance in <ref type="bibr" target="#b40">Qing et al. (2021b)</ref> compared to many early backbones. The result is presented in <ref type="table" target="#tab_6">Table A5</ref>. With TAdaConv, the average mAP on HACS is improved by 1.4%, and the average mAP on Epic-Kitchens-100 action localization is improved by 1.0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I COMPARISON OF TRAINING PROCEDURE</head><p>In this section, we compare the training procedure of TSN and TAda2D on Kinetics-400 and Something-Something-V2. The results are presented in <ref type="figure">Fig. A2</ref>. TAda2D demonstrates a stronger fitting ability and generality even from the early stages of the training, despite that the initial state of TAda2D is identical to that of TSN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kinetics 400</head><p>Something-Something-V2 <ref type="figure">Figure A2</ref>: Training and validation on Kinetics-400 and Something-Something-V2. On both datasets, TAda2D shows a stronger capability of fitting the data and a better generality to the validation set. Further, TAda2D reduces the overfitting problem in Something-Something-V2. In this section, we compare our TAdaConv with previous dynamic filters in two perspectives, respectively the difference in the methodology and in the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.1 COMPARISON IN THE METHODOLOGY LEVEL</head><p>For the former comparison, we include <ref type="table" target="#tab_7">Table A6</ref> to show the differences in different approaches, which is a full version of <ref type="table" target="#tab_1">Table 1</ref>. We compare TAdaConv with several representative approaches in image and in videos, respectively CondConv , DynamicFilter <ref type="bibr" target="#b22">(Jia et al., 2016)</ref>, DDF <ref type="bibr" target="#b63">(Zhou et al., 2021)</ref> and TAM <ref type="bibr" target="#b34">(Liu et al., 2021b</ref>).</p><p>The first difference in the methodology level lies in the source of weights, where previous approaches obtain weights by mixture of experts or generation completely dependent on the input. Mixture of experts denotes W = n ? n W n , where ? n is a scalar obtained by a function f , i.e., W = n f (x) n W n . Completely generated means the weights are only dependent on the input, i.e., W = g((x)), where g generates complete kernel for the convolution. In comparison, the weights in TAdaConv are obtained by calibration, i.e,, W = ?W b , where ? is a vector calibration weight and ? = h((x)) where h(.) generates the calibration vector for the convolutions. Hence, this fundamental difference in how to obtain the convolution weights makes the previous approaches difficult to exploit pre-trained weights, while TAdaConv can easily load pre-trained weights in W b . This ability is essential for video models to speed up the convergence.</p><p>The second difference lies in the ability to perform temporal modelling. The ability to perform temporal modelling does not only mean the ability to generate weights according to the whole sequence in dynamic filters for videos, but it also requires the model to generate different weights for the same set of frames with different orders. For example, weights generated by the global descriptor obtained by global average pooling over the whole video GAP st does not have the temporal modelling ability, since they can not generate different weights if the order of the frames in the input sequence are reversed or randomized. Hence, most image based approaches based on global de- <ref type="table" target="#tab_8">Table A7</ref>: Performance comparison with other dynamic filters. Our Init. denotes initializing the calibration weights to ones so that the initial calibrated weights is identical to the pre-trained weights. Temp. Varying is short for temporally varying, which indicates different weights for different temporal locations (frames). * denotes that the branch was originally not designed for generating filter or calibration weights, but we slightly modified the structure so that it can be used for calibration weight generation. (Numbers in brackets) show the performance improvement brought by our initialization scheme for calibration weights. scriptor vectors (such as CondConv and DynamicFilter) or based on adjacent spatial contents (DDF) can not achieve temporal modelling. TAM generates convolution weights for temporal convolutions based on temporally local descriptors obtained by the global average pooling over the spatial dimension GAP s , which yields different weights if the sequence changes. Hence, in this sense, TAM has the temporal modelling abilities. In contrast, TAdaConv exploits both temporally local and global descriptors to utilize not only local but also global temporal contexts. Details on the source of the weight generation process is also shown in <ref type="table" target="#tab_8">Table A7</ref>.</p><p>The third difference lies in whether the weights generatd are shared for different locations. For CondConv, DynamicFilter and TAM, their generated weights are shared for all locations, while for DDF, the weights are varied according to spatial locations. In comparison, TAdaConv generate temporally adaptive weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.2 COMPARISON IN THE PERFORMANCE LEVEL</head><p>Since TAdaConv is fundamentally different from previous approaches in the generation of calibration weights, it is difficult to directly compare the performance on video modelling, especially for those that are not designed for video modelling. However, since the calibration weight in TAdaConv ? is completely generated, i.e., ? = f ((x)), we can use other dynamic filters to generate the calibration weights for TAdaConv. Since MoE based approaches such as CondConv were essentially designed for applications with less memory constraint but high computation requirements, it is not suitable for video applications since it would be too memory-heavy for video models. Hence, we apply approaches that generate complete kernel weights to generate calibration weights, and compare them with TAdaConv. The performance is listed in <ref type="table" target="#tab_8">Table A7</ref>.</p><p>It is worth noting that these approaches originally generate weights that are randomly initialized. However, as is shown in <ref type="table" target="#tab_7">Table 6</ref>, our initialization strategy for the calibration weights are essential for yielding reasonable results, we further apply our initialization on these existing approaches to see whether their generation function is better than the one in TAdaConv. In the following paragraphs, we provide details for applying representative previous dynamic filters in TAdaConv to generate the calibration weight.</p><p>For DynamicFilter <ref type="bibr" target="#b22">(Jia et al., 2016)</ref>, the calibration weight ? is generated using an MLP over the global descriptor that is obtained by performing global average pooling over the whole input GAP st , i.e., ? = MLP(GAP st (x)). In this case, the calibration weights are shared between different time steps.</p><p>For DDF <ref type="bibr" target="#b63">(Zhou et al., 2021)</ref>, we only use the channel branch since it is shown in <ref type="table" target="#tab_8">Table 7</ref> that it is better to leave the spatial structure unchanged for the base kernel. Similarly, the weights in DDF are also generated by applying an MLP over the global descriptor, i.e., ? = MLP(GAP st (x)). The difference between DDF and DynamicFilter is that for different time step, DDF generates a different calibration weight.</p><p>The original structure of TAM <ref type="bibr" target="#b34">(Liu et al., 2021b)</ref> only generates kernel weights with its global branch, and uses local branch to generate attention maps over different time steps. In our experiments, we modify the TAM a little bit and further make the local branch to generate kernel calibration weights as well. Hence, for only-global version of TAM, the calibration weights are calculated as follows: ? = G(GAP s (x)), where GAP s denotes global average pooling over the spatial dimension and G denotes the global branch in TAM. In this case, calibration weights are shared for all temporal locations. For local+global version of TAM, the calibration weight are calculated by combining the results of the local L and the global branch G, i.e., ? = G(GAP s (x)) ? L(GAP s (x)), where ? denotes element-wise multiplication with broadcasting. This means in this case, the calibration weights are temporally adaptive. Note that this is our modified version of TAM. The original TAM does not have a temporally adaptive convolution weights.</p><p>The results in <ref type="table" target="#tab_8">Table A7</ref> show that (a) without our initialization strategy, previous approaches that generate random weights at initialization are not suitable for generating the calibration weights in TAdaConv; (b) our initialization strategy can conveniently change this and make previous approaches yield reasonable performance when they are used for generating calibration weights; and (c) the calibration weight generation function in TAdaConv, which combines the local and global context, outperform all previous approaches for calibration.</p><p>Further, when we compare TAdaConv without global information with TAM (local*+global branch), it can be seen that although both approach generates temporally varying weights from the frame descriptors GAP s (x) with shape C ? T , our TAdaConv achieves a notably higer performance. Adding the global information enables TAdaConv to achieve a more notable lead in the comparison with previous dynamic filters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparisons between TAdaConv and the spatial convolutions in video models. (a) Standard spatial convolutions in videos share the kernel weights between different frames. (b) Our</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Standard convolutions used in video models. (b) Our TAdaConv using non-linear weight calibrations with global temporal context. (c) The temporal feature aggregation scheme used in TAda2D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The classification performance of TAda2D (a) with different channels (C.) and stages (S.) enabled; (b) in comparison with other state-of-the-arts. R(2+1)D Spreading sth. onto sth. GT Burying sth. in sth. TSN Pilling sth. up TAda2D Burying sth. in sth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Grad-CAM visualization and prediction comparison between TSN, R(2+1)D and TAda2D (more examples in Fig. A3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>x)(C ? 1) 59.2 and GAPs(x)(C ? T )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with other dynamic filters.</figDesc><table><row><cell></cell><cell>Temporal Location Pretrained</cell></row><row><cell>Operations</cell><cell>modelling adaptive weights</cell></row><row><cell>CondConv</cell><cell></cell></row><row><cell>DynamicFilter</cell><cell></cell></row><row><cell>DDF</cell><cell></cell></row><row><cell>TAM</cell><cell></cell></row><row><cell>TAdaConv</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of (2+1)D convolution and TAdaConv in FLOPs and number of parameters. Example setting for operation: C o = C i = 64, k = 3, T = 8, H = W = 56 and r = 4. Example setting for network: ResNet-50 with input resolution 8 ? 224 2 . Colored numbers denote the extra FLOPs/Params. added to 2D convolutions or ResNet-50. Refer to Appendix D for model structures.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Plug-in evaluation of TAdaConv in existing video models on K400 and SSV2 datasets.+1.29 63.30 +2.99    </figDesc><table><row><cell>Base Model</cell><cell cols="5">TAdaConv Frames Params. GFLOPs K400</cell><cell>?</cell><cell>SSV2</cell><cell>?</cell></row><row><cell cols="8">8 8 4+32 75.85 SlowFast 4?16 32.5M 54.52 74.56 SlowOnly 8?8 35.6M 54.53 34.5M 36.10 75.03 4+32 37.7M 36.11 76.47 +1.44 59.80 +3.09 -60.31 --56.71 -</cell></row><row><cell>SlowFast 8?8</cell><cell></cell><cell>8+32 8+32</cell><cell>34.5M 37.7M</cell><cell>65.71 65.73</cell><cell cols="3">76.19 77.43 +1.24 63.88 +2.34 -61.54 -</cell></row><row><cell></cell><cell></cell><cell>8</cell><cell>28.1M</cell><cell>49.55</cell><cell>73.63</cell><cell>-</cell><cell>61.06</cell><cell>-</cell></row><row><cell>R(2+1)D</cell><cell>(2d)</cell><cell>8</cell><cell>31.2M</cell><cell>49.57</cell><cell cols="3">75.19 +1.56 62.86 +1.80</cell></row><row><cell></cell><cell>(2d+1d)</cell><cell>8</cell><cell>34.4M</cell><cell>49.58</cell><cell cols="3">75.36 +1.73 63.78 +2.72</cell></row><row><cell>R3D</cell><cell>(3d)</cell><cell>8 8</cell><cell>47.0M 50.1M</cell><cell>84.23 84.24</cell><cell cols="3">73.83 74.91 +1.08 62.85 +2.99 -59.86 -</cell></row></table><note>Notation indicates our own implementation. See Appendix D for details on the model structure.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">Model TAdaConv K. G. Top-1</cell></row><row><cell>TSN</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32.0</cell></row><row><cell></cell><cell>Lin.</cell><cell>1</cell><cell></cell><cell>37.5</cell></row><row><cell></cell><cell>Lin.</cell><cell>3</cell><cell></cell><cell>56.5</cell></row><row><cell></cell><cell cols="4">Non-Lin. (1, 1) 36.8</cell></row><row><cell></cell><cell cols="4">Non-Lin. (3, 1) 57.1</cell></row><row><cell>Ours</cell><cell cols="4">Non-Lin. (1, 3) 57.3</cell></row><row><cell></cell><cell cols="4">Non-Lin. (3, 3) 57.8</cell></row><row><cell></cell><cell>Lin.</cell><cell>1</cell><cell></cell><cell>53.4</cell></row><row><cell></cell><cell cols="4">Non-Lin. (1, 1) 54.4</cell></row><row><cell></cell><cell cols="4">Non-Lin. (3, 3) 59.2</cell></row></table><note>Calibration weight generation. K: kernel size; Lin./Non-Lin.: linear/non-linear weight generation; G: global information g.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Feature aggregation scheme. FA: feature aggregation; Sc: shortcut for convolution feature; SepBN: separate batch norm.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Benefit of dynamic calibration. T.V.: temporally varying. *: w/o our init.</figDesc><table><row><cell cols="3">Calibration T.V. Top-1 Top-1*</cell></row><row><cell>None</cell><cell>-</cell><cell>32.0</cell></row><row><cell>Learnable</cell><cell>34.3 45.4</cell><cell>32.6 43.8</cell></row><row><cell>Dynamic</cell><cell>51.2 53.8</cell><cell>41.7 49.8</cell></row><row><cell>TAda</cell><cell>59.2</cell><cell>47.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Calibration dimension.</figDesc><table><row><cell cols="2">Cal. dim. ?Parms. ? GFLOPs Top-1</cell></row><row><cell>Cin</cell><cell>3.16M 0.016 63.8</cell></row><row><cell>Cout</cell><cell>3.16M 0.016 63.4</cell></row><row><cell cols="2">Cin ? Cout 4.10M 0.024 63.7</cell></row><row><cell>k 2</cell><cell>2.24M 0.009 62.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Comparison with the top approaches on Something-Something-V2</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table><row><cell>Top-1</cell><cell>Top-5</cell></row></table><note>Comparison with the state-of-the-art approaches over action classification on Epic- Kitchens-100 (Damen et al., 2020). indicates our own implementation for fair comparison. ? indicates the main evaluation metric for the dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Comparison with the state-of-the-art approaches on Kinetics 400<ref type="bibr" target="#b24">(Kay et al., 2017)</ref>.</figDesc><table><row><cell>Model</cell><cell>Pretrain</cell><cell>Frames</cell><cell cols="3">GFLOPs Top-1 Top-5</cell></row><row><cell>TSM (Lin et al., 2019a)</cell><cell>IN-1K</cell><cell>8?3?10</cell><cell>43</cell><cell>74.1</cell><cell>N/A</cell></row><row><cell>SmallBigNet (Li et al., 2020a)</cell><cell>IN-1K</cell><cell>8?3?10</cell><cell>57</cell><cell>76.3</cell><cell>92.5</cell></row><row><cell>TANet (Liu et al., 2021b)</cell><cell>IN-1K</cell><cell>8?3?10</cell><cell>43</cell><cell>76.3</cell><cell>92.6</cell></row><row><cell>TANet (Liu et al., 2021b)</cell><cell>IN-1K</cell><cell>16?3?10</cell><cell>86</cell><cell>76.9</cell><cell>92.9</cell></row><row><cell>SlowFast 4?16 (Feichtenhofer et al., 2019)</cell><cell>-</cell><cell>(4+32)?3?10</cell><cell>36.1</cell><cell>75.6</cell><cell>92.1</cell></row><row><cell>SlowFast 8?8 (Feichtenhofer et al., 2019)</cell><cell>-</cell><cell>(8+32)?3?10</cell><cell>65.7</cell><cell>77.0</cell><cell>92.6</cell></row><row><cell>TDN (Wang et al., 2021)</cell><cell>IN-1K</cell><cell>(8+32)?3?10</cell><cell>47</cell><cell>76.6</cell><cell>92.8</cell></row><row><cell>TDN (Wang et al., 2021)</cell><cell>IN-1K</cell><cell>(16+64)?3?10</cell><cell>94</cell><cell>77.5</cell><cell>93.2</cell></row><row><cell>CorrNet (Wang et al., 2020)</cell><cell>IN-1K</cell><cell>32?1?10</cell><cell>115</cell><cell>77.2</cell><cell>N/A</cell></row><row><cell>TAda2D (Ours)</cell><cell>IN-1K</cell><cell>8?3?10</cell><cell>43</cell><cell>76.7</cell><cell>92.6</cell></row><row><cell>TAda2D (Ours)</cell><cell>IN-1K</cell><cell>16?3?10</cell><cell>86</cell><cell>77.4</cell><cell>93.1</cell></row><row><cell>TAda2DEn (Ours)</cell><cell>IN-1K</cell><cell>(8+16)?3?10</cell><cell>129</cell><cell>78.2</cell><cell>93.5</cell></row><row><cell>MViT-B (Fan et al., 2021)</cell><cell>-</cell><cell>16?1?5</cell><cell>70.5</cell><cell>78.4</cell><cell>93.5</cell></row><row><cell>MViT-B (Fan et al., 2021)</cell><cell>-</cell><cell>32?1?5</cell><cell>170</cell><cell>80.2</cell><cell>94.4</cell></row><row><cell>TimeSformer (Bertasius et al., 2021)</cell><cell>IN-21K</cell><cell>8?3?1</cell><cell>196</cell><cell>78.0</cell><cell>93.7</cell></row><row><cell>ViViT-L (Arnab et al., 2021)</cell><cell>IN-21K</cell><cell>16?3?4</cell><cell>1446</cell><cell>80.6</cell><cell>94.6</cell></row><row><cell>Swin-T (Liu et al., 2021a)</cell><cell>IN-1K</cell><cell>32?3?4</cell><cell>88</cell><cell>78.8</cell><cell>93.6</cell></row><row><cell>TAdaConvNeXt-T (Ours)</cell><cell>IN-1K</cell><cell>16?3?4</cell><cell>47</cell><cell>78.4</cell><cell>93.5</cell></row><row><cell>TAdaConvNeXt-T (Ours)</cell><cell>IN-1K</cell><cell>32?3?4</cell><cell>94</cell><cell>79.1</cell><cell>93.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table /><note>Action localization evaluation on HACS and Epic-Kitchens-100. ? indicates the main evaluation metric for the dataset, i.e., average mAP for action localization.HACS Epic-Kitchens-100 Model @0.5 @0.6 @0.7 @0.8 @0.9 Avg.? Task @0.1 @0.2 @0.3 @0.4 @0.5 Avg.? TSN 43.6 37.7 31.9 24.6 15.0 28.6 Verb 15.98 15.01 14.09 12.25 10.01 13.47 Noun 15.11 14.15 12.78 10.94 8.89 12.37 Act.? 10.24 9.61 8.94 7.96 6.79 8.71 TAda2D 48.7 42.7 36.2 28.1 17.3 32.3 Verb 19.70 18.49 17.41 15.50 12.78 16.78 Noun 20.54 19.32 17.94 15.77 13.39 17.39 Act.? 15.15 14.32 13.59 12.18 10.65 13.18</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table A2 :</head><label>A2</label><figDesc>Model structure of R3D, R(2+1)D and R2D that we used in our experiments. Blue and green fonts indicate respectively the default convolution operation and optional operation that can be replaced by TAdaConv. (Better viewed in color.)</figDesc><table><row><cell>Stage</cell><cell></cell><cell>R3D</cell><cell></cell><cell></cell><cell>R(2+1)D</cell><cell></cell><cell cols="4">R2D (default baseline) output sizes</cell></row><row><cell>Sampling</cell><cell></cell><cell cols="2">interval 8, 1 2</cell><cell></cell><cell cols="2">interval 8, 1 2</cell><cell></cell><cell cols="2">interval 8, 1 2</cell><cell>8?224?224</cell></row><row><cell>conv1</cell><cell></cell><cell cols="2">3?7 2 , 64 stride 1, 2 2</cell><cell></cell><cell cols="2">1?7 2 , 64 stride 1, 2 2</cell><cell></cell><cell cols="2">1?7 2 , 64 stride 1, 2 2</cell><cell>8?112?112</cell></row><row><cell>res2</cell><cell>? ?</cell><cell>1?1 2 , 64 3?3 2 , 64 1?1 2 , 256</cell><cell>? ? ?3</cell><cell>? ? ? ?</cell><cell>1?1 2 , 64 1?3 2 , 64 3?1 2 ,64 1?1 2 , 256</cell><cell>? ? ? ? ?3</cell><cell>? ?</cell><cell>1?1 2 , 64 1?3 2 , 64 1?1 2 , 256</cell><cell>? ? ?3</cell><cell>8?56?56</cell></row><row><cell>res3</cell><cell>? ?</cell><cell>1?1 2 , 128 3?3 2 , 128 1?1 2 , 512</cell><cell>? ? ?4</cell><cell>? ? ? ?</cell><cell>1?1 2 , 128 1?3 2 , 128 3?1 2 ,128 1?1 2 , 512</cell><cell>? ? ? ? ?4</cell><cell>? ?</cell><cell>1?1 2 , 128 1?3 2 , 128 1?1 2 , 512</cell><cell>? ? ?4</cell><cell>8?28?28</cell></row><row><cell>res4</cell><cell>? ?</cell><cell>1?1 2 , 256 3?3 2 , 256 1?1 2 , 1024</cell><cell>? ? ?6</cell><cell>? ? ? ?</cell><cell>1?1 2 , 256 1?3 2 , 256 3?1 2 ,256 1?1 2 , 1024</cell><cell>? ? ? ? ?6</cell><cell>? ?</cell><cell>1?1 2 , 256 1?3 2 , 256 1?1 2 , 1024</cell><cell>? ? ?6</cell><cell>8?14?14</cell></row><row><cell>res5</cell><cell>? ?</cell><cell>1?1 2 , 512 3?3 2 , 512 1?1 2 , 2048</cell><cell>? ? ?3</cell><cell>? ? ? ?</cell><cell>1?1 2 , 512 1?3 2 , 512 3?1 2 ,512 1?1 2 , 2048</cell><cell>? ? ? ? ?3</cell><cell>? ?</cell><cell>1?1 2 , 512 1?3 2 , 512 1?1 2 , 2048</cell><cell>? ? ?3</cell><cell>8?7?7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">global average pool, fc</cell><cell></cell><cell></cell><cell></cell><cell>1?1?1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>, our TAda2D achieves an overall improvement of 31.7%. Our</cell></row><row><cell>per-category analysis shows an mean improvement of 30.35% over all the classes. The largest</cell></row><row><cell>improvement is observed in class 0 (78.5%, Approaching something with your camera), 32 (78.4%,</cell></row><row><cell>Moving away from something with your camera), 30 (74.3%, Lifting up one end of something without</cell></row><row><cell>letting it drop down), 44 (66.2%, Moving something towards the camera) and 41 (66.1%, Moving</cell></row><row><cell>something away from the camera)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table A4 :</head><label>A4</label><figDesc>Ablation studies.</figDesc><table><row><cell cols="2">(a) Ablation studies on ker-</cell><cell>(b) Ablation studies on kernel size with</cell><cell cols="2">(c) Ablation studies on</cell></row><row><cell cols="2">nel size with linear calibration</cell><cell>non-linear calibration weight generation</cell><cell cols="2">reduction ratio r for</cell></row><row><cell cols="2">weight generation function.</cell><cell>function.</cell><cell cols="2">K1 = K2 = 3.</cell></row><row><cell>Kernel size</cell><cell>Top-1</cell><cell>K2=1 K2=3 K2=5 K2=7</cell><cell cols="2">Ratio r Top-1</cell></row><row><cell>1</cell><cell>37.5</cell><cell>K1=1 36.8 57.1 57.8 57.9</cell><cell>1</cell><cell>57.79</cell></row><row><cell>3</cell><cell>56.5</cell><cell>K1=3 57.3 57.8 57.9 58.0</cell><cell>2</cell><cell>57.83</cell></row><row><cell>5</cell><cell>57.3</cell><cell>K1=5 57.6 57.9 58.2 57.9</cell><cell>4</cell><cell>57.78</cell></row><row><cell>7</cell><cell>56.5</cell><cell>K1=7 57.4 57.6 58.0 57.6</cell><cell>8</cell><cell>57.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table A5 :</head><label>A5</label><figDesc>Plug-in evaluation of TAdaConv on the action localization on HACS and Epic-Kitchens. ? indicates the main evaluation metric for the dataset. 'S.F.' is SlowFast network.</figDesc><table><row><cell></cell><cell>HACS</cell><cell>Epic-Kitchen-100</cell></row><row><cell>Model</cell><cell cols="2">@0.5 @0.6 @0.7 @0.8 @0.9 Avg.? Task @0.1 @0.2 @0.3 @0.4 @0.5 Avg.?</cell></row><row><cell></cell><cell></cell><cell>Verb 19.93 18.92 17.90 16.08 13.24 17.21</cell></row><row><cell>S.F. 8?8</cell><cell>50.0 44.1 37.7 29.6 18.4 33.7</cell><cell>Noun 17.93 16.83 15.53 13.68 11.41 15.07</cell></row><row><cell></cell><cell></cell><cell>Act.? 14.00 13.19 12.37 11.18 9.52 12.04</cell></row><row><cell></cell><cell></cell><cell>Verb 19.96 18.71 17.65 15.41 13.35 17.01</cell></row><row><cell cols="2">S.F. 8?8 + TAdaConv 51.7 45.7 39.3 31.0 19.5 35.1</cell><cell>Noun 20.17 18.90 17.58 15.83 13.18 17.13</cell></row><row><cell></cell><cell></cell><cell>Act.? 14.90 14.12 13.32 12.07 10.57 13.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table A6 :</head><label>A6</label><figDesc>Approach comparison between different dynamic filters. The weights column denotes how weights in respective approaches are obtained. The pre-trained weights colmun shows whether the weight generation can exploit pre-trained models such as ResNet<ref type="bibr" target="#b17">(He et al., 2016)</ref>.</figDesc><table><row><cell></cell><cell></cell><cell>Temporal Location Pretrained</cell></row><row><cell>Operations</cell><cell>Weights</cell><cell>Modelling Adaptive weights</cell></row><row><cell cols="2">CondConv DynamicFilter Completely generated W = g(x) Mixture of experts W = n f (x)nWn</cell><cell></cell></row><row><cell>DDF</cell><cell>Completely generated W = g(x)</cell><cell></cell></row><row><cell>TAM</cell><cell>Completely generated W = g(x)</cell><cell></cell></row><row><cell>TAdaConv</cell><cell>Calibrated from a base weight W = h(x)W b</cell><cell></cell></row><row><cell cols="3">J COMPARISON WITH EXISTING DYNAMIC FILTERS</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The performances are referenced from the official release of the EK100 dataset<ref type="bibr" target="#b8">(Damen et al., 2020)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This research is supported by the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme (Project #A18A2b0046), by the RIE2020 Industry Alignment Fund -Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s), and by Alibaba Group through Alibaba Research Intern Program.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pretending to throw sth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TSN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Squeezing</head><p>sth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R(2+1)D</head><p>Turning sth. upside down</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TAda2D</head><p>Pretending to throw sth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head><p>Putting sth. underneath sth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TSN</head><p>Turning the camera downwards when filming sth</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R(2+1)D</head><p>Putting sth. into sth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TAda2D</head><p>Putting sth. underneath sth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head><p>Pulling sth. onto sth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TSN</head><p>Moving sth. and sth.</p><p>so that they pass each other</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R(2+1)D</head><p>Putting sth. onto sth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TAda2D</head><p>Pulling sth. onto sth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head><p>Putting sth. behind sth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TSN</head><p>Pretending to put sth. behind sth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R(2+1)D</head><p>Putting sth. behind sth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TAda2D</head><p>Putting sth. behind sth. <ref type="figure">Figure A3</ref>: Further qualitative evaluations on the Something-Something-V2 dataset. In most cases, TAda2D captures meaningful areas in the videos for the correct classification. Further, the activated region of TAda2D also lasts longer along the temporal dimension compared to other two models, thanks to the global temproal context in the weight generation function G.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6836" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic region-aware convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8064" to="8073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11030" to="11039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic relu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="351" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13256</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Rescaling egocentric vision. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Revisiting spatial invariance with low-rank local connectivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gamaleldin</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2868" to="2879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6824" to="6835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Towards training stronger video vision transformers for epic-kitchens-100 action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhurong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo H Ang</forename><genName>Jr</genName></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05058</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Smallbignet: Integrating core and contextual views for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1092" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning dynamic routing for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8553" to="8562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Micronet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12289</idno>
		<title level="m">Towards image recognition with extremely low flops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Revisiting dynamic convolution via matrix decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<title level="m">Video swin transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal adaptive module for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03545</idno>
		<title level="m">Trevor Darrell, and Saining Xie. A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adafuse: Adaptive temporal fusion network for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Ching</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A stronger baseline for ego-centric action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><forename type="middle">H</forename><surname>Ang</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06942</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Changxin Gao, and Nong Sang. Temporal context aggregation network for temporal action proposal refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="485" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Exploring stronger feature for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13014</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Statistics of natural images: Scaling in the woods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">814</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Natural image statistics and neural representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruno A Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of neuroscience</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1193" to="1216" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Video classification with channelseparated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5552" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Video modeling with correlation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Tdn: Temporal difference networks for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1895" to="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Skipnet: Learning dynamic routing in convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="409" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dynamic filtering with large sampling field for convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandrajit</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="185" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Condconv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04971</idno>
		<title level="m">Conditionally parameterized convolutions for efficient inference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Hacs: Human action clips and segments dataset for recognition and temporal localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8668" to="8678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Neural epitome search for architecture-agnostic network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05642</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Decoupled dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiong</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6647" to="6656" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
