<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Actor and Action Video Segmentation from a Sentence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
							<email>kgavrilyuk@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">QUVA Lab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ghodrati</surname></persName>
							<email>a.ghodrati@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">QUVA Lab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
							<email>zhenyangli@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">QUVA Lab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cees</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
							<email>cgmsnoek@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">QUVA Lab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Actor and Action Video Segmentation from a Sentence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper strives for pixel-level segmentation of actors and their actions in video content. Different from existing works, which all learn to segment from a fixed vocabulary of actor and action pairs, we infer the segmentation from a natural language input sentence. This allows to distinguish between fine-grained actors in the same super-category, identify actor and action instances, and segment pairs that are outside of the actor and action vocabulary. We propose a fully-convolutional model for pixel-level actor and action segmentation using an encoder-decoder architecture optimized for video. To show the potential of actor and action video segmentation from a sentence, we extend two popular actor and action datasets with more than 7,500 natural language descriptions. Experiments demonstrate the quality of the sentence-guided segmentations, the generalization ability of our model, and its advantage for traditional actor and action segmentation compared to the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of this paper is pixel-level segmentation of an actor and its action in video, be it a person that climbs, a car that jumps or a bird that flies. Xu et al. <ref type="bibr" target="#b28">[29]</ref> defined this challenging computer vision problem in an effort to lift video understanding beyond the more traditional work on spatio-temporal localization of human actions inside a tube, e.g. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref>. Many have shown since that joint actor and action inference is beneficial over their independent segmentation, e.g. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>. Where all existing works learn to segment from a fixed set of predefined actor and action pairs, we propose to segment actors and their actions in video from a natural language sentence input, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>We are inspired by recent progress in vision and language solutions for challenges like object retrieval <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref>, person search <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref>, and object tracking <ref type="bibr" target="#b14">[15]</ref>. To arrive at object segmentation from a sentence, Hu et al. <ref type="bibr" target="#b5">[6]</ref> rely on an LSTM network to encode an input sentence into a vector representation, before a fully convolutional network extracts a spatial feature map from an image and outputs an upsampled response map for the target object. Li et al. <ref type="bibr" target="#b14">[15]</ref> propose object tracking from a sentence. Without specifying a bounding box, they identify a target object from the sentence and track it throughout a video. The target localization of their network is similar to Hu et al. <ref type="bibr" target="#b5">[6]</ref>, be it that they introduce a dynamic convolutional layer to allow for dynamic adaptation of visual filters based on the input sentence. In effect making the textual embedding convolutional before the matching. Like <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref> we also propose an endto-end trainable solution for segmentation from a sentence that embeds text and images into a joint model. Rather than relying on LSTMs we prefer a fully-convolutional model from the start, including dynamic filters. Moreover, we optimize our model for the task of segmenting an actor and its action in video, rather than in an image, allowing us to exploit both RGB and Flow.</p><p>The first and foremost contribution of this paper is the new task of actor and action segmentation from a sentence. As a second contribution we propose a fully-convolutional model for pixel-level actor and action segmentation using an encoder-decoder neural architecture that is optimized for video and end-to-end trainable. Third, to show the potential of actor and action segmentation from a sentence we extend the A2D <ref type="bibr" target="#b28">[29]</ref> and J-HMDB <ref type="bibr" target="#b8">[9]</ref> datasets with more than 7,500 textual sentences describing the actors and actions appearing in the video content. And finally, our experiments demonstrate the quality of the sentence-guided segmentations, the generalization ability of our model, and its advantage for traditional actor and action segmentation compared to the state-of-the-art. Before detailing our model, we first discuss related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Actor and action segmentation</head><p>Xu et al. <ref type="bibr" target="#b28">[29]</ref> pose the problem of actor and action segmentation in video and introduce the challenging Actor-Action Dataset (A2D) containing a fixed vocabulary of 43 actor and action pairs. They build a multi-layer conditional random field model and assign to each supervoxel from a video a label from an actor-action product space. In <ref type="bibr" target="#b27">[28]</ref>, Xu and Corso propose a grouping process to add longranging interactions to the conditional random field. Yan et al. <ref type="bibr" target="#b30">[31]</ref> show a multi-task ranking model atop supervoxel features allows for weakly-supervised actor and action segmentation using only video-level tags for training. Rather than relying on supervoxels, Kalogeiton et al. <ref type="bibr" target="#b9">[10]</ref> propose a multi-task network architecture to jointly train an actor and action detector for a video. They extend their bounding box detections to pixel-wise segmentations by using stateof-the-art segmentation proposals <ref type="bibr" target="#b21">[22]</ref> afterwards.</p><p>The above works are limited to model interactions between actors and actions from a fixed predefined set of label pairs. Our work models the joint actor and action space using an open set of labels as rich as language. This has the advantage that we are able to distinguish between fine-grained actors in the same super-category, e.g. a parrot or a duck rolling, and identify different actor and action instances. Thanks to a pre-trained word embedding, our model is also able to infer the segmentation from words that are outside of the actor and action vocabulary but exist in the embedding. Instead of generating intermediate supervoxels or segmentation proposals for a video, we follow a pixel-level model using an encoder-decoder neural architecture that is completely end-to-end trainable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Actor localization from a sentence</head><p>Recently, works appeared that localize a human actor from an image <ref type="bibr" target="#b13">[14]</ref> or video <ref type="bibr" target="#b29">[30]</ref> based on a sentence. In <ref type="bibr" target="#b13">[14]</ref>, Li et al. introduce a person description dataset with sentence annotations and person samples from five existing person re-identification datasets. Their accompanying neural network model captures word-image relations and esti-mates the affinity between a sentence and a person image. Closer to our work is <ref type="bibr" target="#b29">[30]</ref>, where Yamaguchi et al. propose spatio-temporal person search in video. They supplement thousands of video clips from the ActivityNet dataset <ref type="bibr" target="#b0">[1]</ref> with person descriptions. Their person retrieval model first proposes candidate tubes, ranks them based on a query in a joint visual-textual embedding and then outputs a final ranking.</p><p>Similar to <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref>, we also supplement existing datasets with sentence descriptions, in our case A2D <ref type="bibr" target="#b28">[29]</ref> and J-HMDB <ref type="bibr" target="#b8">[9]</ref>, but for the purpose of actor and action segmentation. Where <ref type="bibr" target="#b29">[30]</ref> demonstrates the value of sentences describing human actors for action localization in video, we generalize to actions performed by any actor. Additionally, where <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref>, simplify their localization to a bounding box around the human actor of interest, we output a pixel-wise segmentation of both actor and action in video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Action localization from a sentence</head><p>Both Gao et al. <ref type="bibr" target="#b3">[4]</ref> and Hendricks et al. <ref type="bibr" target="#b4">[5]</ref> consider retrieving a specific temporal interval containing actions via a sentence. In contrast, our work offers a unique opportunity to study spatio-temporal segmentation from a sentence, with a diverse set of actors and actions.</p><p>Jain et al. <ref type="bibr" target="#b7">[8]</ref> follow a zero-shot protocol and demonstrate spatio-temporal action localization is feasible from just a sentence describing a (previously unknown) action class. They first generate a set of action tubes, encode each of them by thousands of object classifier responses, and compute a word2vec similarity between the high-scoring object categories inside an action proposal and the action query. Mettes and Snoek <ref type="bibr" target="#b17">[18]</ref> also follow a zero-shot regime and match sentences to actions in a word2vec space, but rather than relying on action proposals and object classifiers, they prefer object detectors only, allowing to query for spatio-temporal relations between human actors and objects. Different from their zero-shot setting, we operate in a supervised regime. We also aim for spatio-temporal localization of actions in video, but rather than generating bounding boxes, we prefer a pixel-wise segmentation over actions performed by any actor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>Given a video and a natural language sentence as a query, we aim to segment the actor and its action in each frame of the video as specified by the query. To achieve this, we propose a model which combines both video and language information to perform pixel-wise segmentation according to the input query. We do so by generating convolutional dynamic filters from the textual representation and convolving them with the visual representation of different resolutions to output a segmentation mask. Our model consists of three  <ref type="figure">Figure 2</ref>: Our RGB model for actor and action video segmentation from a natural language sentence consists of three main components: a convolutional neural network to encode the expression, a 3D convolutional neural network to encode the video, and a decoder that performs a pixel-wise segmentation by convolving dynamic filters generated from the encoded textual representation with the encoded video representation. The same model is applied to the Flow input.</p><p>main components: a textual encoder, a video encoder and a decoder, as illustrated in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Textual Encoder</head><p>Given an input natural language sentence as a query that describes the actor and action, we aim to encode it in a way that enables us to perform segmentation of the specified actor and action in video. Different from <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref> who aim to train word embeddings from scratch on the ReferIt Dataset <ref type="bibr" target="#b11">[12]</ref>, we rely on word embeddings obtained from a large collection of text documents. Particularly, we are using a word2vec model pre-trained on the Google News Dataset <ref type="bibr" target="#b19">[20]</ref>. It enables us to handle words beyond the ones of the sentences in the training set. In addition, we are using a simple 1D convolutional neural network instead of an LSTM to encode input sentences, which we will further detail in our ablation study.</p><p>Details. Each word of the input sentence is represented as a 300-dimensional word2vec embedding, without any further preprocessing. All the word embeddings are fixed without fine-tuning during training. The input sentence is then represented as a concatenation of its individual word representations, e.g. a 10-word sentence is represented by a 10 ? 300 matrix. Each sentence is additionally padded to have the same size. The network consists of a single 1D convolutional layer with a temporal filter size equal to 2 and with the same output dimension as the word2vec representation. After the convolutional layer we apply the ReLU activation function and perform max-pooling to obtain a representation for the whole sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Video Encoder</head><p>Given an input video, we aim to obtain a visual representation that encodes both the actor and action information, while preserving the spatial information that is necessary to perform pixel-wise segmentation. Different from <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref> who use a 2D image-based model our model takes advantage of the temporal dynamics of the video as well. Recently, Carreira and Zisserman <ref type="bibr" target="#b1">[2]</ref> proposed to inflate the 2D filters of a convolutional neural network to 3D filters (I3D) to better exploit the spatio-temporal nature of video. By pre-training on both image object dataset ImageNet <ref type="bibr" target="#b22">[23]</ref> and video action dataset Kinetics <ref type="bibr" target="#b10">[11]</ref> their model achieves state-of-the-art results for action classification. We adopt the I3D model to obtain a visual representation from video.</p><p>Moreover, we also follow the well-known two-stream approach <ref type="bibr" target="#b23">[24]</ref> to combine appearance and motion information, which was successfully applied earlier to a wide range of video understanding tasks such as action classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref> and detection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33]</ref>. We study the effect of having RGB and Flow inputs for actor and action segmentation in our ablation study.</p><p>Details. Frames of all videos are padded to have the same size. As visual feature representation for both the RGB and Flow input, we use the output of the inception block before the last max-pooling layer of the I3D network followed by an average pooling over the temporal dimension. To obtain a more robust descriptor at each spatial location, L2-normalization is applied to every spatial position in the feature map. Following <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>, we also append the spatial coordinates of each position as extra channels to the visual representation to allow learning spatial qualifiers like "left of" or "above".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decoding with dynamic filters</head><p>To perform pixel-wise segmentation from a natural language sentence we rely on dynamic convolutional filters, as earlier proposed in <ref type="bibr" target="#b14">[15]</ref>. Unlike static convolutional filters that are used in conventional convolutional neural networks, dynamic filters are generated depending on the input, in our case on the encoded sentence representation. It enables us to transfer textual information to the visual domain. Different from <ref type="bibr" target="#b14">[15]</ref>, we notice better results with a tanh activation function and L2-normalization on the features. In addition, we generate dynamic filters for several resolutions with different network parameters.</p><p>Given a sentence representation T , we generate dynamic filters f r for each resolution r ? R with a separate single layer fully-connected network:</p><formula xml:id="formula_0">f r = tanh(W r f T + b r f ),<label>(1)</label></formula><p>where tanh is the hyperbolic tangent function and f r has the same number of channels as representation V r t for video input at timestep t and resolution r. Then the dynamic filters are convolved with V r t to obtain a pixel-wise segmentation response map for resolution r at timestep t:</p><formula xml:id="formula_1">S r t = f r * V r t ,<label>(2)</label></formula><p>To obtain a segmentation mask with the same resolution as the input video, we further employ a deconvolutional neural network. Different from <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>, who apply deconvolution on the segmentation response maps, we use the deconvolutional layers on the video representation V r t directly. It enables us to better handle small objects and output smoother segmentation predictions. In addition, it helps to obtain more accurate segmentations for high overlap values as we will show in the experiments.</p><p>Details. Each of our deconvolutional networks consists of two blocks with one deconvolutional layer with kernel size 8 ? 8 and stride 4, followed by a convolutional layer with a kernel size of 3 ? 3 and a stride of 1. We use only the highest-resolution response map for the final segmentation prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>Our training sample consists of an input video clip, an input sentence and a binary ground truth segmentation mask Y r for each resolution r ? R of the frame in the middle of each input video clip. For each training sample we define a loss, while taking into account multiple resolutions, which helps for better flow of gradients in the model similar to a skip-connection approach:</p><formula xml:id="formula_2">L = r?R ? r L r (3) L r = 1 r 2 r i=1 r j=1 L r ij (4)</formula><p>where ? r is a weight for resolution r. In this paper we consider R = {32, 128, 512} and we further discuss the importance of using losses of all resolutions in our ablation study. The pixel-wise L r ij loss is a logistic loss defined as follows:</p><formula xml:id="formula_3">L r ij = log(1 + exp (?S r ij Y r ij ))<label>(5)</label></formula><p>where S r ij is a response value of our model at pixel (i, j) for resolution r and Y r ij is a binary label at pixel (i, j) for resolution r.</p><p>Details. We train our model using the Adam optimizer <ref type="bibr" target="#b12">[13]</ref> with a learning rate of 0.001 and other parameters of the optimizer set to the default values. We divide the learning rate by 10 every 5, 000 iterations and train for 15, 000 iterations in total. We finetune only the last inception block of the video encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">A2D Sentences</head><p>The Actor-Action Dataset (A2D) by Xu et al. <ref type="bibr" target="#b28">[29]</ref> serves as the largest video dataset for the general actor and action segmentation task. It contains 3,782 videos from YouTube with pixel-level labeled actors and their actions. The dataset includes eight different actions, while a total of seven actor classes are considered to perform those actions. We follow <ref type="bibr" target="#b28">[29]</ref>, who split the dataset into 3,036 training videos and 746 testing videos.</p><p>As we are interested in pixel-level actor and action segmentation from sentences, we augment the videos in A2D with natural language descriptions about what each actor is doing in the videos. Following the guidelines set forth in <ref type="bibr" target="#b11">[12]</ref>, we ask our annotators for a discriminative referring expression of each actor instance if multiple objects are considered in a video. The annotation process resulted in a total of 6,656 sentences, including 811 different nouns, 225 verbs and 189 adjectives. Our sentences enrich the actor and action pairs from the A2D dataset with finer granularities. For example, the actor adult in A2D may be annotated with man, woman, person and player in our sentences, while action rolling may also refer to flipping, sliding, moving and running when describing different actors in different scenarios. Our sentences contain on average more words than the ReferIt dataset [12] (7.3 vs 4.7), even when we leave out prepositions, articles and linking verbs (4.5 vs 3.6). This makes sense as our sentences contain a variety of verbs while existing referring expression datasets mostly ignore verbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">J-HMDB Sentences</head><p>J-HMDB <ref type="bibr" target="#b8">[9]</ref> contains 928 video clips of 21 different actions annotated with a 2D articulated human puppet that provides scale, pose, segmentation and a coarse viewpoint for the humans involved in each action. We augment the videos with sentences following the same protocol as for A2D Sentences. We ask annotators to return a natural language description of what the target object is doing in each video. We obtain 928 sentences, including 158 different nouns, 53 verbs and 23 adjectives. The most popular actors are man, woman, boy, girl and player, while shooting, pouring, playing, catching and sitting are the most popular actions.</p><p>We show sentence-annotated examples of both datasets in <ref type="figure" target="#fig_2">Figure 3</ref> and provide more details on the datasets in the supplemental material. The sentence annotations and the code of our model will be available at https://kgavrilyuk.github.io/ publication/actor_action/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Study</head><p>In the first set of experiments we study the impact of individual components on our proposed model.</p><p>Setup. We select A2D Sentences for these set of experiments and use the train split for training and the test split for evaluation. The input to our model is a sentence describing what to segment and a video clip of N RGB frames around the frame to be segmented.</p><p>Evaluation. We adopt the widely used intersectionover-union (IoU) metric to measure segmentation quality. As aggregation metric we consider overall IoU, which is computed as total intersection area of all test data over the total union area.  Results on A2D Sentences. We first evaluate the influence of the number of input frames on our visual encoder and the segmentation result. We run our model with N = 1, 4, 8, 16 and we get 48.2%, 52.2%, 52.8%, and 53.6% respectively in terms of overall IoU. It reveals the important role of the large temporal context for actor and action video segmentation. Therefore, we choose N = 16 for all remaining experiments.</p><p>Next we compare our 1D convolutional textual encoder with an LSTM encoder. We follow the same setting for LSTM as in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>, we use a final hidden state of LSTM as textual representation for the whole sentence. The dimension of the hidden state is set to 1, 000. We represent words by the same word2vec embedding model for both models. We observe that our simple 1D convolutional textual encoder outperforms LSTM in terms of overall IoU: 53.6% for our encoder and 51.8% for LSTM. We also experi-  <ref type="table">Table 1</ref>: Segmentation from a sentence on A2D Sentences. Object segmentation baselines <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref> as proposed in the original papers, or fine-tuned on the A2D Sentences train split (denoted by ). Our model outperforms both baselines for all metrics. Incorporating Flow in our video model further improves results.</p><p>mented with bidirectional LSTM which slightly improves results over vanilla LSTM to 52.1%. Therefore, we select the convolutional neural network to encode the textual input in the remaining experiments. We further investigate the importance of our multiresolution loss. We compare the setting when we are using all three resolutions to compute the loss (? r = 1, r ? {32, 128, 512}) with the setting when only the highest resolution is used (? 32,128 = 0, ? 512 = 1). In terms of overall IoU the multi-resolution setting performs 53.6% while single resolution performs 49.4%. This demonstrates the benefit of the multi-resolution loss in our model.</p><p>In the last experiment we study the impact of the twostream <ref type="bibr" target="#b23">[24]</ref> approach for our task. We make a comparison for two type of inputs -RGB and Flow. For both streams we use 16 frames as input. The RGB stream produces better results than Flow: 53.6% for RGB and 49.5% for Flow. We then explore a fusion of RGB and Flow streams by computing a weighted average of the response maps from each stream. When we set the weight for RGB 2 times larger than Flow, it further improves our results to 55.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Segmentation from a sentence</head><p>In this experiment, we segment a video based on a given natural language sentence on the newly annotated A2D Sentences and J-HMDB Sentences datasets and compare our proposed model with the baseline methods.</p><p>Setup. As there is no prior work for video segmentation from a sentence, we select two methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>, which can be used for the related task of image segmentation from a sentence, as our baselines. To be precise, we compare with the segmentation model of <ref type="bibr" target="#b5">[6]</ref> and the lingual specification model of <ref type="bibr" target="#b14">[15]</ref>. We report baseline results in two training settings. In the first one, the baselines are trained solely on the ReferIt dataset <ref type="bibr" target="#b11">[12]</ref>, as indicated in the original papers. In the second setting we further fine-tune the baseline models using the training videos from A2D Sentences. We train our model only on the train split of A2D Sentences. During test, we follow <ref type="bibr" target="#b28">[29]</ref> and evaluate the models on each frame of the test videos for which segmentation annotation is availablearound one to three frames per video. The input to both baseline models is an RGB frame with a sentence description. For our model, we use the same sentence as input but instead of a single RGB frame we employ 16 frames around the frame to be segmented as this setting shows the best results in our ablation study.</p><p>Evaluation. In addition to overall IoU, we also consider mean IoU as aggregation. The mean IoU is computed as the average over the IoU of each test sample. While the overall IoU favors large segmented regions, mean IoU treats large and small regions equally. In addition, following <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>, we also measure precision at five different overlap values ranging from 0.5 to 0.9 as well as the mean average precision over .50 : .05 : .95 <ref type="bibr" target="#b15">[16]</ref>.</p><p>Results on A2D Sentences. In <ref type="table">Table 1</ref>, we report the results on the A2D Sentences dataset. The model of <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b14">[15]</ref>, pretrained on ReferIt <ref type="bibr" target="#b11">[12]</ref>, performs modestly as this dataset contains rich sentences describing objects, but it provides less information about actions. Fine-tuning these two baselines on A2D Sentences helps improve their performance by incorporating the notion of actions into the models. Our model outperforms both baselines for all metrics using RGB frames as input, bringing 3.5% absolute improvement in mAP , 2.1% in overall IoU and 6.7% in mean IoU. Fusion of RGB and Flow streams further improves our results. The larger improvement in mean IoU compared to overall IoU indicates our model is especially better on segmenting small objects. The results in mAP show the benefit of our model for larger overlap values. We visualize some of the sentence-guided segmentation results in <ref type="figure">Figure 4</ref>. First of all, our model can tackle the scenarios when the actor is not in the frame, e.g. in the second video. The model stops generating the segmentation once the man has left the camera's view. Our model can also tackle the scenarios when "car jumping into the water" "man with a purple backpack walking on the right" "woman in green dress is walking on the street" "black and white dog rolling on the meadow" "person is watching a dog" "small white dog walking on the right" <ref type="figure">Figure 4</ref>: Visualized segmentation results from our model on A2D Sentences. The first row shows a video with single actor and action, while the video in the second row contains similar types of actors performing the same action. In the third row, we illustrate a video with three sentences describing not only different actors, but also the same type of actor performing different actions. The colored segmentation masks are generated from the sentence with the same color above each video. the actor is performing an action which is different from the one specified in the sentence, e.g. in the first video. The model doesn't output any segmentation for the frames in which the car is not in the jumping state. It shows the potential of our model for spatio-temporal video segmentation. Second, in contrast to segmentation from actor-action labels, we can see from the second video that our segmentation from a sentence enables to distinguish the instances of the same actor-action pair by richer descriptions. In the third video, our model confuses two dogs, still we easily segment different types of actors.</p><p>Results on J-HMDB Sentences. We further evaluate the generalization ability of our model and the baselines. We test the models, finetuned or trained on A2D Sentences, on all 928 videos of J-HMDB Sentences dataset without any additional finetuning. For each video, we uniformly sample three frames for evaluation following the same setting as in the previous experiment. We report our results in <ref type="table" target="#tab_3">Table 2.</ref> J-HMDB Sentences focuses exclusively on human actions and 4 out of 21 actions overlap with actions in A2D Sentences, namely climb stairs, jump, walk, and run. Consistent with the results on A2D Sentences, our method provides a more accurate segmentation for higher overlap values which is shown by mAP. We attribute the better generalization ability to two aspects. The baselines rely on the VGG16 <ref type="bibr" target="#b24">[25]</ref> model to represent images, while we are using the video-specific I3D model. The second aspect comes from our textual representation, which can exploit similarity in descriptions of A2D Sentences and J-HMDB Sentences.   <ref type="table">Table 3</ref>: Semantic segmentation results on the A2D dataset using actor, action and actor+action as input respectively. Even though our method is not designed for this setting, it outperforms the state-of-the-art in most of the cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Segmentation from actor and action pairs</head><p>Finally, we segment a video from a predefined set of actor and action pairs and compare it with the state-of-the-art segmentation models on the original A2D dataset <ref type="bibr" target="#b28">[29]</ref>.</p><p>Setup. Instead of input sentences, we train our model on the 43 valid actor and action pairs provided by the dataset, such as adult walking and dog rolling. We use these pairs as textual input to our model. Visual input is kept the same as before. As our model explicitly requires a textual input for a given video, we select a subset of pairs from all possible pairs as queries to our model. For this purpose, we finetune a multi-label classification network on A2D dataset and select the pairs with a confidence score higher than 0.5. We use this reduced set of pairs as queries to our model and pick the class label with the highest response for each pixel. The classification network contains an RGB and a Flow I3D model where the number of neurons in the last layer is set to 43 and the activation function is replaced by a sigmoid for multi-label classification. During training, we finetune the last inception block and the final layer of both models on random 64-frame video clips. We randomly flip each frame horizontally in the video clip and then extract a 224 ? 224 random crop. We train for 3, 000 iterations with the Adam optimizer and fix the learning rate to 0.001. During test, we extract 32-frame clips over the video and average the scores across all the clips and across RGB and Flow streams to obtain the final score for a given video. For this multi-label classification we obtain mean average precision of 70%, compared to 67% in <ref type="bibr" target="#b28">[29]</ref>.</p><p>Evaluation. We report the class-average pixel accuracy, global pixel accuracy and mean IoU as in <ref type="bibr" target="#b9">[10]</ref>. Pixel accuracy is the percentage of pixels for which the label is correctly predicted, either over all pixels (global) or first computed for each class separately and then averaged over classes (class-average).</p><p>Results on A2D. We compare our approach with the state-of-the-art in <ref type="table">Table 3</ref>. Even though our method is not designed for this setting, it outperforms all the competitors for joint actor and action segmentation (last 3 columns of <ref type="table">Table 3</ref>). Particularly, we improve the state-of-the-art by a margin of 4.9% in terms of class-average accuracy and 5.1% in terms of Mean IoU. In addition to joint actor and action segmentation, we report results for actor and action segmentation separately. For actor segmentation the method by Kalogeiton et al. <ref type="bibr" target="#b9">[10]</ref> is slightly better in terms of class-average accuracy, for all other metrics and settings our method sets a new state-of-the-art. Our improvement is particularly notable on action segmentation where we outperform the state-of-the-art by 8.8% in terms of classaverage accuracy and 7.2% in terms of Mean IoU. It validates that our method is suitable for both actor and action segmentation, be it individually or combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduce the new task of actor and action video segmentation from a sentence. Our encoder-decoder neural architecture for pixel-level segmentation explicitly takes into account the spatio-temporal nature of video. To enable sentence-guided segmentation with our model, we extended two existing datasets with sentence-level annotations describing actors and their actions in the video content. Experiments show the feasibility and robustness, as well as the model's ability to adapt to the task of semantic segmentation of actor and action pairs, outperforming the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material for: Actor and Action Video Segmentation from a Sentence</head><p>In this supplementary material, we first report annotation statistics on both the A2D Sentences and J-HMDB Sentences datasets in Section S1. In Section S2, we show more segmentation results of our proposed model followed by a qualitative comparison of our video-based model with the image-based models of Hu et al.</p><p>[S1] and Li et al. <ref type="bibr">[S2]</ref> in Section S3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1. Dataset statistics</head><p>We show some statistics of the annotated sentences on A2D and J-HMDB datasets. <ref type="figure" target="#fig_0">Figure S1</ref> shows the most frequent nouns and verbs in the A2D Sentences dataset. Segmentation from a sentence allows us to distinguish between the fine-grained actors in the same super-category. For example while in the normal A2D dataset <ref type="bibr">[S3]</ref> there is a general 'adult' category, we annotate fine-grained human actors like {man, woman, guy, person, girl, boy, ...} in A2D Sentences. Furthermore, natural language sentences enable us to make use of a richer set of verbs to describe the same type of action, e.g. {jumping (up and down), bouncing, falling} all are representative for the action label 'jumping' in the regular A2D dataset. Likewise, {flipping, turning, rolling, rotating} are representative for the action label 'rolling', and {moving, running, chasing} are representative for 'running'. <ref type="figure" target="#fig_4">Figure S2</ref> shows the most frequent nouns and verbs in the J-HMDB Sentences dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2. Segmentation results on A2D Sentences</head><p>In this section, we visualize more results of the sentenceguided segmentation using our model. <ref type="figure" target="#fig_2">Figure S3</ref> illustrates videos with only one type of actor performing the same action. Our model segments both deformable (e.g., the 'woman' in the second video) and non-deformable (e.g., the 'ball' in the first video) objects. Also, it can handle reflecting surfaces, indicated by the 'ball' example. The third video demonstrates the ability of our model to distinguish instances among the same actor and action type by language cues like the spatial location provided in the sentence descriptions. <ref type="figure">Figure S4</ref> illustrates videos showing human actions. While the first two videos prove the ability of our model to recognize different human actions, the last video shows a failure case of our model. The model is asked to segment 'man' and 'woman' separately, while it segments both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3. Baseline comparison on A2D Sentences</head><p>In this section, we show a qualitative comparison of our model with two image-based baselines by Hu et al.</p><p>[S1] and Li et al. <ref type="bibr">[S2]</ref> in <ref type="figure">Figure S5</ref>. The first two rows verify that our model is able to segment relatively small actors, while both baselines struggle. The next two rows demonstrate the better segmentation accuracy of our model in comparison to the baseline models. For example, in the fourth row our model segments the car as a whole, while both baselines segment parts of the car only. In the last row, we illustrate the ability of our model to better distinguish between different types of actors.  "metal ball bouncing up and down" "woman is crawling on the grass like spiderman" "a bird on the left is flying" "a bird on the back of other bird with the same species is flying outside" <ref type="figure" target="#fig_2">Figure S3</ref>: Visualized segmentation results from our model on A2D Sentences. In all rows we show examples with only one type of actor performing the same action. The first two videos illustrate examples with one single instance while the last video contains two instances. The colored segmentation masks are generated from the sentence with the same color above each video. "a soldier is crawling" "soldier is standing on the ground" "man standing on the left" " a man is climbing a rock" "man walking with a woman on the beach" "woman walking with a man on the beach" <ref type="figure">Figure S4</ref>: Visualized segmentation results from our model on A2D Sentences. In the first two rows we show examples with one type of actor performing different actions. The last row illustrates a failure case of our model. The colored segmentation masks are generated from the sentence with the same color above each video. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>From a natural language input sentence our proposed model generates a pixel-level segmentation of an actor and its action in video content.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>"</head><label></label><figDesc>small white fluffy puppy biting the cat" "yellow car is flipping over onto its roof" "red ball is rolling on a bowling floor"A2D Sentences"man standing up from the sofa" "man in white top and black pants throwing darts" "boy in gray shirt and black shorts swinging baseball"J-HMDB Sentences</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>A2D Sentences and J-HMDB Sentences example videos, ground truth segments and sentence annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure S1 :</head><label>S1</label><figDesc>Most frequent nouns (top) and verbs (bottom) in the A2D Sentences dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure S2 :</head><label>S2</label><figDesc>Most frequent nouns (top) and verbs (bottom) in the J-HMDB Sentences dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>is rolling" "ball is bouncing in the room" "a black dog is walking on the left" "cat climbing wall" "a girl is rolling on the ground" Groundtruth Figure S5: Qualitative results on A2D Sentences. Columns from left to right are frame to segment, groundtruth segmentation, our model output, output of Hu et al. and output of Li et al. Above each example there is a sentence used as input for all methods describing what to segment in the frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Overlap mAPIoU P@0.5 P@0.6 P@0.7 P@0.8 P@0.9 0.5:0.95 Overall Mean</figDesc><table><row><cell>Hu et al. [6]</cell><cell>7.7</cell><cell>3.9</cell><cell>0.8</cell><cell>0.0</cell><cell>0.0</cell><cell>2.0</cell><cell>21.3</cell><cell>12.8</cell></row><row><cell>Li et al. [15]</cell><cell>10.8</cell><cell>6.2</cell><cell>2.0</cell><cell>0.3</cell><cell>0.0</cell><cell>3.3</cell><cell>24.8</cell><cell>14.4</cell></row><row><cell>Hu et al. [6]</cell><cell>34.8</cell><cell>23.6</cell><cell>13.3</cell><cell>3.3</cell><cell>0.1</cell><cell>13.2</cell><cell>47.4</cell><cell>35.0</cell></row><row><cell>Li et al. [15]</cell><cell>38.7</cell><cell>29.0</cell><cell>17.5</cell><cell>6.6</cell><cell>0.1</cell><cell>16.3</cell><cell>51.5</cell><cell>35.4</cell></row><row><cell>This paper: RGB</cell><cell>47.5</cell><cell>34.7</cell><cell>21.1</cell><cell>8.0</cell><cell>0.2</cell><cell>19.8</cell><cell>53.6</cell><cell>42.1</cell></row><row><cell>This paper: RGB + Flow</cell><cell>50.0</cell><cell>37.6</cell><cell>23.1</cell><cell>9.4</cell><cell>0.4</cell><cell>21.5</cell><cell>55.1</cell><cell>42.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Overlap mAPIoU P@0.5 P@0.6 P@0.7 P@0.8 P@0.9 0.5:0.95 Overall Mean</figDesc><table><row><cell>Hu et al. [6]</cell><cell>63.3</cell><cell>35.0</cell><cell>8.5</cell><cell>0.2</cell><cell>0.0</cell><cell>17.8</cell><cell>54.6</cell><cell>52.8</cell></row><row><cell>Li et al. [15]</cell><cell>57.8</cell><cell>33.5</cell><cell>10.3</cell><cell>0.6</cell><cell>0.0</cell><cell>17.3</cell><cell>52.9</cell><cell>49.1</cell></row><row><cell>This paper</cell><cell>69.9</cell><cell>46.0</cell><cell>17.3</cell><cell>1.4</cell><cell>0.0</cell><cell>23.3</cell><cell>54.1</cell><cell>54.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Segmentation from a sentence on J-HMDB Sentences using best settings per model on A2D Sentences, demonstrating generalization ability. Our model generates more accurate segmentations for higher overlap values.</figDesc><table><row><cell></cell><cell></cell><cell>Actor</cell><cell></cell><cell></cell><cell>Action</cell><cell></cell><cell cols="2">Actor and Action</cell><cell></cell></row><row><cell></cell><cell cols="9">Class-Average Global Mean IoU Class-Average Global Mean IoU Class-Average Global Mean IoU</cell></row><row><cell>Xu et al. [29]</cell><cell>45.7</cell><cell>74.6</cell><cell>-</cell><cell>47.0</cell><cell>74.6</cell><cell>-</cell><cell>25.4</cell><cell>76.2</cell><cell>-</cell></row><row><cell>Xu et al. [28]</cell><cell>58.3</cell><cell>85.2</cell><cell>33.4</cell><cell>60.5</cell><cell>85.3</cell><cell>32.0</cell><cell>43.3</cell><cell>84.2</cell><cell>19.9</cell></row><row><cell>Kalogeiton et al. [10]</cell><cell>73.7</cell><cell>90.6</cell><cell>49.5</cell><cell>60.5</cell><cell>89.3</cell><cell>42.2</cell><cell>47.5</cell><cell>88.7</cell><cell>29.7</cell></row><row><cell>This paper</cell><cell>71.4</cell><cell>92.8</cell><cell>53.7</cell><cell>69.3</cell><cell>92.5</cell><cell>49.4</cell><cell>52.4</cell><cell>91.7</cell><cell>34.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Tall: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Darrell. Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ob-jects2action: Classifying and localizing actions without any video example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Joint learning of object and action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ReferIt game: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tracking by natural language specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Spatial-aware object embeddings for zero-shot localization and classification of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Localizing actions from video labels and pseudo-annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-region two-stream r-cnn for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spatiotemporal deformable part models for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Actor-action semantic segmentation with grouping process models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Can humans fly? action understanding with multiple classes of actors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Spatiotemporal person retrieval via natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weakly supervised actor-action segmentation via robust multi-task ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Natural language person retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tracking by natural language specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Can humans fly? action understanding with multiple classes of actors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
