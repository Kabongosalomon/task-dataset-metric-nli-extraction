<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Space-time Mixing Attention for Video Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
							<email>adrian@adrianbulat.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>Perez-Rua</surname></persName>
							<email>j.perez-rua@samsung.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Samsung AI</orgName>
								<address>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swathikiran</forename><surname>Sudhakaran</surname></persName>
							<email>swathikir.s@samsung.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Samsung AI</orgName>
								<address>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
							<email>brais.a@samsung.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Samsung AI</orgName>
								<address>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
							<email>g.tzimiropoulos@qmul.ac.uk</email>
							<affiliation key="aff4">
								<orgName type="department">Samsung AI Cambridge Queen Mary</orgName>
								<orgName type="institution">University of London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Space-time Mixing Attention for Video Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper is on video recognition using Transformers. Very recent attempts in this area have demonstrated promising results in terms of recognition accuracy, yet they have been also shown to induce, in many cases, significant computational overheads due to the additional modelling of the temporal information. In this work, we propose a Video Transformer model the complexity of which scales linearly with the number of frames in the video sequence and hence induces no overhead compared to an image-based Transformer model. To achieve this, our model makes two approximations to the full space-time attention used in Video Transformers: (a) It restricts time attention to a local temporal window and capitalizes on the Transformer's depth to obtain full temporal coverage of the video sequence. (b) It uses efficient space-time mixing to attend jointly spatial and temporal locations without inducing any additional cost on top of a spatial-only attention model. We also show how to integrate 2 very lightweight mechanisms for global temporal-only attention which provide additional accuracy improvements at minimal computational cost. We demonstrate that our model produces very high recognition accuracy on the most popular video recognition datasets while at the same time being significantly more efficient than other Video Transformer models. Code will be made available. Preprint. Under review. arXiv:2106.05968v2 [cs.CV] 11 Jun 2021 Recent work by [3, 1] has focused on reducing the cost O(T 2 S 2 ) of the full space-time attention of Eq. 4. Bertasius et al. [3] proposed the factorised attention:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video recognition -in analogy to image recognition -refers to the problem of recognizing events of interest in video sequences such as human activities. Following the tremendous success of Transformers in sequential data, specifically in Natural Language Processing (NLP) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5]</ref>, Vision Transformers were very recently shown to outperform CNNs for image recognition too <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30]</ref>, signaling a paradigm shift on how visual understanding models should be constructed. In light of this, in this paper, we propose a Video Transformer model as an appealing and promising solution for improving the accuracy of video recognition models.</p><p>A direct, natural extension of Vision Transformers to the spatio-temporal domain is to perform the self-attention jointly across all S spatial locations and T temporal locations. Full space-time attention though has complexity O(T 2 S 2 ) making such a model computationally heavy and, hence, impractical even when compared with the 3D-based convolutional models. As such, our aim is to exploit the temporal information present in video streams while minimizing the computational burden within the Transformer framework for efficient video recognition.  <ref type="figure">Figure 1</ref>: Different approaches to space-time self-attention for video recognition. In all cases, the key locations that the query vector, located at the center of the grid in red, attends are shown in orange. Unlike prior work, our key vector is constructed by mixing information from tokens located at the same spatial location within a local temporal window. Our method then performs self-attention with these tokens. Note that our mechanism allows for an efficient approximation of local space-time attention at no extra cost when compared to a spatial-only attention model.</p><p>A baseline solution to this problem is to consider spatial-only attention followed by temporal averaging, which has complexity O(T S 2 ). Similar attempts to reduce the cost of full space-time attention have been recently proposed in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref>. These methods have demonstrated promising results in terms of video recognition accuracy, yet they have been also shown to induce, in most of the cases, significant computational overheads compared to the baseline (spatial-only) method due to the additional modelling of the temporal information.</p><p>Our main contribution in this paper is a Video Transformer model that has complexity O(T S 2 ) and, hence, is as efficient as the baseline model, yet, as our results show, it outperforms recently/concurrently proposed work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref> in terms of efficiency (i.e. accuracy/FLOP) by significant margins. To achieve this, our model makes two approximations to the full space-time attention used in Video Transformers: (a) It restricts time attention to a local temporal window and capitalizes on the Transformer's depth to obtain full temporal coverage of the video sequence. (b) It uses efficient space-time mixing to attend jointly spatial and temporal locations without inducing any additional cost on top of a spatial-only attention model. <ref type="figure">Fig. 1</ref> shows the proposed approximation to space-time attention. We also show how to integrate two very lightweight mechanisms for global temporal-only attention, which provide additional accuracy improvements at minimal computational cost. We demonstrate that our model is surprisingly effective in terms of capturing long-term dependencies and producing very high recognition accuracy on the most popular video recognition datasets, including Something-Something-v2 <ref type="bibr" target="#b14">[15]</ref>, Kinetics <ref type="bibr" target="#b3">[4]</ref> and Epic Kitchens <ref type="bibr" target="#b6">[7]</ref>, while at the same time being significantly more efficient than other Video Transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Video recognition: Standard solutions are based on CNNs and can be broadly classified into two categories: 2D-and 3D-based approaches. 2D-based approaches process each frame independently to extract frame-based features which are then aggregated temporally with some sort of temporal modeling (e.g. temporal averaging) performed at the end of the network <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. The works of <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> use the "shift trick" <ref type="bibr" target="#b39">[40]</ref> to have some temporal modeling at a layer level. 3D-based approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31]</ref> are considered the current state-of-the-art as they can typically learn stronger temporal models via 3D convolutions. However, they also incur higher computational and memory costs. To alleviate this, a large body of works attempt to improve their efficiency via spatial and/or temporal factorization <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>CNN vs ViT: Historically, video recognition approaches tend to mimic the architectures used for image classification (e.g. from AlexNet <ref type="bibr" target="#b20">[21]</ref> to <ref type="bibr" target="#b17">[18]</ref> or from ResNet <ref type="bibr" target="#b15">[16]</ref> and ResNeXt <ref type="bibr" target="#b41">[42]</ref> to <ref type="bibr" target="#b13">[14]</ref>). After revolutionizing NLP <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b27">28]</ref>, very recently, Transformer-based architectures showed promising results on large scale image classification too <ref type="bibr" target="#b10">[11]</ref>. While self-attention and attention were previously used in conjunction with CNNs at a layer or block level <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b28">29]</ref>, the Vision Transformer (ViT) of Dosovitskiy et al. <ref type="bibr" target="#b10">[11]</ref> is the first convolution-free, Transformer-based architecture that achieves state-of-the-art results on ImageNet <ref type="bibr" target="#b8">[9]</ref>.</p><p>Video Transformer: Recently/concurrently with our work, vision transformer architectures, derived from <ref type="bibr" target="#b10">[11]</ref>, were used for video recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref>, too. Because performing full space-time attention is computationally prohibitive (i.e. O(T 2 S 2 )), their main focus is on reducing this via temporal and spatial factorization. In TimeSformer <ref type="bibr" target="#b2">[3]</ref>, the authors propose applying spatial and temporal attention in an alternating manner reducing the complexity to O(T 2 S + T S 2 ). In a similar fashion, ViViT <ref type="bibr" target="#b0">[1]</ref> explores several avenues for space-time factorization. In addition, they also proposed to adapt the patch embedding process from <ref type="bibr" target="#b10">[11]</ref> to 3D (i.e. video) data. Our work proposes a completely different approximation to full space-time attention that is also efficient. To this end, we firstly restrict full space-time attention to a local temporal window which is reminiscent of <ref type="bibr" target="#b1">[2]</ref> but applied here to space-time attention and video recognition. Secondly, we define a local joint space-time attention which we show that can be implemented efficiently via the "shift trick" <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Video Transformer: We are given a video clip X ? R T ?H?W ?C (C = 3, S = HW ). Following ViT <ref type="bibr" target="#b10">[11]</ref>, each frame is divided into K ? K non-overlapping patches which are then mapped into visual tokens using a linear embedding layer E ? R 3K 2 ?d . Since self-attention is permutation invariant, in order to preserve the information regarding the location of each patch within space and time, we also learn two positional embeddings, one for space: p s ? R 1?S?d and one for time: p t ? R T ?1?d . These are then added to the initial visual tokens. Finally, the token sequence is processed by L Transformer layers.</p><p>The visual token at layer l, spatial location s and temporal location t is denoted as:</p><formula xml:id="formula_0">z l s,t ? R d , l = 0, . . . , L ? 1, s = 0, . . . , S ? 1, t = 0, . . . , T ? 1.<label>(1)</label></formula><p>In addition to the T S visual tokens extracted from the video, a special classification token z l cls ? R d is prepended to the token sequence <ref type="bibr" target="#b9">[10]</ref>. The l?th Transformer layer processes the visual tokens Z l ? R (T S+1)?d of the previous layer using a series of Multi-head Self-Attention (MSA), Layer Normalization (LN), and MLP (R d ? R 4d ? R d ) layers as follows:</p><formula xml:id="formula_1">Y l = MSA(LN(Z l?1 )) + Z l?1 ,<label>(2)</label></formula><formula xml:id="formula_2">Z l+1 = MLP(LN(Y l )) + Y l .<label>(3)</label></formula><p>The main computation of a single full space-time Self-Attention (SA) head boils down to calculating:</p><formula xml:id="formula_3">y l s,t = T ?1 t =0 S?1 s =0 Softmax{(q l s,t ? k l s ,t )/ d h }v l s ,t , s=0,...,S?1 t=0,...,T ?1<label>(4)</label></formula><p>where q l s,t , k l s,t , v l s,t ? R d h are the query, key, and value vectors computed from z l s,t (after LN) using embedding matrices W q , W k , W v ? R d?d h . Finally, the output of the h heads is concatenated and projected using embedding matrix W h ? R hd h ?d .</p><p>The complexity of the full model is:</p><formula xml:id="formula_4">O(3hT Sdd h ) (qkv projections) +O(2hT 2 S 2 d h ) (MSA for h attention heads) +O(T S(hd h )d) (multi-head projection) +O(4T Sd 2 ) (MLP)</formula><p>. From these terms, our goal is to reduce the cost O(2T 2 S 2 d h ) (for a single attention head) of the full space-time attention which is the dominant term. For clarity, from now on, we will drop constant terms and d h to report complexity unless necessary. Hence, the complexity of the full space-time attention is O(T 2 S 2 ).</p><p>Our baseline is a model that performs a simple approximation to the full space-time attention by applying, at each Transformer layer, spatial-only attention:</p><formula xml:id="formula_5">y l s,t = S?1 s =0 Softmax{(q l s,t ? k l s ,t )/ d h }v l s ,t , s=0,...,S?1 t=0,...,T ?1<label>(5)</label></formula><p>the complexity of which is O(T S 2 ). Notably, the complexity of the proposed space-time mixing attention is also O(T S 2 ). Following spatial-only attention, simple temporal averaging is performed on the class tokens z f inal = 1 T t z L?1 t,cls to obtain a single feature that is fed to the linear classifier.</p><formula xml:id="formula_6">y l s,t = T ?1 t =0 Softmax{(q l s,t ? k l s,t )/ d h }v l s,t , y l s,t = S?1 s =0 Softmax{q l s,t ?k l s ,t )/ d h }? l s ,t , s = 0, . . . , S ? 1 t = 0, . . . , T ? 1 ,<label>(6)</label></formula><p>whereq l s,t ,k l s ,t? l s ,t are new query, key and value vectors calculated from? l s,t 1 . The above model reduces complexity to O(T 2 S + T S 2 ). However, temporal attention is performed for a fixed spatial location which is ineffective when there is camera or object motion and there is spatial misalignment between frames.</p><p>The work of <ref type="bibr" target="#b0">[1]</ref> is concurrent to ours and proposes the following approximation: L s Transformer layers perform spatial-only attention as in Eq. 5 (each with complexity O(S 2 )). Following this, there are L t Transformer layers performing temporal-only attention on the class tokens z Ls t . The complexity of the temporal-only attention is, in general, O(T 2 ).</p><p>Our model aims to better approximate the full space-time self-attention (SA) of Eq. 4 while keeping complexity to O(T S 2 ), i.e. inducing no further complexity to a spatial-only model.</p><p>To achieve this, we make a first approximation to perform full space-time attention but restricted to a local temporal window [?t w , t w ]: The complexity of the local self-attention of Eq. 7 is O((2t w + 1)T S 2 ). To reduce this even further, we make a second approximation on top of the first one as follows: the attention between spatial locations s and s according to the model of Eq. 7 is: </p><formula xml:id="formula_7">y l s,t = t+tw t =t?tw S?1 s =0 Softmax{(q l s,t ? k l s ,t )/ d h }v l s ,t = t+tw t =t?tw V l t a l t , s=0,...,S?1 t=0,...,T ?1 (7) where V l t = [v l 0,t ; v l 1,t ; . . . ; v l S?1,t ] ? R d h ?S and a l t = [a l 0,t , a l 1,t , . . . , a l S?1,t ] ? R S is</formula><formula xml:id="formula_8">t+tw t =t?tw Softmax{(q l s,t ? k l s ,t )/ d h }v l s ,t ,<label>(8)</label></formula><formula xml:id="formula_9">[k l s ,t?tw ; . . . ; k l s ,t+tw ] ? R (2tw+1)d h .</formula><p>Note that to match the dimensions of q l s,t and k l s ,?tw:tw a further projection of k l s ,?tw:tw to R d h is normally required which has complexity O((2t w + 1)d 2 h ) and hence compromises the goal of an efficient implementation. To alleviate this, we use the "shift trick" <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b23">24]</ref> which allows to perform both zero-cost dimensionality reduction, spacetime mixing and attention (between q l s,t and k l s ,?tw:    </p><formula xml:id="formula_10">tw ) in O(d h ). In particular, each t ? [?t w , t w ] is assigned d t h channels from d h (i.e. t d t h = d h ). Let k l s ,t (d t h ) ? R d t</formula><formula xml:id="formula_11">[k l s ,t?tw (d t?tw h ), . . . , k l s ,t+tw (d t+tw h )] ? R d h .<label>(9)</label></formula><p>This has the complexity of a spatial-only attention (O(T S 2 )) and hence it is more efficient than previously proposed video transformers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref>. Our model also provides a better approximation to the full space-time attention and as shown by our results it significantly outperforms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Temporal Attention aggregation: The final set of the class tokens z L?1 t,cls , 0 ? t ? L ? 1 are used to generate the predictions. To this end, we propose to consider the following options: (a) simple temporal averaging z f inal = 1 T t z L?1 t,cls as in the case of our baseline. (b) An obvious limitation of temporal averaging is that the output is treated purely as an ensemble of per-frame features and, hence, completely ignores the temporal ordering between them. To address this, we propose to use a lightweight Temporal Attention (TA) mechanism that will attend the T classification tokens. In particular, a z f inal token attends the sequence [z L?1 0,cls , . . . , z L?1 T ?1,cls ] using a temporal Transformer layer and then fed as input to the classifier. This is akin to the (concurrent) work of <ref type="bibr" target="#b0">[1]</ref> with the difference being that in our model we found that a single TA layer suffices whereas <ref type="bibr" target="#b0">[1]</ref> uses L t . A consequence of this is that the complexity of our layer is</p><formula xml:id="formula_13">O(T ) vs O(2(L t ? 1)T 2 + T ) of [1].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary token:</head><p>As an alternative to TA, herein, we also propose a simple lightweight mechanism for information exchange between different frames at intermediate layers of the network. Given the set of tokens for each frame t, Z l?1 t ? R (S+1)?d h (constructed by concatenating all tokens z l?1 s,t , s = 0, . . . , S), we compute a new set of R tokens Z l r,t = ?(Z l?1 t ) ? R R?d h which summarize the frame information and hence are named "Summary" tokens. These are, then, appended to the visual tokens of all frames to calculate the keys and values so that the query vectors attend the original keys plus the Summary tokens. Herein, we explore the case that ?(.) performs simple spatial averaging z l 0,t = 1 S s z l s,t over the tokens of each frame (R = 1 for this case). Note that, for R = 1, the extra cost that the Summary token induces is O(T S).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X-ViT:</head><p>We call the Video Transformer based on the proposed (a) space-time mixing attention and (b) lightweight global temporal attention (or summary token) as X-ViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Datasets: We trained and evaluated the proposed models on the following datasets (all datasets are publicly available for research purposes):</p><p>Kinetics-400 and 600: The Kinetics <ref type="bibr" target="#b18">[19]</ref> dataset consists of short clips (typically 10 sec long) sampled from YouTube and labeled using 400 and 600 classes, respectively. Due to the removal of certain videos from YouTube, the versions of the datasets used in this paper consist of approximately 261k clips for Kinetics-400 and 457k for Kinetics-600. Note, that these numbers are lower than the original datasets and thus might induce a negative performance bias when compared with prior works.</p><p>Something-Something-v2 (SSv2): The SSv2 <ref type="bibr" target="#b14">[15]</ref> dataset consists of 220,487 short videos, with a length between 2 and 6 seconds that picture humans performing pre-defined basic actions with everyday objects. Since the objects and backgrounds in the videos are consistent across different action classes, this dataset tends to require stronger temporal modeling. Due to this, we conducted most of our ablation studies on SSv2 to better analyze the importance of the proposed components.</p><p>Epic Kitchens-100 (Epic-100): Epic-100 is an egocentric large scale action recognition dataset consisting of more than 90,000 action segments that span across 100 hours of recordings in native environments, capturing daily activities <ref type="bibr" target="#b7">[8]</ref>. The dataset is labeled using 97 verb and 300 noun classes. The evaluation results are reported using the standard action recognition protocol: the network predicts the "verb" and the "noun" using two heads. The predictions are then merged to construct an "action" which is used to calculate the accuracy.</p><p>Network architecture: The backbone models closely follow the ViT architecture <ref type="bibr" target="#b10">[11]</ref>. Most of the experiments were performed using the ViT-B/16 variant (L = 12, h = 12, d = 768, K = 16), where L represents the number of transformer layers, h the number of heads, d the embedding dimension and K the patch size. We initialized our models from a pretrained ImageNet-21k <ref type="bibr" target="#b8">[9]</ref> ViT model. The spatial positional encoding p s was initialized from the pretrained 2D model and the temporal one, p t , with zeros so that it does not have a great impact on the tokens early on during training. The models were trained on 8 V100 GPUs using PyTorch <ref type="bibr" target="#b25">[26]</ref>.</p><p>Testing details: Unless otherwise stated, we used ViT-B/16 and T = 8 frames. We mostly used Temporal Attention (TA) for temporal aggregation. We report accuracy results for 1 ? 3 views (1 temporal clip and 3 spatial crops) departing from the common approach of using up to 10 ? 3 views <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b13">14]</ref>. The 1 ? 3 views setting was also used in Bertasius et al. <ref type="bibr" target="#b2">[3]</ref>. To measure the variation between runs, we trained one of the 8-frame models 5 times. The results varied by ?0.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation studies</head><p>Throughout this section we study the effect of varying certain design choices and different components of our method. Because SSv2 tends to require a more fine-grained temporal modeling, unless otherwise specified, all results reported, in this subsection, are on the SSv2. Effect of local window size: <ref type="table" target="#tab_2">Table 1</ref> shows the accuracy of our model by varying the local window size [?t w , t w ] used in the proposed space-time mixing attention. Firstly, we observe that the proposed model is significantly superior to our baseline (t w = 0) which uses spatial-only attention. Secondly, a window of t w = 1 produces the best results. This shows that more gradual increase of the effective window size that is attended is more beneficial compared to more aggressive ones, i.e. the case where t w = 2.</p><p>A performance degradation for the case t w = 2 could be attributed to boundary effects (handled by filling with zeros) which are aggravated as t w increases. Based on these results, we chose to use t w = 1 for the models reported hereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of SA position:</head><p>We explored which layers should the proposed space-time mixing attention operation be applied to within the Transformer. Specifically, we explored the following variants:  Applying it to the first L/2 layers, to the last L/2 layers, to every odd indexed layer and finally, to all layers. As the results from <ref type="table" target="#tab_3">Table 2a</ref> show, the exact layers within the network that self-attention is applied to do not matter; what matters is the number of layers it is applied to. We attribute this result to the increased temporal receptive field and cross-frame interactions.</p><p>Effect of temporal aggregation: Herein, we compare the two methods used for temporal aggregation: simple temporal averaging <ref type="bibr" target="#b35">[36]</ref> and the proposed Temporal Attention (TA) mechanism. Given that our model already incorporates temporal information through the proposed space-time attention, we also explored how many TA layers are needed. As shown in <ref type="table" target="#tab_3">Table 2b</ref> replacing temporal averaging with one TA layer improves the Top-1 accuracy from 62.5% to 64.4%. Increasing the number of layers further yields no additional benefits. We also report the accuracy of spatial-only attention plus TA aggregation. In the absence of the proposed space-time mixing attention, the TA layer alone is unable to compensate, scoring only 56.6% as shown in <ref type="table" target="#tab_3">Table 2d</ref>. This highlights the need of having both components in our final model. For the next two ablation studies, we therefore used 1 TA layer. Effect of space-time mixing qkv vectors: Paramount to our work is the proposed space-time mixing attention of Eq. 10 which is implemented by constructingk l s ,?tw:tw and? l s ,?tw:tw efficiently via channel indexing (see Eq. 9). Space-time mixing though can be applied in several different ways in the model. For completeness, herein, we study the effect of space-time mixing to various combinations for the key, value and to the input token prior to qkv projection. As shown in <ref type="table" target="#tab_3">Table 2c</ref>, the combination corresponding to our model (i.e. space-time mixing applied to the key and value) significantly outperforms all other variants by up to 2%. This result is important as it confirms that our model, derived from the proposed approximation to the local space-time attention, gives the best results when compared to other non-well motivated variants.</p><p>Effect of amount of space-time mixing: We define as ?d h the total number of channels imported from the adjacent frames in the local temporal window [?t w , t w ] (i.e. tw t =?tw,t =0 d t h = ?d h ) when LGD-3D R101 <ref type="bibr" target="#b26">[27]</ref> 81.5 95.6 10 ? 3 -SlowFast R101+NL <ref type="bibr" target="#b13">[14]</ref> 81.8 95.1 10 ? 3 3,480 X3D-XL <ref type="bibr" target="#b12">[13]</ref> 81.9 95.5 10 ? 3 1,452 TimeSformer-HR <ref type="bibr" target="#b2">[3]</ref> 82.4 96.0 1 ? 3 5,110 ViViT-L/16x2 <ref type="bibr" target="#b0">[1]</ref> 82 . Herein, we study the effect of ? on the model's accuracy. As the results from <ref type="table" target="#tab_3">Table 2d</ref> show, the optimal ? is between 25% and 50%. Increasing ? to 100% (i.e. all channels are coming from adjacent frames) unsurprisingly degrades the performance as it excludes the case t = t when performing the self-attention.</p><p>Effect of Summary token: Herein, we compare Temporal Attention with Summary token on SSv2 and Kinetics-400. We used both datasets for this case as they require different type of understanding: fine-grained temporal (SSv2) and spatial content (K400). From <ref type="table" target="#tab_4">Table 3</ref>, we conclude that the Summary token compares favorable on Kinetics-400 but not on SSv2 showing that is more useful in terms of capturing spatial information. Since the improvement is small, we conclude that 1 TA layer is the best global attention-based mechanism for improving the accuracy of our method adding also negligible computational cost.</p><p>Effect of the number of input frames: Herein, we evaluate the impact of increasing the number of input frames T from 8 to 16 and 32. We note that, for our method, this change results in a linear increase in complexity. As the results from <ref type="table" target="#tab_9">Table 7</ref> show, increasing the number of frames from 8 to 16 offers a 1.8% boost in Top-1 accuracy on SSv2. Moreover, increasing the number of frames to 32 improves the performance by a further 0.2%, offering diminishing returns. Similar behavior can be observed on Kinetics and Epic-100 in <ref type="table" target="#tab_8">Tables 6 and 8</ref>.</p><p>Effect of number of tokens: Herein, we vary the number of input tokens by changing the patch size K. As the results from <ref type="table" target="#tab_5">Table 5</ref> show, even when the number of tokens decreases significantly (ViT-B/32) our approach is still able to produce models that achieve satisfactory accuracy. The benefit of that is having a model which is significantly more efficient.</p><p>Effect of the number of crops at test time. Throughout this work, at test time, we reported results using 1 temporal and 3 spatial crops (i.e. 1 ? 3). This is noticeable different from the current practice of using up to 10 ? 3 crops <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>To showcase the behavior of our method, herein, we test the effect of increasing the number of crops on Kinetics-400. As the results from <ref type="figure" target="#fig_5">Fig. 3</ref> show, increasing the number of crops beyond two temporal views (i.e. 2 ? 3), yields no additional gains. Our findings align with the ones from the work of Bertasius et al. <ref type="bibr" target="#b2">[3]</ref> that observes the same properties for the transformer-based architectures.</p><p>Latency and throughput considerations: While the channel shifting operation used by the proposed space-time mixing attention is zero-FLOP, there is still a small cost associated with memory movement operations. In order to ascertain that the induced cost does not introduce noticeable performance degradation, we benchmarked a Vit-B/16 (8? frames) model using spatial-only attention and the proposed one on 8 V100 GPUs and a batch size of 128. The spatial-only attention model has a throughput of 312 frames/second while our model 304 frames/second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to state-of-the-art</head><p>Our best model uses the proposed space-time mixing attention in all the Transformer layers and performs temporal aggregation using a single lightweight temporal transformer layer as described in Section 3. Unless otherwise specified, we report the results using the 1 ? 3 configuration for the views (1 temporal and 3 spatial) for all datasets.   On Kinetics-400, we match the current state-of-the-art results while being significantly faster than the next two best recently/concurrently proposed methods that also use Transformer-based architectures: 20? faster than ViVit <ref type="bibr" target="#b0">[1]</ref> and 8? than TimeSformer-L <ref type="bibr" target="#b2">[3]</ref>. Note that both models from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> and ours were initialized from a ViT model pretrained on ImageNet-21k <ref type="bibr" target="#b8">[9]</ref> and take as input frames at a resolution of 224 ? 224px. Similarly, on Kinetics-600 we set a new state-of-the-art result. See <ref type="table" target="#tab_6">Table 4</ref>.</p><p>On SSv2 we match and surpass the current state-of-the-art, especially in terms of Top-5 accuracy (ours: 90.8% vs ViViT: 89.8% <ref type="bibr" target="#b0">[1]</ref>) using models that are 14? (16 frames) and 9? (32 frames) faster.</p><p>Finally, we observe similar outcomes on Epic-100 where we set a new state-of-the-art, showing particularly large improvements especially for "Verb" accuracy, while again being more efficient. We presented a novel approximation to the full space-time attention that is amenable to an efficient implementation and applied it to video recognition. Our approximation has the same computational cost as spatial-only attention yet the resulting Video Transformer model was shown to be significantly more efficient than recently/concurrently proposed Video Transformers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref>. By no means this paper proposes a complete solution to video recognition using Video Transformers. Future efforts could include combining our approaches with other architectures than the standard ViT, removing the dependency on pre-trained models and applying the model to other video-related tasks like detection and segmentation. Finally, further research is required for deploying our models on low power devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Full space-time attention:O(T 2 S 2 ) (b) Spatial-only attention: O(T S 2 )(c) TimeSformer<ref type="bibr" target="#b2">[3]</ref>:O(T 2 S + T S 2 ) (d) Ours: O(T S 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the vector with the corresponding attention weights. Eq. 7 shows that, for a single Transformer layer, y l s,t is a spatio-temporal combination of the visual tokens in the local window [?t w , t w ]. It follows that, after k Transformer layers, y l+k s,t will be a spatio-temporal combination of the visual tokens in the local window [?kt w , kt w ] which in turn conveniently allows to perform spatio-temporal attention over the whole clip. For example, for t w = 1 and k = 4, the local window becomes [?4, 4] which spans the whole video clip for the typical case T = 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Detailed self-attention computation graph for (a) full space-time attention and (b) the proposed space-time mixing approximation. Notice that in our case only S tokens participate instead of TS. The temporal information is aggregated by indexing channels from adjacent frames. Tokens of identical colors share the same temporal index.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2</head><label>2</label><figDesc>shows how the key vectork l s ,?tw:tw is constructed. In a similar way, we also construct a new value vector? l s ,?tw:tw . Finally, the proposed approximation to the full space-time attention is given by: ?k l s ,?tw:tw )/ d h }? l s ,?tw:tw , s=0,...,S?1 t=0,...,T ?1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Effect of the number of temporal crops at test time as measured on Kinetics 400 in terms of Top 1 accuracy. For each temporal crop, 3 spatial clips are sampled, for a total of t crops ? 3 clips. Notice that beyond t crops = 2 no additional accuracy gains are observed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>i.e. it requires the calculation of 2t w + 1 attentions, one per temporal location over [?t w , t w ]. Instead, we propose to calculate a single attention over [?t w , t w ] which can be achieved by q</figDesc><table><row><cell>l s,t attending</cell></row><row><cell>k l s ,?tw:tw</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Effect of local window size. To isolate its effect from that of temporal aggregation, the models were trained using temporal averaging. Note, that (Bo.) indicates that only features from the boundaries of the local window were used, ignoring the intermediate ones.</figDesc><table><row><cell>Variant</cell><cell cols="2">Top-1 Top-5</cell></row><row><cell>t w = 0</cell><cell>45.2</cell><cell>71.4</cell></row><row><cell>t w = 1</cell><cell>62.5</cell><cell>87.8</cell></row><row><cell>t w = 2</cell><cell>60.5</cell><cell>86.4</cell></row><row><cell>t w = 2 (Bo.)</cell><cell>60.4</cell><cell>86.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Effect of (a) proposed SA position, (b) temporal aggregation and number of Temporal Attention (TA) layers, (c) space-time mixing qkv vectors and (d) amount of mixed channels on SSv2.(a) Effect of applying the proposed SA to certain layers. Effect of space-time mixing. x denotes the input token before qkv projection. Query produces equivalent results with key and thus omitted.</figDesc><table><row><cell cols="3">Transform. layers Top-1 Top-5 1st half 61.7 86.5 2nd half 61.6 86.3</cell><cell>(c) x key</cell><cell>value</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell>Half (odd. pos)</cell><cell>61.2</cell><cell>86.4</cell><cell></cell><cell></cell><cell>56.6</cell><cell>83.5</cell></row><row><cell>All</cell><cell>62.6</cell><cell>87.8</cell><cell></cell><cell></cell><cell>63.1</cell><cell>88.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>63.1</cell><cell>88.8</cell></row><row><cell cols="3">(b) Effect of number of TA layers. 0 corresponds to</cell><cell></cell><cell></cell><cell>62.5</cell><cell>88.6</cell></row><row><cell>temporal averaging.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64.4</cell><cell>89.3</cell></row><row><cell>#. TA layers</cell><cell cols="2">Top-1 Top-5</cell><cell cols="4">(d) Effect of amount of mixed channels. * uses</cell></row><row><cell>0 (temp. avg.)</cell><cell>62.4</cell><cell>87.8</cell><cell cols="2">temp. avg. aggregation.</cell><cell></cell><cell></cell></row><row><cell>1 2 3</cell><cell>64.4 64.5 64.5</cell><cell>89.3 89.3 89.3</cell><cell>0%* 0% 45.2 56.6</cell><cell>25% 64.3</cell><cell>50% 64.4</cell><cell>100% 62.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison between TA and Summary token on SSv2 (left) and Kinetics-400 (right).</figDesc><table><row><cell cols="2">Summary TA Top-1 Top-5</cell><cell cols="2">Summary TA Top-1 Top-5</cell></row><row><cell>62.4</cell><cell>87.8</cell><cell>77.8</cell><cell>93.7</cell></row><row><cell>63.7</cell><cell>88.9</cell><cell>78.7</cell><cell>93.7</cell></row><row><cell>63.4</cell><cell>88.9</cell><cell>78.0</cell><cell>93.2</cell></row><row><cell>64.4</cell><cell>89.3</cell><cell>78.5</cell><cell>93.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Effect of number of tokens on SSv2.</figDesc><table><row><cell>Variant</cell><cell cols="2">Top-1 Top-5</cell><cell>FLOPs (?10 9 )</cell></row><row><cell>ViT-B/32</cell><cell>60.5</cell><cell>87.4</cell><cell>95</cell></row><row><cell>ViT-L/32</cell><cell>61.8</cell><cell>88.3</cell><cell>327</cell></row><row><cell>ViT-B/16</cell><cell>64.4</cell><cell>89.3</cell><cell>425</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state-of-the-art on the Kinetics-600 dataset. T ? is the number of frames used by our method.</figDesc><table><row><cell>Method</cell><cell cols="4">Top-1 Top-5 Views FLOPs (?10 9 )</cell></row><row><cell>AttentionNAS [38]</cell><cell>79.8</cell><cell>94.4</cell><cell>-</cell><cell>1,034</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison with state-of-the-art on the Kinetics-400. T ? is the number of frames used by our method.</figDesc><table><row><cell>Method</cell><cell cols="4">Top-1 Top-5 Views FLOPs (?10 9 )</cell></row><row><cell>bLVNet [12]</cell><cell>73.5</cell><cell>91.2</cell><cell>3 ? 3</cell><cell>840</cell></row><row><cell>STM [17]</cell><cell>73.7</cell><cell>91.6</cell><cell>-</cell><cell>-</cell></row><row><cell>TEA [23]</cell><cell>76.1</cell><cell>92.5</cell><cell>10 ? 3</cell><cell>2,100</cell></row><row><cell>TSM R50 [24]</cell><cell>74.7</cell><cell>-</cell><cell>10 ? 3</cell><cell>650</cell></row><row><cell>I3D NL [39]</cell><cell>77.7</cell><cell>93.3</cell><cell>10 ? 3</cell><cell>10,800</cell></row><row><cell>CorrNet-101 [35]</cell><cell>79.2</cell><cell>-</cell><cell>10 ? 3</cell><cell>6,700</cell></row><row><cell>ip-CSN-152 [33]</cell><cell>79.2</cell><cell>93.8</cell><cell>10 ? 3</cell><cell>3,270</cell></row><row><cell>LGD-3D R101 [27]</cell><cell>79.4</cell><cell>94.4</cell><cell>-</cell><cell>-</cell></row><row><cell>SlowFast 8?8 R101+NL [14]</cell><cell>78.7</cell><cell>93.5</cell><cell>10 ? 3</cell><cell>3,480</cell></row><row><cell>SlowFast 16?8 R101+NL [14]</cell><cell>79.8</cell><cell>93.9</cell><cell>10 ? 3</cell><cell>7,020</cell></row><row><cell>X3D-XXL [13]</cell><cell>80.4</cell><cell>94.6</cell><cell>10 ? 3</cell><cell>5,823</cell></row><row><cell>TimeSformer-L [3]</cell><cell>80.7</cell><cell>94.7</cell><cell>1 ? 3</cell><cell>7,140</cell></row><row><cell>ViViT-L/16x2 [3]</cell><cell>80.6</cell><cell>94.7</cell><cell>4 ? 3</cell><cell>17,352</cell></row><row><cell>X-ViT (8?) (Ours)</cell><cell>78.5</cell><cell>93.7</cell><cell>1 ? 3</cell><cell>425</cell></row><row><cell>X-ViT (16?) (Ours)</cell><cell>80.2</cell><cell>94.7</cell><cell>1 ? 3</cell><cell>850</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparison with state-of-the-art on SSv2. T ? is the number of frames used by our method. * -denotes models pretrained on Kinetics-600.</figDesc><table><row><cell>Method</cell><cell cols="4">Top-1 Top-5 Views FLOPs (?10 9 )</cell></row><row><cell>TRN [45]</cell><cell>48.8</cell><cell>77.6</cell><cell>-</cell><cell>-</cell></row><row><cell>SlowFast+multigrid [41]</cell><cell>61.7</cell><cell>-</cell><cell>1 ? 3</cell><cell>-</cell></row><row><cell>TimeSformer-L [3]</cell><cell>62.4</cell><cell>-</cell><cell>1 ? 3</cell><cell>7,140</cell></row><row><cell>TSM R50 [24]</cell><cell>63.3</cell><cell>88.5</cell><cell>2 ? 3</cell><cell>-</cell></row><row><cell>STM [17]</cell><cell>64.2</cell><cell>89.8</cell><cell>-</cell><cell>-</cell></row><row><cell>MSNet [22]</cell><cell>64.7</cell><cell>89.4</cell><cell>-</cell><cell>-</cell></row><row><cell>TEA [23]</cell><cell>65.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ViViT-L/16x2 [3]</cell><cell>65.4</cell><cell>89.8</cell><cell>4 ? 3</cell><cell>11,892</cell></row><row><cell>X-ViT (8?) (Ours)</cell><cell>64.4</cell><cell>89.3</cell><cell>1 ? 3</cell><cell>425</cell></row><row><cell>X-ViT (16?) (Ours)</cell><cell>66.2</cell><cell>90.6</cell><cell>1 ? 3</cell><cell>850</cell></row><row><cell>X-ViT* (16?) (Ours)</cell><cell>67.2</cell><cell>90.8</cell><cell>1 ? 3</cell><cell>850</cell></row><row><cell>X-ViT (32?) (Ours)</cell><cell>66.4</cell><cell>90.7</cell><cell>1 ? 3</cell><cell>1,270</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Comparison with state-of-the-art on Epic-100. T ? is the #frames used by our method. Results for other methods are taken from<ref type="bibr" target="#b0">[1]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">Action Verb Noun</cell></row><row><cell>TSN [36]</cell><cell>33.2</cell><cell>60.2</cell><cell>46.0</cell></row><row><cell>TRN [45]</cell><cell>35.3</cell><cell>65.9</cell><cell>45.4</cell></row><row><cell>TBN [20]</cell><cell>36.7</cell><cell>66.0</cell><cell>47.2</cell></row><row><cell>TSM [20]</cell><cell>38.3</cell><cell>67.9</cell><cell>49.0</cell></row><row><cell>SlowFast [14]</cell><cell>38.5</cell><cell>65.6</cell><cell>50.0</cell></row><row><cell>ViViT-L/16x2 [1]</cell><cell>44.0</cell><cell>66.4</cell><cell>56.8</cell></row><row><cell>X-ViT (8?) (Ours)</cell><cell>41.5</cell><cell>66.7</cell><cell>53.3</cell></row><row><cell>X-ViT (16?) (Ours)</cell><cell>44.3</cell><cell>68.7</cell><cell>56.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">More precisely, Eq. 6 holds for h = 1 heads. For h &gt; 1, the different heads? l,h s,t are concatenated and projected to produce? l s,t .</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09849</idno>
		<title level="m">The best of both worlds: Combining recent advances in neural machine translation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11579</idno>
		<title level="m">A2-nets: Double attention networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13256</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Rescaling egocentric vision. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">More is less: Learning efficient video representations by big-little network and depthwise temporal aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pistoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00869</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5842" to="5850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Epic-fusion: Audio-visual temporal binding for egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5492" to="5501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Motionsqueeze: Neural motion feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="345" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06803</idno>
		<title level="m">Temporal adaptive module for video recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<title level="m">An imperative style, high-performance deep learning library</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with local and global diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12056" to="12065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5552" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video modeling with correlation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2740" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attentionnas: Spatiotemporal attention cell search for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="449" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shift: A zero flop, zero parameter alternative to spatial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Golmant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholaminejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A multigrid method for efficiently training video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
