<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optical Flow Based Motion Detection for Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Man</forename><surname>Lo</surname></persName>
							<email>kamanphoebe@gmail.com</email>
						</author>
						<title level="a" type="main">Optical Flow Based Motion Detection for Autonomous Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motion detection is a fundamental but challenging task for autonomous driving. In particular scenes like highway, remote objects have to be paid extra attention for better controlling decision. Aiming at distant vehicles, we train a neural network model to classify the motion status using optical flow field information as the input. The experiments result in high accuracy, showing that our idea is viable and promising. The trained model also achieves an acceptable performance for nearby vehicles. Our work is implemented in PyTorch. Open tools including nuScenes, FastFlowNet and RAFT are used. Visualization videos are available at https://www.youtube.com/playlist? list=PLVVrWgq4OrlBnRebmkGZO1iDHEksMHKGk .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Introduction</head><p>Motion detection, or moving object detection, is a computer vision related technique for detecting the physical movement of an object relative to its background. It is widely used in various areas like smart homes, surveillance and security, and also plays a crucial role in autonomous driving. To make better future plan on controlling during driving, vehicles need to monitor the road condition well. Careful inspection for faraway environment is required for scenes that allow high-speed driving like highways or quiet roads. However, the perception range of lidar and radar sensors are not always far enough to cover distant objects and thus computer vision based methods should be applied under these circumstances. Traditional methods of motion detection rely on the difference of pixels between frames. Therefore, detecting motion in the distance, especially those in radial direction, is a challenging issue since they are usually just a few pixels changes.</p><p>Optical flow estimation is a commonly used technique in motion detection tasks for providing velocity information. It is calculated based on the brightness constancy constraint, supposing the timestamps of two consecutive frames are close enough that the brightness of the same positions in real world will remain unchanged. In this paper, we use different algorithms to obtain optical flow field information of vehicles in between 30 to 70 meters from the nuScenes <ref type="bibr" target="#b1">[2]</ref> dataset, and feed them into neural network ResNet18 <ref type="bibr" target="#b3">[4]</ref> as inputs. The model then outputs the binary prediction of motion status, i.e., still or moving. Our experiments show that the moving targets are successfully detected with a high correct rate. We also use the trained model to infer nearby vehicles and obtain a reasonable accuracy.</p><p>The rest of the paper is organized as follows: Section II gives a brief review of relevant works. Section III demonstrates the framework of our work, followed by the experimental details and results in Section IV. Finally, conclusions and possible future work are presented in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Related Work</head><p>In this section, basic approaches regarding motion detection are firstly reviewed, together with specific topics relevant to our work, namely small object motion detection and autonomous driving. After that, we dive deeper into optical flow algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Motion detection</head><p>Traditional methods used in detecting motion can be mainly divided into four categories: background subtraction, frame difference, temporal difference and optical flow estimation. A previous review done by <ref type="bibr">Manchanda and Sharma [7]</ref> includes works using these approaches to detect motion for general purposes. The works they list were published from 2009 to 2015. There is also some afterwards improvement based on the basic methods in recent years, such as <ref type="bibr" target="#b14">[15]</ref> <ref type="bibr" target="#b13">[14]</ref>[1] <ref type="bibr" target="#b4">[5]</ref>.</p><p>For recent works about motion detection in autonomous driving, both <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b12">[13]</ref> make use of CRF related model. The former targets at a specific range while the latter jointly feeds disparity map and optical flow field as model input. Our work is inspired by <ref type="bibr" target="#b8">[9]</ref>. Yet we concentrate solely on motion classification for bounding boxes so far while their work combines object detection and motion segmentation. Besides, we exploit the original optical flow information instead of converting it to RGB images so as to prevent normalization in this process and preserve numerical precision. More details about our implementation are stated in Section III.</p><p>Focusing on the topic of small object motion detection, existing works usually focus on insects, like <ref type="bibr" target="#b11">[12]</ref> <ref type="bibr" target="#b9">[10]</ref>, which entirely differ from autonomous driving in appearance and background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optical flow</head><p>FastFlowNet <ref type="bibr" target="#b5">[6]</ref> and RAFT <ref type="bibr" target="#b10">[11]</ref> achieve state-of-the-art speed and accuracy respectively for estimating optical flow field. FastFlowNet is 10? faster than RAFT while RAFT obtains a F1 error of 5.10% on the KITTI <ref type="bibr" target="#b2">[3]</ref> dataset, which is only half of the value of FastFlowNet. The two algorithms are used and are compared with each other in our work. Example inferences of FastFlowNet and RAFT using the same raw image pair are depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Method</head><p>The framework of our work is presented in this section, starting with the pipeline and followed by details of labeling and data preprocessing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pipeline</head><p>The overview of our work is outlined below: 1. Select keyframe pairs that contain target vehicles from nuScenes 2. Generate optical flow field for all keyframe pairs via Fast-FlowNet or RAFT 3. Label the objects as still or moving by estimating their velocity 4. Extract optical flow information of objects within the modified 2D bounding boxes after some preprocessing and feed them into the neural network 5. Train a binary classifier from scratch using ResNet18 along with some necessary adjustments of layers B. Labeling Data of 2D bounding box and binary motion ground truth are recorded in every label. The former is marked by the coordinates xmin, xmax, ymin and ymax, which are simply deduced from the eight corners of the original 3D bounding box by picking the minimum and the maximum of x and y.</p><p>The motion ground truth is decided based on the velocity calculated as</p><formula xml:id="formula_0">= 2 ? 1 2 ? 1<label>(1)</label></formula><p>where is given with respect to the global coordinate system. If the absolute value of velocity | | ? 2 m/s, then the object is marked as moving or else it is still.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data preprocessing</head><p>To determine whether an object is moving, we need the optical flow information not only of the object itself, but also of the background around. Therefore, some preprocessing has to be done on the 2D bounding box before inputting to the network, as mentioned in the 4th step of the pipeline. First, the box is reshaped to be a square with side length ( ?, ? ? ). Then triple the length of sides and if needed, pad the box with edge values. Finally, cut off data outside the box and resize it to 224 ? 224 using bilinear interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Experiments</head><p>In this section, we first detail the dataset used and experimental setup. Then the results are discussed. The construction and evaluation of a generalized dataset are presented at last. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>Our model is trained and evaluated on the filtered nuScenes <ref type="bibr" target="#b1">[2]</ref> dataset. nuScenes comprises 1000 diverse scenes, covering different locations, time and weather conditions. For simplicity, we exclude scenes of "night", "rain" and "lightning", thereby 604 scenes are remained. We then collect keyframe pairs that contain any of the seven types of vehicles within the specific distance and visibility range as shown in <ref type="table" target="#tab_0">Table 1</ref>. After that, optical flow field of the frame pairs is calculated through Fast-FlowNet or RAFT, and is saved as .npy file. As a result, we obtain 18460 objects in total, while 16467 of them used as training set and 1993 for evaluation. Considering the amount of data is rather small, random horizontal flip with probability 0.5 is performed for data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental setup</head><p>The model architecture we used is chosen to be ResNet18 <ref type="bibr" target="#b3">[4]</ref>. However, since the input is not in the form of RGB images, we have to train the model from scratch instead of applying a pretrained model. Therefore, the number of output channels of the first convolutional layer is modified to be 64 to make the network adapt to our input. The size of final output is also changed to 1, which should be a number within [0, 1]. If the output value greater than 0.5, the object will be classified as moving, otherwise it will be still. The rest of the model structure stay unchanged. <ref type="table" target="#tab_1">Table 2</ref> lists the settings of hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental result</head><p>The accuracy of our model is greater than 90%, as shown in <ref type="table" target="#tab_2">Table 3</ref>. The model trained with FastFlowNet input unexpect-   edly has better performance than the one with input generated by RAFT, which achieves state-of-the-art accuracy for optical flow estimation. Note that it is not fair to compare our results directly with other motion detection methods, since we simplify the data a lot. Nonetheless, the pretty values still reflect the feasibility of our idea of work. Predictions are visualized for intuitive understanding of the model performance. Several correct and incorrect inferences are depicted in <ref type="figure" target="#fig_1">Figure 2</ref>. We sum up two main reasons for the wrong classifications:</p><p>? Unclear optical flow for remote or slow objects. These kinds of objects are always tough to deal with because of the tiny difference in distance in the visual world, hence the unclear flow which confuses the network. ? Being affected by the background or foreground. As mentioned in Section III, the surrounding flow information is also included as part of the input. Therefore, it could be misleading when there are some other objects involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Generalization</head><p>We extend our filtered dataset for inference by adding nonkeyframes and nearby objects from nuScenes. To start with, optical flow is calculated for frame pairs at 4-frame intervals. Since there is no annotation for non-keyframes in nuScenes, bounding box positions and ground truths of objects are estimated based on the information of their corresponding keyframes. The former are calculated by linear interpolation while the latter remain the same as the closest previous keyframes with respect to the non-keyframes. Vehicles closer than 30 meters, but with visibility requirements as before, are also taken into account. Evaluating on this generalized dataset, the F-score of our model significantly drops to 60%, as shown in <ref type="table" target="#tab_3">Table 4</ref>. Our visualization videos contain inference of generalization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. Conclusion and future work</head><p>In this paper, we have investigated the effect of binary motion classification for annotated remote vehicles by inputting optical flow information into a neural network. The experimental result reports that our model can successfully detect the motion and the high accuracy illustrates the great potential of our idea. Cases failed to be correctly inferred are mainly caused by unclear optical flow and misleading surrounding flow information. Our trained model is not so applicable to nearby objects yet the performance might be dramatically enhanced if the model is trained with them. There is still much room for improvement for our work:</p><p>? Remove the strict rules for filtering data to adapt to concrete use ? Train the optical flow model from scratch (by selfsupervised learning), rather than apply pretrained models ? Construct an end-to-end classification network architecture, leaving the middle stages regarding generating optical flow field to be implicit</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of optical flow predictions on the nuScenes dataset. From left to right: the preceding raw image of a keyframe pairs, flow visualization of FastFlowNet and RAFT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) Correct predictions (b) Incorrect predictions due to unclear flow (c) Incorrect predictions due to misleading surround Visualization of inference using the model trained with FastFlowNet input. Blue boxes and red boxes represent still and moving objects respectively. These are the frames of the generalized dataset so nearby objects are also detected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Settings for filtering the nuScenes dataset.</figDesc><table><row><cell>Setting</cell><cell>Description</cell></row><row><cell></cell><cell>vehicle.car</cell></row><row><cell></cell><cell>vehicle.emergency.ambulance</cell></row><row><cell></cell><cell>vehicle.emergency.police</cell></row><row><cell>Target categories</cell><cell>vehicle.truck</cell></row><row><cell></cell><cell>vehicle.bus.bendy</cell></row><row><cell></cell><cell>vehicle.bus.rigid</cell></row><row><cell></cell><cell>vehicle.construction</cell></row><row><cell>Distance</cell><cell>30m -70m</cell></row><row><cell>Visibility</cell><cell>80% -100%</cell></row><row><cell>Sensor</cell><cell>CAM_FRONT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Hyperparameter setting for ResNet18 model training.</figDesc><table><row><cell>Function</cell><cell cols="2">Hyperparameter Setting</cell></row><row><cell>Dataloader</cell><cell>Batch size</cell><cell>128</cell></row><row><cell></cell><cell>Algorithm</cell><cell>SGD</cell></row><row><cell>Optimizer</cell><cell>Learning rate Weight decay</cell><cell>0.01 0.01</cell></row><row><cell></cell><cell>Momentum</cell><cell>0.9</cell></row><row><cell></cell><cell>Schedule</cell><cell>StepLR</cell></row><row><cell>Scheduler</cell><cell>Step size</cell><cell>10</cell></row><row><cell></cell><cell>Gamma</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Quantitative performance of models with optical flow input generated by various algorithms.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">The pretrained models</cell></row><row><cell cols="4">of both optical flow algorithms are trained on the KITTI [3]</cell></row><row><cell>dataset.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Optical flow algorithm</cell><cell>F1 (%)</cell><cell>Precision (%)</cell><cell>Recall (%)</cell></row><row><cell>FastFlowNet</cell><cell>92.9</cell><cell>94.3</cell><cell>91.7</cell></row><row><cell>RAFT</cell><cell>89.5</cell><cell>89.7</cell><cell>89.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Quantitative performance of evaluation on the generalized dataset. The pretrained model of optical flow algorithm is trained on KITTI.</figDesc><table><row><cell>Optical flow algorithm</cell><cell>F1 (%)</cell><cell>Precision (%)</cell><cell>Recall (%)</cell></row><row><cell>FastFlowNet</cell><cell>60.4</cell><cell>63.0</cell><cell>62.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The code for generating optical flow field information is based on the corresponding original projects, FastFlowNet and RAFT.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Moving object detection system based on the modified temporal difference and otsu algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oussama</forename><surname>Boufares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Boussif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureddine</forename><surname>Aloui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 18th International Multi-Conference on Systems, Signals Devices (SSD)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1378" to="1382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Be?bom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11027</idno>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
			<publisher>?RR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An efficient optical flow based motion detection method for non-stationary scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiagang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Control And Decision Conference (CCDC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5272" to="5277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fastflownet: A lightweight network for fast optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingtong</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analysis of computer vision based techniques for motion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumati</forename><surname>Manchanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanu</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference -Cloud System and Big Data Engineering (Confluence)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="445" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Motion object detection and tracking optimization in autonomous vehicles in specific range with optimized deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahimeh</forename><surname>Nezhadalinaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Mahdizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faezeh</forename><surname>Jamshidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 7th International Conference on Web Research (ICWR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="53" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modnet: Motion and appearance based moving object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mennatullah</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heba</forename><surname>Mahgoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Zahran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>El-Sallab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2859" to="2864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A method for detection of small moving objects in uav videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladan</forename><surname>Stojni?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Risojevi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Mu?tra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedran</forename><surname>Jovanovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janja</forename><surname>Filipi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Kezi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zdenka</forename><surname>Babi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">RAFT: recurrent all-pairs field transforms for optical flow. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A feedback neural network for small target motion detection in cluttered backgrounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jigen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigang</forename><surname>Yue</surname></persName>
		</author>
		<idno>abs/1805.00342</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense scene flow based coarse-to-fine rigid moving object detection for autonomous vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongtong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="23492" to="23501" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Moving object detection based on improved three frame difference and background subtraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxiong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Industrial Informatics -Computing Technology, Intelligent Technology, Industrial Information Integration (ICIICII)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="79" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Moving target detection based on improved gaussian mixture background subtraction in video images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Kasabov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="152612" to="152623" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
