<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CYCLEMLP: A MLP-LIKE ARCHITECTURE FOR DENSE PREDICTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoufa</forename><surname>Chen</surname></persName>
							<email>shoufach@connect.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
							<email>xieenze@connect.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjian</forename><surname>Ge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runjian</forename><surname>Chen</surname></persName>
							<email>rjchen@connect.hku.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
							<email>liangding@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<email>pluo@cs.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CYCLEMLP: A MLP-LIKE ARCHITECTURE FOR DENSE PREDICTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a simple MLP-like architecture, CycleMLP, which is a versatile backbone for visual recognition and dense predictions. As compared to modern MLP architectures, e.g. , MLP-Mixer (Tolstikhin et al., 2021), ResMLP <ref type="bibr" target="#b49">(Touvron et al., 2021a)</ref>, and gMLP <ref type="bibr" target="#b35">(Liu et al., 2021a)</ref>, whose architectures are correlated to image size and thus are infeasible in object detection and segmentation, CycleMLP has two advantages compared to modern approaches. (1) It can cope with various image sizes. (2) It achieves linear computational complexity to image size by using local windows. In contrast, previous MLPs have O(N 2 ) computations due to fully spatial connections. We build a family of models which surpass existing MLPs and even state-of-the-art Transformer-based models, e.g. Swin Transformer <ref type="bibr" target="#b36">(Liu et al., 2021b)</ref>, while using fewer parameters and FLOPs. We expand the MLP-like models' applicability, making them a versatile backbone for dense prediction tasks. CycleMLP achieves competitive results on object detection, instance segmentation, and semantic segmentation. In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mIoU on ADE20K dataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot robustness on ImageNet-C dataset. Code is available at https: //github.com/ShoufaChen/CycleMLP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Vision models in computer vision have been long dominated by convolutional neural networks (CNNs) <ref type="bibr" target="#b28">(Krizhevsky et al., 2012;</ref><ref type="bibr">He et al., 2016)</ref>. Recently, inspired by the successes in Natural Language Processing (NLP) field, Transformers <ref type="bibr" target="#b51">(Vaswani et al., 2017</ref>) are adopted into the computer vision community. Built with self-attention layers, multi-layer perceptrons (MLPs), and skip connections, Transformers make numerous breakthroughs on visual tasks <ref type="bibr" target="#b26">(Dosovitskiy et al., 2020;</ref><ref type="bibr" target="#b36">Liu et al., 2021b)</ref>. More recently, <ref type="bibr" target="#b47">(Tolstikhin et al., 2021;</ref><ref type="bibr" target="#b35">Liu et al., 2021a)</ref> have validated that building models solely on MLPs and skip connections without the self-attention layers can achieve surprisingly promising results on ImageNet <ref type="bibr" target="#b24">(Deng et al., 2009)</ref>  Despite promising results on visual recognition tasks, these MLP-like models can not be used in dense prediction tasks (e.g., object detection and semantic segmentation) due to the three challenges: (1) Current models are composed of blocks with non-hierarchical architectures, which make the model infeasible to provide pyramid and high-resolution feature representations.</p><p>(2) Current models cannot deal with flexible input scales due to the Spatial FC as shown in <ref type="figure" target="#fig_0">Figure 1b</ref>. The spatial FC is configured by an image-size related weight 1 . Thus, this  <ref type="bibr" target="#b47">(Tolstikhin et al., 2021;</ref><ref type="bibr" target="#b49">Touvron et al., 2021a;</ref><ref type="bibr" target="#b35">Liu et al., 2021a</ref>) has a global receptive field in the spatial dimension. However, its parameter size is fixed and it has quadratic computational complexity to image scale. <ref type="bibr">(c)</ref> Our proposed Cycle Fully-Connected Layer (Cycle FC) has linear complexity the same as channel FC and a larger receptive field than Channel FC. denotes the output position. For simplicity, we omit batch dimension and set the feature's width to 1 here for example. Several more general cases can be found in <ref type="figure">Figure 7</ref> (Appendix G). Best viewed in color. structure typically requires the input image with a fixed scale during both the training and inference procedure. It contradicts the requirements of dense prediction tasks, which usually adopt a multi-scale training strategy <ref type="bibr" target="#b15">(Carion et al., 2020)</ref> and different input resolutions in training and inference stages <ref type="bibr" target="#b33">(Lin et al., 2014;</ref><ref type="bibr" target="#b21">Cordts et al., 2016)</ref>.</p><formula xml:id="formula_0">? (a) Channel FC ? (b) Spatial FC ? (c) Cycle FC Stepsize ! 0 =-1 ! 1 =0 ! 2 =1 ! 7 =-1 ! 8 =0 ! = 3, " = 1 ! = , " = 1 ! = 1, " = 1 ! 9 =1 (d) (e) (f)</formula><p>(3) The computational and memory costs of the current MLP models are quadratic to input image sizes for dense prediction tasks (e.g., COCO benchmark <ref type="bibr" target="#b33">(Lin et al., 2014)</ref>).</p><p>To address the first challenge, we construct a hierarchical architecture to generate pyramid features. For the second and third issues, we propose a novel variant of fully connected layer, named as Cycle Fully-Connected Layer (Cycle FC), as illustrated in <ref type="figure" target="#fig_0">Figure 1c</ref>. The Cycle FC is capable of dealing with various image scales and has linear computational complexity to image size.</p><p>Our Cycle FC is inspired by Channel FC layer illustrated in <ref type="figure" target="#fig_0">Figure 1a</ref>, which is designed for channel information communication <ref type="bibr" target="#b32">(Lin et al., 2013;</ref><ref type="bibr" target="#b45">Szegedy et al., 2015;</ref><ref type="bibr">He et al., 2016;</ref><ref type="bibr">Howard et al., 2017)</ref>. The main merit of Channel FC lies in that it can deal with flexible image sizes since it is configured by image-size agnostic weight of C in and C out . However, the Channel FC is infeasible to aggregate spatial context information due to its limited receptive field.</p><p>Our Cycle FC is designed to enjoy Channel FC's merit of taking input with arbitrary resolution and linear computational complexity while enlarging its receptive field for context aggregation. Specifically, Cycle FC samples points in a cyclical style along the channel dimension ( <ref type="figure" target="#fig_0">Figure 1c</ref>). In this way, Cycle FC has the same complexity (both the number of parameters and FLOPs) as channel FC while increasing the receptive field simultaneously. To this end, we adopt Cycle FC to replace the Spatial FC for spatial context aggregation (i.e., token mixing) and build a family of MLP-like models for both recognition and dense prediction tasks.</p><p>The contributions of this paper are as follows: (1) We propose a new MLP-like operator, Cycle FC, which is computational friendly to cope with flexible input resolutions.</p><p>(2) We take the first attempt to build a family of hierarchical MLP-like architectures (CycleMLP) based on Cycle FC operator for dense prediction tasks.</p><p>(3) Extensive experiments on various tasks (e.g., ImageNet classification, COCO object instance detection, and segmentation, and ADE20K semantic segmentation) demonstrate that CycleMLP outperforms existing MLP-like models and is comparable to and sometimes better than CNNs and Transformers on dense predictions.  <ref type="figure">Figure 2</ref>: ImageNet accuracy v.s. model capacity.</p><p>All models are trained on ImageNet-1K <ref type="bibr" target="#b24">(Deng et al., 2009</ref>) without extra data. CycleMLP surpasses existing MLP-like models such as MLP-Mixer <ref type="bibr" target="#b47">(Tolstikhin et al., 2021)</ref>, ResMLP <ref type="bibr" target="#b49">(Touvron et al., 2021a)</ref>, gMLP <ref type="bibr" target="#b35">(Liu et al., 2021a)</ref>, S 2 -MLP  and <ref type="bibr">ViP (Hou et al., 2021)</ref>.</p><p>Related Work. Convolution Neural Networks (CNNs) has dominated the visual backbones for several years <ref type="bibr" target="#b28">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b42">Simonyan &amp; Zisserman, 2014;</ref><ref type="bibr">He et al., 2016)</ref>. <ref type="bibr" target="#b26">(Dosovitskiy et al., 2020)</ref> introduced the first pure Transformer-based <ref type="bibr" target="#b51">(Vaswani et al., 2017)</ref> model into computer vision and achieved promising performance, especially pre-trained on the large scale JFT dataset. Recently, some works <ref type="bibr" target="#b47">(Tolstikhin et al., 2021;</ref><ref type="bibr" target="#b49">Touvron et al., 2021a;</ref><ref type="bibr" target="#b35">Liu et al., 2021a)</ref> removed the attention in Transformer and proposed pure MLP-based models. Please see Appendix A for a comprehensive review of the literature on the visual backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHOD</head><p>In this section, we introduce CycleMLP models for vision tasks including recognition and dense predictions. To begin with, in Sec. 2.1 we formulate our proposed novel operator, Cycle FC, which serves as a basic component for building CycleMLP models. Then we compare Cycle FC with Channel FC and multi-head attention adopted in recent Transformer-based models <ref type="bibr" target="#b26">(Dosovitskiy et al., 2020;</ref><ref type="bibr" target="#b48">Touvron et al., 2020;</ref><ref type="bibr" target="#b36">Liu et al., 2021b)</ref> in Sec. 2.2. Finally, we present the detailed configurations of CycleMLP models in Sec. 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CYCLE FULLY-CONNECTED LAYER</head><p>Notation. We denote an input feature map as X ? R H?W ?Cin , where H, W denote the height and width of the image and C in is the number of feature channels. We use subscripts to index the feature map. For example, X i,j,c is the value of c th channel at the spatial position (i, j) and X i,j,: are values of all channels at the spatial (i, j).</p><p>The motivation behind Cycle FC is to enlarge receptive field of MLP-like models to cope with downstream dense prediction tasks while maintaining the computational efficiency. As illustrated in <ref type="figure" target="#fig_0">Figure 1a</ref>, Channel FC applies weighting matrix on X along the channel dimension on fixed position (i, j). However, Cycle FC introduces a receptive field of (S H , S W ), where S H and S W are stepsize along with the height and width dimension respectively (illustrated in <ref type="figure" target="#fig_0">Figure 1 (d)</ref>). The basic Cycle FC operator can be formulated as below:</p><formula xml:id="formula_1">CycleFC(X) i,j,: = Cin c=0 X i+?i(c),j+?j (c),c ? W mlp c,: + b<label>(1)</label></formula><p>where W mlp ? R Cin?Cout and b ? R Cout are parameters of Cycle FC. ? i (c) and ? j (c) are the spatial offset of the two axis on the c th channel, which are defined as below:</p><formula xml:id="formula_2">? i (c) = (c mod S H ) ? 1, ? j (c) = ( c S H mod S W ) ? 1<label>(2)</label></formula><p>Examples. We provide several examples <ref type="figure" target="#fig_0">(Figure 1 (d</ref>)-(f)) to illustrate the stepsize. For the sake of visualization convenience, we set the tensor's W = 1. Thus, these three examples naturally all have S W = 1. <ref type="figure" target="#fig_0">Figure 1 (d)</ref> illustrates the offsets along two axis when S H = 3, that is ? j (c) ? 0 and ? i (c) = {?1, 0, 1, ?1, 0, 1, ? ? ? } when c = 0, 1, 2, ? ? ? , 8. <ref type="figure" target="#fig_0">Figure 1</ref> (e) shows that when S H = H, Cycle FC has a global receptive field. <ref type="figure" target="#fig_0">Figure 1</ref> (f) shows that when S H = 1, there will be no offset along either axis and thus Cycle FC degrades to Channel FC <ref type="figure" target="#fig_0">(Figure 1 (a)</ref>). We also provide a more general case where W = 1 and S H = 3, S W = 3 in <ref type="figure">Figure 7</ref> (Appendix).</p><p>The offsets ? i (c) and ? j (c) enlarge the receptive field of Cycle FC as compared to Channel FC <ref type="figure" target="#fig_0">(Figure 1a</ref>), which applies weights solely on the same spatial position for all channels. The larger receptive field in return brings improvements on dense prediction tasks like semantic segmentation and object detection as shown in <ref type="table">Table 1</ref>. Meanwhile, Cycle FC still maintains computational efficiency and flexibility on input resolution. Both the FLOPs and the number of parameters are linear to the spatial scale which are exactly the same as those of Channel FC. In contrast, although Spatial FC has a global receptive field over the whole spatial space, its computational cost is quadratic to the image scale. Besides, it fails to handle inputs with different resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">COMPARISON BETWEEN MULTI-HEAD SELF-ATTENTION (MHSA) AND CYCLE FC</head><p>Inspired by <ref type="bibr" target="#b20">Cordonnier et al. (2020)</ref>, when re-parametried properly, a multi-head self-attention layer with N h heads can be formulated as below, which is similar to a convolution with kernel size ? N h ? ? N h . (Please refer to Appendix C for detailed derivation)  (4) equation 4 shows that only the weights of W mhsa on spatial shift (? i (c) + 1, ? j (c) + 1) are taken into account in W mlp . This indicates that Cycle FC introduce an inductive bias that the weighting matrix in MHSA should be sparse. Thus Cycle FC inherits the large receptive field introduced in MHSA.</p><formula xml:id="formula_3">MHSA(X) i,j,: = h?{1,2,...,N h } X i+?i(h),j+?j (h),: W mhsa,h + b (3) where W mhsa,h ? R Cin?Cout is the parameter matrix for h th head in MHSA. b ? R Cout is the bias vector. {? i (h), ? j (h)} = {(0, 0), (1, 0), (?1, 0), ? ? ? } contains all possible positional shift in convolution with kernel size ? N h ? ? N h .</formula><p>The receptive field in Cycle FC is enlarged to (S H , S W ), which enables Cycle FC to tackle with downstream dense prediction tasks better. Meanwhile, with the sparsity inductive bias, Cycle FC maintains computational efficiency in MLP-based methods as compared to convolution and multihead self-attention. The parameter size in Cycle FC is C in ?C out while W mhsa ? R K?K?Cin?Cout .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">OVERALL ARCHITECTURE</head><p>Patch Embedding. Given the raw input image with the size of H ? W ? 3, our model first splits it into patches by a patch embedding module <ref type="bibr" target="#b26">(Dosovitskiy et al., 2020)</ref>. Each patch is then treated as a "token". Specifically, we follow <ref type="bibr">(Fan et al., 2021;</ref><ref type="bibr" target="#b53">Wang et al., 2021a)</ref> to adopt an overlapping patch embedding module with the window size 7 and stride 4. These raw patches are further projected to a higher dimension (denoted as C) by a linear embedding layer. Therefore, the overall patch embedding module generates the features with the shape of H 4 ? W 4 ? C. CycleMLP Block. Then, we sequentially apply several Cycle FC Bloc blocks. Comparing with the previous MLP blocks <ref type="bibr" target="#b47">(Tolstikhin et al., 2021;</ref><ref type="bibr" target="#b49">Touvron et al., 2021a;</ref><ref type="bibr" target="#b35">Liu et al., 2021a)</ref> visualized in <ref type="figure" target="#fig_5">Figure 5</ref> (Appendix), the key difference of Cycle FC block is that it utilizes our proposed Cycle Fully-Connected Layer (Cycle FC) for spatial projection and advances the models in context aggregation and information communication. Specifically, the Cycle FC block consists of three parallel Cycle FCs, which have stepsizes S H ? S W of 1 ? 7, 7 ? 1, and 1 ? 1. This design is inspired by the factorization of convolution <ref type="bibr" target="#b46">(Szegedy et al., 2016</ref>) and criss-cross attention <ref type="bibr">(Huang et al., 2019)</ref>. Then, there is a channel-MLP with two linear layers and a <ref type="bibr">GELU (Hendrycks &amp; Gimpel, 2016)</ref> non-linearity in between. A LayerNorm (LN) <ref type="bibr" target="#b12">(Ba et al., 2016)</ref>     <ref type="bibr">, 2016)</ref>. The number of tokens (feature scale) is maintained within each stage. At each stage transition, the channel capacity of the processed tokens is expanded while the number of tokens is reduced. This strategy effectively reduces the spatial resolution complexity. Overall, each of our model variants has four stages, and the output feature at the last stage has a shape of H 32 ? W 32 ? C 4 . These stage settings are widely utilized in both CNN <ref type="bibr" target="#b42">(Simonyan &amp; Zisserman, 2014;</ref><ref type="bibr">He et al., 2016)</ref> and Transformer <ref type="bibr" target="#b54">(Wang et al., 2021b;</ref><ref type="bibr" target="#b36">Liu et al., 2021b)</ref> models. Therefore, CycleMLP can conveniently serve as a general-purpose visual backbone and a generic replacement for existing backbones.</p><p>Model Variants. The design principle of the model's macro structure is mainly inspired by the philosophy of hierarchical Transformer <ref type="bibr" target="#b54">(Wang et al., 2021b;</ref><ref type="bibr" target="#b36">Liu et al., 2021b)</ref> models, which reduce the number of tokens at the transition layers as the network goes deeper and increase the channel dimension. In this way, we can build a hierarchical architecture that is critical for dense prediction tasks <ref type="bibr" target="#b33">(Lin et al., 2014;</ref><ref type="bibr" target="#b70">Zhou et al., 2017)</ref>. Specifically, we build two model zoos following two widely used Transformer architectures, PVT <ref type="bibr" target="#b54">(Wang et al., 2021b)</ref> and Swin <ref type="bibr" target="#b36">(Liu et al., 2021b)</ref>. Models in PVT-style are named from CycleMLP-B1 to CycleMLP-B5 and in Swin-Style are named as CycleMLP-T, -S, and -B, which represent models in tiny, small, and base sizes. These models are built by adapting several architecture-related hyper-parameters, including S i , C i , E i , and L i which represent the stride of the transition, the token channel dimension, the number of blocks, and the expansion ratio respectively at Stage i. Detailed configurations of these models are in <ref type="table">Table 11</ref> (Appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>In this section, we first examine CycleMLP by conducting experiments on ImageNet-1K <ref type="bibr" target="#b24">(Deng et al., 2009</ref>) image classification. Then, we present a bunch of baseline models achieved by Cy-cleMLP in dense prediction tasks, i.e., COCO <ref type="bibr" target="#b33">(Lin et al., 2014)</ref> object detection, instance segmentation, and ADE20K <ref type="bibr" target="#b70">(Zhou et al., 2017)</ref> semantic segmentation.    All models are trained on 224?224 and evaluated on various resolutions without fine-tuning. Left: Absolute top-1 accuracy; Right: Accuracy difference relative to that tested on 224?224. The superiority of CycleMLP's robustness becomes more significant when scale varies to a greater extent. <ref type="table" target="#tab_7">Table 4</ref> further details the ablation study on the structure of CycleMLP block. It is observed that the top-1 accuracy drops significantly after removing one of the three parallel branches, especially when discarding the 1?7 or 7?1 branch. To eliminate the probability that the fewer parameters and FLOPs cause the performance drop, we further use two same branches (denoted as "" in <ref type="table" target="#tab_7">Table 4</ref>) and one 1?1 branch to align the parameters and FLOPs. The accuracy still drops relative to CycleMLP, which further demonstrates the necessity of these three unique branches.</p><p>Resolution adaptability. One remarkable advantage of CycleMLP is that it can take arbitraryresolution images as input without any modification. On the contrary, GFNet <ref type="bibr" target="#b41">(Rao et al., 2021)</ref> needs to interpolate the learnable parameters on the fly when the input scale is different from the one for training. We compare the resolution adaptability by directly evaluating models at a broad spectrum of resolutions using the weight pre-trained on 224?224, without fine-tuning. <ref type="figure" target="#fig_3">Figure 3</ref> (left) shows that the absolute Top-1 accuracy on ImagNet and <ref type="figure" target="#fig_3">Figure 3 (right)</ref> shows the accuracy differences between one specific resolution and the resolution of 224?224. Compared with DeiT and GFNet, CycleMLP is more robust when resolution varies. In particular, at the 128?128, CycleMLP saves more than 2 points drop compared to GFNet. Furthermore, at higher resolution, the performance drop of CycleMLP is less than GFNet. Note that the superiority of CycleMLP becomes more significant when the resolution changes to a greater extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">OBJECT DETECTION AND INSTANCE SEGMENTATION</head><p>Settings. We conduct object detection and instance segmentation experiments on COCO <ref type="bibr" target="#b33">(Lin et al., 2014)</ref> dataset. We first follow the experimental settings of PVT <ref type="bibr" target="#b54">(Wang et al., 2021b)</ref>, which are introduced in Appendix. E.2. The corresponding results are presented in <ref type="table" target="#tab_9">Table 6</ref>. Then, in order to compare fairly with Swin Transformer, which adopts a different experimental recipe with PVT, we further follow the experimental settings of Swin with our CycleMLP-S model and the results are presented in <ref type="table" target="#tab_10">Table 7</ref>.    Results. As shown in  Moreover, we also visualized the receptive field following <ref type="bibr" target="#b58">(Xie et al., 2021)</ref>, and the results are visualized in <ref type="figure" target="#fig_4">Figure 4</ref>, which demonstrate that our CycleMLP has a larger effective receptive field than Swin.   <ref type="bibr" target="#b28">(Krizhevsky et al., 2012)</ref> errors is used as the robustness metric. The lower, the better.</p><formula xml:id="formula_4">Backbone RetinaNet 1? Mask R-CNN 1? Param AP AP50 AP75 APS APM APL Param AP b AP b 50 AP b 75 AP m AP m 50 AP m 75</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">ROBUSTNESS</head><p>We further conduct experiments on ImageNet-C (Hendrycks &amp; Gimpel, 2016) to analyze the robustness ability of the CycleMLP, following <ref type="bibr" target="#b38">(Mao et al., 2021)</ref> and results are presented in <ref type="table" target="#tab_15">Table 10</ref>. Compared with both Transformers (e.g. DeiT and Swin) and existing MLP models (e.g. MLP-Mixer, ResMLP, gMLP), CycleMLP achieves a stronger robustness ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>We present a versatile MLP-like architecture, CycleMLP, in this work. CycleMLP is built upon the Cycle Fully-Connected Layer (Cycle FC), which is capable of dealing with variable input scales and can serve as a generic, plug-and-play replacement of vanilla FC layers. Experimental results demonstrate that CycleMLP outperforms existing MLP-like models on ImageNet classification and achieves promising performance on dense prediction tasks, i.e., object detection, instance segmentation and semantic segmentation. This work indicates that an attention-free architecture can also serve as a general vision backbone.  <ref type="bibr">et al., 2017)</ref> connected each layer to every other layer in a feed-forward fashion, strengthening feature propagation and reducing the number of parameters. In parallel with these architecture design works, some other works also made significant contributions to the popularity of CNNs, including normalization <ref type="bibr">(Ioffe &amp; Szegedy, 2015;</ref><ref type="bibr" target="#b12">Ba et al., 2016)</ref>, data augmentation <ref type="bibr" target="#b22">(Cubuk et al., 2020;</ref><ref type="bibr" target="#b66">Yun et al., 2019;</ref><ref type="bibr" target="#b67">Zhang et al., 2017)</ref>, etc.</p><p>Transformer-based Models. Transformers were first proposed by Vaswani et al.for machine translation and have since become the dominant choice in many NLP tasks <ref type="bibr" target="#b25">(Devlin et al., 2018;</ref><ref type="bibr" target="#b52">Wang et al., 2018;</ref><ref type="bibr" target="#b14">Brown et al., 2020)</ref>. Recently, transformer have also led to a series of breakthroughs in computer vision community since the invention of ViT <ref type="bibr" target="#b26">(Dosovitskiy et al., 2020)</ref>, and have been working as a de facto standard for various tasks, e.g., image classification <ref type="bibr" target="#b26">(Dosovitskiy et al., 2020;</ref><ref type="bibr" target="#b48">Touvron et al., 2020;</ref>, detection and segmentation <ref type="bibr" target="#b54">(Wang et al., 2021b;</ref><ref type="bibr" target="#b36">Liu et al., 2021b;</ref><ref type="bibr" target="#b58">Xie et al., 2021)</ref>, video recognition <ref type="bibr">(Wang et al., 2021c;</ref><ref type="bibr" target="#b13">Bertasius et al., 2021;</ref><ref type="bibr" target="#b11">Arnab et al., 2021;</ref><ref type="bibr">Fan et al., 2021)</ref> and so on. Moreover, there has also been lots of interest in adopting transformer to cross aggregate multiple modality information <ref type="bibr" target="#b40">(Radford et al., 2021;</ref><ref type="bibr">Gabeur et al., 2020;</ref><ref type="bibr">Dzabraev et al., 2021)</ref>. Furthermore, combining CNNs and transformers is also explored in <ref type="bibr" target="#b43">(Srinivas et al., 2021;</ref><ref type="bibr" target="#b64">Li et al., 2021;</ref><ref type="bibr" target="#b56">Wu et al., 2021;</ref><ref type="bibr" target="#b50">Touvron et al., 2021b)</ref>.</p><p>MLP-based Models. MLP-based models <ref type="bibr" target="#b47">(Tolstikhin et al., 2021;</ref><ref type="bibr" target="#b49">Touvron et al., 2021a;</ref><ref type="bibr" target="#b35">Liu et al., 2021a</ref>) differ from the above discussed CNN-and Transformer-based models because they resort to neither convolution nor self-attention layers. Instead, they use MLP layers over feature patches on spatial dimensions to aggregate the spatial context. These MLP-based models share similar macro structures but differ from each other in the detailed design of the micro block. In addition, MLP-based models provide more efficient computation than transformer-based models since they do not need to calculate affinity matrix using key-query multiplication. Concurrent to our work, S 2 -MLP ) utilizes a spatial-shift operation for spatial information communication.</p><p>The similar aspect between our work and S 2 -MLP lies in that we all conduct MLP operations along the channel dimension. However, our Cycle FC is different from S 2 -MLP in: (1) S 2 -MLP achieves communications between patches by splitting feature maps along channel dimension into several groups and shifting different groups in different directions. It introduces extra splitting and shifting operations on the feature map. On the contrary, we propose a novel operator-Cycle Fully-Connected Layer-for spatial context aggregation. It does not modify the feature map and is formulated as a generic, plug-and-play MLP unit that can be used as a direct replacement of vanilla without any adjustments.</p><p>(2) We design a pyramid structure for and conduct extensive experiments on classification, object detection, instance segmentation, and semantic segmentation. However, the output feature map of S 2 -MLP has only one single scale in low resolution, which is unsuitable for dense prediction tasks. Only ImageNet classification is evaluated on S 2 -MLP. We compared Cycle FC with S 2 -MLP in details in the Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B COMPARISON OF MLP BLOCKS</head><p>We summary MLP blocks proposed by recent MLP-related works in <ref type="figure" target="#fig_5">Figure 5</ref>. We notice that existing MLP blocks, i.e., MLP-Mixer, ResMLP and gMLP share similar method of Spatial Proj: Transpose ? Fully-Connected over spatial dimension ? Transpose back. These models can not cope with variable image scales as the FC layers in Spatial Proj are configured by the seq len.</p><p>The blocks used for building CycleMLP consist of our proposed novel Cycle FC, whose configuration has nothing to do with image scales and can naturally deal with dynamic image scales. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C FROM MULTI-HEAD SELF-ATTENTION TO CONVOLUTION</head><p>In this section, we provide details in how MHSA can be transferred into a convolution-like operator in equation 3. To start with, the a MHSA layer can be formulated as below:</p><formula xml:id="formula_5">MHSA(X) = concat h?{1,...,N h } [SA h (X)]W out + b<label>(5)</label></formula><p>where W out ? R (N h Cout)?C out and b ? R C out are parameters for the final linear projection. SA h is the h th self-attention module. Then we reshape X into X ? R HW ?Cin and let T = H ? W , which indicates that there are T tokens in X. SA h can be defined as follow:</p><p>SA(X) t,: = softmax(A t,:</p><formula xml:id="formula_6">)V + b A = (Q + P )(K + P )<label>(6)</label></formula><p>where V = XW val , Q = XW qry , K = XW key are respectively the value, query and key matrix with learnable matrices W v ? R Cin?Cout , W q ? R Cin?C k , W k ? R Cin?C k . P ? R T ?Cin is the positional embedding matrix containing positional information for every input token, which can be replaced by the output of any function f P that encodes the position of tokens. And A ? R T ?T is the attention matrix where each element A i,j is the attention score between the i th and j th token in X.</p><p>With absolute positional encoding, the second line in equation 6 can be expanded as <ref type="bibr" target="#b20">(Cordonnier et al., 2020)</ref>:</p><p>A q,k =(Xq,: + Pq,:)W qry ((X k,: + P k,: )W key ) =Xq,:W qry (X k,: W key ) + Xq,:W qry (P k,: W key ) + Pq,:W qry (X k,: W key ) + Pq,:W qry (P k,: W key )</p><p>When we apply relative positional encoding scheme in , A is re-parametried into:</p><p>A q,k = X q,: W qry (X k,: W key ) +X q,: W qry (r ? q,k? key ) +u(X k,: W key ) +v(r ? q,k? key ) <ref type="formula">(8)</ref> where r ? q,k is a positional encoding for relative distance ? q,k = (? 1 , ? 2 ) between token q and k in X.</p><p>W key is introduced to only pertain to the positional encoding r q,k . u and v are learnable parameter vectors that replace the original P q,: W qry term, which implies that the attention bias remains the same regardless of the absolution positions of the query. If we set W qry = W key = 0 and W key = I, the first three terms in equation 8 vanish and A q,k = vr q,k . We set {? i (h), ? j (h)} = {(0, 0), (1, 0), (?1, 0), ? ? ? } contains all possible positional shift in convolution with kernel size</p><formula xml:id="formula_8">? N h ? ? N h . For each head h, let r q,k = ( ? q,k 2 , ? 1 , ? 2 ) and v h = ?? h (1, ?2? i (h), ?2? j (h))</formula><p>, each softmax attention matrix becomes: </p><formula xml:id="formula_9">softmax(A h ) q,k = 1 if ? q,k = (? i (h), ? j (h)) 0 otherwise<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ARCHITECTURE VARIANTS</head><p>In order to conduct fair and convenient comparison, we build two model zoos: the one is in PVT-Style (named as CycleMLP-B1 to -B5) and the other in Swin-Style (named as CycleMLP-T, -S and -B). These models are scaled up by adapting several architecture-related hyper-parameters, including S i , C i , E i and L i which represent the stride of the transition, the token channel dimension, the number of blocks and the expansion ratio respectively at Stage i. Detailed configurations of these models are in <ref type="table">Table 11</ref>.</p><formula xml:id="formula_10">E EXPERIMENTAL SETUPS E.1 IMAGENET CLASSIFICATION</formula><p>Settings. We train our models on the ImageNet-1K dataset <ref type="bibr" target="#b24">(Deng et al., 2009)</ref>, which contains 1.2M training images and 50K validation images evenly spreading 1,000 categories. We follow the standard practice in the community by reporting the top-1 accuracy on the validation set. Our code is implemented based on PyTorch <ref type="bibr" target="#b39">(Paszke et al., 2019)</ref> framework and heavily relies on the timm (Wightman, 2019) repository. For apple-to-apple comparison, our training strategy is mostly adopted from DeiT <ref type="bibr" target="#b48">(Touvron et al., 2020)</ref>, which includes RandAugment (Cubuk et al., 2020), Mixup <ref type="bibr" target="#b67">(Zhang et al., 2017)</ref>, Cutmix <ref type="bibr" target="#b66">(Yun et al., 2019)</ref> random erasing <ref type="bibr">(Zhong et al., 2020)</ref> and stochastic depth <ref type="bibr">(Huang et al., 2016)</ref>. The optimizer is AdamW <ref type="bibr" target="#b37">(Loshchilov &amp; Hutter, 2017)</ref> with the momentum of 0.9 and weight decay of 5?10 ?2 by default. The cosine learning rate schedule is adopted with the initial value of 1?10 ?3 . All models are trained for 300 epochs on 8 Tesla V100 GPUs with a total batch size of 1024.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F SAMPLING STRATEGIES</head><p>We explore more sampling strategies in this subsection, including random sampling and dilated sampling inspired by dilated convolution <ref type="bibr" target="#b62">(Yu &amp; Koltun, 2016;</ref> (as shown in <ref type="figure">Figure 6</ref>). We also compare the dense sampling method with ours.</p><p>Random sampling. As shown in <ref type="table" target="#tab_5">Table 13</ref>, we conduct experiments with random sampling for three independent trials and observe that the averaged Top-1 accuracy on ImageNet-1K drops by 1.3%. We hypothesize that the decreased performance is caused by the fact that random sampling will totally disturb the semantic information of objects, which is essential to image recognition. Compared with the random sampling strategy, our cyclical sampling is able to aggregate the adjacent pixels, which benefits in capturing the semantic information.</p><p>Dilated Stepsize ( <ref type="figure">Figure 6</ref>). As shown in <ref type="table" target="#tab_5">Table 13</ref>, we observe the result of dilated sampling is better than the random one (+1.0% acc) but lower than ours (?0.5% acc). In fact, compared with the random sampling, dilated solutions take their advantages in local information aggregation. However, compared with the cyclical sampling strategy, dilated solutions lose the fine-grained information for recognition. It may hurt the accuracy performance to some extent.</p><p>Dense sampling. we conduct ablation studies by using dense sampling strategies (i.e., vanilla convolution with kernel size 1?3 and 3?1). Since dense sampling strategies incredibly increase the models' parameters and FLOPs, we do not have enough time to thoroughly optimize the model for 300 epochs. Therefore, for fair comparisons, we conducted extra ablation studies on training models for 100 epochs with the strictly same learning configurations. The results shown in <ref type="table" target="#tab_7">Table 14</ref> demonstrate that the sparse sampling strategy (ours) outperforms the dense one. The comparison indicates that the dense sampling strategies introduce redundant parameters, which makes the model hard to optimize. Our sparse sampling strategy with fewer parameters is proven to be efficient and optimization-friendly.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G VISUALIZATION EXAMPLES</head><p>For easier understanding of our proposed CycleMLP, we visualize several instances of CycleMLP in <ref type="figure">Figure 7</ref>, including general case with stepsize 3?3 (7a), even stepsize <ref type="formula" target="#formula_7">(7b)</ref>, and examples where stepsize along height or width equals to 1 (7c, 7d).</p><p>We note that given specific number of input and output channels, no matter how the stepsize changes, the number of parameters of the CycleMLP does not change. Therefore, there is a trade-off of representation abilities between spatial and channel dimensions, which will be discussed in details in following experimental analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments:</head><p>We further conduct experiments on CycleMLPs with stepsize of 2?7, 7?2, 7?3, 3?7, and 4?4, respectively. The results are summarized <ref type="table" target="#tab_8">Table 15</ref>. For fair comparisons, all the models in the above table have the same parameters and FLOPs. We observe that the model with stepsize of 1?7 and 7?1 achieves the best performance, especially for semantic segmentation on ADE20K. To analyze the impact of stepsize on the performance, we take <ref type="figure">Figure 7</ref> for better illustration. One can see that enlarging the stepsize can expand the spatial receptive field. However, at a cost, it will reduce the number of periods (groups) running along the channel dimension, which may hurt the channel-wise representation abilities. Taking a feature map with C = 18 for example, the CycleMLP with stepsize 3?3 <ref type="figure">(Figure 7(a)</ref>) runs through only 2 channel groups (curly brackets in the <ref type="figure">figure)</ref>. However, the CycleMLP with stepsize 3?1 <ref type="figure">(Figure 7(c)</ref>) will run through 6 groups in total, making better use of the representation in the channel dimension. That's to say, there is a trade-off between spatial and channel representation. We empirically found that CyCleMLP with stepsize of 1?7 and 7?1 achieves the best performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a)-(c): motivation of Cycle Fully-Connected Layer (Cycle FC) compared to Channel FC and Spatial FC. (a) Channel FC aggregates features in the channel dimension with spatial size '1'. It can handle various input scales but cannot learn spatial context. (b) Spatial FC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(d)-(f): Three examples of different stepsizes. Orange blocks denote the sampled positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Resolution adaptability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Effective Receptive Field (ERF). We visualize the ERFs of the last stage for bothSwin (Liu et al.,  2021b)  and CycleMLP. Best viewed with zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of MLP blocks in details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Substitute softmax(A h ) into equation 5 and we get MHSA(X) i,j,: = h?{1,2,...,N h } X i+?i(h),j+?j (h),: W mhsa,h + b (10)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>classification.</figDesc><table><row><cell cols="3">FC Stepsize O(HW )</cell><cell>Scale Variable</cell><cell>ImgNet Top-1</cell><cell>COCO AP</cell><cell>ADE20K mIoU</cell></row><row><cell>Channel</cell><cell>1</cell><cell>HW</cell><cell></cell><cell>79.4</cell><cell>35.0</cell><cell>36.3</cell></row><row><cell>Spatial</cell><cell>-</cell><cell>H 2 W 2</cell><cell></cell><cell>80.9</cell><cell></cell></row><row><cell>Cycle</cell><cell>7</cell><cell>HW</cell><cell></cell><cell>81.6</cell><cell>41.7</cell><cell>42.4</cell></row></table><note>Table 1: Comparison of three types of FC operators.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Further, we stack all W mhsa,h together and reshape it into W mhsa ? R K?K?Cin?Cout . Then a relationship between W mlp and W mhsa can be formulated as follow.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>layer is applied before both parallel Cycle FC layers and channel-MLP modules. A residual connection(He et al., 2016)  is applied after each module.</figDesc><table><row><cell>Model</cell><cell cols="3">Param FLOPs Top-1</cell></row><row><cell>EAMLP-14</cell><cell>30M</cell><cell>-</cell><cell>78.9</cell></row><row><cell>EAMLP-19</cell><cell>55M</cell><cell>-</cell><cell>79.4</cell></row><row><cell>Mixer-B/16</cell><cell cols="3">59M 12.7G 76.4</cell></row><row><cell>Mixer-B/16  ?</cell><cell cols="3">59M 12.7G 77.3</cell></row><row><cell>ResMLP-S12</cell><cell>15M</cell><cell>3.0G</cell><cell>76.6</cell></row><row><cell>ResMLP-S24</cell><cell>30M</cell><cell>6.0G</cell><cell>79.4</cell></row><row><cell cols="4">ResMLP-B24 116M 23.0G 81.0</cell></row><row><cell>gMLP-Ti</cell><cell>6M</cell><cell>1.4G</cell><cell>72.3</cell></row><row><cell>gMLP-S</cell><cell>20M</cell><cell>4.5G</cell><cell>79.6</cell></row><row><cell>gMLP-B</cell><cell cols="3">73M 15.8G 81.6</cell></row><row><cell>S 2 -MLP-wide</cell><cell cols="3">71M 14.0G 80.0</cell></row><row><cell>S 2 -MLP-deep</cell><cell cols="3">51M 10.5G 80.7</cell></row><row><cell>ViP-Small/7</cell><cell>25M</cell><cell>6.9G</cell><cell>81.5</cell></row><row><cell cols="4">ViP-Medium/7 55M 16.3G 82.7</cell></row><row><cell>ViP-Large/7</cell><cell cols="3">88M 24.4G 83.2</cell></row><row><cell>AS-MLP-T</cell><cell>28M</cell><cell>4.4G</cell><cell>81.3</cell></row><row><cell>AS-MLP-S</cell><cell>50M</cell><cell>8.5G</cell><cell>83.1</cell></row><row><cell>AS-MLP-B</cell><cell cols="3">88M 15.2G 83.3</cell></row><row><cell cols="2">CycleMLP-B1 15M</cell><cell>2.1G</cell><cell>79.1</cell></row><row><cell cols="2">CycleMLP-B2 27M</cell><cell>3.9G</cell><cell>81.6</cell></row><row><cell cols="2">CycleMLP-B3 38M</cell><cell>6.9G</cell><cell>82.6</cell></row><row><cell cols="4">CycleMLP-B4 52M 10.1G 83.0</cell></row><row><cell cols="4">CycleMLP-B5 76M 12.3G 83.1</cell></row><row><cell>CycleMLP-T</cell><cell>28M</cell><cell>4.4G</cell><cell>81.3</cell></row><row><cell>CycleMLP-S</cell><cell>50M</cell><cell>8.5G</cell><cell>82.9</cell></row><row><cell>CycleMLP-B</cell><cell cols="3">88M 15.2G 83.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>ImageNet-1K classification for MLP-like models.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparison with SOTA models on ImageNet-1K without extra data. Stage. The blocks with the same architecture are stacked to form one Stage (He et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation on three parallel branches.</figDesc><table><row><cell>Stepsize</cell><cell>ImgNet Top-1</cell><cell>ADE20K mIoU</cell></row><row><cell>3</cell><cell>81.6</cell><cell>42.4</cell></row><row><cell>5</cell><cell cols="2">81.6 (+0.0) 43.2 (+0.8)</cell></row><row><cell>7</cell><cell cols="2">81.6 (+0.0) 43.9 (+1.5)</cell></row><row><cell>9</cell><cell cols="2">81.5 (?0.1) 43.2 (+0.8)</cell></row><row><cell>We adopt CycleMLP-B2 variant for this abla-</cell><cell></cell><cell></cell></row><row><cell>tion study. Double check marks () denote</cell><cell></cell><cell></cell></row><row><cell>two same branches.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>StepsizeComparison with MLP-like Models. We first compare CycleMLP with existing MLP-like models and the results are summarized inTable 2andFigure 2. The accuracy-FLOPs tradeoff of CycleMLP consistently outperforms existing MLP-like models<ref type="bibr" target="#b47">(Tolstikhin et al., 2021;</ref><ref type="bibr" target="#b49">Touvron et al., 2021a;</ref><ref type="bibr" target="#b35">Liu et al., 2021a;</ref> Guo et al., 2021; Hou et al., 2021) under a wide range of FLOPs, which we attribute to the effectiveness of our Cycle FC. Specifically, compared with one of the pioneering MLP work, i.e., gMLP<ref type="bibr" target="#b35">(Liu et al., 2021a)</ref>, CycleMLP-B2 achieves the same top-1 accuracy (81.6%) as gMLP-B while reducing more than 3? FLOPs (3.9G for CycleMLP-B2 and 15.8G for gMLP-B). Furthermore, compared with existing SOTA MLP-like model, i.e., ViP (Hou et al., 2021), our model CycleMLP-B utilizes less FLOPs (15.2G) than ViP-Large/7 (24.4G, the largest one of ViP family) while achiving higher top-1 accuracy.It is noted that all previous MLP-like models listed inTable 2do not conduct experiments on dense prediction tasks due to the incapability of dealing with variable input scales, which is discussed in Sec. 1. However, CycleMLP solved this issue by adopting Cycle FC. The experimental results on dense prediction tasks are presented in Sec. 3.3 and Sec. 3.4.Comparison with SOTA Models.Table 3further compares CycleMLP with previous state-ofthe-art CNN, Transformer and Hybrid architectures. It is interesting to see that CycleMLP models achieve comparable performance to Swin Transformer<ref type="bibr" target="#b36">(Liu et al., 2021b)</ref>, which is the state-ofthe-art Transformer-based model. Specifically, CycleMLP-B achieves slightly better top-1 accuracy (83.4%) than Swin-B (83.3%) with similar parameters and FLOPs. GFNet<ref type="bibr" target="#b41">(Rao et al., 2021)</ref> utilizes the fast Fourier transform (FFT)<ref type="bibr" target="#b19">(Cooley &amp; Tukey, 1965)</ref> to learn spatial information and achieves similar performance as CycleMLP on ImageNet-1K classification. However, the architecture of GFNet is correlated with the input resolution, and extra operation (parameter interpolation) is required when input scale changes, which may hurt the performance of dense predictions. We will thoroughly compare CycleMLP with GFNet in Sec. 3.4 on ADE20K.3.2 ABLATION STUDYIn this subsection, we conduct extensive ablation studies to analyze each component of our design. Unless otherwise stated, We adopt CycleMLP-B2 instantiation in this subsection.</figDesc><table><row><cell>ablation: CycleMLP</cell></row><row><cell>achieves the highest mIoU on ADE20K</cell></row><row><cell>when stepsize is 7. However, the stepsize</cell></row><row><cell>has negligible influence on the ImageNet</cell></row><row><cell>classification.</cell></row></table><note>Cycle Fully-Connected Layer. To demonstrate the advantage of the Cycle FC, we compare CycleMLP-B2 with two other baseline models equipped with channel FC and Spatial FC as spatial context aggregation operators, respectively. The differences of these operators are visualized in Fig- ure 1, and the comparison results are shown in Table 1. CycleMLP-B2 outperforms the counterparts built on both Spatial and Channel FC for ImageNet classification, COCO object detection, instance segmentation, and ADE20K semantic segmentation. The results validate that Cycle FC is capable of serving as a general-purpose, plug-and-play operator for spatial information communication and context aggregation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Backbone</cell><cell cols="2">AP b AP b 50</cell><cell>AP b 75</cell><cell cols="2">AP m AP m 50</cell><cell>AP m 75</cell><cell cols="2">Params FLOPs</cell></row><row><cell>ResNet50 (He et al., 2016)</cell><cell>41.0</cell><cell>61.7</cell><cell>44.9</cell><cell>37.1</cell><cell>58.4</cell><cell>40.1</cell><cell>44M</cell><cell>260G</cell></row><row><cell cols="2">PVT-Small (Wang et al., 2021b) 43.0</cell><cell>65.3</cell><cell>46.9</cell><cell>39.9</cell><cell>62.5</cell><cell>42.8</cell><cell>44M</cell><cell>245G</cell></row><row><cell>Swin-T (Liu et al., 2021b)</cell><cell>46.0</cell><cell>68.2</cell><cell>50.2</cell><cell>41.6</cell><cell>65.1</cell><cell>44.8</cell><cell>48M</cell><cell>264G</cell></row><row><cell>CycleMLP-T (ours)</cell><cell>46.4</cell><cell>68.1</cell><cell>51.1</cell><cell>41.8</cell><cell>64.9</cell><cell>45.1</cell><cell>48M</cell><cell>260G</cell></row></table><note>Object detection and instance segmentation on COCO val2017 (Lin et al., 2014). We compare CycleMLP with various backbones including ResNet (He et al., 2016), ResNeXt (Xie et al., 2017) and PVT (Wang et al., 2021b).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>The instance segmentation results of different backbones on the COCO val2017 dataset. Mask R-CNN frameworks are employed.</figDesc><table><row><cell cols="3">Results. Firstly, as shown in Table 6, CycleMLP-based RetinaNet consistently surpasses the CNN-</cell></row><row><cell cols="3">based ResNet (He et al., 2016), ResNeXt (Xie et al., 2017) and Transformer-based PVT (Wang</cell></row><row><cell cols="3">et al., 2021b) under similar parameter constraints, indicating that CycleMLP can serve as an ex-</cell></row><row><cell cols="3">cellent general-purpose backbone. Furthermore, using Mask R-CNN (He et al., 2017) for instance</cell></row><row><cell cols="3">segmentation also demonstrates similar comparison results. Furthermore, from Table 7, the Cy-</cell></row><row><cell cols="3">cleMLP can achieve a slightly better performance than Swin Transformer.</cell></row><row><cell>3.4 SEMANTIC SEGMENTATION</cell><cell></cell><cell></cell></row><row><cell>Backbone</cell><cell cols="2">Semantic FPN Param mIoU (%)</cell></row><row><cell>ResNet18 (He et al., 2016)</cell><cell>15.5M</cell><cell>32.9</cell></row><row><cell>PVT-Tiny (Wang et al., 2021b)</cell><cell>17.0M</cell><cell>35.7</cell></row><row><cell>CycleMLP-B1 (ours)</cell><cell>18.9M</cell><cell>40.8</cell></row><row><cell>ResNet50 (He et al., 2016)</cell><cell>28.5M</cell><cell>36.7</cell></row><row><cell>PVT-Small (Wang et al., 2021b)</cell><cell>28.2M</cell><cell>39.8</cell></row><row><cell>Swin-T  ? (Liu et al., 2021b)</cell><cell>31.9M</cell><cell>41.5</cell></row><row><cell>GFNet-Tiny (Rao et al., 2021)</cell><cell>26.6M</cell><cell>41.0</cell></row><row><cell>CycleMLP-B2 (ours)</cell><cell>30.6M</cell><cell>43.4</cell></row><row><cell>ResNet101 (He et al., 2016)</cell><cell>47.5M</cell><cell>38.8</cell></row><row><cell cols="2">ResNeXt101-32x4d (Xie et al., 2017) 47.1M</cell><cell>39.7</cell></row><row><cell>PVT-Medium (Wang et al., 2021b)</cell><cell>48.0M</cell><cell>41.6</cell></row><row><cell>GFNet-Small (Rao et al., 2021)</cell><cell>47.5M</cell><cell>42.5</cell></row><row><cell>CycleMLP-B3 (ours)</cell><cell>42.1M</cell><cell>44.3</cell></row><row><cell>PVT-Large (Wang et al., 2021b)</cell><cell>65.1M</cell><cell>42.1</cell></row><row><cell>Swin-S  ? (Liu et al., 2021b)</cell><cell>53.2M</cell><cell>45.2</cell></row><row><cell>CycleMLP-B4 (ours)</cell><cell>55.6M</cell><cell>45.1</cell></row><row><cell>GFNet-Base (Rao et al., 2021)</cell><cell>74.7M</cell><cell>44.8</cell></row><row><cell cols="2">ResNeXt101-64x4d (Xie et al., 2017) 86.4M</cell><cell>40.2</cell></row><row><cell>CycleMLP-B5 (ours)</cell><cell>79.4M</cell><cell>45.5</cell></row></table><note>Settings. We conduct semantic segmentation experiments on ADE20K (Zhou et al., 2017) dataset and present the detailed settings in Appendix. E.3. Table 8 and Table 9 show the experimental results using training recipes from PVT and Swin respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Semantic segmentation on ADE20K<ref type="bibr" target="#b70">(Zhou et al., 2017)</ref> val. All models are equipped with Semantic FPN<ref type="bibr" target="#b27">(Kirillov et al., 2019)</ref>.</figDesc><table /><note>? Results are from GFNet (Rao et al., 2021).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>, CycleMLP outperforms ResNet (He et al., 2016) and PVT (Wang</cell></row><row><cell>et al., 2021b) significantly with similar parameters. Moreover, compared to the state-of-the-art</cell></row><row><cell>Transformer-based backbone, Swin Transformer (Liu et al., 2021b), CycleMLP can obtain com-</cell></row></table><note>parable or even better performance. Specifically, CycleMLP-B2 surpasses Swin-T by 0.9 mIoU with slightly less parameters (30.6M v.s. 31.9M). Although GFNet (Rao et al., 2021) achieves similar performance as CycleMLP on ImageNet clas- sification, CycleMLP notably outperforms GFNet on ADE20K semantic segmentation where input scale varies. We attribute the superiority of CycleMLP under a scale-variable scenario to the capa- bility of dealing with arbitrary scales. On the contrary, GFNet (Rao et al., 2021) requires additional</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>The semantic segmentation results of different backbones on the ADE20K validation set.</figDesc><table /><note>heuristic operation (weight interpolation) when the input scale varies, which may hurt the perfor- mance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Robustness on ImageNet-C (Hendrycks &amp; Dietterich, 2019). The mean corruption error (mCE) normalized by AlexNet</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Maksim Dzabraev, Maksim Kalashnikov, Stepan Komkov, and Aleksandr Petiushko. Mdmmt: Multidomain multimodal transformer for video retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3354-3363, 2021. A LITERATURE ON VISION MODEL CNN-based Models. Originally introduced over twenty years ago (LeCun et al., 1989), convolutional neural networks (CNN) have been widely adopted since the success of the AlexNet (Krizhevsky et al., 2012) which outperformed prevailing approaches based on hand-crafted image features. There have been several attempts made to improve the design of CNN-based models. VGG (Simonyan &amp; Zisserman, 2014) demonstrated a state-of-the-art performance on ImageNet via deploying small (3 ? 3) convolution kernels to all layers. He et al.introduced skip-connections in ResNets (He et al., 2016), enabling a model variant with more than 1000 layers. DenseNet (Huang</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>The semantic segmentation results of different backbones on the ADE20K validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 13 :</head><label>13</label><figDesc>Comparison with dilated and random sampling. For random sampling, we conduct the experiments for three independent trials with three seeds(S=1, 2, 3).</figDesc><table><row><cell>Operators</cell><cell cols="2">Dense Params FLOPs Top-1 Acc</cell></row><row><cell>Conv: 1?3 + 3?1</cell><cell>34.3M 5.1G</cell><cell>75.0</cell></row><row><cell>CycleMLP: 1?3 + 3?1</cell><cell>26.8M 3.9G</cell><cell>76.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 14 :</head><label>14</label><figDesc>Comparison with dense sampling: On the consideration of training time, we only train both models for 100 epochs for fair comparison.</figDesc><table><row><cell>branch1</cell><cell>branch2</cell><cell>ImgNet Top-1</cell><cell>ADE20K mIoU</cell></row><row><cell>7?1</cell><cell>1? 7</cell><cell>81.6</cell><cell>43.9</cell></row><row><cell>7?2</cell><cell>2?7</cell><cell>81.5</cell><cell>43.4</cell></row><row><cell>7?3</cell><cell>3?7</cell><cell>81.4</cell><cell>42.7</cell></row><row><cell>4?4</cell><cell>4?4</cell><cell>81.5</cell><cell>43.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 15 :</head><label>15</label><figDesc>Comparison on different stepsizes (e.g., even stepsize and odd stepsize), including 7?2, 4?4.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We omit bias here for discussion convenience.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. Ping Luo is supported by the General Research Fund of HK No.27208720,  No.17212120, and the HKU-TCL Joint Research Center for Artificial Intelligence. <ref type="table">Table 11</ref>: Instantiations of the CycleMLP with varying complexity. The E i and L i denote the expand ratio and number of repeated layers. Our design principle is inspired by the philosophy of ResNet (He et al., 2016), where the channel dimension increases while the spatial resolution shrinks with the layer going deeper.</p><p>Further kernel optimization for Cycle FC may bring a faster speed but is beyond the scope of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 COCO INSTANCE SEGMENTATION</head><p>We conduct object detection and instance segmentation experiments on COCO <ref type="bibr" target="#b33">(Lin et al., 2014)</ref> dataset, which contains 118K and 5K images for train and validation splits. We adopt the mmdetection <ref type="bibr" target="#b16">(Chen et al., 2019)</ref> toolbox for all experiments in this subsection. To evaluate the our CycleMLP backbones, we adopt two widely used detectors, i.e., RetinaNet <ref type="bibr" target="#b34">(Lin et al., 2017)</ref> and Mask R- CNN (He et al., 2017). All backbones are initialized with ImageNet pre-trained weights and other newly added layers are initialized via Xavier (Glorot &amp; Bengio, 2010). We use the AdamW <ref type="bibr" target="#b37">(Loshchilov &amp; Hutter, 2017)</ref> optimizer with the initial learning rate of 1?10 ?4 . All models are trained on 8 Tesla V100 GPUs with a total batch size of 16 for 12 epochs (i.e., 1? training scheduler). The input images are resized to the shorted side of 800 pixels and the longer side does not exceed 1333 pixels during training. We do not use the multi-scale <ref type="bibr" target="#b15">(Carion et al., 2020;</ref><ref type="bibr" target="#b71">Zhu et al., 2020;</ref> training strategy. In the testing stage, the shorter side of input images is resized to 800 pixels while no constraint on the longer side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 ADE20K SEMANTIC SEGMENTATION</head><p>We conduct semantic segmentation experiments on ADE20K <ref type="bibr" target="#b70">(Zhou et al., 2017)</ref> dataset, which covers a broad range of 150 semantic categories. ADE20K contains 20K training, 2K validation and 3K testing images. We adopt the mmsegmenation (Contributors, 2020) toolbox as our codebase in this subsection. The experimental settings mostly follow PVT <ref type="bibr" target="#b54">(Wang et al., 2021b)</ref>, which trains models for 40K iterations on 8 Tesla V100 GPUs with 4 samples per GPU. The backbone is initialized with the pre-trained weights on ImageNet. All models are optimized by AdamW <ref type="bibr" target="#b37">(Loshchilov &amp; Hutter, 2017)</ref>. The initial learning rate is configured as 2 ? 10 ?4 with the polynomial decay parameter of 0.9. Input images are randomly resized and cropped to 512?512 at the training phase. During testing, we scale the images to the shorted side of 512. We adopt the simple approach Semantic FPN <ref type="bibr" target="#b27">(Kirillov et al., 2019)</ref> as the semantic segmentation method following <ref type="bibr" target="#b54">(Wang et al., 2021b)</ref> for fair comparison.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-H-Ti</forename><surname>Gfnet</surname></persName>
		</author>
		<idno>FFT 224 2 15M 2.0G 80.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-S</forename><surname>Deit</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Trans 224 2 22M 4.6G 79.8</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-T</forename><surname>Swin</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Trans 224 2 29M 4.5G 81.3</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-H-S</forename><surname>Gfnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fft</surname></persName>
		</author>
		<idno>224 2 32M 4.5G 81.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-H-B</forename><surname>Gfnet</surname></persName>
		</author>
		<idno>FFT 224 2 54M 8.4G 82.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Trans 224 2 61M 9</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pvt-L</surname></persName>
		</author>
		<idno>8G 81.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-B</forename><surname>Vit</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Trans 224 2 86M 17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-B</forename><surname>Deit</surname></persName>
		</author>
		<idno>5G 81.8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Trans 384 2 86M 55</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-B</forename><surname>Deit</surname></persName>
		</author>
		<idno>4G 83.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Trans 224 2 88M 15</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-B</forename><surname>Swin</surname></persName>
		</author>
		<idno>4G 83.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-B</forename><surname>Cyclemlp</surname></persName>
		</author>
		<idno>MLP 224 2 88M 15.2G 83.4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An algorithm for the machine calculation of complex fourier series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Cooley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">90</biblScope>
			<biblScope unit="page" from="297" to="301" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the relationship between selfattention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJlnC1rKPB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<title level="m">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12424</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">As-mlp: An axial shifted mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08391</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08050</idno>
		<title level="m">Pay attention to mlps</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Rethinking the design principles of robust vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gege</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuefeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaokai</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07926</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00645</idno>
		<title level="m">Global filter networks for image classification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16519" to="16529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14454" to="14463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Resmlp: Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Herv? J?gou. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13797</idno>
		<title level="m">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8741" to="8750" />
		</imprint>
	</monogr>
	<note>2021c. Ross Wightman. Pytorch image models</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15203</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="191" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">S 2 -mlp: Spatial-shift mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfeng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07477</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI 16</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Method Backbone val MS mIoU Params FLOPs</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">ResNet-101 45</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Danet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="69" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">ResNet-101 44</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Deeplabv3+</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="63" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Acnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
		<idno>2020) ResNet-101 46.0 69M 1249G</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="101" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">2020) ResNet-101 45</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ocrnet (yuan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="56" to="923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">ResNet-101 44</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Upernet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="86" to="1029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">2020) HRNet-w48 45</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ocrnet (yuan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="71" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">ResNeSt-101 46</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Deeplabv3+</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="66" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Deeplabv3+</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno>ResNeSt-200 48.4 88M 1381G</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Upernet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<idno>2021b) 45.8 60M 945G AS-MLP-T</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Upernet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<idno>2021b) 49.5 81M 1038G AS-MLP-S</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="81" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-S</forename><surname>Cyclemlp</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="81" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Upernet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">]</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sh ? Sw</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="3" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">]</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sh ? Sw</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="3" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">]</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sh ? Sw</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="3" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">]</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sh ? Sw</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Examples of Stepsize cases: Here we separate the feature map along the width dimension for convenient visualization. denotes the output position. We place the absolute coordinates (h, w, c) of the sampled points at the left of the feature. Sampled points within a curly bracket ({) belong to the same period (group)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>Dash lines link two cyclical periods</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
