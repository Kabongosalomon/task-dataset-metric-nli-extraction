<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RoME: Role-aware Mixture-of-Expert Transformer for Text-to-Video Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Burak</forename><surname>Satar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Joo</forename><forename type="middle">Hwee</forename><surname>Lim</surname></persName>
						</author>
						<title level="a" type="main">RoME: Role-aware Mixture-of-Expert Transformer for Text-to-Video Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video understanding</term>
					<term>video retrieval</term>
					<term>trans- former</term>
					<term>multi-modal</term>
					<term>hierarchical</term>
					<term>cross-modal</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Seas of videos are uploaded daily with the popularity of social channels; thus, retrieving the most related video contents with user textual queries plays a more crucial role. Most methods consider only one joint embedding space between global visual and textual features without considering the local structures of each modality. Some other approaches consider multiple embedding spaces consisting of global and local features separately, ignoring rich inter-modality correlations.</p><p>We propose a novel mixture-of-expert transformer RoME that disentangles the text and the video into three levels; the roles of spatial contexts, temporal contexts, and object contexts. We utilize a transformer-based attention mechanism to fully exploit visual and text embeddings at both global and local levels with mixture-of-experts for considering inter-modalities and structures' correlations. The results indicate that our method outperforms the state-of-the-art methods on the YouCook2 and MSR-VTT datasets, given the same visual backbone without pre-training. Finally, we conducted extensive ablation studies to elucidate our design choices.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Video content has become significantly abundant with the usage of mobile phones and social media. The rapid growth and the complex nature of videos have motivated various research such as video retrieval, visual question answering, video captioning, etc. Specifically, text-to-video retrieval aims to retrieve relevant video clips, given that a user text query becomes even more compelling since it is used daily by billions of users over various video-based applications. In this work, we focus only on text-to-video retrieval as the text allows semantic queries, which are commonly used to search for relevant videos.</p><p>Semantic mapping and embedding are necessary to align textual and video features to achieve reasonable text-to-video retrieval performance. The traditional models <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> are inadequate to retrieve compositional video clips since they make use of the textual queries only in the form of keywords. Thus, many recent works utilize fine-grained textual descriptions <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b6">[7]</ref>. However, these works apply only one joint embedding space for text and video matching, which causes the loss of fine-grained details <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Although there are some current methods with multiple embedding spaces, they treat each space individually without considering their correlations. For instance, <ref type="bibr" target="#b2">[3]</ref> uses only global features and soft attention at every level, lacking interaction between visual features. <ref type="bibr" target="#b9">[10]</ref> uses only self-attention to model correlations between Manuscript submitted June <ref type="bibr" target="#b25">26,</ref><ref type="bibr">2022</ref> visual features, which would make the model computationally expensive.</p><p>We propose a novel Role-aware Mixture-of-Expert transformer model (RoME) for the text-to-video retrieval task by exploiting different experts from textual and visual representations. This paper is an extension of our ICIP paper <ref type="bibr" target="#b10">[11]</ref> with the following summary of the innovations: 1) Firstly, as in our conference version <ref type="bibr" target="#b10">[11]</ref>, we also disentangle the video and textual features into hierarchical structures with the motivation from various cognitive science studies <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> showing that people perceive events hierarchically from the object, scene, and event. Then, we calculate the similarities between the corresponding embedding levels in the text and video.</p><p>2) Secondly, different from our conference version, which applies the same attention scheme at both global and local level features, we only use inter-modality attention for the global level features which depict scenes and events, while we use intra-and inter-modality attention for the local level features, which contains fine-grained action and object. The intuition behind this is that global features are complementary to local representations through contexts; however, the local features are usually noise which can influence the global representations if applied symmetrically.</p><p>3) Thirdly, we strengthen our model by applying the weighted sum to the video encodings, which is better than the common approach of simply averaging the embeddings. While the previous approach assumes that encoding level is equally important, our results show that contribution of each level is different and could change based on the dataset. 4) Lastly, we conduct a more comprehensive experiment and outperform all the SOTA methods at YouCook2 and MSR-VTT datasets when using the same visual features to demonstrate our method's strong performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>For images, the technique by Lee et al. <ref type="bibr" target="#b13">[14]</ref> retrieves imagetext by embedding visual regions and words in a shared space. Images with corresponding words and image regions show a high degree of resemblance. Karpathy et al. <ref type="bibr" target="#b14">[15]</ref> deconstruct images and sentences into several areas and words in their proposal to use maximum alignment to calculate global matching similarity. And image descriptions are broken down into various roles by Wu et al. <ref type="bibr" target="#b15">[16]</ref>. For videos, before the deep learning era, the researchers <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> used only textual keywords to retrieve short video clips with simple actions. However, it is insufficient to address video clips with complex actions since many works use natural language as queries. <ref type="figure">Fig. 1</ref>. Illustration of our model for video retrieval task. The input contains the extracted features from the video clips and their corresponding textual captions in training. The caption is disentangled into three levels in the textual encoding part through semantic role labeling and GCNs. In the video encoding part, the video expert features, namely 2D and 3D, are exploited by various transformers. While we use only self-attention at the global level, we utilize both self-attention and cross-modal attention at the local levels. We apply text-video representation matching as the last step. The weighted sum of visual experts is used to calculate similarities for the three-level embeddings.</p><p>These works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref> typically applies only one embedding space to measure the gap between text and video. Although some of them use multimodal features from the video, having joint embedding space causes the loss of the fine-grained. For instance, Mithun et al. <ref type="bibr" target="#b3">[4]</ref> and Liu et al. <ref type="bibr" target="#b7">[8]</ref> utilize multimodal features, including image, motion, and audio. Dong et al. <ref type="bibr" target="#b8">[9]</ref> use mean pooling, biGRU, and CNN as three branches to encode consecutive videos and texts. In addition to this semantic gap, Luo et al. <ref type="bibr" target="#b17">[18]</ref> use only self-attentions among all modalities, and their method is highly dependent on pretraining. Kim et al. <ref type="bibr" target="#b18">[19]</ref> use only soft-attentions and ignore interaction between visual features. While Yang et al. <ref type="bibr" target="#b19">[20]</ref> utilize a fine-grained alignment with an extra loss and exploit special arrangement for hard negatives, they still use only selfattentions among all modalities by ignoring the interaction between visual features. Various other works focused on aligning local features to close the semantic gap <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, however, with partial progress to the problem. The sequential interaction of videos and texts is proposed to be combined by Yu et al. <ref type="bibr" target="#b21">[22]</ref>.</p><p>Various methods <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref> apply BERT models to the vision encoders for better video and text matching. For instance, Sun et al. <ref type="bibr" target="#b22">[23]</ref> transform visual features into visual tokens and feed them into the network along with textual tokens. However, visual tokens may not handle the fine-grained details. To address this, Zhu et al. <ref type="bibr" target="#b23">[24]</ref> uses local visual features in a similar BERT-like network by ignoring global features. Rather than transformer-based approaches, some studies such as Tan et al. <ref type="bibr" target="#b25">[26]</ref> utilize LSTM and GNNs to exploit cross-modal features. Liu et al. <ref type="bibr" target="#b7">[8]</ref> utilizes only a gating mechanism to modulate embeddings as an alternative. However, it is challenging to capture high-level inter-modality information. Moreover, pre-training is used to bring a boost to the results. While a giant instructional dataset <ref type="bibr" target="#b5">[6]</ref> is common among the community, Alayrac et al. <ref type="bibr" target="#b26">[27]</ref> exploit another huge dataset on audio <ref type="bibr" target="#b27">[28]</ref>. Various studies focus on improving the quality of pre-training as well. While Gabeur et al. <ref type="bibr" target="#b28">[29]</ref> mask their modalities respectively, Wang et al. <ref type="bibr" target="#b29">[30]</ref> exploit object features for that aim. However, we are motivated on development without pre-training, although our work can benefit from pretrained models with experimental results in Sec. IV. Some recent works have multiple embedding spaces and use both global/local features with various attention-based mechanisms. For example, Ging et al. <ref type="bibr" target="#b30">[31]</ref> exploit cross-attentions and self-attentions even though the interaction between visual features is limited. However, the action and object-related features are not considered. For fine-grained action retrieval, Wray separates action phrases into verbs and nouns, but this is difficult to apply to sentences with more intricate compositions. The hierarchical modeling of movies and paragraphs is proposed by Zhang et al. <ref type="bibr" target="#b31">[32]</ref>; however, it does not apply to the breakdown of single sentences. Gabeur et al. <ref type="bibr" target="#b9">[10]</ref> use only self-attentions to learn the text-video correspondence even with many visual modalities. Also, having a large input size because of many visual expert features causes a high computation power due to the nature of transformers. Gabeur et al. <ref type="bibr" target="#b9">[10]</ref> develop their model based on Miech et al. <ref type="bibr" target="#b4">[5]</ref>. While the latter uses word2vec vectors and gated units, the former uses transformers in the textual and visual encoding part. Second, Gabeur et al. exploit three more video experts than Miech et al.. Chen et al. <ref type="bibr" target="#b2">[3]</ref> uses only global 2D CNN features as a visual backbone and ignores interaction between visual feature levels. They suggest transforming textual and visual features into three semantic roles. Many works use various frozen video expert features as well. For instance, while <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref> use seven video expert features, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b32">[33]</ref> use four of them. Our method could be named as a mixture of experts because we exploit the contextual interaction between the experts at every hierarchical level. These video experts could also be used in other vision-language-related tasks, such as video captioning <ref type="bibr" target="#b33">[34]</ref> and VQA <ref type="bibr" target="#b34">[35]</ref>.</p><p>Lastly, Our recent effort <ref type="bibr" target="#b10">[11]</ref> exploits fine-grained details in the global and local visual features, has multiple embedding spaces to close the semantic gap, and utilizes specially arranged transformers to make use of modality-specific and modality-complement features. However, our earlier work uses these transformers regardless of the hierarchical level. In other words, the same mechanisms are applied at every level. More-over, the model simply averages the similarity scores at every level by ignoring the importance of the discriminative feature in the various expert features. Furthermore, it lacks extensive experiments and ablations to justify the contributions. The limitations of <ref type="bibr" target="#b10">[11]</ref> are addressed with the extension in this paper. Our method exploits intra-modality correlation at the global level by preventing noise from local fine-grained local features with self-attention. Likewise, we utilize inter-modality correlations at the local-level features with the Mixture-of-Expert model to combine the local and global video experts. Besides, apply weighted sum to the video experts along with extensive experiments and ablations. <ref type="figure">Figure 1</ref> introduces our model, which has three main parts: textual encoding, video encoding, and text-video matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Textual Encoding</head><p>To comply with disentangled video features, we also disentangle the textual features hierarchically. Every video clip has at least one descriptive textual sentence in a hierarchically structured way. For instance, while a sentence could define global features, action and entity-related words would define the local features.</p><p>By following recent works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b10">[11]</ref>, we utilize <ref type="bibr" target="#b35">[36]</ref> for handling semantic role labeling to disentangle the structure of sentences in the form of graphs. To exploit the semantic relationship among the words, verbs are connected to the sentence by showing the temporal relations with their directed edges, and then the objects are connected to verbs. We define the relationship between actions and entities as r ij , in which i refers to action nodes and j refers to verb nodes. Before applying GCN, we use bidirectional LSTM to have contextualized word embeddings. While we apply soft attention for sentencelevel embedding, we implement max-pooling over verbs and objects for their corresponding embeddings.</p><p>Rather than using relational GCN <ref type="bibr" target="#b36">[37]</ref> by learning weight matrix for each semantic role which would be computationally expensive, we follow Chen et al. <ref type="bibr" target="#b2">[3]</ref> to utilize factorised weights in two steps. Firstly, the semantic role matrix W r , which is specific matrix for different semantic roles are multiplied with initialized node embeddings g i {g e , g a , g o } in Eq. 1. r ij is an one-hot vector referring to the edge type from node i to j. refers to element-wise multiplication.</p><formula xml:id="formula_0">g 0 i = g i W r r ij<label>(1)</label></formula><p>Secondly, a graph attention network is applied to neighbor nodes. Then W t matrix, which is shared across all relations types, is used to exploit attended nodes in Eq. 2. ? refers to the outcome after attention is implemented for every node.</p><formula xml:id="formula_1">g l+1 i = g l i + W l+1 t j?Ni ? ij (g l j )<label>(2)</label></formula><p>After applying these two formulas, we gather the textual representation for global and local features; for sentences, verbs, and words, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Video Encoding</head><p>While disentangling language queries into hierarchical features is relatively easy, parsing videos into hierarchical features could be challenging. We propose different transformer models for global and local levels. While we use only intra-modality features at the global level, we utilize both intra-modality and inter-modality specific features at the local levels.</p><p>The intuition behind this is that while the global features could guide the local representations through the cross-modal attention at the action or object level, the local features may give noise to the global representations when used at the appearance level. We use transformer encoder and decoders to implement this idea as inspired from <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>.</p><p>Calculating appearance-level video encodings is relatively easy. The 2D feature F S is fed into the self-attention layer to encode into z s . Then, a feed-forward layer FF outputs the final contextualized appearance feature. Normalization of the layer is done under the Norm function.</p><formula xml:id="formula_2">z s = Norm MultiHead(F S , F S , F S ) + F S E S = Norm FF(z s ) + z s<label>(3)</label></formula><p>We follow Vaswani et al. <ref type="bibr" target="#b37">[38]</ref>'s multi-headed attention layers as formulated in Eq. 4. During the training process, all the W matrices are learned. While query Q is the same as key K and value V in the self-attention layers, the query Q could be from another source in the cross-modal attention layers. Layer normalization and the residual connection are also applied after every attention layer.</p><formula xml:id="formula_3">MultiHead(Q, K, V ) = Concat(Head 1 , ..., Head h )W O Head i = Attention(QW Q i , KW K i , V W V i ) Attention(Q, K, V ) = ? QK T ? d V<label>(4)</label></formula><p>Complementary features are applied with cross-modal attention to calculate the verb E A and noun E O level encodings. For instance, we explain the formula for verb level in Eq. 5. First, the appearance level F S and noun level features F O are concatenated. Then, a multi-headed self-attention is applied, followed by a feed-forward layer FF. This generates contextspecific feature s e of the verb level.</p><formula xml:id="formula_4">f e = Concat(F S , F O ) z e = Norm MultiHead(f e , f e , f e ) + f e s e = Norm FF(z e ) + z e<label>(5)</label></formula><p>Then, self-attention of verb level features is calculated as in the first row of Eq. 5.</p><p>Finally, cross-modal attention is applied such that the contextualized verb-level features would attend to the contextualized appearance and object-level features. Then, a feedforward linear layer FF and layer normalization are applied. We repeat the same process to calculate the noun-level embeddings. Then, these three-level video embeddings are used in the video-text matching part.</p><formula xml:id="formula_5">z a = Norm MultiHead(F A , F A , F A ) + F A c e = Norm MultiHead(s e , z a , z a ) + z a E A = Norm FF(c e ) + c e (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Video-Text Matching</head><p>Some works such as <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b10">[11]</ref> average the similarities of the different embedding levels, assuming that they are equally important. Other works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref> apply weighted sum but predicted from textual encodings. On the other hand, we apply weighted sum to the video encodings as the video encodings, especially at the local level, become contextualized via crossmodal attention with the other modalities. The guidance of the textual encodings may not be necessary.</p><p>We simply use cosine similarity with the video and textual embeddings to calculate the matching score by adding a weighting calculated on video embeddings only. We utilize contrastive ranking loss <ref type="bibr" target="#b2">[3]</ref> in training by aiming to have positive and negative pairs larger than a pre-set margin. If v and c symbolize visual and textual representation, the positive pairs and the negative pairs could be presented like following, (v p , c p ) and (v p , c n ) / (v n , c p ), respectively. A pre-set margin is referred to as ? to calculate the contrastive loss.</p><formula xml:id="formula_6">s(V, C) = 3 i=1 w i (V ) &lt; v i , c i &gt; ||v i || 2 ||c i || 2 (7) w i (V ) = e h(V ) ? ai 3 j=1 e h(V ) ? aj<label>(8)</label></formula><p>W i (v) represents the weight for the ith video expert feature. We feed the expert encodings into a linear layer and then apply a softmax operation. Since we have three linear layers, we would calculate three different weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We share our experimental results on YouCook2 and MSR-VTT datasets for the video retrieval task and present various ablations results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>YouCook2 dataset: This dataset <ref type="bibr" target="#b39">[40]</ref> is collected from YouTube only from cooking-related videos with 89 different recipes. They are all captured in various kitchen environments from the third-person viewpoint. The video clips are labeled by imperative English sentences along with temporal boundaries referring to the actions. The annotation was done manually by humans. Video clips are extracted by using the formal annotation file. We feed our model with the validation dataset for evaluation following the other studies since the test set is not publicly available. Normally, the dataset has 10,337 clips and 3,492 clips in the training and validation set, respectively. However, some of the raw videos are not available due to being removed or becoming private. This makes 8% of training clips and 7% of validation clips missing in total. Thus, we use 9,496 clips and 3,266 clips in the training and validation set, respectively.</p><p>MSR-VTT dataset: It <ref type="bibr" target="#b40">[41]</ref> contains 10K video clips with 20 descriptions for each video and collected by a commercial video search engine. The annotation is done by humans. This dataset has various train/test splits. One of the most used splits is called 1k-B. It has 6,656 clips for training and 1,000 clips for testing. The full split, which is another widely used split, contains 6,513 video clips for training, 497 for validation, and 2990 for testing. While we test our method mostly in this 1k-B split, we also report in the full split as well.</p><p>Video features: We extract our own features from the raw videos of YouCook2 by following Miech et al. <ref type="bibr" target="#b5">[6]</ref>. To extract appearance features we use two dimensional Convolutional Neural Networks of ResNet-152 <ref type="bibr" target="#b41">[42]</ref> model, pre-trained on Imagenet <ref type="bibr" target="#b42">[43]</ref>. To extract action level features we utilize three dimensional Convolutional Neural Networks of ResNeXt-101 <ref type="bibr" target="#b43">[44]</ref> model, pre-trained on Kinetics <ref type="bibr" target="#b44">[45]</ref>. Later on, we implement max-pooling on temporal dimension, making the dimension (1, 2048) for each feature set. The process is also depicted in <ref type="figure" target="#fig_1">Figure 2</ref>  Evaluation metrics. Given a textual query, we rank all the video candidates such that the video associated with the textual query is ranked as high as possible. Recall (R@k) and Median Rank (MedR) are used to evaluate the model. For instance, R@k is calculated if the ground truth video is in the first kth in the ranking list, where k is {1,5,10}. Higher recall values refer to better performance. MedR denotes the median rank of correctly predicted video clips in the list. The lower MedR is, the performance is better.</p><p>Training. Glove embeddings <ref type="bibr" target="#b45">[46]</ref> is used to set the word embedding with the size of 300 for the textual embedding. To parse the text embeddings hierarchically, we follow <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Graph convolutions are applied with two layers, outputting 1024 dimensions. We apply a linear layer to each video expert feature embeddings to be compatible with the textual layer, making their dimension 1024 as well. The ? margin is set to 0.2. Our mini-batch size is 32. While the epoch is 100 for the YouCook2 dataset, it is 50 for the MSR-VTT dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on YouCook2</head><p>Table I compares our approach with SOTA methods for video retrieval task on YouCook2 dataset. It includes all the methods that report their results without pre-training. The above part of <ref type="table" target="#tab_0">Table I</ref> shows the methods when using the same feature set. We overpass all the recent papers in all metrics, except the last metric for one work <ref type="bibr" target="#b30">[31]</ref>, using the same feature backbone. Please note that the HGR method by Chen et al. <ref type="bibr" target="#b2">[3]</ref> is reported by Satar et al. <ref type="bibr" target="#b10">[11]</ref>. All the models in the table use pre-extracted features; in other words, they are not end-to-end models, making the comparison fair. The lower part of <ref type="table" target="#tab_0">Table I</ref> introduces the results with pretraining and different visual feature backbones. We see higher results for two reasons. UniVL <ref type="bibr" target="#b17">[18]</ref>, COOT <ref type="bibr" target="#b30">[31]</ref>, and TACo <ref type="bibr" target="#b19">[20]</ref> methods use S3D features pre-trained on HowTo100M. Since HowTo100M is similar to YouCook2 as an instructional video dataset, it would be more useful than pre-training on Kinetics. On the other hand, MMVC <ref type="bibr" target="#b32">[33]</ref> uses S3D features pre-trained on Kinetics. However, this model exploits additional audio and Automatic Speech Recognition (ASR) features which could boost the performance. <ref type="table" target="#tab_0">Table II</ref> compares our approach with SOTA methods when using the same feature set on the MSR-VTT dataset. These include all the methods that report text-to-video retrieval results without pre-training. We overpass all the methods in all metrics. In particular, our method surpasses the performance by 2.9% on R@1 over the CE method <ref type="bibr" target="#b7">[8]</ref>, which uses four features, even though we only use two of them. Moreover, our method improves the performance by 1.9% on R@1 over the best SOTA approach <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on MSR-VTT</head><p>For the full split, we overpass all the methods that we have similar feature sets. However, we surpass the CE method <ref type="bibr" target="#b7">[8]</ref> only in the first two metrics. One of the main reasons that CE overpasses our method in the last two metrics could be the advantage of using various other visual experts such as audio and ASR. Secondly, the testing set becomes tripled than 1k-B split, increasing the advantage of having many visual expert features. Thirdly, our models' textual encoding part may not handle having twenty captions for each video clip, as it is included in MSR-VTT. A transformer-based approach such as BERT may perform better, although we did not apply any experiments with BERT. Moreover, it is hard to make a comparison among some approaches. For example, Alayrac et al. <ref type="bibr" target="#b6">[7]</ref> report their result when pre-trained on HowTo100M and a zero-shot setting. TeachText-CE <ref type="bibr" target="#b47">[48]</ref> uses various textual feature experts as long as seven different visual feature experts, making it hard to compare with our result, which is still added to the table for reference. Nevertheless, we overpass the CE method in the first two metrics, which could be more critical in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments for Video-to-Text Retrieval</head><p>Although the video-to-text retrieval task is not our primary motivation, we report it in <ref type="table" target="#tab_0">Table III</ref> to show that our model could also be used in this aim. For YouCook2, only one method reports their video-to-text retrieval results when there is no pre-training, which is reported by <ref type="bibr" target="#b10">[11]</ref>. We overpass the compared methods in all metrics with a high margin on both datasets without pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Studies</head><p>We share extensive ablation studies about design choices on weighted sum, model design, and video feature settings.</p><p>1) Ablation on Weighting Encodings: Some recent papers add weighting to only textual representation. We do an ablation study by adding weighting to the only visual representation, only textual representation, and both. Our results in <ref type="table" target="#tab_0">Table IV</ref> indicate: 1) Applying weighted sum over textual or visual encodings generally performs better than average weighting. It justifies the idea that every embedding level impacts the model differently. 2) Weighted sum on only visual encodings brings slightly higher results than having weighted sum on only textual encodings. We assume it is because it ignores discriminate signals from the video experts by making the model text-dependent when using weighted sum on only textual encodings.</p><p>If we examine the calculated weights of each visual level on the YouCook2 dataset, the results are around 0.45, 0.30, and 0.25 for appearance, action, and object levels, respectively. The weights are calculated in Eq. 7 and 8, Sec.III. While the 2D feature contributes the most, the other two feature levels affect the results considerably and similarly. It is because the dataset contains crucial objects and actions. The calculated weights of the visual encodings in the MSR-VTT dataset are around 0.47, 0.43, and 0.10 for appearance action and object levels, showing the same importance for appearance features. However, while action features contribute more, the contribution of object-level features decreases sharply. It is mainly due to the nature of the dataset, where there are fewer objects but more continual actions.</p><p>2) Ablation on Model Design: Our proposed model suggests that different attention methods for global and local feature levels increase the result. We could see this increase in <ref type="table">Table V</ref>. The model we use for ablation studies applies only self-attention for every level. We compare this model with our proposed model, where we use self-attention in the appearance level but cross-modal and self-attention for the other two levels. Average weighting is applied for both models. All the feature experts have the dimensions of 2048, which makes the comparison fair. The results are improved by proposing different attention mechanisms for global and local levels.</p><p>3) Ablation on Visual Feature Settings: While all the variations in our model surpass the baseline in the same feature set, the highest result comes with a concatenated 2D and 3D feature setting in the <ref type="table" target="#tab_0">Table VI</ref>, showing that early fusion gives a boost since the features are correlated. Average weighting is applied to the models. 4) Extended Ablations: The following ablation presents comprehensive insights on visual feature setting and model  design choice. The above part shows that our visual feature setting choice, which is the concatenation of 2D and 3D features, brings higher results even on a different model design than our proposed model. Similarly, the below part indicates that our model design choice performs better even using different feature settings than our proposed approach. Average weighting is applied to the models. These results support our two arguments that using self-attention only at the global level with cross-modal and self-attention at the local levels along with concatenated 2D and 3D features brings the best results. V. CONCLUSION In this work, we propose a novel hierarchical MoE transformer to transform the text and video features into three semantic roles and then align them in three joint embedding spaces. To better use the intra-modality and inter-modality video features, we employ self-attention and cross-modal attention at the local levels and only use self-attention at the global level. Moreover, the weighted sum of the video features boosts the results by exploiting the discriminative video features. Our approach surpasses SOTA methods on the YouCook2 and MSR-VTT datasets by using the same visual feature set without pre-training. We also share an extensive ablation study allowing us to have deeper insights.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>. For MSR-VTT, we gather appearance level features of ResNet-152 model from Chen et al. [3] and action level features of ResNeXt-101 model from Liu et al. [8], respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Feature extraction and pre-processing method for our experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I WE</head><label>I</label><figDesc>COMPARE OUR APPROACH WITH SOTA METHODS FOR VIDEO RETRIEVAL ON YOUCOOK2 VALIDATION SET. WE OVERPASS THEM IN ALL METRICS EXCEPT THE LAST ONE BY USING THE SAME FEATURE SET WHEN NO PRE-TRAINING. THE LOWER PART SHOWS THE METHODS WITH PRE-TRAINING.</figDesc><table><row><cell>Method</cell><cell>Visual Backbone</cell><cell>Batch Size</cell><cell>R@1?</cell><cell cols="2">Text-to-Video Retrieval R@5? R@10?</cell><cell>MdR?</cell></row><row><cell>Random</cell><cell>-</cell><cell>-</cell><cell>0.03</cell><cell>0.15</cell><cell>0.3</cell><cell>1675</cell></row><row><cell>UniVL (v2-FT-Joint), 2020 [18]</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>32</cell><cell>3.4</cell><cell>10.8</cell><cell>17.8</cell><cell>76</cell></row><row><cell>Satar et al., 2021 [11]</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>32</cell><cell>4.5</cell><cell>13.2</cell><cell>20.0</cell><cell>85</cell></row><row><cell>Miech et al., 2019 [6]</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>-</cell><cell>4.2</cell><cell>13.7</cell><cell>21.5</cell><cell>65</cell></row><row><cell>HGR*, 2020 [3]</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>32</cell><cell>4.8</cell><cell>14.0</cell><cell>20.3</cell><cell>85</cell></row><row><cell></cell><cell>Resnet-152 (ImageNet) +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HGR*, 2020 [3]</cell><cell>ResNeXt-101 (Kinetics) +</cell><cell>32</cell><cell>4.7</cell><cell>14.1</cell><cell>20.0</cell><cell>87</cell></row><row><cell></cell><cell>Faster R-CNN (MS COCO)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HGLMM, 2015 [47]</cell><cell>Fisher Vectors</cell><cell>-</cell><cell>4.6</cell><cell>14.3</cell><cell>21.6</cell><cell>75</cell></row><row><cell></cell><cell>Resnet-152 (ImageNet) +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Satar et al., 2021 [11]</cell><cell>ResNeXt-101 (Kinetics) +</cell><cell>32</cell><cell>5.3</cell><cell>14.5</cell><cell>20.8</cell><cell>77</cell></row><row><cell></cell><cell>Faster R-CNN (MS COCO)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SwAMP, 2021 [19]</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>128</cell><cell>4.8</cell><cell>14.5</cell><cell>22.5</cell><cell>57</cell></row><row><cell>TACo, 2021 [20]</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>128</cell><cell>4.9</cell><cell>14.7</cell><cell>21.7</cell><cell>63</cell></row><row><cell>COOT, 2020 [31]</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>32</cell><cell>5.9</cell><cell>16.7</cell><cell>24.8</cell><cell>49.7</cell></row><row><cell>RoME</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>32</cell><cell>6.3</cell><cell>16.9</cell><cell>25.2</cell><cell>53</cell></row><row><cell>UniVL (FT-Joint), 2020 [18]</cell><cell>S3D (HowTo100M)</cell><cell>32</cell><cell>7.7</cell><cell>23.9</cell><cell>34.7</cell><cell>21</cell></row><row><cell>MMCV, 2022 [33]</cell><cell>S3D (Kinetics)</cell><cell>32</cell><cell>16.6</cell><cell>37.4</cell><cell>48.3</cell><cell>12</cell></row><row><cell>COOT, 2020 [31]</cell><cell>S3D (HowTo100M)</cell><cell>64</cell><cell>16.7</cell><cell>40.2</cell><cell>52.3</cell><cell>9</cell></row><row><cell>TACo, 2021 [20]</cell><cell>S3D (HowTo100M)</cell><cell>128</cell><cell>16.6</cell><cell>40.3</cell><cell>53.1</cell><cell>9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II WE</head><label>II</label><figDesc>COMPARE OUR APPROACH WITH SOTA METHODS FOR VIDEO RETRIEVAL ON MSR-VTT 1K-B SPLIT AND FULL SPLIT. WE OVERPASS THEM IN ALL METRICS WHEN NO PRE-TRAINING WITH THE SAME FEATURE SET ON 1K-B SPLIT. ON FULL SPLIT, WE ONLY OVERPASS THEM IN THE FIRST TWO METRICS, WHICH ARE MORE FUNCTIONAL TO REAL-WORLD APPLICATIONS.</figDesc><table><row><cell>Method</cell><cell>Visual Backbone</cell><cell>Batch Size</cell><cell>R@1?</cell><cell cols="2">Text-to-Video Retrieval R@5? R@10?</cell><cell>MdR?</cell></row><row><cell></cell><cell cols="2">Split 1k-B</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random</cell><cell>-</cell><cell>-</cell><cell>0.1</cell><cell>0.5</cell><cell>1.0</cell><cell>500</cell></row><row><cell>MoEE, 2018 [5]</cell><cell>SENet-154 (ImageNet) + R(2+1)D (IG-65m) +</cell><cell>-</cell><cell>13.6</cell><cell>37.9</cell><cell>51.0</cell><cell>10</cell></row><row><cell>JPoSE, 2019 [49]</cell><cell>TSN + Flow</cell><cell>-</cell><cell>14.3</cell><cell>38.1</cell><cell>53.0</cell><cell>9</cell></row><row><cell>UniVL (v1*), 2020 [18]</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>-</cell><cell>14.6</cell><cell>39.0</cell><cell>52.6</cell><cell>10</cell></row><row><cell>SwAMP, 2021 [19]</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>128</cell><cell>15.0</cell><cell>38.5</cell><cell>50.3</cell><cell>10</cell></row><row><cell></cell><cell>SENet-154 (ImageNet) +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CE, 2019 [8]</cell><cell>R(2+1)D (IG-65m) +</cell><cell>-</cell><cell>18.2</cell><cell>46.0</cell><cell>60.7</cell><cell>7</cell></row><row><cell></cell><cell>Six more visual experts</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TACo, 2021 [20]</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>128</cell><cell>19.2</cell><cell>44.7</cell><cell>57.2</cell><cell>7</cell></row><row><cell>RoME</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>32</cell><cell>21.1</cell><cell>50.0</cell><cell>63.1</cell><cell>5</cell></row><row><cell></cell><cell cols="2">Full Split</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random</cell><cell>-</cell><cell>-</cell><cell>0.1</cell><cell>0.5</cell><cell>1.0</cell><cell>500</cell></row><row><cell>Dual Encoding, 2019 [9]</cell><cell>Resnet-152 (ImageNet)</cell><cell>128</cell><cell>7.7</cell><cell>22.0</cell><cell>31.8</cell><cell>32.0</cell></row><row><cell>Alayrac et al., 2020 [7]</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>-</cell><cell>9.9</cell><cell>24.0</cell><cell>32.4</cell><cell>29.5</cell></row><row><cell>HGR, 2020 [3]</cell><cell>Resnet-152 (ImageNet)</cell><cell>128</cell><cell>9.2</cell><cell>26.2</cell><cell>36.5</cell><cell>24</cell></row><row><cell></cell><cell>SENet-154 (ImageNet) +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CE, 2019 [8]</cell><cell>R(2+1)D (IG-65m) +</cell><cell>-</cell><cell>10.0</cell><cell>29.0</cell><cell>42.2</cell><cell>16</cell></row><row><cell></cell><cell>Six more visual experts</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoME</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>32</cell><cell>10.7</cell><cell>29.6</cell><cell>41.2</cell><cell>17</cell></row><row><cell></cell><cell>SENet-154 (ImageNet) +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TeachText-CE, 2021 [48]</cell><cell>R(2+1)D (IG-65m) +</cell><cell>64</cell><cell>11.8</cell><cell>32.7</cell><cell>45.3</cell><cell>13.0</cell></row><row><cell></cell><cell>Six more visual experts</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III WE</head><label>III</label><figDesc>COMPARE OUR APPROACH WITH SOTA METHODS FOR VIDEO-TO-TEXT RETRIEVAL ON YOUCOOK2 VALIDATION SET AND MSR-VTT 1K-B SPLIT. WE OVERPASS THEM IN ALL METRICS WHEN THERE IS NO PRE-TRAINING WITH THE SAME FEATURE SET.</figDesc><table><row><cell>Method</cell><cell>Visual Backbone</cell><cell>Batch Size</cell><cell>R@1?</cell><cell cols="2">Video-to-Text Retrieval R@5? R@10?</cell><cell>MdR?</cell></row><row><cell></cell><cell cols="2">YouCook2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HGR*, 2020 [3]</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>32</cell><cell>4.1</cell><cell>13.0</cell><cell>19.0</cell><cell>85</cell></row><row><cell></cell><cell>Resnet-152 (ImageNet) +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HGR*, 2020 [3]</cell><cell>ResNeXt-101 (Kinetics) +</cell><cell>32</cell><cell>4.2</cell><cell>13.2</cell><cell>18.9</cell><cell>84</cell></row><row><cell></cell><cell>Faster R-CNN (MS COCO)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoME</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>32</cell><cell>4.9</cell><cell>15.9</cell><cell>24.1</cell><cell>55</cell></row><row><cell></cell><cell cols="2">MSR-VTT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>JPoSE, 2019 [49]</cell><cell>TSN + Flow</cell><cell>-</cell><cell>16.4</cell><cell>41.3</cell><cell>54.4</cell><cell>8.7</cell></row><row><cell></cell><cell>SENet-154 (ImageNet) +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CE, 2019 [8]</cell><cell>R(2+1)D (IG-65m) +</cell><cell>-</cell><cell>18.0</cell><cell>46.0</cell><cell>60.3</cell><cell>6.5</cell></row><row><cell></cell><cell>Six more visual experts</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoME</cell><cell>Resnet-152 (ImageNet) + ResNeXt-101 (Kinetics)</cell><cell>32</cell><cell>27.9</cell><cell>58.5</cell><cell>73.1</cell><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV ABLATIONTABLE V</head><label>IVV</label><figDesc>FOR WEIGHTING OPTIONS FOR VISUAL AND TEXTUAL ENCODINGS ON YOUCOOK2 VALIDATION SET AND MSR-VTT 1K-B SPLIT. 'AVERAGE WEIGHTING' OPTION MEANS THAT WE AVERAGE BOTH VISUAL AND TEXTUAL ENCODINGS AFTER CALCULATING THE SIMILARITY SCORE. 'WEIGHTED SUM ON ONLY VISUAL ENCODINGS' MEANS THAT WE APPLY WEIGHTED SUM ON VISUAL ENCODINGS WHILE AVERAGING THE TEXTUAL ENCODINGS BEFORE CALCULATING THE SIMILARITY SCORE. IMPLEMENTING WEIGHTED SUM ON VISUAL ENCODINGS BOOST THE PERFORMANCE. ABLATION STUDY FOR MODEL DESIGN CHOICE ON YOUCOOK2 VALIDATION SET AND MSR-VTT 1K-B SPLIT. SUGGESTING DIFFERENT ATTENTION MECHANISMS FOR GLOBAL AND LOCAL LEVELS IMPROVES THE RESULT.</figDesc><table><row><cell>Weighting Option</cell><cell cols="3">Visual Features Appearance Action</cell><cell cols="2">Object</cell><cell cols="2">R@1?</cell><cell>R@5?</cell><cell>R@10?</cell><cell>MdR?</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">YouCook2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average</cell><cell></cell><cell>2D + 3D</cell><cell>2D + 3D</cell><cell cols="2">2D + 3D</cell><cell>6</cell><cell></cell><cell>16.7</cell><cell>24.6</cell><cell>55</cell></row><row><cell>Weighted Sum on both textual and visual encodings</cell><cell></cell><cell>2D + 3D</cell><cell>2D + 3D</cell><cell cols="2">2D + 3D</cell><cell>5.9</cell><cell></cell><cell>16.8</cell><cell>24.7</cell><cell>54</cell></row><row><cell>Weighted Sum on only textual encodings</cell><cell></cell><cell>2D + 3D</cell><cell>2D + 3D</cell><cell cols="2">2D + 3D</cell><cell>6.1</cell><cell></cell><cell>17</cell><cell>24.8</cell><cell>52</cell></row><row><cell>Weighted Sum on only visual encodings</cell><cell></cell><cell>2D + 3D</cell><cell>2D + 3D</cell><cell cols="2">2D + 3D</cell><cell>6.3</cell><cell></cell><cell>16.9</cell><cell>25.2</cell><cell>53</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">MSR-VTT</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average</cell><cell></cell><cell>2D + 3D</cell><cell>2D + 3D</cell><cell cols="2">2D + 3D</cell><cell>20.8</cell><cell></cell><cell>49.1</cell><cell>62.8</cell><cell>6</cell></row><row><cell>Weighted Sum on both textual and visual encodings</cell><cell></cell><cell>2D + 3D</cell><cell>2D + 3D</cell><cell cols="2">2D + 3D</cell><cell>20.6</cell><cell></cell><cell>49.0</cell><cell>62.3</cell><cell>6</cell></row><row><cell>Weighted Sum on only textual encodings</cell><cell></cell><cell>2D + 3D</cell><cell>2D + 3D</cell><cell cols="2">2D + 3D</cell><cell>21.1</cell><cell></cell><cell>49.7</cell><cell>62.9</cell><cell>6</cell></row><row><cell>Weighted Sum on only visual encodings</cell><cell></cell><cell>2D + 3D</cell><cell>2D + 3D</cell><cell cols="2">2D + 3D</cell><cell>21.1</cell><cell></cell><cell>50.0</cell><cell>63.1</cell><cell>5</cell></row><row><cell>Attention</cell><cell></cell><cell cols="3">Visual Features Appearance Action</cell><cell cols="2">Object</cell><cell cols="2">R@1?</cell><cell>R@5?</cell><cell>R@10?</cell><cell>MdR?</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">YouCook2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Self-attention for all levels</cell><cell></cell><cell>2D + 3D</cell><cell cols="2">2D + 3D</cell><cell cols="2">2D + 3D</cell><cell>6</cell><cell>16.2</cell><cell>24.5</cell><cell>55</cell></row><row><cell cols="2">Self-attention in the appearance level, Cross-modal + Self-attention for others</cell><cell>2D + 3D</cell><cell cols="2">2D + 3D</cell><cell cols="2">2D + 3D</cell><cell>6</cell><cell>16.7</cell><cell>24.6</cell><cell>55</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">MSR-VTT</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Self-attention for all levels</cell><cell></cell><cell>2D + 3D</cell><cell cols="2">2D + 3D</cell><cell cols="2">2D + 3D</cell><cell cols="2">20.4</cell><cell>48.1</cell><cell>62.6</cell><cell>6</cell></row><row><cell cols="2">Self-attention in the appearance level, Cross-modal + Self-attention for others</cell><cell>2D + 3D</cell><cell cols="2">2D + 3D</cell><cell cols="2">2D + 3D</cell><cell cols="2">20.8</cell><cell>49.1</cell><cell>62.8</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI ABLATIONTABLE VII THE</head><label>VIVII</label><figDesc>ON VISUAL FEATURE SETTING ON YOUCOOK2 DATASET. CONCATENATED APPEARANCE AND ACTION FEATURES BRINGS AN INCREASE, RATHER THAN USING THEM IN THE DIFFERENT LEVELS SEPARATELY. ABOVE PART SHOWS THE ABLATION ON VISUAL FEATURE SETTINGS WITH ANOTHER MODEL DESIGN ON YOUCOOK2 VALIDATION SET. THE RESULT SHOWS THAT CONCATENATED 2D AND 3D FEATURES PERFORMS THE BEST. THE BELOW PART INDICATES THE ABLATION ON MODEL DESIGN CHOICE WITH ANOTHER FEATURE SETTING ON YOUCOOK2 DATASET. THE RESULT SHOWS THAT OUR DESIGN CHOICE PERFORMS BETTER THAN HAVING ONLY SELF-ATTENTIONS.</figDesc><table><row><cell>Attention</cell><cell>Appearance</cell><cell>Visual Features Action</cell><cell>Object</cell><cell>R@1?</cell><cell>R@5?</cell><cell>R@10?</cell><cell>MdR?</cell></row><row><cell>Self-attention in the appearance level, Cross-modal + Self-attention for others</cell><cell>2D</cell><cell>2D</cell><cell>2D</cell><cell>5.7</cell><cell>16.7</cell><cell>24.1</cell><cell>56</cell></row><row><cell>Self-attention in the appearance level, Cross-modal + Self-attention for others</cell><cell>2D</cell><cell>3D</cell><cell>RoI</cell><cell>5.7</cell><cell>16.6</cell><cell>24.2</cell><cell>54</cell></row><row><cell>Self-attention in the appearance level, Cross-modal + Self-attention for others</cell><cell>2D + 3D</cell><cell>2D + 3D</cell><cell>2D + 3D</cell><cell>6</cell><cell>16.7</cell><cell>24.6</cell><cell>55</cell></row><row><cell>Attention</cell><cell cols="2">Visual Features Appearance Action</cell><cell>Object</cell><cell>R@1?</cell><cell>R@5?</cell><cell>R@10?</cell><cell>MdR?</cell></row><row><cell cols="5">Visual Feature Setting Ablation on Another Model Design</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Self-attention for all levels</cell><cell>2D</cell><cell>2D</cell><cell>2D</cell><cell>5.5</cell><cell>16.3</cell><cell>24.1</cell><cell>58</cell></row><row><cell>Self-attention for all levels</cell><cell>2D</cell><cell>3D</cell><cell>RoI</cell><cell>5.3</cell><cell>16.2</cell><cell>23.1</cell><cell>57</cell></row><row><cell>Self-attention for all levels</cell><cell>2D + 3D</cell><cell>2D + 3D</cell><cell>2D + 3D</cell><cell>6</cell><cell>16.2</cell><cell>24.5</cell><cell>55</cell></row><row><cell cols="5">Model Design Ablation on Another Feature Setting</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Self-attention for all levels</cell><cell>2D</cell><cell>3D</cell><cell>RoI</cell><cell>5.3</cell><cell>16.3</cell><cell>23.1</cell><cell>57</cell></row><row><cell>Self-attention in the appearance level, Cross-modal + Self-attention for others</cell><cell>2D</cell><cell>3D</cell><cell>RoI</cell><cell>5.7</cell><cell>16.6</cell><cell>24.2</cell><cell>54</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research is supported by the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme (Project A18A2b0046).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic concept discovery for large-scale zero-shot event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Composite concept discovery for zero-shot video event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habibian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">ICMR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fine-grained video-text retrieval with hierarchical graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<idno type="DOI">10.1145/3206025.3206064</idno>
		<ptr target="https://doi.org/10.1145/3206025.3206064" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on ICMR</title>
		<meeting>the 2018 ACM on ICMR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02516</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Use what you have: Video retrieval using representations from collaborative experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in arXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dual encoding for zero-example video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9338" to="9347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-modal Transformer for Video Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic role aware correlation transformer for text to video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Satar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hongyuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="1334" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Segmentation in the perception and memory of events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kurby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zacks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="72" to="81" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceiving, remembering, and communicating structure in events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Zacks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Y</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental psychology. General</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="58" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08024</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2598339</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2016.2598339" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="664" to="676" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unified visual-semantic embeddings: Bridging vision and language with structured meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6602" to="6611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Avlnet: Learning audio-visual language representations from instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boggust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Univl: A unified video and language pre-training model for multimodal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Swamp: Swapped assignment of multi-modal pairs for crossmodal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05814</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Taco: Token-aware cascade contrastive learning for video-text alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">08</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Polysemous visual-semantic embedding for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">VD-BERT: A Unified Vision and Dialog Transformer with BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="3325" to="3338" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">wman: Weaklysupervised moment alignment network for text-based video segment retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJx4rerFwB" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Selfsupervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Masking modalities for cross-modal video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2111.01300" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Object-aware video-language pre-training for retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Shou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2112.00656" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coot: Cooperative hierarchical transformer for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ging</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/ff0abbcc0227c9124a804b084d161a2d-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="605" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1810.07212" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Masking modalities for cross-modal video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-modal dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">VLMo: Unified vision-language pre-training with mixture-of-modality-experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Simple bert models for relation extraction and semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC, Proceedings</title>
		<imprint>
			<publisher>Springer/Verlag</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need,&quot; in NIPS</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">BERT: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17344" />
	</analytic>
	<monogr>
		<title level="j">in AAAI</title>
		<imprint>
			<biblScope unit="page" from="7590" to="7598" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="6546" to="6555" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">EMNLP. ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Teachtext: Crossmodal generalized distillation for text-video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-V</forename><surname>Bogolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">VI. BIOGRAPHY SECTION Burak Satar is a third-year Ph.D. candidate majoring in video understanding with multi-modal features in a joint program at the School of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen ; A*star</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singapore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">under SINGA scholarship. He received his M.Sc. degree in Electronic Engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Computer Science and Engineering Nanyang Technological University, Singapore and Institute for Infocomm Research</orgName>
		</respStmt>
	</monogr>
	<note>ICCV. His research interests include causality in vision, multimedia retrieval, and computer vision</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">He is currently a Research Scientist with the Institute for Infocomm Research, A*STAR, Singapore. His research interests include multimedia content analysis and segmentation</title>
	</analytic>
	<monogr>
		<title level="m">Hongyuan Zhu received the B.S.degree in software engineering from the University of Macau, in 2010, and the Ph.D. degree in computer engineering from Nanyang Technological University</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Singapore (and its predecessors) in October 1990. His research experience includes connectionist expert systems, neural-fuzzy systems, handwritten recognition, multi-agent systems, content-based image retrieval, scene/object recognition, and medical image analysis, with over 280 international refereed journal and conference papers and 25 patents (awarded and pending)</title>
	</analytic>
	<monogr>
		<title level="m">ACM MM 2013, the Best Paper Honorable Mention in ACM SIGIR 2016, and TOMM best paper</title>
		<meeting><address><addrLine>Hangzhou, China; Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-01" />
		</imprint>
		<respStmt>
			<orgName>Hanwang Zhang is currently an Assistant Professor at the School of Computer Science and Engineering, Nanyang Technological University, Singapore. He was a research scientist at the Department of Computer Science, Columbia University, USA. He received the B.Eng (Hons.) degree in computer science from Zhejiang University ; National University of Singapore ; National University of Singapore and his Ph.D. degree in Computer Science &amp; Engineering from the University of New South Wales ; Computer Science and Engineering, Nanyang Technological University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">D. Thesis Award of School of Computing</note>
	<note>He was also the co-Director of IPAL (Image, Pervasive Access Lab), a French-Singapore Joint Lab (UMI 2955. He was bestowed the title of &apos;Chevalier dans l&apos;ordre des Palmes Academiques&apos; by the French Government in 2008 and the National Day Commendation Medal by the Singapore Government in 2010</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
