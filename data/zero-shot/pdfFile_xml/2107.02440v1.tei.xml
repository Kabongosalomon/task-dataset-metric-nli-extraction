<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hierarchical Dual Model of Environment-and Place-Specific Utility for Visual Place Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><forename type="middle">Varma</forename><surname>Keetha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Milford</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourav</forename><surname>Garg</surname></persName>
						</author>
						<title level="a" type="main">A Hierarchical Dual Model of Environment-and Place-Specific Utility for Visual Place Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual Place Recognition (VPR) approaches have typically attempted to match places by identifying visual cues, image regions or landmarks that have high "utility" in identifying a specific place. But this concept of utility is not singular -rather it can take a range of forms. In this paper, we present a novel approach to deduce two key types of utility for VPR: the utility of visual cues 'specific' to an environment, and to a particular place. We employ contrastive learning principles to estimate both the environment-and place-specific utility of Vector of Locally Aggregated Descriptors (VLAD) clusters in an unsupervised manner, which is then used to guide local feature matching through keypoint selection. By combining these two utility measures, our approach achieves state-of-the-art performance on three challenging benchmark datasets, while simultaneously reducing the required storage and compute time. We provide further analysis demonstrating that unsupervised cluster selection results in semantically meaningful results, that finer grained categorization often has higher utility for VPR than high level semantic categorization (e.g. building, road), and characterise how these two utility measures vary across different places and environments. Source code is made publicly available at https://github.com/Nik-V9/HEAPUtil.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Mobile robot localization can be challenging due to extreme variations in scene appearance and camera viewpoint, which affect key robot capabilities including semantic scene understanding and Visual Place Recognition (VPR). Several solutions have been proposed in the literature to improve VPR, including contrastive representation learning <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, domain translation <ref type="bibr" target="#b2">[3]</ref>, sequential matching <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b5">[5]</ref>, semantic saliency <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref> and hierarchical matching <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref>. Many of these methods tend to learn salient visual cues that can improve VPR, but which are typically non-interpretable <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[10]</ref>, or in the case of those based on explicit semantics, require human input for interpretation and performance enhancement <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b7">[7]</ref>.</p><p>In this work, we propose novel visual utility estimation techniques that not only lead to state-of-the-art VPR performance, but also offer semantic interpretability. For example, they enable insights into the role of coarse (buildings) vs fine-grained semantics (parts of buildings), as well as the varying relevance of a particular semantic class (road) in different environmental contexts (city vs rail traverse). We present a novel hierarchical VPR pipeline that uses global descriptors to guide local feature matching in a more unified <ref type="bibr" target="#b0">1</ref> The author is with the Indian Institute of Technology (ISM) Dhanbad, India. <ref type="bibr" target="#b1">2</ref> The authors are with the School of Electrical Engineering and Robotics, QUT, Brisbane, Australia. The authors acknowledge continued support from the Queensland University of Technology (QUT) through the Centre for Robotics.  <ref type="figure">Fig. 1</ref>. We propose Environment-Specific (ES) and Place-Specific (PS) utility estimation methods that determine the relevance of unique visual cues in a reference map. Our combined ES and PS utility, estimated from the global NetVLAD descriptors, guides SuperPoint's keypoint selection (cyan mask) to obtain correct feature correspondences, where vanilla SuperPoint fails due to matches found on pedestrians and vehicles. manner. Moving beyond existing hierarchical VPR methods which only pass candidate hypotheses from the coarse to fine stage, we also estimate the visual utility of different elements in the scene through a VLAD-based global descriptor (NetVLAD <ref type="bibr" target="#b0">[1]</ref>). In particular, we use the cluster-level VLAD representations to estimate a cluster's utility in environmentspecific and place-specific manner for a given reference map. Here, environment-specific refers to estimating utility at a global level applicable to all the places, whereas placespecific refers to estimating utility at a local level applicable to that particular place only. Without requiring any special iterative training, the proposed method is tested on different environments, where we show that the combination of global environment-specific and local place-specific utility leads to informed local feature matching and reduces storage and compute requirements (see <ref type="figure">Fig 1)</ref>.</p><p>We make the following specific contributions:</p><p>? an unsupervised method for estimating the global environment-specific (ES) and local place-specific (PS) utility of visual elements represented as VLAD clusters; ? a more unified hierarchical global-to-local VPR pipeline where utility estimated from global descriptors guides local feature matching; ? a combined ES and PS utility-based method which achieves state-of-the-art VPR performance while offering reduced storage and compute time properties; and ? a 'bridge' between human semantics and automated segmentation-based understanding of visual relevance for VPR, achieved through several visualizations and qualitative insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Global and Local Descriptors for VPR</head><p>VPR is commonly posed as an image retrieval problem, where an image is described by a global descriptor or a set of local descriptors and keypoints to match with other images. Recent surveys <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref> have reviewed the many representations used to describe images for VPR, ranging from hand-crafted features such as SIFT <ref type="bibr" target="#b15">[15]</ref> to learned global descriptors such as NetVLAD <ref type="bibr" target="#b0">[1]</ref>, AP-GeM <ref type="bibr" target="#b1">[2]</ref>, and DeLG <ref type="bibr" target="#b16">[16]</ref>; local descriptors such as SuperPoint <ref type="bibr" target="#b10">[10]</ref> and DeLF <ref type="bibr" target="#b16">[16]</ref>; and local matchers such as SuperGlue <ref type="bibr" target="#b17">[17]</ref>.</p><p>Furthermore, hierarchical approaches have been used in several VPR, and SLAM pipelines where global descriptors are used to retrieve top candidates and local feature matching is used to obtain the best match amongst the top candidates <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>. One such approach, HF-Net <ref type="bibr" target="#b9">[9]</ref> proposed a 'monolithic' CNN to simultaneously learn global NetVLAD descriptors and local SuperPoint features for 6-DoF localization. However, in such hierarchical approaches, the global descriptors are not used to guide the local feature matching, and are limited to providing top matching candidates. In this context, our proposed hierarchical method uses unsupervised utility estimated from global descriptors to guide local feature matching, and is applicable to any existing hierarchical VPR method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visual Feature Selection</head><p>The VPR problem has been posed in many ways ranging from a classification task <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b16">[16]</ref> to contrastive learning <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b21">[21]</ref>. Most methods share the core intuition of attempting to represent images of the same place similarly. However, environments are filled with distractors, and so much work has attempted to automatically learn and identify the areas of an image with the most utility for the VPR task. a) Semantics Based: As surveyed recently <ref type="bibr" target="#b22">[22]</ref>, visual semantics is an emerging area of research in the field of robotics with huge potential for VPR and localization. A number of methods have demonstrated the use of semantic information or distinctive and informative visual elements for improving VPR <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b11">[11]</ref>. However, these methods rely largely on human-based semantic categories, where relevant information is retained based on human intuition of the semantic classes, for example, buildings <ref type="bibr" target="#b7">[7]</ref>, roads <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b11">[11]</ref>, lanes <ref type="bibr" target="#b23">[23]</ref> and the skyline <ref type="bibr" target="#b25">[25]</ref>. Such approaches also tend to require segmentation masks or supervision involving semantic labels to endow the system with higher-level semantic knowledge <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b6">[6]</ref>. More recently, <ref type="bibr" target="#b27">[27]</ref> explored using more fine-grained semantic categories beyond those derived from humans, showing promising potential. b) Region and Attention Based: Past methods have adopted region-based approaches, including grid-based region selection, region proposal networks, Hashing based landmark detection, and Convolutional Neural Network (CNN) activations based region extraction <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>. Other approaches involve using attention to weigh image features based on a relevance criterion <ref type="bibr" target="#b32">[32]</ref>. However, even though semantics and saliency are strongly intertwined, there are no methods that leverage learned descriptors' inherent semantic properties for this problem. In this context, our proposed framework estimates the utility of NetVLAD <ref type="bibr" target="#b0">[1]</ref> clusters, leveraging their intrinsic semantic properties. c) Place-Specific Feature Selection: Place-specific learning has been explored previously <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref> but these methods require significant training. <ref type="bibr" target="#b36">[36]</ref> proposed an image-specific and spatially-localized detection of confusing features using local Term frequency-Inverse document frequency (tf-idf) weighting, however, it does not consider a simultaneous global environment-level utility as proposed in this work. Furthermore, we show that our method can be employed under challenging appearance conditions and is not limited to city-like environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head><p>In this section, we first present the proposed unsupervised techniques to estimate environment-and place-specific feature utility for VPR. We then describe our unified approach to hierarchical VPR, where the coarse global descriptor matching stage guides the fine local feature matching stage via keypoint filtering based on utility estimates (see <ref type="figure" target="#fig_0">Fig 2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Utility Estimation</head><p>VLAD based place representations <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[6]</ref> have been demonstrated to achieve high performance VPR, in particular the recent deep learning based adaptations such as NetVLAD <ref type="bibr" target="#b0">[1]</ref>. Our approach here is motivated by the observation that the cluster assignment of NetVLAD descriptors has inherent semantic properties, whose level of detail varies with the number of clusters. For example, a NetVLAD descriptor with 16 clusters 1 results in cluster assignment that is analogous to human-based semantics while also comprising fine-grained segmentation of typical broad semantic classes like buildings and roads. These observations lead to the hypothesis we pursue here: instead of comparing the full concatenated VLAD descriptor, if we compare the aggregated residuals at cluster level, the distribution of cluster-wise distances within the reference map can be used to estimate that particular cluster's utility for VPR. The second component of this hypothesis is that clusters with lower cluster-wise distances tend to cause high perceptual aliasing. We formulate this procedure in accordance with the well established max-margin based contrastive learning regime, as described in the following subsections.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Maximizing cluster-wise margins:</head><p>Contrastive learning has been demonstrated to achieve state-of-the-art performance for representing places <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b21">[21]</ref>. With the use of triplets, that is, an anchor (X a ), positive (X p ) and a negative (X n ), descriptors are typically learnt such that the margin, ?, between the anchor-negative distance and the anchor-positive distance is maximized.</p><formula xml:id="formula_0">? = X a ? X n 2 ? X a ? X p 2<label>(1)</label></formula><p>In this work, we maximize this margin in a non-iterative manner since only the reference map traverse is used, unlike the typical use of multiple place views <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. We further adapt the computation of this margin with two key points: a) the triplets are represented with cluster-wise aggregated residuals instead of considering a full concatenated VLAD vector and b) the anchor-positive distance in our case approaches zero, since we only consider a single traverse where nearby images are similar. The margin is thus computed for each cluster independently using only the anchor-negative distances, and is referred to as utility from here. Thus, the higher the utility of a cluster, the lower the perceptual aliasing it causes. This process can be used at both the environment-level (that is, globally across the full reference map) and place-level (that is, specific to individual local places), respectively referred to as environment-specific and place-specific, as discussed further here.</p><p>2) Place-Specific Utility: High-utility clusters tend to indicate salient areas of a particular place. Based on this observation, we formulate a per cluster place-specific utility estimation which, when sorted, gives us a relative saliency ranking for each cluster at the particular place in the reference map.</p><p>For a given Image I (considered as a unique place) from a particular geographical location in the Reference Map R, a positive localization radius P exists where places (images) within this radius are considered positives. Also, a nonnegative localization radius 2P is considered beyond which all places (images) are considered negatives. Let us suppose that there are Z a negatives in R for anchor Image a such that n 1 , n 2 , ? ? ? , n z represent each negative. Then the place-specific utility of cluster K, given that it exists, at that particular place with anchor image a in R can be formulated as:</p><formula xml:id="formula_1">( PS U K ) a = 1 Z a n z ? n=n 1 (V K ) a ? (V K ) n 2<label>(2)</label></formula><p>where V K represents the sum of residuals for the cluster K.</p><p>3) Environment-Specific Utility: At an environment-level, clusters with a low variance in residual values across the reference map contain objects with high perceptual aliasing for that particular environment. We hypothesize that such clusters vary by property of the specific environment and such global utility can guide the place-specific utility to avoid non-relevant clusters for that environment while also preventing transient errors.</p><p>Considering all N places 2 in the reference map R, the Environment-Specific utility, ES U K , of a cluster K, is formulated as:</p><formula xml:id="formula_2">ES U K = 1 N(N ? 1) N ? a=1 n z ? n=n 1 (V K ) a ? (V K ) n 2<label>(3)</label></formula><p>where V K represents the cluster-level representation of a place, that is, the sum of residuals, for cluster K. Once the utility values for all the K clusters are determined, k-means segregation <ref type="bibr" target="#b2">3</ref> with k=2 is used to divide the clusters into two bins. VLAD clusters falling in the bin with high utility are regarded as environment-specific high utility clusters, while others are discarded as dustbin clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Unified Hierarchical Visual Place Recognition</head><p>When local feature matching pipelines compare local descriptors, they can struggle to discard distractors within an image or areas of the image with high perceptual aliasing. We propose to increase their robustness to aliasing by filtering the local descriptors and keypoints based on our place-specific and environment-specific utility estimated from global descriptors, with an additional benefit being the reduced storage and compute requirements. Distinct from existing work, our hierarchical VPR better unifies the global and local feature stages as the keypoint utility is directly estimated through the utility of VLAD clusters of the global descriptors.</p><p>1) Global Descriptor Matching: We use NetVLAD representation as our global descriptor. A query image descriptor is matched with the reference global descriptors using Euclidean distance to retrieve top C matching candidates.</p><p>In order to obtain a cluster-level segmentation mask, we replace NetVLAD's differentiable soft cluster assignment with its original counterpart of hard cluster assignment, leading to a mask of size 40?30 corresponding to the spatial dimensions of NetVLAD's last convolutional layer tensor. This cluster assignment mask is then rescaled to the original image size 640 ? 480. These cluster assignments along with the top C candidate matches are then passed on to the local feature matching stage.</p><p>2) Local Feature Matching: For local feature matching, we use SuperPoint <ref type="bibr" target="#b10">[10]</ref> (SP) descriptors/keypoints. Given the reference images database R, cluster assignment mask from the global descriptors and keypoint spatial locations, we employ the place-specific and environment-specific utility for SP descriptors/keypoints filtering in the following manner: a) Environment-Specific keypoint filtering: Based on the unsupervised environment specific utility estimation, we select the SP keypoints corresponding to environment-specific high utility clusters. b) Place-Specific keypoint filtering: Once the place-specific utility of each cluster for an Image I in the Reference map R is estimated, the utility values of all the clusters are sorted to obtain a relative cluster saliency ranking for that specific Image I. Based on this place-specific cluster saliency ranking, we use the Top X Clusters to select SP keypoints. c) Combined keypoint filtering: We propose a combination of the environment-specific and place-specific utility approaches. Initially, the SP keypoints of an image are subsampled using the Top X clusters formulation, and then a further filtering is performed to obtain keypoints belonging only to environment-specific high utility clusters.</p><p>Once the filtered SP keypoints for all the images in the reference map are obtained, we consider two state-of-the-art feature matching pipelines to match the SP descriptors of the query with the filtered SP descriptors of the top C reference candidates obtained through global descriptor matching: i) Based on SuperPoint's matching pipeline <ref type="bibr" target="#b10">[10]</ref>, the descriptors of query and reference image are first matched using absolute Euclidean distance-based Nearest Neighbor (NN) search with mutual NN cross-check, which is followed by geometric verification using RANSAC based homography with a pixel threshold of 3.</p><p>ii) Based on SuperGlue's matching pipeline <ref type="bibr" target="#b17">[17]</ref>, a graph neural network takes as input SuperPoint keypoints and descriptors to produce inliers between an image pair.</p><p>For a matched image pair, inliers are used to compute the match score:</p><formula xml:id="formula_3">c f inal = arg max c?C p I p Q + p c<label>(4)</label></formula><p>where p I , p Q and p c are the number of inliers, the number of SP keypoints in the query, and the number of filtered SP keypoints in the candidate reference image, respectively. Amongst the top C candidates, the candidate with the highest match score is selected as the final match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We used three widely used benchmark datasets to evaluate our proposed approach: Berlin Kudamm <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b31">[31]</ref>, Oxford Robot Car <ref type="bibr" target="#b40">[40]</ref>, and Nordland <ref type="bibr" target="#b41">[41]</ref>. All the datasets present challenging scenarios for VPR in terms of substantial viewpoint shift and drastic shift in visual appearance due to seasonal cycles or time of day, as described below:</p><p>1) Berlin Kudamm: This dataset is downloaded from the crowd-sourced photo mapping platform Mapillary where two different perspectives of the same route are captured. In this dataset, confusing objects and dynamic distractors such as vehicles and pedestrians with homogeneous scenes lead to perceptual aliasing. The substantial viewpoint shift in particular adds to the complexity. The total traverse span is about 3 Km, where the reference traverse contains 314 frames and the query traverse has 280 frames. In both the traverses, all the frames are geotagged.</p><p>2) Oxford RobotCar: This dataset contains traverses of Oxford city captured during different seasonal cycles and times of the day. We use a subsampled version of the Overcast Summer and Autumn Night traverses <ref type="bibr" target="#b4">4</ref> . We use GPS data to subsample the original data to obtain a total traverse span of 1.5 Km, resulting in a total of 213 frames in summer and 251 frames during the night, with frame spacing of approximately 5-6 meters. The Overcast Summer and Autumn Night traverse provide a drastic shift in visual appearance due to season and time of day.</p><p>3) Nordland: This dataset captures a 728 km train journey during different seasonal cycles. We use the Summer and Winter traverse for our experiments. We use the first 7000 images from both the traverses, which are uniformly subsampled to obtain 1000 images for the reference summer traverse and 467 for the query winter traverse. The combination of widespread vegetation and occasional unique objects in this dataset presents a challenging scenario on top of extreme appearance variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation</head><p>We use Recall@1 as the performance metric since the output of a VPR system can be typically employed for precise 6-DoF SLAM/localization <ref type="bibr" target="#b42">[42]</ref>. For a given localization radius, Recall@K is defined as the ratio of correctly retrieved queries within the top K predictions to the total number of queries. We use a ground truth localization radius of 50 meters, 45 meters and 1 frame respectively for Berlin, Oxford and Nordland datasets. For our place-specific utility estimation, we use top 10 clusters and for the combined environment-and placespecific system, we use top X ? 1 clusters, where X is the number of useful clusters determined by the environmentspecific system. We provide full parameter sweeps in the results section for sensitivity analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baseline Comparisons</head><p>We use vanilla NetVLAD (Pitts30K trained <ref type="bibr" target="#b0">[1]</ref>), vanilla SuperPoint <ref type="bibr" target="#b10">[10]</ref> and vanilla SuperPoint + SuperGlue as baseline in the results, represented as NetVLAD, Vanilla Super-Point (SP), and Vanilla SP + SuperGlue (SG) respectively. In all our local feature based methods including other baselines (described below), we use NetVLAD top-20 candidates to select the final match using local feature matching.</p><p>We also provide two additional baselines: Semantic Segmentation Consistency and Cluster Consistency, which employ human-level semantics in place of our ES &amp; PS utility in the proposed hierarchical VPR framework. These baselines are based on previous work leveraging human-level semantics <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b11">[11]</ref> or cluster-based fine-grained semantics <ref type="bibr" target="#b27">[27]</ref>, where semantic or cluster label consistency across reference and query is used to improve the matching framework.</p><p>For the Semantic Consistency baseline, we generate semantic segmentation masks for all reference and query images based on the Cityscapes <ref type="bibr" target="#b39">[39]</ref> scheme. We then filter the SP keypoints to only retain points belonging to buildings, vegetation, and roads for Oxford and Berlin (this particular choice of semantic classes is similar to the selection by <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b6">[6]</ref>); and the ones belonging to buildings, vegetation and terrain for Nordland. In <ref type="bibr" target="#b6">[6]</ref>, implicit keypoint correspondences are first obtained and then semantic label consistency is imposed. For the Semantic Consistency baseline considered in this paper, we first obtain the keypoints from the chosen semantic classes and then use the vanilla SuperPoint's feature matching pipeline to find keypoint correspondences. Similarly, for the Cluster Consistency baseline, we select the clusters corresponding to the aforementioned semantic classes respectively for all three datasets based on visual inspection.</p><p>Finally, we also demonstrate the use of our proposed method on an existing hierarchical 6-DoF localization pipeline HF-Net <ref type="bibr" target="#b9">[9]</ref> but in the context of VPR. In <ref type="table" target="#tab_2">Table I</ref>, we include results for HF-Net's MobileNetVLAD as a global descriptor, vanilla HF-Net (using their global and local descriptors in accordance with the vanilla SP pipeline) and proposed utility-based keypoint filtering applied to vanilla HF-Net's local descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS &amp; DISCUSSION</head><p>In this section, we first present the key quantitative results from testing the proposed framework on three benchmark datasets. We then provide a qualitative analysis with visualizations and insights from both Environment-Specific and Place-Specific cluster utility. <ref type="table" target="#tab_2">Table I</ref> and <ref type="figure">Fig 3 show</ref> the performance of the proposed pipeline on all three benchmark datasets. We also present all the seven baselines' performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Performance Characteristics</head><p>Across all datasets and all baseline matching systems (SP, SP+SG, HF-Net), it can be observed that the Environment-Specific (ES) utility results in improved recall in most cases with noticeable reduction in storage and compute time. On the other hand, the Place-Specific (PS) system performs close to its respective vanilla method but significantly reduces storage and compute requirements. Hence, the combined ES + PS method balances the trade-off between recall and efficiency advantages, leading to improved efficiency than ES alone and consistently superior recall performance as compared to the vanilla methods (with only exception being the Oxford dataset when using SP+SG and HF-Net). <ref type="figure">Fig 3 shows</ref> the full performance curves for the PS and ES+PS systems for SuperPoint-based matching. Performance saturates at a relatively small number of top-X clusters. When employing a combined filtering approach based on both ES and PS Utility, this peak performance is improved and achieved rather earlier.</p><p>1) Storage &amp; Compute Time Benefits: <ref type="figure">Fig 4(a)</ref> shows the ES utility-based filtering resulting in a consistent reduction of the reference map size, measured in terms of total number of local descriptors stored. This leads to reduction in compute time. In particular, for the Oxford dataset, where perceptual aliasing is high due to day-night matching, the ES utilitybased filtering leads to an increase in performance while only requiring storage of 55% of the original descriptors.</p><p>Both the standalone PS and combined PS+ES system is able to retain near peak performance while reducing the number of Top-X clusters selected for PS utility. In particular for the Berlin dataset, PS+ES continues to outperform the vanilla system even with 40% storage and 70% computetime requirements. At 20% storage and 50% compute-time, performance is still competitive with Vanilla SP. Similar trends can be observed for the Oxford and Nordland datasets. Further computational and storage gains are likely achievable with complementary methods including quantization, binarization, hashing and dimension reduction <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b37">[37]</ref>.</p><p>2) Effect of Vocabulary/Cluster Size: To further understand the effect of the VLAD vocabulary size (number of clusters) on the proposed pipeline's performance, we present an ablation using 8, 16, 64, 96, and 128 clusters in the proposed pipeline. <ref type="figure">Fig 4(b)</ref> shows the performance of Vanilla NetVLAD, Vanilla SuperPoint and ES Utility methods on   Berlin for varying numbers of clusters. It can be observed that the use of 16 clusters for ES offers relatively bigger jump in performance than other cluster size values, which even surpasses high baseline performance for cluster size 96 and 128. Although the distinctiveness of individual clusters increases with the vocabulary size, the results suggest that their relative utility ranking does not remain meaningful enough to noticeably improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Analysis of ES &amp; PS Utility</head><p>Our quantitative analyses presented above show how the proposed ES and PS utility methods achieve state-of-theart performance while also reducing the overall storage and compute time requirements. In this section, we present insights and visualizations of segmentation masks obtained from ES, PS and their combined utility estimation. Visualization of ES utility (second column), PS utility (third column) and ES+PS utility (last column). For ES visualizations, low utility is represented in red and high in gray..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Environment-Specific Utility:</head><p>Prior work <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b11">[11]</ref> was based on an assumption that a handful of broad-level semantic classes such as buildings, vegetation, and roads are more important to VPR. <ref type="figure">Fig 5 depicts</ref> patches from low and high ES utility clusters for all three datasets. It can be observed that generic distractors such as cars, pedestrians and sky are discarded by ES utility, which helps in improving performance and is in line with a broad semantics-based utility. However, a cluster representing "a large white planar patch with a window grid" in the Berlin dataset (top left, red) and "large road patches" in the Oxford dataset (top right, red) are deemed to have low utility due to their frequent occurrence, leading to high perceptual aliasing. This demonstrates that determining feature utility for VPR based on a broad-level semantic class is not sufficient, thus fine-grained representations (often a subset of a broad-level semantic class) specific to an environment can effectively determine cluster utility to improve performance.</p><p>In the Nordland dataset, there is a small watermark at the top-right corner of the image, which often leads to false local keypoint matches for vanilla SuperPoint. While this specific example could be trivially filtered, it demonstrates a key property of the methods presented here. As shown in <ref type="figure">Fig 5,</ref> in the Nordland cluster patches, this frequently-occurring watermark is assigned to a low environment-specific utility cluster which represents "text based visual elements".  <ref type="figure" target="#fig_2">Fig 6(c)</ref> right) specific to a particular place is marked as more important than other elements. Similarly, in <ref type="figure" target="#fig_2">Fig 6 c)</ref> right, the road visible on the left side is a highly-informative place-specific cue and is considered as high utility by the PS system. As a particular characteristic of the Nordland dataset, we observed that pixels belonging to vegetation, clouds and sky sometimes emerged as a single cluster and thus cumulatively assigned a middling place-specific utility. This could potentially be mitigated through a dynamic selection of cluster size per place and can be explored in future work. 3) Combined ES + PS Utility: <ref type="figure">Fig 7 shows</ref> how ES and PS Utility complement each other and their combination leads to better utility ranking. For instance, in Berlin, both PS and ES utility consider buildings as important on both sides of the road but ES discards the window panes as discussed previously, leading to improved ES+PS performance. Similarly, as shown in <ref type="figure">Fig 7 for</ref> Nordland, PS assigned high utility to a small part of the railway track and watermark but ES utility discarded them. Hence, in the combined ES+PS system, the PS utility-based filtering becomes more robust to perceptual aliasing while also avoiding transient errors.</p><p>4) Case Study: Frequent Virtual Landmark: In order to analyze the effect of uniqueness or frequency of an object in the reference traverse on utility estimation, we present a controlled experiment on the Oxford Summer day traverse. We introduce a unique virtual landmark (traffic sign) in the image such that it is well-aligned with the road across the traverse. We introduce this landmark in the traverse at t t h image to create four duplicate reference traverses where t varies as 50, 10, 4, and 1 for Sparse, Moderate, High, and Dense setting respectively. As the frequency of the added landmark increased, the PS and ES utility values of that particular cluster decreased. In <ref type="figure" target="#fig_5">Fig 8,</ref> it can be observed that for the Sparse setting, the landmark is marked as a salient object since it only appears at very few places across the traverse. Conversely, for the Dense scenario, it can be observed that the landmark is marked as the least salient by both PS and ES utility estimation. This study shows the effectiveness of the proposed unsupervised method where regardless of any prior knowledge of the virtual landmark or specific training, the utility is correctly estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>Discriminatively identifying individual places from a set of already-seen images is a critical and challenging problem for VPR. Estimating the uniqueness of visual cues and their relevance to VPR is crucial in the context of this problem. In this research, we proposed a novel approach to deduce the utility of visual cues 'specific' to an environment and a particular place, unified through a pipeline that guides keypoint filtering at the local feature matching stage. Our proposed pipeline leads to consistent state-of-the-art performance on three standard benchmark datasets exhibiting challenging appearance change and viewpoint shift, while simultaneously reducing the storage and compute time requirements.</p><p>A number of areas are of interest for further investigation. We employed a contrastive learning approach here but there may be other better suited learning schemes. Moving beyond the two utility measures investigated here, it may be profitable to learn a much larger number of measures of utility, for example learning a measure of utility for local areas that lies partway between specific places and whole environment measures. The ultimate ideal number of utility measures may depend on their complementarity, which could also be assessed. While we have shown here that finer grained semantic segmentation below broad classes like road or building may have particular utility for VPR, further research could investigate the relationship between humandefined categories and those which are most useful for VPR. Collectively, future work in this area will help further improve the capabilities of these systems while also bridging the divide between human navigation and autonomous navigation systems, with potential additional benefits in areas like human robot interaction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Schematic of our proposed approach. In the offline stage, global and local descriptors are extracted from the reference database images, and environment-specific (ES) and place-specific (PS) utility is estimated to further filter the local keypoints. During the online localization stage, for a given query, the top C matching candidates are retrieved from the reference database using global descriptor matching. The final place match is then obtained through local feature matching of query image features with the high utility features of the candidates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>(a) Performance of ES + PS Utility with respect to storage and compute time relative to vanilla SuperPoint for Oxford and Berlin, and (b) performance variations on Berlin with respect to the vocabulary size, using Vanilla NetVLAD, Vanilla SuperPoint and ES Utility. Example patches from clusters with low (red) &amp; high (blue) environment-specific utility for Berlin, Oxford and Nordland.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Visualizations of PS utility ranking of different clusters, where utility decreases from blue to green to red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 7. Visualization of ES utility (second column), PS utility (third column) and ES+PS utility (last column). For ES visualizations, low utility is represented in red and high in gray..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 )</head><label>2</label><figDesc>Place-Specific Utility: Fig 6 shows the importance ranking visualizations based on PS utility, with blue being most useful and red the least. While the behavior of PS is similar to ES, notable examples can be observed in an open vegetative environment of Nordland, where an extra set of railway track (cyan colored in Fig 6(c) left) and a part of road (blue and cyan colored in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Visualization of ES (left) and PS (right) utility ranking when a virtual landmark is placed (a) sparsely or (b) densely in the traverse. For ES visualizations, low utility is represented in red and high in gray.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I QUANTITATIVE</head><label>I</label><figDesc>RESULTS: PERFORMANCE COMPARISON ON THREE BENCHMARK DATASETS. Performance Vs Top-X cluster keypoints. Results displayed via single markers at X = 16 use all the clusters and their keypoints (except Semantic Consistency).</figDesc><table><row><cell></cell><cell></cell><cell>Berlin</cell><cell></cell><cell></cell><cell>Oxford</cell><cell></cell><cell></cell><cell>Nordland</cell><cell></cell></row><row><cell>Methods</cell><cell cols="9">Recall@1 Storage Time Recall@1 Storage Time Recall@1 Storage Time</cell></row><row><cell>NetVLAD [1]</cell><cell>38.21</cell><cell>-</cell><cell>-</cell><cell>46.61</cell><cell>-</cell><cell>-</cell><cell>9.21</cell><cell>-</cell><cell>-</cell></row><row><cell>Vanilla SuperPoint (SP) [10]</cell><cell>46.07</cell><cell>1</cell><cell>1</cell><cell>72.11</cell><cell>1</cell><cell>1</cell><cell>14.99</cell><cell>1</cell><cell>1</cell></row><row><cell>Semantic Consistency</cell><cell>44.64</cell><cell>0.66</cell><cell>0.83</cell><cell>64.14</cell><cell>0.81</cell><cell>0.90</cell><cell>13.91</cell><cell>0.58</cell><cell>0.90</cell></row><row><cell>Cluster Consistency</cell><cell>43.21</cell><cell>0.49</cell><cell>0.74</cell><cell>58.96</cell><cell>0.62</cell><cell>0.80</cell><cell>11.99</cell><cell>0.69</cell><cell>0.96</cell></row><row><cell>Ours: ES Utility</cell><cell>50.36</cell><cell>0.79</cell><cell>0.88</cell><cell>74.10</cell><cell>0.53</cell><cell>0.76</cell><cell>16.06</cell><cell>0.96</cell><cell>0.99</cell></row><row><cell>Ours: PS Utility</cell><cell>47.14</cell><cell>0.48</cell><cell>0.73</cell><cell>69.32</cell><cell>0.37</cell><cell>0.71</cell><cell>14.56</cell><cell>0.90</cell><cell>0.97</cell></row><row><cell>Ours: ES + PS Utility</cell><cell>49.64</cell><cell>0.70</cell><cell>0.84</cell><cell>74.10</cell><cell>0.48</cell><cell>0.74</cell><cell>16.06</cell><cell>0.94</cell><cell>0.98</cell></row><row><cell>Vanilla SP + SuperGlue (SG) [17]</cell><cell>59.64</cell><cell>1</cell><cell>1</cell><cell>86.45</cell><cell>1</cell><cell>1</cell><cell>20.34</cell><cell>1</cell><cell>1</cell></row><row><cell>Ours: ES Utility</cell><cell>61.07</cell><cell>0.79</cell><cell>0.88</cell><cell>86.06</cell><cell>0.53</cell><cell>0.76</cell><cell>20.12</cell><cell>0.96</cell><cell>0.99</cell></row><row><cell>Ours: PS Utility</cell><cell>51.43</cell><cell>0.48</cell><cell>0.73</cell><cell>82.86</cell><cell>0.37</cell><cell>0.71</cell><cell>20.34</cell><cell>0.90</cell><cell>0.97</cell></row><row><cell>Ours: ES + PS Utility</cell><cell>59.64</cell><cell>0.70</cell><cell>0.84</cell><cell>86.06</cell><cell>0.48</cell><cell>0.74</cell><cell>20.98</cell><cell>0.94</cell><cell>0.98</cell></row><row><cell>HF-Net's MobileNetVLAD [9]</cell><cell>35.36</cell><cell>-</cell><cell>-</cell><cell>60.55</cell><cell>-</cell><cell>-</cell><cell>16.91</cell><cell>-</cell><cell>-</cell></row><row><cell>Vanilla HF-Net</cell><cell>46.78</cell><cell>1</cell><cell>1</cell><cell>86.00</cell><cell>1</cell><cell>1</cell><cell>27.83</cell><cell>1</cell><cell>1</cell></row><row><cell>Ours: ES Utility</cell><cell>48.57</cell><cell>0.79</cell><cell>0.86</cell><cell>84.46</cell><cell>0.52</cell><cell>0.77</cell><cell>29.34</cell><cell>0.97</cell><cell>0.99</cell></row><row><cell>Ours: PS Utility</cell><cell>45.35</cell><cell>0.63</cell><cell>0.79</cell><cell>81.67</cell><cell>0.47</cell><cell>0.75</cell><cell>26.34</cell><cell>0.90</cell><cell>0.97</cell></row><row><cell>Ours: ES + PS Utility</cell><cell>47.85</cell><cell>0.70</cell><cell>0.81</cell><cell>85.66</cell><cell>0.47</cell><cell>0.75</cell><cell>28.69</cell><cell>0.95</cell><cell>0.98</cell></row><row><cell>Fig. 3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1"><ref type="bibr" target="#b16">16</ref> was chosen as being the closest to typical number of semantic classes for road-based datasets such as Cityscapes<ref type="bibr" target="#b39">[39]</ref>. Please refer to Section V-A.2 for an ablation study on vocabulary size.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Unlike SLAM, for global re-localization the map size is known a priori.<ref type="bibr" target="#b2">3</ref> The use of terms 'segregation' and 'bins' instead of clustering and clusters for k-means is intentional to avoid confusing it with VLAD clusters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Originally 2015-03-17-11-08-44 and 2014-12-16-18-44-24 in<ref type="bibr" target="#b40">[40]</ref> </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5297" to="5307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning with average precision: Training image retrieval with a listwise loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R D</forename><surname>Souza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5107" to="5116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Addressing challenging place recognition tasks using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="2349" to="2355" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Seqnet: Learning descriptors for sequencebased hierarchical place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Delta descriptors: Change-based place representation for robust visual localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5120" to="5127" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Lost? appearance-invariant place recognition for opposite viewpoints using visual semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suenderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05526</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantics-aware visual localization under challenging perceptual conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Appearance-only SLAM at large scale with FAB-MAP 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1100" to="1123" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From coarse to fine: Robust hierarchical localization at large scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dymczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Superpoint: Selfsupervised interest point detection and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="224" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Xview: Graph-based semantic multi-view localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gawel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Don</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1687" to="1694" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey on deep visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Masone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Where is your place, visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual place recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lowry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unifying deep local and global features for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="726" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4938" to="4947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Orb-slam: a versatile and accurate monocular slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on robotics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1147" to="1163" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lsd-slam: Large-scale direct monocular slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sch?ps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning features at scale for visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3223" to="3230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fine-tuning cnn image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1655" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantics for robotic mapping, perception and interaction: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cosgun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
		<idno type="DOI">10.1561/2300000059</idno>
		<ptr target="http://dx.doi.org/10.1561/2300000059" />
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Robotics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="224" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Laneloc: Lane marking based localization using highly accurate maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kn?ppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Localization from semantic observations via the matrix permanent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Atanasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Pappas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="73" to="99" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Skyline-based localisation for aggressively manoeuvring robots using uv sensors and spherical harmonics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Differt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5615" to="5622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic signatures for urban visual localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Soheilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gouet-Brunet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Content-Based Multimedia Indexing (CBMI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fine-grained segmentation networks: Self-supervised segmentation for improved long-term visual localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="31" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Only look once, mining distinctive landmarks from convnet for visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Maffra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A holistic visual place recognition approach using lightweight cnns for significant viewpoint and appearance changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khaliq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ehsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcdonald-Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="561" to="569" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning context flexible attention model for long-term visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4015" to="4022" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Place recognition with convnet landmarks: Viewpoint-robust, condition-robust, training-free</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pepperell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems XI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep image retrieval: Learning global representations for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="241" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">What makes paris look like paris?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saurabh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Abhinav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Josef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (SIG-GRAPH)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning and calibrating per-location classifiers for visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="907" to="914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scene signatures: Localised and point-less features for localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mcmanus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Avoiding confusing features in place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="748" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">24/7 place recognition by view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1808" to="1817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">1 year, 1000 km: The oxford robotcar dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Single-view place recognition under seasonal changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Olid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>F?cil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPNIV Workshop at</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Vpr-bench: An open-source visual place recognition evaluation framework with quantifiable viewpoint and appearance change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kooij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcdonald-Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ehsan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards life-long visual localization using an efficient matching of binary sequences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Alcantarilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Robot. Autom</title>
		<imprint>
			<biblScope unit="page" from="6328" to="6335" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Relocalization under substantial appearance changes using hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vysotska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. Intell. Robot. Syst. Worksh</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
