<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 BANet: A Blur-aware Attention Network for Dynamic Scene Deblurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Jen</forename><surname>Tsai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Tsung</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Chung-Chi</forename><surname>Tsai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 BANet: A Blur-aware Attention Network for Dynamic Scene Deblurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image deblurring</term>
					<term>blur-aware attention module</term>
					<term>region-wise pooling attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image motion blur results from a combination of object motions and camera shakes, and such blurring effect is generally directional and non-uniform. Previous research attempted to solve non-uniform blurs using self-recurrent multiscale, multi-patch, or multi-temporal architectures with selfattention to obtain decent results. However, using self-recurrent frameworks typically leads to a longer inference time, while interpixel or inter-channel self-attention may cause excessive memory usage. This paper proposes a Blur-aware Attention Network (BANet), that accomplishes accurate and efficient deblurring via a single forward pass. Our BANet utilizes region-based selfattention with multi-kernel strip pooling to disentangle blur patterns of different magnitudes and orientations and cascaded parallel dilated convolution to aggregate multi-scale content features. Extensive experimental results on the GoPro and RealBlur benchmarks demonstrate that the proposed BANet performs favorably against the state-of-the-arts in blurred image restoration and can provide deblurred results in real-time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Dynamic scene deblurring or blind motion deblurring aims to restore a blurred image with little knowledge about the blur kernel. Scene blur caused by camera shakes, object motions, low shutter speeds, or low frame rates not only degrades the quality of taken images/videos but also results in information loss. Therefore, removing such blurring artifacts to recover image details becomes essential to many downstream Performance comparison on the GoPro test dataset in terms of deblurring quality and runtime complexity. The proposed BANet performs favorably against the state-of-the-art methods in both accuracy and efficiency.</p><p>vision applications, such as facial detection <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, text recognition <ref type="bibr" target="#b2">[3]</ref>, moving object segmentation <ref type="bibr" target="#b3">[4]</ref>, etc., where clean and sharp images are appreciated. Although significant progress has been made in conventional and deep-learningbased approaches <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>, we observe a compromise between accuracy and speed. Owing to this observation, we target to develop an efficient and effective algorithm in this paper for blurred image restoration with its current performance in accuracy and speed shown in <ref type="figure">Fig. 1</ref>.</p><p>Deep-learning-based approaches usually reach superior results, given their better feature representation capability toward dynamic scenes. Among the state-of-the-art architectures for deblurring, self-recurrent models have been widely adopted to leverage blurred image repeatability in either multiple scales (MS) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, multiple patch levels (MP) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, or multiple temporal behaviors (MT) <ref type="bibr" target="#b7">[8]</ref>, as shown in <ref type="figure">Fig. 2</ref>(a)-(c). Specifically, the MS models distill multi-scale blur information in a self-recurrent manner and restore blurred images based on the extracted coarse-to-fine features <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. However, scaling a blurred image to a lower resolution often results in losing edge information <ref type="bibr" target="#b7">[8]</ref>. In contrast, the MP models split a blurred input image into multiple patches to estimate and then remove motion blurs of different scales <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b12">[13]</ref>. However, splitting the blurred input and features into equal-sized non-overlapping patches may cause cause contextual information discontinuity, sub-optimal for handling nonuniform blur in dynamic scenes. In <ref type="bibr" target="#b7">[8]</ref> In addition to model architectures, recent research studies <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> further exploit self-attention to address blur non-uniformity. Suin et al. <ref type="bibr" target="#b12">[13]</ref> utilize MP-based processing with self-attention to extract features for areas with global and local motions. However, using a self-recurrent mechanism to generate multi-scale features often leads to a significantly longer inference time. To shorten the latency, Purohit and Rajagopalan <ref type="bibr" target="#b13">[14]</ref> selectively aggregate features through learnable pixel-wise attention <ref type="bibr" target="#b14">[15]</ref> enabled by deformable convolutions for modeling local blurs in a single forward pass. Despite its effectiveness, self-attention exploring pixel-wise or channelwise correlations via trainable filters often causes high memory usage, thus only applicable to small-scale features <ref type="bibr" target="#b13">[14]</ref>. Furthermore, motion blurs coming from object motions manifest smeared effects and produce directional and local averaging artifacts, which cannot be handled well by inter-pixel/channel correlations.</p><p>This paper proposes a Blur-aware Attention Network (BANet) to overcome the above-mentioned issues. BANet is an efficient yet effective single-forward-pass model, as illustrated in <ref type="figure">Fig. 2(d)</ref>, which achieves state-of-the-art deblurring performance while working in real-time, as shown in <ref type="figure">Fig. 1</ref>. Specifically, our model stacks multiple layers of the Blur-Aware Module (BAM) for removing motion blurs. BAM separates the deblurring process into two branches, Blur-aware Attention (BA) and Cascaded Parallel Dilated Convolution (CPDC), where BA locates region-wise blur orientations and magnitudes while CPDC adaptively removes blurs based on the attended blurred features. Based on an observation of directional and regional averaging artifacts caused by dynamic blurs, the proposed BA derives region-wise attention by using computationally inexpensive regional averaging to capture blurred patterns of different orientations and magnitudes globally and locally. To derive the orientations and magnitudes of different blurred regions in an image, we reassemble horizontal and vertical blurred responses to catch irregular blur orientations and utilize multi-scale kernels to learn the magnitudes. CDPC leverages two cascaded multiscale dilated convolutions to deblur image features. As a result, BANet possesses the superior deblurring capability and can support subsequent real-time applications superbly.</p><p>In short, our contributions are two-fold. First, BANet is featured with a novel BAM module that exploits region-wise attention to capture blur orientations and magnitudes, making BANet capable of disentangling blur contents of different degrees in dynamic scenes. With the disentangled region-wise blurred patterns, it then utilizes cascaded multi-scale dilated convolution to restore blurred features. Second, our efficient single-forward-pass deep networks perform favorably against state-of-the-art methods with fast inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Conventional Methods</head><p>Dynamic Scene image deblurring is a highly ill-posed problem since blurs stem from various factors in the real world. Conventional image deblurring studies often make different assumptions, such as uniform <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b18">[19]</ref> or nonuniform <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b23">[24]</ref> blurs, and image priors <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b28">[29]</ref>, to model blur characteristics. Namely, these methods impose different constraints on the estimated blur kernels, latent images, or both with handcrafted regularization terms for blur removal. Nevertheless, these methods often attempt to solve a nonconvex optimization problem and involve heuristic parameter tuning that is entangled with the camera pipeline; thus, they cannot generalize well to complex real-world examples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deblurring via Learning</head><p>Learning-based approaches with self-recurrent modules gain great success in single-image deblurring. Particularly, the coarse-to-fine schemes can gradually restore a sharp image on different resolutions (MS) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, fields of view (MP) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b12">[13]</ref>, or temporal characteristics (MT) <ref type="bibr" target="#b7">[8]</ref>. Despite the success, self-recurrent models usually lead to longer inference runtime. Recently, non-recurrent methods <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b33">[34]</ref> were proposed for efficient deblurring. For instance, Kupyn et al. <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> suggested using conditional generative adversarial networks to restore blurred images. However, these methods do not well address non-uniform blurs in dynamic scenes, often causing blur artifacts in the deblurred images. To address this issue, Yuan et al. <ref type="bibr" target="#b31">[32]</ref> proposed a spatially variant deconvolution network with optical flow estimation to guide deformable convolutions and capture moving objects during model training. Li et al. <ref type="bibr" target="#b32">[33]</ref> proposed a depth-guided model for deblurring. However, the optical flow and depth information may not always correlate with blur, which may cause less effective deblurring. Cho et al. <ref type="bibr" target="#b33">[34]</ref> proposed an efficient multi-scale deblurring structure with a multi-input multi-output. With multi-scale input, the process adopts a shallow convolution to turn the images into attention masks and multiply them by the same scales' features. However, its simple feature attention mechanism may not be able to extract blur information comprehensively from an input image, hence limiting its deblurring performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Self-attention</head><p>Self-attention (SA) <ref type="bibr" target="#b34">[35]</ref> has been widely adopted to advance the fields of image processing <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b35">[36]</ref> and computer vision <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Recent advances <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> revealed that attention is beneficial for learning inter-pixel correlations to emphasize different local features for removing non-uniform blur. Specifically, Purohit et al. <ref type="bibr" target="#b13">[14]</ref> proposed to deblur using SA to explore pixel-wise correlation for non-local feature adaptation. However, since SA requires much memory in O(H 2 W 2 ) space, where H and W are the height and width of the input to SA, the method can only apply SA to the smallest-scale features (from a 1280 ? 720 blurred input to 160 ? 90 SA's input), limiting the efficacy of SA. Also, motion blurs cause directional and local averaging artifacts, which merely pixel-wise SA may not address well. Suin et al. <ref type="bibr" target="#b12">[13]</ref> proposed an MP architecture with less memoryintensive SA by using global average pooling with space complexity O(d a d c HW ), where d a is the channel dimension of the components query and key in SA, d c is the dimension of the component value, and d a d c &lt; HW . Despite the method's less space complexity, compressing pixel information into the channel domain may lose spatial information, thus degrading deblurring performance. In contrast, we propose an efficient and low memory-cost regional averaging SA to capture non-uniform blur information more accurately. It is with space complexity O(CHW ), where C is the number of output channels. It can deblur high-resolution input images and achieve superior performance in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head><p>We present the blur-aware attention network (BANet) to address the potential issues in two commonly used techniques for deblurring: self-recurrence and self-attention. Selfrecurrent algorithms result in longer inference time due to repeatedly accessing input blurred images. Self-attention based on inter-pixel or inter-channel correlations is memory intensive and cannot explicitly capture regional blurring information. Instead, the proposed BANet is a one-pass residual network consisting of a series of stacked blur-aware modules (BAMs), which serve as the building blocks, to disentangle different patterns of blurriness and remove blurs based on the attended blurred features.</p><p>As illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, BANet starts with two convolutional layers, which contain a stride of 2 to downsample the input image to half resolution. BANet employs one transposed convolutional layer to upsample features to the original size. In between, we stack a set of BAMs to correlate regions with similar blur and extract multi-scale content features. A BAM consists of two components, BA and CPDC, where BA distills global and local blur orientations and magnitudes, and CPDC captures multi-scale blurred patterns to eliminate blurs adaptively. Combining BA and CPDC, BAM is a residuallike architecture that derives both global and local multi-scale blurring features in a learnable manner. We detail the two key components, BA and CPDC, in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HxWxC</head><p>HxWxC HxWxC A. Blur-aware Attention (BA)</p><p>To accurately restore the motion area displaying directional and averaging artifacts caused by object motions and camera shakes, we propose a region-based self-attention module, called BA, to capture such effects in the global (image) and local (patch) scales. As shown in <ref type="figure">Fig. 4</ref>, BA contains two cascaded parts: multi-kernel strip pooling (MKSP) and attention refinement (AR). MKSP catches multi-scale blurred patterns of different magnitudes and orientations, followed by AR to refine them locally. a) Multi-Kernel Strip Pooling (MKSP): Hou et al. <ref type="bibr" target="#b38">[39]</ref> presented an SP (strip pooling) method that uses horizontal and vertical one-pixel long kernels to extract long-range bandshape context information for scene parsing. SP averages the input features within a row or a column individually and then fuses the two thin-strip features to discover global cross-region dependencies. Let the input feature maps x = [x i,j,c ] ? R H?W ?C , where C denotes the number of channels. Applying SP to x generates a vertical and a horizontal tensor followed by a 1D convolutional layer with a kernel size of 3. This produces a vertical tensor</p><formula xml:id="formula_0">y v = [y v i,c ] ? R H?C and a horizontal tensor y h = [y h j,c ] ? R W ?C , where y v i,c = 1 W W ?1 j=0 x i,j,c and y h j,c = 1 H H?1 i=0 x i,j,c .</formula><p>The SP operation, after a convolution layer, fuses the two tensors into y = [y i,j,c ] ? R H?W ?C , where y i,j,c = y v i,c + y h j,c , and then turns the fused tensor into an attention mask M sp as</p><formula xml:id="formula_1">M sp = ? sig (f 1 (y)),<label>(1)</label></formula><p>where f 1 is a 1 ? 1 convolutional layer and ? sig (?) is the sigmoid function. Although SP has shown its effects on segmenting band-shape objects for scene parsing, it is unsuitable to directly apply SP to an image deblurring task, aiming to locate blurred patterns that tend to involve different orientations and magnitudes, and restore a sharp image. Motivated by SP, we propose MKSP that adopts strip pooling with different kernel sizes to discover regional and directional averaging artifacts caused by dynamic blurs.</p><p>MKSP combines and compares multiple sizes/scales of averaging results followed by concatenation and convolution to catch blurred patterns of different magnitudes and orientations. The idea behind our design is to reassemble different orientations by horizontal and vertical operations on multiscale results, e.g., the difference between consecutive kernel sizes, and reveal the scales of blurred patterns. We apply convolutional layers to automatically discover these blur-aware operations on the feature level to learn irregular attended features rather than a fixed cropping method on the image level used in MP methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b12">[13]</ref>. MKSP averages the input tensors within rows and columns by adaptive average pooling to generate H ? n ? C and n ? W ? C long features, where n ? {1, 3, 5, 7} represents different scales. Thus, MKSP generates four pairs of tensors, each of which has a vertical and a horizontal tensor followed by a 1D (for n = 1) or 2D (for the rest) convolutional layer with the kernel size of 3 or 3 ? 3, respectively. This produces the vertical tensor y v,n ? R H?n?C and the horizontal tensor y h,n ? R n?W ?C , where the vertical tensor is</p><formula xml:id="formula_2">y v,n i,j,c = 1 K h K h ?1 k=0 x i,(j?S h +k),c ,<label>(2)</label></formula><p>where the horizontal stride S h = W n and the horizontalstrip kernel size K h = W ? (n ? 1)S h . Symmetrically, the horizontal tensor is defined by</p><formula xml:id="formula_3">y h,n i,j,c = 1 K v Kv?1 k=0 x (i?Sv+k),j,c ,<label>(3)</label></formula><p>where the vertical stride S v = H n and the vertical-strip kernel size</p><formula xml:id="formula_4">K v = H ? (n ? 1)S v .</formula><p>After determining the horizontal and vertical magnitudes, the orientations of blur patterns are estimated jointly considering the two orthogonal magnitudes. More specifically, MKSP, after a 1D (for n = 1) or 2D (for the rest) convolutional layer, fuses each pair of tensors (y v,n , y h,n ) into a tensor </p><p>Similar to SP, we concatenate all the fused tensors to yield an attention mask as M mksp = f out (y 1 ? y 3 ? y 5 ? y 7 ), where ? stands for the concatenation operation, and f out (?) = ? sig (Conv(? ReLU (Conv(?)))) represents a non-linear mapping function consisting of two 3?3 convolutional layers. The first layer uses the ReLU activation function, and the second uses a sigmoid function. As shown in <ref type="figure" target="#fig_2">Fig. 5</ref>, the proposed MKSP can generate attention masks that better fit objects or local scenes than those by using SP with only H ?1 and 1?W kernels used, which yields rough band-shape masks. b) Attention Refinement (AR): After obtaining the globally attended features by the element-wise multiplication of attention masks M mksp and input tensor x, we further refine these features locally via a simple attention mechanism using f AR (?). The final output of our BA block through the MKSP and AR stages is computed as</p><formula xml:id="formula_6">f AR (x) ?x,<label>(5)</label></formula><p>where ? represents element-wise multiplication, andx = M mksp ?x denotes the global features extracted using MKSP. Figs. 6(c) and (d) demonstrate that cascading MKSP with AR can refine the attended feature maps. The proposed BA facilitates the attention mechanism applied to deblurring since it requires less memory, i.e. O(HW C), where C represents the channel dimension, than those adopted in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. It disentangles blurred contents with different magnitudes and orientations. <ref type="figure">Fig. 7</ref> showcases three examples of blur content disentanglement using BA, where we witness that background scenes are differentiated from the foreground scenes because those objects closer to the camera move faster, thus more blurred. <ref type="figure">Fig. 8</ref> shows more examples of attention maps yielded by BA, which implicitly acts as a gate for propagating relevant blur contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cascaded Parallel Dilated Convolution (CPDC)</head><p>Atrous convolution, also called dilated convolution, has been widely applied to computer-vision tasks <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> for enlarging receptive fields and extracting features from objects with different scales without increasing the kernel size. Inspired by this, we design a cascaded parallel dilated convolution (CPDC) block with multiple dilation rates to capture multi-scale blurred objects. Instead of stacking dilated convolutional layers with different rates in parallel, which we call parallel dilated convolution (PDC), our CPDC block cascades two sets of PDC with a single convolutional layer working as a fusion bridge. It can distill patterns more beneficial to deblurring before passing through the second PDC. As an example, <ref type="figure">Fig. 9</ref>(a) shows a PDC block consisting of three 3?3 dilated convolutional layers with a dilation rate D (D = 1, 3, and 5), each of which outputs features with half the number of input channels. After concatenation, the number of the output channels of the PDC block increases by 1.5 times. As shown in <ref type="figure">Fig. 9(b)</ref>, our CPDC block consists of two PDC blocks bridged by a 3 ? 3 convolutional layer, which would be more effective in aggregating multi-scale content information for deblurring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss function</head><p>In BANet, we utilize the Charbonnier loss as suggested in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b41">[42]</ref>:</p><formula xml:id="formula_7">L char = ||R ? Y|| 2 + ? 2 ,<label>(6)</label></formula><p>where R and Y respectively denote the restored image and the ground-truth image, and ? = 10 ?3 as in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b41">[42]</ref>. In addition, to enhance the restoration performance, we add an FFT loss to supervise the results in the frequency domain, as adopted in MIMO-UNet+ <ref type="bibr" target="#b33">[34]</ref>:</p><formula xml:id="formula_8">L F F T = ||F(R) ? F(Y)|| 1 ,<label>(7)</label></formula><p>where F represents the fast Fourier transform function. At last, we optimize BANet using the total loss L as</p><formula xml:id="formula_9">L = L char + ?L F F T ,<label>(8)</label></formula><p>where ? is set to 0.01 empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>This section evaluates the proposed method. In the following, we first describe the experimental setup, then compare our method with the state-of-the-arts, and finally conduct ablation studies to analyze the effectiveness of individual components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>We evaluate the BANet on three image deblurring benchmark datasets: 1) GoPro <ref type="bibr" target="#b5">[6]</ref>   raw images and RealBlur-J from JPEG images. We train our model using Adam optimizer with parameters ? 1 = 0.9 and ? 2 = 0.999. We set the initial learning rate to 10 ?4 , which then decays to 10 ?7 based on the cosine annealing strategy. Following <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b31">[32]</ref>, we utilize random cropping, flipping, and rotation for data augmentation. Lastly, we implement our model with PyTorch library on a computer equipped with Intel Xeon Silver 4210 CPU and NVIDIA 2080ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results</head><p>Quantitative Analysis: We compare our method with 11 latest approaches, including MSCNN <ref type="bibr" target="#b5">[6]</ref>, SRN <ref type="bibr" target="#b9">[10]</ref>, DSD <ref type="bibr" target="#b8">[9]</ref>, DeblurGAN-v2 <ref type="bibr" target="#b30">[31]</ref>, DMPHN <ref type="bibr" target="#b6">[7]</ref>, EDSD <ref type="bibr" target="#b31">[32]</ref>, MTRNN <ref type="bibr" target="#b7">[8]</ref>, RADN <ref type="bibr" target="#b13">[14]</ref>, SAPHN <ref type="bibr" target="#b12">[13]</ref>, MIMO-UNet+ <ref type="bibr" target="#b33">[34]</ref>, and MPR-Net <ref type="bibr" target="#b11">[12]</ref>, which also handle dynamic deblurring on the GoPro <ref type="bibr" target="#b5">[6]</ref> test set. For HIDE <ref type="bibr" target="#b42">[43]</ref>, we choose nine recent deblurring methods, including DeblurGAN-v2 <ref type="bibr" target="#b30">[31]</ref>, SRN <ref type="bibr" target="#b9">[10]</ref>, HAdeblur <ref type="bibr" target="#b42">[43]</ref>, DSD <ref type="bibr" target="#b8">[9]</ref>, DMPHN <ref type="bibr" target="#b6">[7]</ref>, MTRNN <ref type="bibr" target="#b7">[8]</ref>, SAPHN <ref type="bibr" target="#b12">[13]</ref>, MIMO-UNet+ <ref type="bibr" target="#b33">[34]</ref>, and MPRNet <ref type="bibr" target="#b11">[12]</ref>, according to their availability in released pre-trained weights. For RealBlur <ref type="bibr" target="#b43">[44]</ref>, we choose four methods that trained on the RealBlur training set, including DeblurGAN-v2 <ref type="bibr" target="#b30">[31]</ref>, SRN <ref type="bibr" target="#b9">[10]</ref>, MPRNet <ref type="bibr" target="#b11">[12]</ref>, and MIMO-UNet+ <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blurred patch DSD MTRNN</head><p>MIMO-Unet+ DMPHN MPRNet BANet+ Blurred Input <ref type="figure">Fig. 10</ref>. Qualitative comparisons on GoPro <ref type="bibr" target="#b5">[6]</ref> test set. The deblurred results listed from left to right are from MTRNN <ref type="bibr" target="#b7">[8]</ref>, DSD <ref type="bibr" target="#b8">[9]</ref>, DMPHN <ref type="bibr" target="#b6">[7]</ref>, MIMO-UNet+ <ref type="bibr" target="#b33">[34]</ref>, MPRNet <ref type="bibr" target="#b11">[12]</ref>, and Ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blurred patch DSD MTRNN</head><p>MIMO-Unet+ DMPHN MPRNet BANet+ Blurred Input <ref type="figure">Fig. 11</ref>. Qualitative comparisons on HIDE <ref type="bibr" target="#b42">[43]</ref> dataset. The deblurred results listed from left to right are from MTRNN <ref type="bibr" target="#b7">[8]</ref>, DSD <ref type="bibr" target="#b8">[9]</ref>, DMPHN <ref type="bibr" target="#b6">[7]</ref>, MIMO-UNet+ <ref type="bibr" target="#b33">[34]</ref>, MPRNet <ref type="bibr" target="#b11">[12]</ref>, and Ours. To better compare with recent approaches, we devise two versions of our model, BANet and BANet+. The only difference between them is the number of channels used in a BAM, and BANet with 128 channels involves 18 million parameters while BANet+ has 40 million parameters with 192 channels. <ref type="table" target="#tab_2">Table I lists the objective scores (PSNR and  SSIM)</ref>, runtime, parameters, and GFLOPs on the GoPro test set for all the compared methods. We observe that the self-recurrent models, MSCNN <ref type="bibr" target="#b5">[6]</ref>, SRN <ref type="bibr" target="#b9">[10]</ref>, DSD <ref type="bibr" target="#b8">[9]</ref>, MTRNN <ref type="bibr" target="#b7">[8]</ref>, SAPHN <ref type="bibr" target="#b12">[13]</ref>, and MPRNet <ref type="bibr" target="#b11">[12]</ref>, consume longer runtime than the non-recurrent ones, i.e., DeblurGAN-v2 <ref type="bibr" target="#b30">[31]</ref>, RADN <ref type="bibr" target="#b13">[14]</ref>, MIMO-UNet+ <ref type="bibr" target="#b33">[34]</ref>, and ours. As reported in  <ref type="bibr" target="#b11">[12]</ref>, by 0.37 dB in PSNR with faster runtime (?113ms) and lower GFLOPs (?172). <ref type="table" target="#tab_2">Table II</ref> shows the quantitative results on HIDE <ref type="bibr" target="#b42">[43]</ref>. As can be seen, BANet outperforms all the compared methods except for MPRNet <ref type="bibr" target="#b11">[12]</ref> with a faster inference time. BANet+ only works comparably to MPRNet <ref type="bibr" target="#b11">[12]</ref> since MPRNet seems to perform favorably on HIDE <ref type="bibr" target="#b42">[43]</ref> particularly, but our model runs much faster.  <ref type="bibr" target="#b30">[31]</ref>, SRN <ref type="bibr" target="#b9">[10]</ref>, MPRNet <ref type="bibr" target="#b11">[12]</ref>, MIMO-UNet+ <ref type="bibr" target="#b33">[34]</ref>, and Ours on RealBlur <ref type="bibr" target="#b43">[44]</ref> test set.</p><p>Qualitative Analysis: <ref type="figure">Fig. 10</ref> and <ref type="figure">Fig. 11</ref> show qualitative comparisons on the GoPro test set and HIDE dataset with previous state-of-the-arts MTRNN <ref type="bibr" target="#b7">[8]</ref>, DSD <ref type="bibr" target="#b8">[9]</ref>, DMPHN <ref type="bibr" target="#b6">[7]</ref>, MIMO-UNet+ <ref type="bibr" target="#b33">[34]</ref>, and MPRNet <ref type="bibr" target="#b11">[12]</ref>. As observed in <ref type="figure">Fig. 10</ref>, MTRNN <ref type="bibr" target="#b7">[8]</ref>, DSD <ref type="bibr" target="#b8">[9]</ref>, DMPHN <ref type="bibr" target="#b6">[7]</ref>, MIMO-UNet+ <ref type="bibr" target="#b33">[34]</ref>, and MPRNet <ref type="bibr" target="#b11">[12]</ref> do not well recover regions with texts or severe blurs whereas BANet can restore those regions better. In <ref type="figure">Fig. 11</ref>, MTRNN <ref type="bibr" target="#b7">[8]</ref>, DSD <ref type="bibr" target="#b8">[9]</ref>, DMPHN <ref type="bibr" target="#b6">[7]</ref>, and MIMO-UNet+ <ref type="bibr" target="#b33">[34]</ref> do not deblur the striped t-shirt and texts well, while BANet recovers those parts better. <ref type="figure">Fig. 12</ref> and <ref type="figure" target="#fig_1">Fig. 13</ref> demonstrate some deblurred results using DeblurGAN-v2 <ref type="bibr" target="#b30">[31]</ref>, SRN <ref type="bibr" target="#b9">[10]</ref>, MPRNet <ref type="bibr" target="#b11">[12]</ref>, MIMO-UNet+ <ref type="bibr" target="#b33">[34]</ref>, and ours, on the RealBlur <ref type="bibr" target="#b43">[44]</ref> test set. As can be seen, although all these models can remove blurs, BANet performs favorably on delicate image details. User Study: We further conduct a user study to evaluate the subjective quality of deblurred results on real blurred images chosen from the RealBlur-J test set. We compare our method (BANet+) against four methods, including MIMO-UNet+ <ref type="bibr" target="#b33">[34]</ref>, MPRNet <ref type="bibr" target="#b11">[12]</ref>, SRN <ref type="bibr" target="#b9">[10]</ref>, and DeblurGAN-v2 <ref type="bibr" target="#b30">[31]</ref>. Note that all the methods are trained on the RealBlur-J training set.</p><p>In the study, 34 subjects aged from 21 to 40 years participated in the study without any prior knowledge of the experiment. Their vision is either normal or corrected to be normal. We picked 16 blurred images with varying scenes for the experiment and obtained the deblurred results using all the compared approaches. Since each method is compared against BANet with all the chosen blurred images in the experiment, we have 16 ? 4 = 64 image pairs in total. Each subject is shown all the image pairs, one at a time, and asked which one he/she prefers in terms of visual quality. Each image pair is displayed randomly and placed side by side. Subjects are asked to check images carefully before choosing without a time limit. <ref type="table" target="#tab_2">Table IV</ref> shows the subjective evaluation results, where the values represent the percentage that the deblurring results with our method are preferred to the counterparts with the other compared methods for all the votes collected. It indicates that our method obtains over 95% preference votes compared to all the compared methods, which again demonstrates that our approach achieves better subjective visual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>In the ablation studies, we do all the experiments on the BANet (18M) version. BAM with Different Components:  the effectiveness of spatial attention for deblurring. Using MKSP in PDC (Net1 vs. Net4) improves PSNR by 0.72 dB, which has much more performance gain compared to using strip pooling (SP) <ref type="bibr" target="#b38">[39]</ref> in Net3 or AR in Net2. Substituting PDC in Net5 with CPDC (Net6), our proposed version of BAM leads to a further performance gain. Thanks to its mechanism for locating blur regions based on both global attention and local convolutions, our BAM attains the best performance while achieving fast inference time.</p><p>Numbers of Stacked BAMs: Using more layers to enlarge the receptive field may improve performance for computer vision or image processing tasks. Nevertheless, stacking more layers for deblurring does not guarantee better performance <ref type="bibr" target="#b12">[13]</ref> and might consume extra inference time. However, using our residual learning-based BAM design, we can stack multiple layers to expand the effective receptive field for better deblurring. In <ref type="table" target="#tab_2">Table VI</ref>, we show performance comparisons with   For a fair comparison, we also compare CPDC against a PDC variant that stacks two PDCs in a series, called PDC 2 , with a similar parameter size, and CPDC still performs better.</p><p>D. Blur-aware Attention vs. Self-Attention RADN <ref type="bibr" target="#b13">[14]</ref> utilizes a similar self-attention (SA) mechanism proposed in <ref type="bibr" target="#b14">[15]</ref> for deblurring. It helps connect regions with similar blurs to facilitate global access to relevant features across the entire input feature maps. However, its high memory usage makes applying it to high-resolution images infeasible. Thus, SA is usually employed in network layers on a smaller scale like in RADN <ref type="bibr" target="#b13">[14]</ref>, where important blur information would be lost due to down-sampling. In contrast, our proposed region-based attention is more suitable for correlating regions with similar blur characteristics. Moreover, it can process highresolution images thanks to its low memory consumption. To further demonstrate our BA's efficacy, we compare the SA <ref type="bibr" target="#b14">[15]</ref> with BA using our BANet (stack-4) as a backbone network, as shown in <ref type="figure">Fig. 14(b)</ref>. Due to the high memory demand for SA (O(H 2 W 2 )) to process 720 ? 1280 images, we adopt our stack-4 model for training. When testing the networks, we separate the input image into eight sub-images for both SA and BA to deblur, each equipped with a single 2080ti GPU. Since our BA requiring lower memory usage (O(CHW ), where C &lt;&lt; H ? W ) can process the image with the full resolution, we also show its result. In <ref type="table" target="#tab_2">Table IX</ref>, SA * and BA * represent deblurring an image with its eight subimages separately, whereas BA for processing the entire image at once. As can be observed, the proposed BA * works much more efficiently than SA * with a comparable result. When deblurring the entire image at once, BA undoubtedly performs the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper proposes a novel blur-aware attention network (BANet) for single image deblurring. BANet consists of stacked blur-aware modules (BAMs) to disentangle regionwise blur contents of different magnitudes and orientations and aggregate multi-scale content features for more accurate and efficient dynamic scene deblurring. We have investigated and examined our design through demonstrations of attention masks and attended feature maps, as well as extensive ablation studies and performance comparisons. Our extensive experiments demonstrate that the proposed BANet achieves realtime deblurring and performs favorably against state-of-theart deblurring methods on the GoPro and RealBlur benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Manuscript received August 16, 2021; revised May 17, 2022 and August 30, 2022; accepted October 13, 2022. Date of publication month day, 2022; date of current version month day, 2022. This work was funded in part by National Science and Technology Council (NSTC) under grants 110-2634-F-002-050, 111-2628-E-A49-025-MY3, 111-2221-E-004-010, and in part by Qualcomm Technologies, Inc., through a Taiwan University Research Collaboration Project, under Grant NAT-487844. The computational and storage resources for this project were provided in part by National Center for Highperformance Computing (NCHC) of National Applied Research Laboratories (NARLabs), Hsinchu, Taiwan. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Wangmeng Zuo. (Corresponding author: Chia-Wen Lin) F.-J. Tsai is with the Department of Electrical Engineering, National Tsing Hua University, Hsinchu 300044, Taiwan. E-mail: fjtsai@gapp.nthu.edu.tw Y.-T. Peng is with the Department of Computer Science, National Chengchi University, Taipei 116011, Taiwan. E-mail: ytpeng@cs.nccu.edu.tw C.-C. Tsai is with Qualcomm Technologies, Inc., San Diego, CA 92121, USA. E-mail: chuntsai@qti.qualcomm.com Y.-Y. Lin is with the Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu 300093, Taiwan. E-mail: lin@cs.nctu.edu.tw C.-W. Lin is with the Department of Electrical Engineering, National Tsing Hua University, Hsinchu 300044, Taiwan, and with the Electronic and Optoelectronic System Research Laboratories, Industrial Technology Research Institute, Hsinchu 310401, Taiwan. (e-mail: cwlin@ee.nthu.edu.tw) Fig. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Architecture of the proposed blur-aware attention networks (BANet). The blur-aware modules (BAM) serve as the building blocks of BANet. The first BAM is detailed in the purple dotted box while the rest are represented by solid purple boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>/PSNR(29.15) Output with MKSP/PSNR(29.47) Visualization of the attention masks of SP and MKSP, and the corresponding effects on the final results on GoPro test set. y n ? R H?W ?C by y n i,j,c = y v,n i, n?j W ,c + y h,n n?i H ,j,c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>(a) Input blurred image in GoPro testing set. (b)-(d) Comparisons among the attended feature maps by using different components of the proposed BA including (b) AR, (c) MKSP, and (d) MKSP + AR. Three disentanglement examples of blurred patterns of different degrees using our BA on GoPro test set. (a) Input blurred images and (b) attended feature maps on different regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Visualization of the blur-aware attended features on GoPro test set, where moving objects in the blurred images are highlighted while background is mostly excluded. These blur-aware masks are crucial for handling blurred images with diverse blur patterns. Architectures of (a) parallel dilated convolution (PDC) and (b) cascaded parallel dilated convolution (CPDC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>that consists of 3, 214 pairs of blurred and sharp images of resolution 720 ? 1280, where 2, 103 pairs are used for training, and the rest for testing, 2) HIDE [43] that contains 2, 025 pairs of HD images, all for testing, and RealBlur [44] that consists of 3, 758 pairs for training and 980 pairs for testing. The RealBlur dataset is further split into two datasets: RealBlur-R collected from The authors from the universities in Taiwan completed the experiments on the datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Architecture of blur-aware attention (BA). It cascades two parts, including multi-kernel strip pooling (MKSP) and attention refinement (AR). It is developed to disentangle blurred contents in an efficient way. See the text for details.</figDesc><table><row><cell>1xWxC Hx1xC 3xWxC 5xWxC 7xWxC Hx3xC Hx5xC Hx7xC</cell><cell>y h,7 y v,5 y v,7 y h,5 y h,3 y v,1 y v,3 y h,1</cell><cell>Conv Conv Conv Conv Conv Conv Conv Conv</cell><cell>HxWxC HxWxC HxWxC HxWxC HxWxC HxWxC HxWxC HxWxC</cell><cell>Add Add Add Add</cell><cell>Concate</cell><cell>Conv</cell><cell>ReLu</cell><cell>Conv</cell><cell>Sigmoid</cell><cell>Conv</cell><cell>Conv Expand ReLu Multiplication Sigmoid</cell></row><row><cell>HxWxC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">HxWxC</cell><cell>HxWxC</cell></row><row><cell>Fig. 4.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I EVALUATION</head><label>I</label><figDesc>RESULTS ON GOPRO TEST SET. THE BEST SCORE IN ITS COLUMN IS HIGHLIGHTED IN BOLD AND THE SECOND BEST IS UNDERLINED. SYMBOL * INDICATES THOSE METHODS WITHOUT RELEASED CODE; THUS WE CITE THE RESULTS FROM THE ORIGINAL PAPERS OR EVALUATE ON THE RELEASED DEBLURRED IMAGES. ALL METHODS ARE TRAINED ON GOPRO TRAINING SET. TIME AND PARAMS ARE MEASURED IN MILLISECOND (MS) AND MILLION (M).</figDesc><table><row><cell>Model</cell><cell cols="5">PSNR ? SSIM ? Time ? Params ? GFLOPs ?</cell></row><row><cell>MSCNN [6]</cell><cell>30.40</cell><cell>0.936</cell><cell>943</cell><cell>12</cell><cell>336</cell></row><row><cell>SRN [10]</cell><cell>30.25</cell><cell>0.934</cell><cell>650</cell><cell>7</cell><cell>167</cell></row><row><cell>DSD [9]</cell><cell>30.96</cell><cell>0.942</cell><cell>1300</cell><cell>3</cell><cell>471</cell></row><row><cell cols="2">DeblurGAN-v2 [31] 29.55</cell><cell>0.934</cell><cell>42</cell><cell>68</cell><cell>42</cell></row><row><cell>DMPHN [7]</cell><cell>31.36</cell><cell>0.947</cell><cell>354</cell><cell>22</cell><cell>235</cell></row><row><cell>EDSD  *  [32]</cell><cell>29.81</cell><cell>0.934</cell><cell>10</cell><cell>1</cell><cell>-</cell></row><row><cell>MTRNN [8]</cell><cell>31.13</cell><cell>0.944</cell><cell>53</cell><cell>3</cell><cell>164</cell></row><row><cell>RADN  *  [14]</cell><cell>31.85</cell><cell>0.953</cell><cell>38</cell><cell>-</cell><cell>-</cell></row><row><cell>SAPHN  *  [13]</cell><cell>32.02</cell><cell>0.953</cell><cell>770</cell><cell>-</cell><cell>-</cell></row><row><cell>MIMO-UNet+ [34]</cell><cell>32.45</cell><cell>0.957</cell><cell>23</cell><cell>16</cell><cell>154</cell></row><row><cell>MPRNet [12]</cell><cell>32.66</cell><cell>0.959</cell><cell>138</cell><cell>20</cell><cell>760</cell></row><row><cell>BANet</cell><cell>32.54</cell><cell>0.957</cell><cell>23</cell><cell>18</cell><cell>264</cell></row><row><cell>BANet+</cell><cell>33.03</cell><cell>0.961</cell><cell>25</cell><cell>40</cell><cell>588</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II EVALUATION</head><label>II</label><figDesc>RESULTS ON HIDE DATASET. THE BEST SCORE IN ITS COLUMN IS HIGHLIGHTED IN BOLD AND THE SECOND BEST IS UNDERLINED. SYMBOL * INDICATES THOSE METHODS WITHOUT RELEASED CODE; THUS WE CITE THE RESULTS FROM THE ORIGINAL PAPERS OR EVALUATE ON THE RELEASED DEBLURRED IMAGES. ALL METHODS ARE TRAINED ON GOPRO TRAINING SET. TIME IS MEASURED IN MILLISECOND (MS).</figDesc><table><row><cell>Model</cell><cell>PSNR ?</cell><cell>SSIM ?</cell><cell>Time ?</cell></row><row><cell>DeblurGAN-v2 [31]</cell><cell>27.40</cell><cell>0.882</cell><cell>42</cell></row><row><cell>SRN [10]</cell><cell>28.36</cell><cell>0.904</cell><cell>424</cell></row><row><cell>HAdeblur  *  [43]</cell><cell>28.87</cell><cell>0.903</cell><cell>-</cell></row><row><cell>DSD [9]</cell><cell>29.10</cell><cell>0.913</cell><cell>1200</cell></row><row><cell>DMPHN [7]</cell><cell>29.10</cell><cell>0.918</cell><cell>341</cell></row><row><cell>MTRNN [8]</cell><cell>29.15</cell><cell>0.918</cell><cell>53</cell></row><row><cell>SAPHN  *  [13]</cell><cell>29.98</cell><cell>0.930</cell><cell>-</cell></row><row><cell>MIMO-UNet+ [34]</cell><cell>30.00</cell><cell>0.930</cell><cell>28</cell></row><row><cell>MPRNet [12]</cell><cell>30.93</cell><cell>0.939</cell><cell>138</cell></row><row><cell>BANet</cell><cell>30.16</cell><cell>0.930</cell><cell>23</cell></row><row><cell>BANet+</cell><cell>30.58</cell><cell>0.935</cell><cell>25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III EVALUATION</head><label>III</label><figDesc>RESULTS ON REALBLUR TEST SET. ALL METHODS ARE TRAINED ON REALBLUR TRAINING SET. TIME IS MEASURED IN MILLISECOND (MS).</figDesc><table><row><cell></cell><cell cols="2">RealBlur-J</cell><cell cols="2">RealBlur-R</cell><cell></cell></row><row><cell>Model</cell><cell cols="5">PSNR ? SSIM ? PSNR ? SSIM ? Time</cell></row><row><cell>DeblurGAN-v2 [31]</cell><cell>29.69</cell><cell>0.870</cell><cell>36.44</cell><cell>0.935</cell><cell>44</cell></row><row><cell>SRN [10]</cell><cell>31.38</cell><cell>0.901</cell><cell>38.65</cell><cell>0.965</cell><cell>420</cell></row><row><cell>MPRNet [12]</cell><cell>31.76</cell><cell>0.922</cell><cell>39.31</cell><cell>0.972</cell><cell>81</cell></row><row><cell>MIMO-UNet+ [34]</cell><cell>31.92</cell><cell>0.919</cell><cell>-</cell><cell>-</cell><cell>23</cell></row><row><cell>BANet</cell><cell>32.00</cell><cell>0.923</cell><cell>39.55</cell><cell>0.971</cell><cell>22</cell></row><row><cell>BANet+</cell><cell>32.42</cell><cell>0.929</cell><cell>39.90</cell><cell>0.972</cell><cell>24</cell></row><row><cell cols="6">Table I, BANet runs faster with fewer parameters and GFLOPs</cell></row><row><cell cols="6">as well as achieves better performance than recurrent-based</cell></row><row><cell cols="6">methods, MSCNN [6], SRN [10], DSD [9], DMPHN [7],</cell></row><row><cell cols="6">MTRNN [8], and SAPHN [13] and non-recurrent methods,</cell></row><row><cell cols="6">such as DeblurGAN-v2 [31] and RADN [14] on the GoPro</cell></row><row><cell cols="6">test set. BANet also performs favorably against an efficient</cell></row><row><cell cols="6">multi-scale model, MIMO-UNet+ [34], with the same run-</cell></row><row><cell cols="6">time and a comparable model size. BANet+ outperforms the</cell></row><row><cell cols="2">best competitor, MPRNet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV RESULTS</head><label>IV</label><figDesc>OF USER STUDY. THE VALUES REPRESENT THE PERCENTAGE THAT OUR METHOD WAS CHOSEN OVER THE OTHER COMPARED METHODS.</figDesc><table><row><cell></cell><cell cols="4">MIMO-UNet+ [34] MPRNet [12] SRN [10] DeblurGAN-v2 [31]</cell></row><row><cell>Ours</cell><cell>95.6%</cell><cell>95.6%</cell><cell>98.5%</cell><cell>99.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table Vshows an ablation on different component combinations in our Blur-Aware Module (BAM) tested on the GoPro test set. As can be seen, adding a simple attention refinement (AR) mechanism to PDC (Net1 vs. Net2) can improve PSNR by 0.41 dB, which showsFig. 13. Examples of deblurred results obtained using DeblurGAN-v2<ref type="bibr" target="#b30">[31]</ref>, SRN<ref type="bibr" target="#b9">[10]</ref>, MPRNet<ref type="bibr" target="#b11">[12]</ref>, MIMO-UNet+<ref type="bibr" target="#b33">[34]</ref>, and Ours on RealBlur<ref type="bibr" target="#b43">[44]</ref> test set.</figDesc><table><row><cell>Blurred Input</cell><cell>Blurred patch</cell><cell>DeblurGAN-v2</cell><cell>SRN</cell><cell>MPRNet</cell><cell>MIMO-Unet+</cell><cell>BANet+</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V ABLATION</head><label>V</label><figDesc></figDesc><table><row><cell></cell><cell cols="6">STUDY ON GOPRO TEST SET USING DIFFERENT COMPONENT</cell></row><row><cell></cell><cell></cell><cell cols="3">COMBINATIONS IN BAM</cell><cell></cell><cell></cell></row><row><cell>Model Net1 Net2 Net3 Net4 Net5 Net6</cell><cell>PDC ? ? ? ? ?</cell><cell>AR ? ? ?</cell><cell>SP ?</cell><cell>MKSP ? ? ?</cell><cell>CPDC ?</cell><cell>PSNR 31.39 31.80 31.81 32.11 32.24 32.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI PERFORMANCE</head><label>VI</label><figDesc>COMPARISONS OF THE STACKING NUMBER OF BAMS IN BANET ON GOPRO TEST SET. Although the quantitative performance improves with the number of BAMs, the improvement became saturated after 12. Therefore, we choose 10 for its excellent balance between efficiency and visual quality. Effectiveness of MKSP and CPDC: InTable VII, we investigate the effects of kernel combination of MKSP on the GoPro test set. MKSP with five kernel sizes of 1, 3, 5, 7, and 9 performs a little worse than the first four sizes (1, 3, 5, and 7), indicating that adding the kernel size of 9 would not catch blur features more accurately, thus not helping with the performance. InTable VIII, we verify that CPDC, which uses a single convolution as a fusion bridge, outperforms PDC.</figDesc><table><row><cell>BANet</cell><cell>stack-4</cell><cell>stack-8</cell><cell>stack-10</cell><cell>stack-12</cell></row><row><cell>PSNR</cell><cell>31.36</cell><cell>32.37</cell><cell>32.54</cell><cell>32.55</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE VII</cell><cell></cell></row><row><cell cols="5">PERFORMANCE COMPARISONS OF STRIP POOLING (SP) AND MKSP ON</cell></row><row><cell></cell><cell cols="3">GOPRO TEST SET WITH PDC.</cell><cell></cell></row><row><cell>BANet</cell><cell>SP</cell><cell>MKSP 135</cell><cell>MKSP 1357</cell><cell>MKSP 13579</cell></row><row><cell>PSNR</cell><cell>31.81</cell><cell>32.03</cell><cell>32.11</cell><cell>32.04</cell></row><row><cell cols="5">various numbers of BAMs stacked in our model on the GoPro</cell></row><row><cell cols="5">test set. We list four versions: stack-4, stack-8, stack-10, and</cell></row><row><cell cols="5">stack-12, corresponding to 4, 8, 10, and 12 BAMs stacked in</cell></row><row><cell>BANet.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VIII ABLATION</head><label>VIII</label><figDesc>STUDY OF CPDC (W/O BA) COMPARED TO PDC (W/O BA) ON GOPRO TEST SET.</figDesc><table><row><cell></cell><cell>PDC 135</cell><cell>PDC 2 135</cell><cell>CDPC</cell></row><row><cell>PSNR</cell><cell>31.39</cell><cell>31.78</cell><cell>32.13</cell></row><row><cell>Parms (M)</cell><cell>6</cell><cell>10</cell><cell>10</cell></row><row><cell>BA</cell><cell>Concate</cell><cell>SA</cell><cell>Concate</cell></row><row><cell>CPDC</cell><cell></cell><cell>CPDC</cell><cell></cell></row><row><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell></row><row><cell cols="4">Fig. 14. Architecture comparisons between (a) our original BAM and (b)</cell></row><row><cell cols="2">BA replaced by SA [15] in BAM.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE IX PERFORMANCE</head><label>IX</label><figDesc>COMPARISON BETWEEN BA AND SA [15] USING BANET (STACK-4) ON GOPRO TEST SET. * REPRESENTS DEBLURRING ON EIGHT SUB-IMAGES INSTEAD OF AN ENTIRE IMAGE.</figDesc><table><row><cell></cell><cell cols="3">SA  BA</cell></row><row><cell>PSNR</cell><cell>31.11</cell><cell>31.09</cell><cell>31.36</cell></row><row><cell>Time (ms)</cell><cell>770</cell><cell>16</cell><cell>12</cell></row></table><note>* BA*</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Coupled learning for facial deblur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="page" from="961" to="972" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deblurring face images using uncertainty guided multi-stream semantic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yasarla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="page" from="6251" to="6263" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blind deblurring of text images using a text-specific hybrid dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="page" from="710" to="723" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint stereo video deblurring, scene flow estimation and moving object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="page" from="1748" to="1761" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Motion blur kernel estimation via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="page" from="194" to="205" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="3883" to="3891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep stacked hierarchical multi-patch network for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="5978" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-temporal recurrent neural networks for progressive non-uniform single image deblurring with incremental temporal training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro. Conf. Comput. Vis</title>
		<meeting>Euro. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="327" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring with parameter selective sharing and nested skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="3848" to="3856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="8174" to="8182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dark and bright channel prior embedded network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="page" from="6885" to="6897" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-stage progressive image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="821" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatially-attentive patchhierarchical network for adaptive motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="3606" to="3615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Region-adaptive dense network for efficient motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2020-02" />
			<biblScope unit="page" from="11" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2009-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Removing camera shake from a single photograph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="page" from="787" to="794" />
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Two-phase kernel estimation for robust motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro. Conf. Comput. Vis</title>
		<meeting>Euro. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010-09" />
			<biblScope unit="page" from="157" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-quality motion deblurring from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single image deblurring using motion density functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro. Conf. Comput. Vis</title>
		<meeting>Euro. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010-09" />
			<biblScope unit="page" from="171" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Space-variant singleimage blind deconvolution for removing camera shake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf. Process. Syst</title>
		<meeting>Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2010-12" />
			<biblScope unit="page" from="829" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast removal of non-uniform camera shake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="463" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Segmentation-free dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="2766" to="2773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-uniform deblurring for shaken images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="491" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Blind image deblurring with local maximum gradient prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image deblurring and denoising using color priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="1550" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deblurring text images via l0-regularized intensity and gradient prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="2901" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deblurring images via dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="2315" to="2328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image deblurring via extreme channels prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017-06" />
			<biblScope unit="page" from="6978" to="6986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="8183" to="8192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="8877" to="8886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient dynamic scene deblurring using spatially variant deconvolution network with optical flow guided training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="3552" to="3561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring by depth guided model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="page" from="5273" to="5288" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking coarse-to-fine approach in single image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4641" to="4650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf. Process. Syst</title>
		<meeting>Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Strip Pooling: Rethinking spatial pooling for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="4002" to="4011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro. Conf. Comput. Vis</title>
		<meeting>Euro. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="404" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Humanaware motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="5571" to="5580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Real-world blur dataset for learning and benchmarking deblurring algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro. Conf. Comput. Vis</title>
		<meeting>Euro. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="184" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu-Jen</surname></persName>
		</author>
		<title level="m">and the M.S. degree in communications engineering from National Tsing-Hua University</title>
		<meeting><address><addrLine>Keelung, Taiwan; Hsinchu, Taiwan; Hsinchu, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>Tsai received the B.S. degree in electrical engineering from National Taiwan Ocean University ; National Tsing-Hua University</orgName>
		</respStmt>
	</monogr>
	<note>respectively. Since 2021, he has been pursuing the Ph.D. degree in the department of electrical engineering. His research interests include image /video restoration and computer vision</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">He joined National Chengchi University in Feb. 2019, where he is currently an Assistant Professor in the computer science department. Before that, he was a Senior Engineer with Qualcomm Technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Tsung</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Member, IEEE) received the Ph.D. in electrical and computer engineering from University of California</title>
		<meeting><address><addrLine>San Diego; Inc., San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>His research interests include image processing. video compression, and machine-learning applications</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
