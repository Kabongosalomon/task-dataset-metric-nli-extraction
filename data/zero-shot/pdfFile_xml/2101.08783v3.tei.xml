<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Person Re-identification Data Augmentation Method with Adversarial Defense Effect</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Mathematics and Information</orgName>
								<orgName type="institution">Fujian Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Mathematics and Information</orgName>
								<orgName type="institution">Fujian Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwen</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Mathematics and Information</orgName>
								<orgName type="institution">Fujian Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Mathematics and Information</orgName>
								<orgName type="institution">Fujian Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Weng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Mathematics and Information</orgName>
								<orgName type="institution">Fujian Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Mathematics and Information</orgName>
								<orgName type="institution">Fujian Normal University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Person Re-identification Data Augmentation Method with Adversarial Defense Effect</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Person Re-identification</term>
					<term>Data Augmentation</term>
					<term>Adversarial Defends</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The security of the Person Re-identification(ReID) model plays a decisive role in the application of ReID. However, deep neural networks have been shown to be vulnerable, and adding undetectable adversarial perturbations to clean images can trick deep neural networks that perform well in clean images. We propose a ReID multi-modal data augmentation method with adversarial defense effect: 1) Grayscale Patch Replacement, it consists of Local Grayscale Patch Replacement(LGPR) and Global Grayscale Patch Replacement(GGPR). This method can not only improve the accuracy of the model, but also help the model defend against adversarial examples; 2) Multi-Modal Defense, it integrates three homogeneous modal images of visible, grayscale and sketch, and further strengthens the defense ability of the model. These methods fuse different modalities of homogeneous images to enrich the input sample variety, the variaty of samples will reduce the over-fitting of the ReID model to color variations and make the adversarial space of the dataset that the attack method can find difficult to align, thus the accuracy of model is improved, and the attack effect is greatly reduced. The more modal homogeneous images are fused, the stronger the defense capabilities is . The proposed method performs well on multiple datasets, and successfully defends the attack of MS-SSIM proposed by CVPR2020 against ReID <ref type="bibr" target="#b9">[10]</ref>, and increases the accuracy by 467 times(0.2% to 93.3%).The code is available at https://github.com/finger-monkey/ReID_Adversarial_Defense</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Although deep learning has been widely studied and fully applied in many computer vision fields, Szegedy et al. <ref type="bibr" target="#b12">[13]</ref> still found interesting weaknesses in deep neural networks. Deep learning models are susceptible to attacks from adversarial samples. Such attacks will cause the neural network to completely change its prediction results, and this shortcoming will be used by attackers, which will have a great negative impact on the reliability and security of the ReID system. These adversarial samples have only added a slight perturbation, so that the human visual system cannot detect this perturbation. The far-reaching implications of such phenomena have attracted many researchers to conduct research in the field of adversarial attack and defense. To examine the robustness of ReID systems is rather important because the insecurity of ReID systems may cause severe losses. For example, criminals may use the adversarial perturbations to cheat the monitoring systems. As far as we know, the defense research against attacks based on deep neural network ReID systems has just begun. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">32]</ref> are the only two current research results on adversarial attacks based on deep learning in ReID field, which shows that ReID also faces the above-mentioned problems. Therefore, it is an urgent problem to develop an effective ReID adversarial defense method. Our work not only provides a simple and effective adversarial defense method for ReID, but also improves the performance of the model. In terms of model accuracy improvement, Multi-Modal Data Augmentation enables the model to fully learn the spatial structure features, which not only strengthens the learning of the underlying features, but also further alleviates the over-fitting risk of ReID model because of the color variation caused, so as to further improve the accuracy of the model. In terms of adversarial defense, images of different modalities retain the homogeneous structure information of visible images, and the consistency of structural information ensures that the model can be trained and converged normally, the fusion of the different modal information can bring rich structural variation to the input data, which will make attack method difficult to align the adversarial space of the dataset. This is equivalent to the original adversarial space being disturbed, which means to produce defense effects. Based on the above analysis, we propose the two methods to effectively improve model accuracy and defensive ability: (1) multi-modal data augmentation, otherwise called Grayscale Patch Replacement, which is used to mine more robust features and enrich the input data variety, not only does it help to improve the performance of the model, but also brings robust defense capability to the model, so this method fully play dual potential of multi-modal information; (2) Multi-Modal Defense, which integrates three homogeneous modal images of visible, grayscale and sketch, and enriches the changes of input data and strengthens the defense ability of the model.</p><p>The proposed method has the following advantages:</p><p>(1) It is a lightweight approach which does not require any additional parameter learning or memory consumption. It can be combined with various CNN models without changing the learning strategy.</p><p>(2) It is a complementary approach to existing data augmentation and defense methods. When these methods are used altogether, it will further improve ReID accuracy and adversarial defense ability.</p><p>Our main contributions are summarized as follows:</p><p>(1)An effective method is proposed in this paper, which will make full use of the structural information of homogeneous modal images and increase the number and diversity of training samples. Hence, it will enhance the model's robustness to color variation. We have conducted a large number of experiments on three ReID datasets, which show the effectiveness of the proposed method.</p><p>(2) Using image scaling and the homogeneity of multi-modal information to perturb adversarial space, the combination method of internal and external defenses provides an effective growth direction for future research on improving the accuracy and defense ability of ReID models, and verifies the effectiveness and versatility of the method in a variety of attack methods and fields.</p><p>The content of the remainder of the paper is as follows: Section 2 presents the related work, Section 3 presents the related algorithms, Section 4 presents the experimental results and analysis, and Section 5 presents a conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>It's well known that data augmentation such as Random Cropping and Random Flipping plays an important role in the field of classification, detection and ReID. All of which increase the diversity of training data, and improve the generalization ability of the model to some extent. The Random Erasing proposed by Zhong et al. <ref type="bibr" target="#b2">[3]</ref> simulates the occlusion problem that is frequently encountered in reality, which randomly erases a part of the image with a certain probability in the training samples to increase the diversity of the training samples. To some extent, it resolves the problems of inadequate generalization ability when the recognition task faces the occlusion problem, so it has become an effective training trick in the field of ReID. Zheng et al. used generative adversarial networks to change the clothes of a pedestrian with the clothes of other pedestrians, this method generates more diversified data and improves the generalization ability of the model <ref type="bibr" target="#b31">[29]</ref>. The kreciprocal encoding is used to reorder the results of the query to improve mAP and Rank-1 <ref type="bibr" target="#b35">[34]</ref>, this trick is known as re-Rank. The above methods partly solve the problem from different views, and effectively help the model to improve the accuracy. We propose a multi-modal data augmentation called Grayscale Patch Replacement, which uses spatial structure information and color information of visible and grayscale modal images to fit each other in the shared space, this method helps the model learn more effective features and enhance robustness. The following cross-domain experiments and adversarial defense experiments have fully shown that this method effectively improves the robustness of the model.</p><p>To achieve the goal of defense, Guo et al. <ref type="bibr" target="#b13">[14]</ref> proposed to adopt more diversified non-differentiable image transformation operations such as depth reduction, JPEG compression, total variance minimization and image quilting to increase the difficulty of network gradient prediction to achieve the purpose of defense. It is noted that most of the training images are in JPG format, Dziugaite <ref type="bibr" target="#b0">[1]</ref> uses JPG image compression method to reduce the impact of adversarial disturbance experimental results show that this method is effective for some adversarial attack algorithms. PixelDefense defense <ref type="bibr" target="#b28">[27]</ref> uses the PixelCNN generation model to change all pixels in each channel and convert the adversarial sample to the normal sample then put in the original model prediction. VectorDefense <ref type="bibr" target="#b30">[28]</ref> converts the bitmap into a vector image space and returns it before classification to avoid being deceived by the adversarial structure. These methods are not simple enough and have not been verified in more fields. The performance of a ReID algorithm in an adversarial attack can evaluate the robustness of an algorithm. MS-SSIM <ref type="bibr" target="#b9">[10]</ref> is an attack method specially proposed for ReID in CVPR2020. It has extremely strong transferable attack capability, and its transferable attack can make the model accuracy drop sharply. At the same time, it has also been demonstrated to be more aggressive than the traditional attack methods such as DeepFool <ref type="bibr" target="#b18">[18]</ref>, NewtonFool <ref type="bibr" target="#b14">[15]</ref>, CW <ref type="bibr" target="#b32">[31]</ref>. As far as we know, this attack method currently has no better defense method. Metric Attack <ref type="bibr" target="#b33">[32]</ref> is another ReID adversarial attack method, which uses distance measurement to achieve an attack on the ReID model. The Multi-Modal Defense we proposed not only has a good defensive effect on the above two ReID attack methods, but is also effective against other traditional attack methods. It is a universal defense method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>Proposed Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Introduction to Multi-Modal Data Augmentation</head><p>Inspired by Zheng <ref type="bibr" target="#b31">[29]</ref>, this paper proposes an effective ReID multi-modal data augmentation method, which consists of Local Grayscale Patch Replacement(LGPR) and Global Grayscale Patch Replacement(GGPR). As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="figure" target="#fig_1">Figure 2</ref>, the input image is randomly replaced with grayscale patch by of homogeneous grayscale ima-ge to make the model more fully mining the underlying features related to the spatial structure, which find a better balance between the color information and the structure information to enhance the generalization ability of the model.</p><p>We conduct experiments in subsections 3.1.1 and 3.1.2 using reID_baseline [30] on the Market-1501 <ref type="bibr" target="#b8">[9]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Introduction to Multi-Modal Data Augmentation</head><p>In the process of model training, we conduct LGPR randomly transformation on the entire batch of training images with a certain probability. For an image I in a minibatch, denote the probability of it undergoing LGPR be pr, and the probability of it being kept unchanged be 1?pr. In this process, it randomly selects a rectangular region in the image and replaces it with the pixels of the same rectangular region in the corresponding grayscale image. Thus, training images which include regions with different levels of grayscale are generated. Among them, sl and sh are the minimum and maximum values of the ratio of the image to the randomly generated rectangle area, and the Se of the rectangle area limited between the minimum and maximum ratio is obtained by Se ? Rand (sl, sh ) ? S, re is a coefficient used to determine the shape of the rectangle. It is limited to the interval (r1, r2). xe and ye are randomly generated by coordinates of the upper left corner of the rectangle. If the coordinates of the rectangle exceed the scope of the image, the area and position coordinates of the rectangle are re-determined. When a rectangle that meets the above requirements is found, the pixel values of the selected region are replaced by the corresponding rectangular region on the grayscale image converted from RGB image. The procedure of LGPR is shown in Algorithm.1.</p><p>Hyper-Parameter Setting. During CNN training, there is a hyper-parameter that needs to be estimated here, that is, the random grayscale probability pl. We take the hyperparameter pl as 0.01, 0.03, 0.05, 0.07, 0.1, 0.2, 0.3,..., 1 for the LGPR experiments. Then we take the value of each parameter for three independent repetitions of the experiments. Finally, we calculate the average of the final result. The results of different pr are shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Obviously, when pl=0.4 or pl=0.7, the model achieves better performance. And the best performance is achieved when pl=0.4. If we do not specify, the hyperparameter is set pl=0.4 in the later experiments.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Global Grayscale Patch Replace(GGPR)</head><p>In the process of model training, we conduct GGPR randomly transformation on the entire batch of training images with a certain probability. When the probability value of GGPR is set to p=0.05, it means that 5% of the input samples are converted into grayscale images to participate in the model training during the training process. This method can be implemented by the RandomGrayscale function in pytorch.</p><p>Hyper-Parameter Setting. During CNN training, there is a hyperparameter that needs to be estimated here, that is, the random grayscale probability pg. We take the hyper-parameter pg as 0.01, 0.03, 0.05, 0.07, 0.1, 0.2, 0.3, ... , 1 for the GGPR experiments. Then we take the value of each parameter for three independent repetitions of the experiments. Finally, we calculate the average of the final result.The results of different pg are shown in <ref type="figure">Figure 4</ref>.</p><p>We can see that when pg=0.05, the performance of the model reaches the maximum value in Rank-1 and mAP in <ref type="figure">Figure.</ref>4. If we do not specify, the hyperparameter is set pg=0.05 in the later experiments.</p><p>Evaluation of GGPR and LGPR. Compared with the best results of GGPR on baseline <ref type="bibr" target="#b9">[10]</ref>, the accuracy of LG-PR is improved by 0.5% and 1.4% on Rank-1 and mAP, re-spectively. Under the same conditions using re-Rank, the accuracy of Rank-1 and mAP is improved by 0% and 0.4%, respectively. Therefore, the advantages of LGPR are more obvious when re-Rank is not used. However, <ref type="figure" target="#fig_2">Figure 3</ref> also shows that the performance improvement brought by LGPR is not stable enough because of the obvious fluctuation in LGPR, while the performance improvement brought by GGPR is very stable. Therefore, we improve the stability of the method by combining GGPR with LGPR. <ref type="figure">Figure 4</ref>: Accuracy under different hyper-parameters on Market-1501 using baseline <ref type="bibr" target="#b9">[10]</ref>.</p><p>Evaluation by Combining GGPR with LGPR. First, we fix the hyper-parameter value of GGPR to pg=0.05, then keep the control variable unchanged to further determine the hyper-parameter of LGPR. Finally, we take the hyper-parameter pl of LGPR to be 0.1, 0.2, ... , 0.7 to conduct combination experiments of GGPR and LGPR, and cond-uct three independent repeated experiments for each parameter pl to get the average value. The result is shown in <ref type="figure" target="#fig_3">Figure 5</ref>. It can be seen from Figure that the performance improvement brought by the combination of GGPR and LGPR is more stable and with less fluctuation, and the comprehensive performance of the model is the best when the hyper-parameter value of LGPR is pl=0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Modal Defense</head><p>A grayscale image can be obtained when a visible image extracts color information, and a sketch image can be obtained when a grayscale image extracts details information. In other words, a grayscale image is a visible image that loses some color information, and a sketch image is a grayscale image that loses some details information. Thus, visible images, grayscale images, and sketch images are homogeneous.  Our Multi-Modal Defense solution is to train the model using visible modal and homogeneous grayscale modal and sketch modal images. As shown in <ref type="figure" target="#fig_4">Figure 6</ref>, we randomly convert a batch of RGB images into grayscale or sketch images with a certain probability in pre-processing stage, and then merge the channels of the grayscale image and the sketch image with the channels of the RGB image. The advantages of using homogeneous modal images can be attributed to two aspects: <ref type="bibr" target="#b0">(1)</ref> The grayscale and sketch images retain the homogeneous structural information of the visible image. It is the key to ensure the convergence of the model. The sketch and grayscale images are different modalities of homogeneous RGB images, so using them for mixed training will not bring negative impact on model performance. <ref type="formula">(2)</ref> The proposed method can simply and efficiently bring rich changes to the input data, and still maintain good performance under a variety of attacks. The procedure is shown in Algorithm.2. <ref type="figure">Figure 7</ref>: The first row of picture is an RGB image, the second row of picture is a partial display of the random fusion of RGB image and grayscale image channels, and the third row of picture is a par-tial display of the random fusion of RGB image and sketch image channels. We can see that our method has rich changes by fusing RGB image with grayscale and sketch image. If p2 ? p 6 I3 ? I; 7 else 8 I1 ? I; 9 return I.</p><p>We transform 15% and 5% RGB images into grayscale and sketch images, respectively, then we take two-thirds grayscale images as GGPR to directly input CNN. Meanwhile, we fuse the remaining grayscale and sketch image with RGB image. In the process of image fusion, we use the grayscale or sketch image to overlay 1 or 2 channels from RGB image channels randomly selected. The fusion effect is shown in <ref type="figure">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments Results and Analysis</head><p>Datasets. We conducted comparison experiments on MTMC17 <ref type="bibr" target="#b25">[24]</ref>, DukeMTMC <ref type="bibr" target="#b46">[45]</ref>, and Market-1501 <ref type="bibr" target="#b8">[9]</ref>. The MSMT17 dataset, created in winter, was presented in 2018 as a new, larger dataset closer to real-life scenes, containing a total of 4,101 individuals and covering multiple scenes and time periods. The DukeMTMC is a large-scale Multi-Target, Multi-Camera (MTMC) tracking dataset, a HD video dataset recorded by 8 synchronous cameras, with more than 7,000 single camera tracks and more than 2,700 individual pedestrians. The Market-1501 dataset was collected in the summer of 2015. It includes 1,501 pedestrians captured by six cameras (five HD cameras and one low-definition camera). These three datasets are currently the largest datasets of ReID, and they are also the most representative because they collectively contain multi-season, multi-time, HD, and low-definition cameras with rich scenes and backgrounds as well as complex lighting variations. We evaluated these three datasets using Rank-k precision and mean Average Precision(mAP). Rank-1 denotes the average accuracy of the first returned result corresponding to each query image; mAP denotes the mean of average accuracy, the query results are sorted according to the similarity, the closer the correct result is to the top of the list, the higher the score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison of Grayscale Patch Replacement and State-of-the-Art</head><p>In this section we will compare the performance of our approach with state-ofthe-art methods on three baselines. The baselines are the ReID_baseline[30], the strong baseline <ref type="bibr" target="#b10">[11]</ref> and FastReID <ref type="bibr" target="#b11">[12]</ref>. Since the model requires more training epochs to fit than the original, we add 1 to 2 times training epochs to the training process. A comparison of the performance of our method with the state-of-the-art methods in three datasets is shown in <ref type="table">Table 1</ref> to <ref type="table" target="#tab_3">Table 3</ref>.</p><p>To our knowledge, applying the proposed approach to fastReID, we have achieved the highest retrieval accuracy currently available on the MTMC17 dataset.</p><p>On the one hand, our method achieves better ReID performance because of exploiting the grayscale transformation, which increases the number and diversity of training samples. On the other hand, exploiting the structural information retained by the grayscale image, the colors of the RGB images and the spatial structural informion of the grayscale images are fitted to each other in the model training, increasing the model's robustness to color changes.  <ref type="table">Table 1</ref>: Performance comparison on MTMC17 datasets. Since using reRank on the MTMC17 dataset requires a lot of hardware cost, we did not make a comparison in this regard.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MTMC17</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Market1501</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross-domain Person Re-identification</head><p>Cross-domain person re-identification aims at adapting the model trained on a labeled source domain dataset to another target domain dataset without any annotation. The performance of a method in cross-domain experiments can test whether the method really improves the robustness of the model. In order to further explore the effectiveness of the proposed method in cross-domain experiments, we use GGPR to conduct the following cross-domain experiments on strong baseline <ref type="bibr" target="#b10">[11]</ref>. The experiments are shown in <ref type="table">Table 4</ref>. Experimental results show that Random Erasing can also significantly improve the performance of the ReID model, but it will cause a significant drop in cross-domain performance. The proposed method can not only signific-antly improve the cross-domain performance of the ReID model, but also be more robust because of learning more discriminative features.</p><p>In <ref type="table">Table 4</ref>, +REA means that the trick of Random Erasing is used in model training, -REA means turning it off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M?D D?M</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank-1 mAP</head><p>Rank-1 mAP <ref type="bibr" target="#b10">[11]</ref>+REA+reRank <ref type="bibr" target="#b10">[11]</ref>  <ref type="table">Table 4</ref>: The performance of different models is evaluated on cross-domain dataset. M?D means that we train the model on Market1501 and evaluate it on DukeMTMC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adversarial Defense</head><p>We use the adversarial samples generated by the MS-SSIM attack on AlignedReID <ref type="bibr" target="#b1">[2]</ref> to conduct migration attack defense experiments. Meanwhile, we use the adversarial samples generated by the Metric Attack attack on HACNN <ref type="bibr" target="#b34">[33]</ref> to conduct another migration attack defense experiment. The targets of the two migration attacks are all strong baseline <ref type="bibr" target="#b10">[11]</ref>. The following experiment adopts the result of using reRank by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Use GGPR to Improve Adversarial Defense</head><p>We use GGPR on the Market-1501 dataset to conduct the following adversarial defense experiments on strong base-line <ref type="bibr" target="#b10">[11]</ref>.</p><p>Experimental details. We use scaling to conduct the ad-versarial defense experiments. The image size of the Market-1501 dataset is 128*64, and the strong baseline <ref type="bibr" target="#b10">[11]</ref> will rescale the image size to 384*128 during the training proc-ess. We first rescale the query image to 100*50, and then rescale it to 384*128 in the inference stage, this operation will destroy the noise structure of the adversarial sample from the exterior of the model without significantly harm-ing the model performance. Finally, using above proposed method to defend the model interior, the images rescaled are input to the model. It is notable that we only use image scaling preprocessing in the inference stage. The experim-ental results show that this combination of internal and ext-ernal defenses works well.</p><p>The MS-SSIM attack and defensive experimental results are shown in  <ref type="table" target="#tab_5">Table 5</ref>. The MS-SSIM attack and defensive experiment.</p><p>Defensive Performance Analysis. After learning multi-modal homogeneous data, we think that the model will ma-ke attack method difficult to align this change in the adversarial space of the dataset, so that the model trained by this method produces defense effects. To further experiments, we use more multi-modal to conduct defense experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Multi-Modal Defense Experiment</head><p>In the MS-SSIM defense experiment, we rescale an image to 110*50 in the preprocessing stage. Comparison of the defense effects of GGPR and Multi-Modal Defense methods under epoch=30 and epoch=40 of iterative attack conditions are shown in <ref type="table">Table.</ref>6. We can see from the Table that only using image scaling and GGPR technology can achieve a better defense effect, and the combination of using scaling and Multi-Modal Defense can further improve the defense effect. It is remarkable that the adversarial noise of the adversarial samples generated by MS-SSIM is already visible to the human visual system under the condition of epoch=40, and the proposed method can still defend well the attack.   Since Adversarial samples are generated from the query set by MS-SSIM, and adversarial samples are generated from the gallary set by Metric Attack. For fair comparison, we sample another query set from the gallary adversarial samples of the Metric Attack attack, and the size of the adversarial samples are all 256*128, and the adversarial defense experiment is conducted on the strong baseline <ref type="bibr" target="#b10">[11]</ref>. In the Metric Attack defense experiment, we rescale the training samples to 110*50 in preprocessing stage. The comparison of the defense effects of GGPR and Multi-Modal Defense methods against Metric Attack attacks is shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, a simple and effective ReID data augmentation method with adversarial defense effects is proposed. Neither does the method require large scale training like GAN, nor introduces any noise. This method increases the number and diversity of training samples through homogeneous grayscale replacement, and avoids over-fitting the color variations introduced during the training process. The effectiveness of the proposed method is verified by experiments on multiple datasets and baselines, and exceeds the performance of the currently optimal algorithm. In addition, the proposed method uses image scaling to break the adversarial structure from the exterior of adversarial image, and then brings a variety of input data through the fusion of homogeneous RGB images, grayscale images and sketch images, which makes attack method difficult to align this change in the adversarial space of the dataset. These operations will ultimately realize the combined defense of internal and external. The effectiveness of the proposed method is verified by testing on the only two attack methods against ReID. Finally, we hope that this work can promote the development of deep learning adversarial defenses and reduce the security issues of reID, and also inspire future researchers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Application of GGPR(left) and LGPR(right) in pipelines of the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Schematic diagram of Random Grayscale Patch Replacement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Accuracy under different hyper-parameters on Market-1501 with using baseline[30].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Model performance of combining GGPR withLGPR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The pipeline of Multi-Modal Defense of proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Input batch RGB images I; Gray transformation probability p1; Sketch transformation probability p2; Output1: batch of grayscale images I1. Output2: batch of RGB-Gray fusion images I2. Output3: batch of RGB-Sketch fusion images I3. Initialization: p1 ? 0.1; p2 ? 0.05; p3 ? 0.05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Schematic diagram of internal and external combined defense.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm1: Local Grayscale Patch Replace Procedure Input</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>: Input image I;</cell></row><row><cell></cell><cell>Image size W and H;</cell></row><row><cell></cell><cell>Area of image S;</cell></row><row><cell></cell><cell>Erasing probability p;</cell></row><row><cell></cell><cell>Erasing area ratio range sl and sh ;</cell></row><row><cell></cell><cell>Erasing aspect ratio range r1 and r2 .</cell></row><row><cell cols="2">Output: Grayscale erased image I  *  .</cell></row><row><cell cols="2">1 Initialization: pl ? Rand (0, 1).</cell></row><row><cell cols="2">2 if p1 ? p then</cell></row><row><cell>3</cell><cell>I  *  ? I;</cell></row><row><cell>4</cell><cell>return I  *  .</cell></row><row><cell cols="2">5 else</cell></row><row><cell>6</cell><cell>while True do</cell></row><row><cell>7</cell><cell>Se ? Rand (sl ,sh )?S;</cell></row><row><cell>8</cell><cell>re ? Rand (r1 ,r2 );</cell></row><row><cell>9</cell><cell>He ? Sqrt(Se?re), We ? Sqrt(Se / re);</cell></row><row><cell>10</cell><cell>xe ? Rand (0,W), ye ? Rand (0,H);</cell></row><row><cell>11</cell><cell>if xe + We ? W and ye + He ? H then</cell></row><row><cell>12</cell><cell>Ie ? (xe ,ye ,xe + We ,ye + He );</cell></row><row><cell>13</cell><cell>I(Ie ) ? Grayscale(Ie);</cell></row><row><cell>14</cell><cell>I  *  ? I;</cell></row><row><cell>15</cell><cell>return I  *</cell></row><row><cell>16</cell><cell>end</cell></row><row><cell cols="2">17 end</cell></row><row><cell>18</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on Market1501 datasets.</figDesc><table><row><cell></cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>PCB [36] (ECCV'18)</cell><cell>92.3</cell><cell>77.4</cell></row><row><cell>AANet [40] (CVPR'19)</cell><cell>93.9</cell><cell>83.4</cell></row><row><cell>IANet [19] (CVPR'19)</cell><cell>94.4</cell><cell>83.1</cell></row><row><cell>Auto-ReID [44] (ICCV'19)</cell><cell>94.5</cell><cell>85.1</cell></row><row><cell>DGNet[29](CVPR'19)</cell><cell>94.8</cell><cell>86.0</cell></row><row><cell>Pyramid [42] (CVPR'19)</cell><cell>95.7</cell><cell>88.2</cell></row><row><cell>ABDNet [43] (ICCV'19)</cell><cell>95.6</cell><cell>88.3</cell></row><row><cell>SONA [38] (ICCV'19)</cell><cell>95.7</cell><cell>88.7</cell></row><row><cell>SCAL [39] (ICCV'19)</cell><cell>95.8</cell><cell>89.3</cell></row><row><cell>CAR [37] (ICCV'19)</cell><cell>96.1</cell><cell>84.7</cell></row><row><cell>Circle Loss [41] (CVPR'20)</cell><cell>96.1</cell><cell>87.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on DukeMTMC datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Market1501</cell><cell></cell><cell></cell></row><row><cell></cell><cell>clean</cell><cell></cell><cell>MS-SSIM</cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell></cell><cell>(epoch=30)</cell><cell></cell></row><row><cell></cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>reID-strong baseline[11]</cell><cell>95.4</cell><cell>94.2</cell><cell>0.2</cell><cell>0.8</cell></row><row><cell>[11]+GGPR(Ours)</cell><cell>96.2</cell><cell>94.7</cell><cell>92.0</cell><cell>89.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison of the defense effects of Ours[1]: GGPR and Ours[2]: Multi-Modal Defense.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 : Methods Market-1501[9] clean Metric Attack Rank-1 mAP Rank-1 mAP</head><label>7</label><figDesc></figDesc><table><row><cell>[attack gallary]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HACNN[33](no reRank)</cell><cell>91.3</cell><cell>77.5</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>[attack query]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>reID-strong baseline[11]</cell><cell>93.5</cell><cell>92.9</cell><cell>56.7</cell><cell>62.6</cell></row><row><cell>[11]+resize[110,50]</cell><cell>92.8</cell><cell>92.3</cell><cell>68.4(+11.7)</cell><cell>72.4(9.8)</cell></row><row><cell>[11]+Ours[1]</cell><cell>94.1(+0.6)</cell><cell>93.1(+0.2)</cell><cell>77.8(+21.1)</cell><cell>79.3(+16.7)</cell></row><row><cell>[11]+Ours[2]</cell><cell>94.1(+0.6)</cell><cell>92.8</cell><cell>87.2(+30.5)</cell><cell>86.1(+23.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparison of the defense effects of GGPR and Multi-Modal Defense in Metric Attack.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00853</idno>
		<title level="m">A study of the effect of JPG compression on adversarial images</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">AlignedReID: Surpassing Human-Level Performance in Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08603</idno>
		<title level="m">Adversarial Examples for Semantic Segmentation and Object Detection</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nayebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09202</idno>
		<title level="m">Biologically inspired protection of deep networks from adversarial attacks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Salience-Guided Cascaded Suppression Network for Person ReIDentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00939</idno>
		<title level="m">Dense Associative Memory is Robust to Adversarial Inputs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darius</forename><surname>Afchar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Nozick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Echizen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00888</idno>
		<title level="m">MesoNet: a Compact Facial Video Forgery Detection Network</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scalable person re-identification:A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transferable, Controllable, and Inconspicuous Adversarial Attacks on Person Re-identification With Deep Mis-Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bag of Tricks and A Strong Baseline for Deep Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02631</idno>
		<title level="m">FastReID: A Pytorch Toolbox for Real-world Person Re-identification</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Countering adversarial images using input transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Objective metrics and gradient descent algorithms for adversarial examples in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uyeong</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACSAC</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="262" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Michael</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Theoretically Principled Trade-off between Robustness and Accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Seyed-Mohsen Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interaction-and-aggregation network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinqian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Ex-plaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning with a strong adversary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03034</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darius</forename><surname>Afchar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Nozick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Echizen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00888</idno>
		<title level="m">MesoNet: a Compact Facial Video Forgery Detection Network</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<title level="m">Qi Tian; Person Transfer GAN to Bridge Domain Gap for Person Re-Identification Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bilinear attention networks for person retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision(ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8030" to="8039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relation-Aware Global Attention for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pixeldefend</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">ICLR(Poster)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Vector-Defense:Vectorization as a Defense to Adversarial Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V M</forename><surname>Kabilan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint Discriminative and Generative Learning for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kautz</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In S&amp;P</title>
		<imprint>
			<biblScope unit="page" from="39" to="57" />
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Metric attack and defense for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10650</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">AdaptiveReID: Adaptive L2 Regularization in Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyang</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heikki</forename><surname>Huttunen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07875</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person ReIDentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Second-order non-local attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Bryan ; Ning) Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poellabauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mixed highorder attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aanet: Attribute attention network for person re-identifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiat-Pin</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharmili</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Circle loss:A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multi-loss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mixed highorder attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Auto-ReID: Searching for a part-aware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijie</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
