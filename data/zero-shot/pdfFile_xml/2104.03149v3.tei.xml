<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Dancette</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Sorbonne Universit?</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>LIP6</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Cad?ne</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Sorbonne Universit?</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>LIP6</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Carney Institute for Brain Science</orgName>
								<orgName type="institution">Brown University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Idiap Research Institute</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Australian Institute for Machine Learning</orgName>
								<orgName type="institution" key="instit2">University of Adelaide</orgName>
								<address>
									<addrLine>5 Valeo.ai</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Sorbonne Universit?</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>LIP6</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce an evaluation methodology for visual question answering (VQA) to better diagnose cases of shortcut learning. These cases happen when a model exploits spurious statistical regularities to produce correct answers but does not actually deploy the desired behavior. There is a need to identify possible shortcuts in a dataset and assess their use before deploying a model in the real world. The research community in VQA has focused exclusively on question-based shortcuts, where a model might, for example, answer "What is the color of the sky" with "blue" by relying mostly on the question-conditional training prior and give little weight to visual evidence. We go a step further and consider multimodal shortcuts that involve both questions and images. We first identify potential shortcuts in the popular VQA v2 training set by mining trivial predictive rules such as co-occurrences of words and visual elements. We then introduce VQA-CounterExamples (VQA-CE), an evaluation protocol based on our subset of Coun-terExamples i.e. image-question-answer triplets where our rules lead to incorrect answers. We use this new evaluation in a large-scale study of existing approaches for VQA. We demonstrate that even state-of-the-art models perform poorly and that existing techniques to reduce biases are largely ineffective in this context. Our findings suggest that past work on question-based biases in VQA has only addressed one facet of a complex issue. The code for our method is available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual Question Answering (VQA) is a popular task that aims at developing models able to answer free-form questions about the contents of given images. The research com-* Equal contribution ? Work done before April 2021 and joining Tesla munity introduced several datasets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> to study various topics such as multimodal fusion <ref type="bibr" target="#b6">[7]</ref> and visual reasoning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22]</ref>. The popular VQA v2 dataset <ref type="bibr" target="#b20">[21]</ref> is the largest dataset of photographs of real scenes and humanprovided questions. Because of strong selection biases and annotation artifacts, these datasets have served as a test-bed for the study of dataset biases and shortcut learning <ref type="bibr" target="#b17">[18]</ref> (we will use the term "shortcut" exclusively in the rest of the paper). These spurious correlations correspond to superficial statistical patterns in the training data that allow predicting correct answers without deploying the desirable behavior. Issues of shortcut learning have become an increasing concern for other tasks in vision and natural language processing <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14]</ref>. In extreme cases, shortcuts in VQA may allow guessing the answer without even looking at the image <ref type="bibr" target="#b0">[1]</ref>. Some shortcuts can be more subtle and involve both textual and visual elements. For instance, training questions containing What sport are strongly associated with the answer tennis when they co-occur with a racket in the image (see <ref type="figure">Figure 1</ref>). However, some examples can be found in the validation set, such as What sport field is in the background ?, that lead to a different answer (soccer) despite a racquet being present in the image. Because of such exceptions, a model that strongly relies on simple co-occurrences will fail on unusual questions and scenes. Our work studies such multimodal patterns and their impact on VQA models.</p><p>The presence of dataset biases in VQA datasets is well known <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref>, but existing evaluation protocols are limited to text-based shortcuts. Our work introduces VQA-CounterExamples (VQA-CE for short) which is an evaluation protocol for multimodal shortcuts. It is easy to reproduce and can be used on any model trained on VQA v2, without requiring retraining. We first start with a method to discover superficial statistical patterns in a given VQA dataset that could be the cause of shortcut learning. We discover a collection of co-occurrences of textual and visual elements that are strongly predictive of certain an- <ref type="figure">Figure 1</ref>. Overview of this work. We first mine simple predictive rules in the training data such as: what + sport + racket V ? tennis. We then search for counterexamples in the validation set that identify some rules as undesirable statistical shortcuts. Finally, we use the counterexamples as a new challenging test set and evaluate existing VQA models like UpDown <ref type="bibr" target="#b2">[3]</ref> and VilBERT <ref type="bibr" target="#b30">[31]</ref>. swers in the training data and often transfer to the validation set. For instance, we discover a rule that relies on the appearance of the words "what","they","playing" together with the object "controller" in the image to always predict the correct answer "wii". We consider this rule to be a shortcut since it could fail on arbitrary images with other controllers, as it happens in the real world. Thus, our method can be used to reflect biases of the datasets that can potentially be learned by VQA models.</p><p>We go one step further and identify counterexamples in the validation set where the shortcuts produce an incorrect answer. These counterexamples form a new challenging evaluation set for our VQA-CE evaluation protocol. We found that the accuracy of existing VQA models is significantly degraded on this data. More importantly, we found that most current approaches for reducing biases and shortcuts are ineffective in this context. They often reduce the average accuracy over the full evaluation set without significant improvement on our set of counterexamples. Finally, we identify shortcuts that VQA models may be exploiting. We find several shortcuts giving predictions highly correlated with existing models' predictions. When they lead to incorrect answers on some examples from the validation set, VQA models also provide incorrect answers. This tends to show that VQA models exploit these multimodal shortcuts. In summary, the contributions of this paper are as follows.</p><p>1. We propose a method to discover shortcuts which rely on the appearance of words in the question and visual elements in the image to predict the correct answer. By applying it to the widely-used VQA v2 training set, we found a high number of multimodal shortcuts that are predictive on the validation set. 2. We introduce the VQA-CE evaluation protocol to assess the VQA models' reliance on these shortcuts. By running a large-scale evaluation of recent VQA approaches, we found that state-of-the-art models exploit these shortcuts and that bias-reduction methods are ineffective in this context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We review existing approaches to discovering potential statistical shortcuts and assess their use by learned models.</p><p>Detecting cases of shortcut learning A first type of approaches consists in detecting the use of shortcuts by leveraging explainability methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b31">32]</ref>. However, they require costly human interpretation, or additional annotations <ref type="bibr" target="#b14">[15]</ref> to assess whether a particular explanation reveals the use of a shortcut. A second type consists in evaluating a model on artificial data or out-of-distribution data. For instance, <ref type="bibr" target="#b18">[19]</ref> artificially modify the texture of natural images to show that convolutional networks trained on ImageNet exploit features related to textures instead of shapes. Also, <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b5">[6]</ref> evaluate vision models on out-ofdistribution data to show that they cannot identify known objects when their poses changed significantly. In this line of works, <ref type="bibr" target="#b23">[24]</ref> focus on evaluating models on adversarial examples and show links with statistical regularities or "nonrobust features" that models exploit. A third type of approaches use specific models with a known type of biases to assess the amount of biases of this type directly in the dataset. For instance, in computer vision, BagNet <ref type="bibr" target="#b8">[9]</ref> obtained high accuracy on ImageNet by using occurrences of small local image features, without using the global spatial context. This suggests that state-of-the-art ImageNet models are biased towards local image features. Similarly, our approach leverages specific shallow models that are constructed to only exploit biases of a certain type.</p><p>This kind of approaches have been used in VQA. Previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21]</ref> used question-only and image-only models to quantify the amount of unimodal shortcuts in a dataset. Instead, our approach is not only able to quantify the amount of shortcuts but also identify these shortcuts. More importantly, our method can identify multimodal shortcuts that combine elements of the question and the image. The closest approach to ours <ref type="bibr" target="#b31">[32]</ref> uses the Apriori algorithm to extract predictive rules that combine the appearance of words and visual contents. However, these <ref type="figure">Figure 2</ref>. Pipeline of the proposed method to detect potential shortcuts in a VQA training set. We detect and label objects in images with a Faster R-CNN model. We then summarize each VQA example with binary indicators representing words in the question, answer, and labels of detected objects. Finally, a rule mining algorithm identifies frequent co-occurrences and extracts a set of simple predictive rules.</p><p>rules are specific to the attention maps and predictions of the VQA model from <ref type="bibr">[28]</ref>. More problematically, they are extracted on the validation set and are mainly used for qualitative purposes. Our approach also relies on the Apriori algorithm but extracts rules directly on the training set, independently of any model, and the predictive capacity of the rules is evaluated on the validation set.</p><p>Evaluating VQA models' reliance on shortcuts Once a class of shortcuts has been identified, a first way to evaluate model's robustness is to build external out-of-distribution evaluation datasets on which using these shortcuts leads to a wrong prediction. In Visual Question Answering, the VQA-Rephrasing <ref type="bibr" target="#b36">[37]</ref> dataset contains multiple rephrased but semantically-identical questions. The goal is to test model's sensitivity to small linguistic variations and will penalize usage of a certain class of question-related shortcuts. Similar datasets exist for natural language processing <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Another type of evaluation methods artificially injects certain kind of shortcuts in the training set and evaluate models on examples that do not possess these shortcuts. The widely used VQA-CP [1] evaluation procedure consists in resplitting the original VQA datasets so that the distribution of answers per question type ("how many", "what color is", etc.) is different between the training and evaluation set. Models that rely on those artificial shortcuts are therefore penalized. VQA-CP was used to develop methods that aim at avoiding learning shortcuts from the question type on this modified training set <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. Similar approaches for VQA exists <ref type="bibr">[13]</ref>. The downside of these approaches is that they focus on artificially introduced shortcuts and only target text-related biases and shortcuts. More importantly, models that have been trained on original datasets, i.e. VQA v2, need to be retrained on their modified versions, i.e. VQA-CP v2. Other concerns have been raised in <ref type="bibr" target="#b42">[43]</ref>. On the contrary, our proposed evaluation method does not require additional data collection or data generation, focuses on multimodal shortcuts, and does not require retraining. We follow guidelines from <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b42">43]</ref> for a better evaluation of the use of shortcuts.</p><p>Finally, the GQA-OOD <ref type="bibr" target="#b28">[29]</ref> dataset extracts from the GQA <ref type="bibr" target="#b22">[23]</ref> validation and testing set example with rare answers, conditioned on the type of question. Thus, it targets question-related shortcuts. It enables the evaluation of models without retraining on a separate training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Detecting multimodal shortcuts for VQA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Our shortcut detection method</head><p>We introduce our method to detect shortcuts relying on textual and visual input. Our approach consists in building a dataset of input-output variables and applying a rule mining algorithm. The code for our method is available online * . In Visual Question Answering (VQA), we consider a training set D train made of n triplets (v i , q i , a i ) i? <ref type="bibr">[1,n]</ref> with v i ? V an image, q i ? Q a question in natural language and a i ? A an answer. VQA is usually casted as a problem of learning a multimodal function f : V ?Q ? A that produces accurate predictions on D test of unseen triplets.</p><p>Mining predictive rules on a training set Our goal is to detect shortcuts that f might use to provide an answer without deploying the desired behavior. To this end, we limit ourselves to a class of shortcuts that we think is often leveraged by f . We display in <ref type="figure">Figure 2</ref> our rule mining process. These shortcuts are short predictive association rules A ? C that associate an antecedent A to a consequent C. Our antecedents are composed of words of the question and salient objects in the image (or image patch), while our consequents are just answers. For instance, the rule {what, color, plant} ? {green} provides the answer "green" when the question contains the words "what", "color" and "plant". These shallow rules are by construction shortcuts. They are predictive on the validation set but do not reflect the complex behavior that needs to be learned to solve the VQA task. For instance, they do not rely on the order of words, neither the position and relationships of visual contents in the image. They lack the context that is required to properly answer the question. Moreover, even rules that seem correct often have a few counterexamples in the dataset, i.e. examples that are matched by the antecedent but the consequent provides the wrong answer. We later use these counterexamples in our evaluation procedure.</p><p>Binary dataset creation To detect these rules, we first encode all question-image-answer triplets of D train as binary vectors. Each dimension accounts for the presence or absence of (a) a word in the question, (b) an object V in the image, represented by its textual detection label from Faster R-CNN, (c) an answer. The number of dimensions of each binary vector is the sum of the size of the dictionary of words (e.g. 13,000 words in VQA v2), the number of detection labels of distinct objects in all images (e.g. 1,600 object labels), and the number of possible answers in the training set (e.g. 3,000 answers). We report results with ground-truth instead of detected labels in the supplementary materials.</p><p>Frequent itemset mining On our binary dataset, we apply the GMiner algorithm <ref type="bibr" target="#b11">[12]</ref> to efficiently find frequent itemsets. An itemset is a set of tokens I = {i 1 , .., i n } that appear very frequently together in the dataset. The support of the itemset is its number of occurrences. For example, the itemset {what, color, plant, green} might be very common in the dataset and have a high support. GMiner takes one parameter, the minimum support. We include an additional parameter, which is the maximum length for an itemset. We detail how we select parameters at the end of this section.</p><p>Rules extraction and filtering The next step is to extract rules from the frequent itemsets. First, we filter out the itemsets that do not contain an answer token, as they cannot be converted to rules. For the others that do contain an answer a, we remove it from the itemset to create the antecedent X ( X = I \ a). The rule is then X ? a. The support s of the rule is the number of occurrences of X in the dataset. The confidence c of the rule is the frequency of correct answers among examples that have X . We then proceed to filter rules. We apply the following three steps: (a) we remove the rules with a confidence on the training set lower than 30% (remove when c &lt; 0.3) (b) if some rules have the same antecedent but different answers, then we keep the rule with the highest confidence and remove the others. For instance, given the rules {is, there} ? yes and {is, there} ? no with a respective confidence of 70% and 30%, we only keep the first one with the answer yes. (c) if a rules r 1 's antecedent is a superset of another rule r 2 's antecedent, if both have the same answer, and r 1 has a lower confidence than r 2 , then we remove r 1 . For instance, given the rules {is, there} ? yes and {is, there, cat} ? yes with a respective confidence of 70% and 60%, we only keep the first one without the word cat. We consider the remaining rules as shortcuts. Note that rules with a confidence of 100% could be considered correct and not shortcuts, but these rules will not influence our evaluation protocol, detailed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Analysis of shortcuts on natural data</head><p>We analyze the shortcuts that our approach can detect on the widely used VQA v2 dataset <ref type="bibr" target="#b20">[21]</ref> made of 1.1M imagequestion-answer examples and based on 200K images from the MS-COCO dataset <ref type="bibr" target="#b29">[30]</ref>. We extract shortcuts with dif-ferent combinations of minimum support and confidence. Each time, we aggregate them into a classifier that we evaluate on the validation set. We detail how to build this kind of classifier in Section 4.2. We select the support and confidence leading to the best overall accuracy. It corresponds to a minimum support of 2.1 ? 10 ?5 (about ?8 examples in training set), and a minimum confidence of 0.3. Once these shortcuts have been detected, we assess their number and type(purely textual, purely visual, or multimodal). We also verify that they can be used to find counterexamples that cannot be accurately answered using shortcuts. Finally, we evaluate their confidence on the validation set. In the next section, we leverage these counterexamples with our VQA-CE evaluation protocol to assess model's reliance on shortcuts.</p><p>Words-only and objects-only shortcuts First, we show that our approach is able to detect shortcuts that are purely textual or visual. In the first row of <ref type="figure" target="#fig_0">Figure 3</ref>, we display a shortcut detected on VQA v2 that only accounts for the appearance of words in the question. It predicts the answer "white" when the words "what", "color", "is", "snow" appear at any position in the question. In the training set, these words appear in 95 examples and 90.62% of them have the "white" answer. This shortcut is highly predictive on the validation set and gets 95.65% of correct answers over 92 examples. We also display an example on which exploiting the shortcut leads to the correct answer, and a counterexample on which the shortcut fails because the question was about "the color of the snow suit" which is "pink". In the second row, we show a shortcut that only accounts for the appearance of visual objects. It predicts "yes" when a "frisbee", a "tree", a "hand" and a "cap" appear in the image. However, this kind of shortcuts is usually less predictive since they cannot exploit the question-type information which is highly correlated with certain answers, i.e. "what color" is usually answered by a color.</p><p>Multimodal shortcuts Then, we show that our approach is able to detect multimodal shortcuts. They account for the appearance of both words and visual objects V . In the third row of <ref type="figure" target="#fig_0">Figure 3</ref>, we display a multimodal shortcut that predicts "tennis" when the words what, sport and a racket V appear. It is a common shortcut with a confidence of 98.05% based on a support of 667 examples in the training set. It is also highly predictive on the validation set with 98.97% confidence and 291 support. At first sight, it is counterintuitive that this simple rule is a shortcut but answering complex questions is not about detecting frequent words and objects in images that correlate with an answer. In fact, this shortcut is associated to counterexamples where it fails to answer accurately. Here, the sport that can be played in the background is not tennis but soccer.</p><p>Number of shortcuts and statistics per type Here we show that our approach can be used to detect a high number of multimodal shortcuts. Overall, it detects 1.12M shortcuts on the VQA v2 training set. As illustrated in <ref type="figure" target="#fig_1">Figure 4</ref>, since there are 413K examples, it is often the case that several shortcuts can be applied to the same example. This is the main reason behind the high number of shortcuts For instance, the antecedent {animals, what, giraffe V } overlaps with {animals, these, what, giraffe V }. Among all the shortcuts that our method can detect, only 50k are textual, 77k are visual and 1M are multimodal. In other words, 90% are multimodal. In addition to being more numerous, they are also more predictive. For instance, the most confident shortcut that matches an example, highlighted in green in <ref type="figure" target="#fig_1">Figure 4</ref>, is multimodal 91.80% of the time. Finally, 3K examples are not matched by any shortcut antecedents. They have unusual question words or visual content. We later do not take them into account in our VQA-CE evaluation protocol. We display some representative examples in the supplementary materials. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence distribution on training and unseen data</head><p>In the supplementary materials, we display the confidence distribution of these shortcuts. We observe that our shortcuts are predictive on unseen data that follows the training set distribution. The number of shortcuts that reach a confidence between 0.9 and 1.0 is as high on the validation set as it is than on the training set. This means that shortcuts detected on the VQA v2 training set transfer to the validation set. Additionally, most shortcuts obtain a confidence lower than 1.0, which allows finding examples that contradict them by leading to the wrong answers. These counterexamples are the core of our approach to assess the VQA model's reliance on shortcuts which is described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Assessing models' reliance on shortcuts</head><p>The classic evaluation protocol in VQA consists in calculating the average accuracy over all the examples. Instead, we introduce the VQA-CounterExamples evaluation protocol (VQA-CE) that additionally calculates the average accuracy over a specific subset of the validation set. This subset is made of counterexamples that cannot be answered by exploiting shortcuts. Models that do exploit shortcuts are expected to get a lower accuracy. It is how we assess the use of shortcuts. Importantly, our protocol does not require retraining as it was the case with the previous VQA-CP <ref type="bibr" target="#b0">[1]</ref> protocol. We first detail the subsets creation procedure at the core of our VQA-CE protocol. Then we run extensive experiments to assess the use of shortcuts on many VQA models and bias-reduction methods. Finally, we identify shortcuts that are often exploited by VQA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Our VQA-CE evaluation protocol</head><p>Subsets creation using shortcuts By leveraging the shortcuts that we have detected before, we build the Counterexamples subset of the VQA v2 validation set. This subset is made of 63,298 examples on which all shortcuts provide the incorrect answer. As a consequence, VQA models that exploit these shortcuts to predict will not be able to get accurate answers on this kind of examples. They will be penalized and obtain a lower accuracy on this subset. On the contrary, we build the non-overlapping Easy subset. It is made of 147,681 examples on which at least one shortcut provides the correct answer. On this subset, VQA models that exploit shortcuts can reach high accuracy. Finally, 3,375 examples are not matched by any shortcut's antecedent. Since these examples do not belong to any of our two subsets, we do not consider them in our analysis. We show in supplementary materials that they have unusual questions and images.</p><p>Distribution of examples Here, we show how the split between our two subsets Counterexamples and Easy affects the distribution of examples. In <ref type="figure" target="#fig_2">Figure 5</ref>, we show that the original distribution of answers is similar to the Easy distribution but dissimilar to the Counterexamples distribution. Highlighted in blue, we display the five most common answers from the Easy distribution. They can be found at the same positions in the original distribution, the two major answers being "yes" and "no". It is not the case in the Counterexamples subset where these answers appear less frequently. Nonetheless, they are still in the top 30 answers which shows that our subsets creation is not a trivial splitting between frequent and rare answers. Similarly, the five most common answers from the Counterexamples subset, highlighted in orange, can be found in the Easy and All subset. We report similar observations for the questions and answer-type distributions in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main results</head><p>In <ref type="table" target="#tab_1">Table 1</ref>, we report results of some baselines, common VQA models, and latest bias-reduction methods fol- lowing our VQA-CE evaluation protocol. Models that exploit shortcuts are expected to get a lower accuracy on the Counterexamples compared to their overall accuracy. All models have been trained on the VQA v2 training set, and evaluated on the VQA v2 validation set. We detail them and discuss our findings in the next paragraphs. We report additional results on VQA v1 and v2 in the supplementary materials.</p><p>Baselines The Question-Only and Image-Only baselines are deep models that only use one modality. They are often used to assess the amount of unimodal shortcuts that a deep model can capture. We report extreme drops in accuracy on our Counterexamples subset compared to the overall accuracy, with a loss of 32.53 points and 22.12 points respectively. This shows that most of the questions that are easily answerable by only using the question, or the image, are filtered out of our Counterexamples subset.</p><p>Aggregating shortcuts to create a classifier In order to evaluate our shortcuts as a whole, we aggregate them to build a VQA classifier. As shown in the preceding section, each training example is associated with shortcuts that can be used to find the correct answer. Among these correct shortcuts, we select the highest-confidence one for each example. This leaves us with 115,718 unique shortcuts. In order to predict an answer for an unseen example, we take the most predicted answer for all its matching shortcuts weighted by the confidence of the shortcuts. For the examples that are not matched by any shortcut, we output "yes", the most common answer. Our shortcut-based clas- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bias-reduction methods</head><p>UpDown <ref type="bibr" target="#b2">[3]</ref> is used as a base architecture for bias-reduction methods RUBi <ref type="bibr" target="#b9">[10]</ref> 61.88 <ref type="bibr">(-1,64)</ref> 32.25 <ref type="bibr">(-1,66)</ref> 75.03 (-1.66) LMH + RMFE <ref type="bibr" target="#b16">[17]</ref> 60.96 <ref type="bibr">(</ref>  <ref type="bibr" target="#b2">[3]</ref>. We also report accuracies on VQA-CP v2 <ref type="bibr" target="#b0">[1]</ref> which focus on question biases, and comes with a different training set and testing set. VilBERT was not evaluated for VQA-CP as it was pretrained on balanced datasets. sifier reaches an overall accuracy of 42.26%, close to the 44.12% of the deep question-only baseline. Interestingly, both use a different class of shortcuts. Ours is mostly based on shallow multimodal shortcuts, not just shortcuts from the question. Since we use the same shortcuts to create our subsets, the shortcut-based classifier reaches a score of 0% on the Counterexamples. On VQA-CP testing set, our classifier reaches 22.44% accuracy. It highlights the difference with our counterexamples subset: VQA-CP does penalize some shortcuts, but there are still some that can be exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VQA models learn shortcuts</head><p>We compare different types of VQA models: SAN <ref type="bibr" target="#b43">[44]</ref> represents the image as a grid of smaller patches and uses a stacked attention mechanism over these patches, instead UpDown <ref type="bibr" target="#b2">[3]</ref> represents the image as a set of objects detected with Faster-RCNN and uses a simpler attention mechanism over them, BLOCK <ref type="bibr" target="#b7">[8]</ref> also relies on the object representations but uses a more complex attention mechanism based on a bilinear fusion, VilBERT <ref type="bibr" target="#b30">[31]</ref> also relies on the object representations but uses a transformer-based model that has been pretrained on the Conceptual Caption dataset <ref type="bibr" target="#b37">[38]</ref>. First, they suffer from a loss of 29 accuracy points on the counterexamples compared to their overall accuracy. This suggests that, despite their differences in modeling, they all exploit shortcuts. Note that comparable losses are reported on VQA-CP v2 <ref type="bibr" target="#b0">[1]</ref> which especially focuses on shortcuts based on question-types. Second, our evaluation protocol can be used to compare two models that get similar overall accuracies:</p><p>UpDown and BLOCK which gets +0.37 points over Up-Down. We can explain that this gain is due to a superior accuracy on the Easy subset with +0.96 and report a loss of -1.00 points on the Counterexamples. These results suggest that the bilinear fusion of BLOCK better captures shortcuts. On the contrary, VilBERT gets a better accuracy on our both subsets. It might be explained by the advantages of pretraining on external data.</p><p>Bias-reduction methods do not work well on natural multimodal shortcuts Our evaluation protocol can also be used to assess the efficiency of common bias-reduction methods. We use publicly available codebases when available, or our own implementation. All methods have been developed on the VQA-CP v2 dataset. It introduces new training and evaluation splits of VQA v2 that follow different distributions conditioned on the question-type. All the studied methods have been applied to UpDown and reached gains ranging from +5 to +20 accuracy points on the VQA-CP testing set. We evaluate them in the more realistic context of the original VQA v2 dataset. We show that their effect on our Counterexamples subset is very small. More specifically, some methods such as RUBi <ref type="bibr" target="#b9">[10]</ref>, LMH+RMFE <ref type="bibr" target="#b16">[17]</ref>, and ESR <ref type="bibr" target="#b38">[39]</ref> have a negative effect on all subsets. Some methods such as LMH [13] and LMH+CSS <ref type="bibr" target="#b10">[11]</ref> slightly improve the accuracy on counterexamples but strongly decrease the accuracy on the Easy subset, and consequently decrease the overall accuracy. As reported in <ref type="bibr" target="#b42">[43]</ref>, most of those methods rely on knowledge about the VQA-CP testing distribution (inversion of the an-  <ref type="bibr" target="#b19">(20)</ref> 71.4 <ref type="bibr" target="#b6">(7)</ref> 100.0 100.0 0.0 <ref type="table">Table 2</ref>. Instances of shortcuts that are highly correlated with VQA models' predictions. We display their antecedent made of words from the question and objects V from the image, and their answer. Their support, i.e. number of examples matched by the antecedent, and confidence, i.e. percentage of correct answers among them, have been calculated on the VQA v2 training and validation sets. We report the correlation coefficients of their predictions with those of three VQA models: UpDown <ref type="bibr" target="#b2">[3]</ref> that uses an object detector, VilBERT <ref type="bibr" target="#b30">[31]</ref> that has been pretrained on a large dataset, and Q-only <ref type="bibr" target="#b20">[21]</ref> that only uses the question. We show some counterexamples in the supplementary material.</p><p>swer distribution conditioned on the question), which no longer applies in our VQA v2 evaluation setting. Finally, we found two methods, LfF <ref type="bibr" target="#b33">[34]</ref> and RandImg <ref type="bibr" target="#b42">[43]</ref> that slightly improve the accuracy on the Counterexamples subset with gains of +0.36 and +0.50, while having a very small impact on the overall accuracy, even reaching small gains in the best case of LfF. It should be noted that LfF is more general than others since it was not designed for the VQA-CP context. Overall, all effects are much smaller compared to their effectiveness on VQA-CP. This suggests that those bias-reduction methods might exploit the distribution shift between VQA-CP training and evaluation splits. They are efficient in this setting but do not work as well to reduce naturally-occurring shortcuts in VQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Identifying most exploited shortcuts</head><p>We introduce a method to identify shortcuts that may be exploited by a given model. On the validation set, we calculate for each shortcut a correlation coefficient between its answer and the predictions of the studied model. Importantly, a 100% correlation coefficient indicates that the model may exploit the shortcut: both always provide the same answers, even on counterexamples on which using the shortcuts leads to the wrong answer.</p><p>In <ref type="table">Table 2</ref>, we report shortcuts that obtain the highest correlation coefficient with UpDown <ref type="bibr" target="#b2">[3]</ref> and VilBERT <ref type="bibr" target="#b30">[31]</ref>. Overall, these shortcuts have a high confidence and support, which means that they are common in the dataset and predictive. Most importantly, they are multimodal. As a consequence, these shortcuts obtain low correlations with Question-Only <ref type="bibr" target="#b20">[21]</ref>. On the contrary, they obtain a 100% correlation coefficient with VilBERT and UpDown. For instance, the second shortcut provides the answer skate-boarding for the appearance of sport, this, what in the question and a skateboard V in the image. It is a common shortcut with a support of 31 examples in the validation set. It gets a correlation of 0% because Question-Only mostly answer baseball for these examples. Its confidence of 87.1% indicates that 4 counterexamples can be found where the shortcut provides the wrong answer. To be correctly answered, they require more than a simple prediction based on the appearance of words and salient visual contents. These results once again confirm that VQA models tend to exploit multimodal shortcuts. It shows the importance of taking them into account in an evaluation protocol for VQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduced a method that discovers multimodal shortcuts in VQA datasets. It gives novel insights on the nature of shortcuts in VQA: they are not only related to the question but are also multimodal. We introduced an evaluation protocol to assess whether a given models exploits multimodal shortcuts. We found that most state-of-the-art VQA models suffer from a significant loss of accuracy in this setting. We also evaluated existing bias-reduction methods. Even the most general-purpose of these methods do not significantly reduce the use of multimodal shortcuts. We hope this new evaluation protocol will stimulate the design of better techniques to learn robust VQA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>The effort from Sorbonne Universit? was partly supported by ANR grant VISADEEP (ANR-20-CHIA-0022). This work was granted access to HPC resources of IDRIS under the allocation 2020-AD011011588 by GENCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Supplementary material</head><p>7.1. Additional statistics about shortcuts <ref type="figure">Figure 6</ref>. Histogram of shortcuts binned per confidence on the VQA v2 training and validation sets. Our shortcuts are detected on the training set and selected to have a confidence above 30%. Even though their confidence could be expected to be lower on the validation set, it still is above 30% for a large number of them, indicating that the selection transfers well to the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence distribution on training and unseen data</head><p>Here we show that shortcuts detected on the VQA v2 training set transfer to the validation set. In <ref type="figure">Figure 6</ref>, we display the confidence distribution of these shortcuts. As told earlier, we only consider shortcuts that reach a confidence greater than 0.3 on the training set. The number of shortcuts decreases when the confidence increases. It is expected to find fewer shortcuts with higher levels of confidence due to the collection procedure of VQA v2 which focused on reducing the amount of data biases and shortcuts. We evaluate on the validation set the same shortcuts detected on the training set and also display the confidence distribution. We show that our shortcuts are predictive on both training data, and unseen data that follows the training set distribution. The number of shortcuts that reach a confidence between 0.9 and 1.0 is even higher on the validation set than on the training set. The confidences are overall slightly lower on the validation set, but a large number of them are still above 0.3, indicating that they generalize to new examples from the same distribution. The great majority of shortcuts, which obtain a confidence lower than 1.0, allows finding examples that contradict them by leading to the wrong answers. We manually verified by looking at these examples that only a minority are wrongly annotated or ambiguous, most of them are counterexamples. These counterexamples are the core of our approach to assess the VQA model's reliance on shortcuts. <ref type="figure" target="#fig_4">Figure 7</ref>  Distribution of examples per answer type In <ref type="figure" target="#fig_5">Figure 8</ref>, we display the distribution of examples in our two subsets per answer type. We see that most yes-no questions are going in the Easy subset, as they are correctly predicted by some rules. On the contrary, for the two other answer types, examples are more evenly distributed between the Easy and Counterexamples subsets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution of examples per question-type In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Examples that are not matched by any rule</head><p>In <ref type="figure" target="#fig_6">Figure 9</ref>, we display some representative examples that are neither in the Easy subset nor in the Counterexamples subset. These examples are not matched by any antecedent of our rules. Their input might be unusual. We do not add these examples to our Counterexamples subset, as they do not contradict the shortcuts we found. We discard them entirely from our analysis. There consists in about 3K of examples. Results with ground-truth visual labels We report in Table 3 the results of our analysis with ground-truth visual labels from the COCO <ref type="bibr" target="#b29">[30]</ref> dataset, instead of labels detected with Faster R-CNN. We make similar observations to the main experiments of the paper: bias-reductions methods often degrade performances, on both easy and counterexamples split. A few methods slightly improve the counterexamples score, but much less than on VQA-CP. The only method which improves both overall and counterexamples scores is LfF <ref type="bibr" target="#b33">[34]</ref>. We observed similar results on the dataset with detected labels, reported in <ref type="table" target="#tab_1">Table 1</ref> of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on VQA v1</head><p>We report in <ref type="table">Table 4</ref> the results of our analysis on the VQA v1 dataset. We observe similar results as in <ref type="table" target="#tab_1">Table 1</ref> from the main paper. Most biasreduction methods degrade performances on the counterexamples split, and only LfF <ref type="bibr" target="#b33">[34]</ref> improves performances on all three splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Rules with supporting examples and counterexamples</head><p>In <ref type="figure">Figure 10</ref>, we display some counterexamples to some rules displayed in <ref type="table">Table 2</ref> of the main paper. Some of those examples are "true" counterexamples, where the input does match the rule's antecedent, but the answer is different. For instance, in the first example of the first rule, the question is actually about the clothes and not the sport, and the man is dressed in a basketball outfit. On the contrary, some examples are there due to an incorrect object detection: in the second example of the first rule, the object detection module detected a skateboard instead of a scooter. Thus, the example is incorrectly matched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approaches Overall</head><p>Counterexamples (ours) Easy (ours) <ref type="figure">Figure 10</ref>. Instances of shortcuts that are highly correlated with VQA models' predictions. We display their antecedent made of words from the question and objects V from the image, and their answer. Their support, i.e. number of examples matched by the antecedent, and confidence, i.e. percentage of correct answers among them, have been calculated on the VQA v2 training and validation sets. We report the correlation coefficients of their predictions with those of three VQA models: UpDown <ref type="bibr" target="#b2">[3]</ref> that uses an object detector, VilBERT <ref type="bibr" target="#b30">[31]</ref> that has been pretrained on a large dataset, and Q-only <ref type="bibr" target="#b20">[21]</ref> that only uses the question. We also display some supporting examples, in blue, and counterexamples, in orange.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Examples of shortcuts found in the VQA v2 dataset. The confidence is the accuracy obtained by applying the shortcut on all examples matching by its antecedent. The support is the number of matching examples. More supporting examples and counterexamples are shown in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Multiple shortcuts can often be exploited to find the correct answer in any given example. The confidence is the percentage of accurate answers among examples that are matched by the shortcut antecedent. The shortcut of highest confidence (in green) is multimodal for 92% of examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Number of examples per answer (30 most frequent ones) in the complete validation set, our Counterexamples subset, and our Easy subset. Answers highlighted in blue and orange are the top 5 answers for the Easy and Counterexamples subsets respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, we display the distribution of examples per question type, and their split between the Easy and the Counterexamples split. We show that examples of a question-type that can be answered by yes or no, such as is, are, does, do, mostly belong to the Easy subset. Examples of a question-type beginning by what, where or why mostly belong to the Counterexamples subset. These examples need to be answered using a richer vocabulary than yes or no. Examples of a question-type beginning by how belong to both subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Distribution of the number of examples per question type. Examples associated to our Counterexamples subset are matched by some shortcuts, but no shortcut leads to the correct answer. Examples associated to our Easy subset are matched by at least one shortcut that leads to the correct answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Number of examples per answer type. "All" corresponds to all the examples from the VQA v2 validation set. Among them, examples associated to our "Counterexamples" subset are matched by some shortcuts, but none of these shortcuts leads to the correct answer. Inversely, examples associated to our Easy subset are matched by at least one shortcut that leads to the correct answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Representative instances of image-question-answer examples that are not matched by any of our shortcuts. These examples have unusual questions, images or answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>VQA-CP v2 [1]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>22.64</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>19.31</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>15.95</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>24.96</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>39.74</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>38.69</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>44.23</cell></row><row><cell></cell><cell>-2.56)</cell><cell>33.14 (-0.77)</cell><cell>73.32 (-3.37)</cell><cell>54.55</cell></row><row><cell>ESR [39]</cell><cell>62.96 (-0.56)</cell><cell>33.26 (-0.65)</cell><cell>76.18 (-0.51)</cell><cell>48.50</cell></row><row><cell>LMH [13]</cell><cell>61.15 (-2.37)</cell><cell>34.26 (+0.35)</cell><cell>73.12 (-3.57)</cell><cell>52.05</cell></row><row><cell>LfF [34]</cell><cell>63.57 (+0.05)</cell><cell>34.27 (+0.36)</cell><cell>76.60 (-0.09)</cell><cell>39.49</cell></row><row><cell>LMH+CSS [11]</cell><cell>53.55 (-9.97)</cell><cell>34.36 (+0.45)</cell><cell>62.08 (-14.61)</cell><cell>58.95</cell></row><row><cell>RandImg [43]</cell><cell>63.34 (-0.18)</cell><cell>34.41 (+0.50)</cell><cell>76.21 (-0.48)</cell><cell>55.37</cell></row></table><note>Results of our VQA-CE evaluation protocol. We report accuracies on VQA v2 full validation set and on our two subsets: Counterexamples and Easy examples. We re-implemented all models and bias-reduction methods.? VilBERT is pretrained on Conceptual Caption and fine-tuned on VQA v2 training set. Scores in (green) and (red) are relative to UpDown</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Sup.) Conf. (Sup.) UpDown VilBERT Question-Onlydoing + man V + surfboard V + hand V ? surfing 86.helmet V + bat V + dirt V ? batter 61.8</figDesc><table><row><cell></cell><cell>Train</cell><cell>Val</cell><cell></cell><cell>Correlations (Val)</cell><cell></cell></row><row><cell>Rule (antecedent ? consequent)</cell><cell>Conf. (6 (115)</cell><cell>87.3 (55)</cell><cell>100.0</cell><cell>100.0</cell><cell>23.6</cell></row><row><cell cols="2">sport + this + what + skateboard V ? skateboarding 98.2 (53)</cell><cell>87.1 (31)</cell><cell>100.0</cell><cell>100.0</cell><cell>0.0</cell></row><row><cell cols="2">holding + this + what + racket V ? tennis racket 75.0 (26)</cell><cell>33.3 (3)</cell><cell>100.0</cell><cell>100.0</cell><cell>33.3</cell></row><row><cell cols="2">played + shorts V + racket V + leg V ? tennis 100.0 (29)</cell><cell>80.0 (5)</cell><cell>100.0</cell><cell>100.0</cell><cell>40.0</cell></row><row><cell cols="2">playing + they + what + controller V ? wii 100.0 (30)</cell><cell>88.9 (9)</cell><cell>100.0</cell><cell>100.0</cell><cell>66.7</cell></row><row><cell cols="2">picture + where + beach V + people V ? beach 100.0 (21)</cell><cell>90.0 (10)</cell><cell>100.0</cell><cell>100.0</cell><cell>90.0</cell></row><row><cell cols="2">taken + where + toilet V ? bathroom 85.2 (22)</cell><cell>80.0 (5)</cell><cell>100.0</cell><cell>100.0</cell><cell>20.0</cell></row><row><cell cols="2">eating + what + pizza V + arm V ? pizza 81.5 (21)</cell><cell>66.7 (6)</cell><cell>100.0</cell><cell>100.0</cell><cell>66.7</cell></row><row><cell cols="2">carrying + is + what + kite V ? kite 66.7 (21)</cell><cell>60.0 (5)</cell><cell>100.0</cell><cell>100.0</cell><cell>0.0</cell></row><row><cell cols="2">gender + of + what + head V ? male 64.1 (24)</cell><cell>66.7 (6)</cell><cell>100.0</cell><cell>100.0</cell><cell>66.7</cell></row><row><cell>position +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t just assume; look and answer: Overcoming priors for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Michael A Alcorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengfei</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shinn</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Block: Bilinear superdiagonal fusion for visual question answering and visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Approximating cnns with bagof-local-features models works surprisingly well on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RUBi: Reducing Unimodal Biases for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Counterfactual samples synthesizing for robust visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3</meeting>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gminer: A fast gpu-based frequent itemset mining method for large-scale data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Hyun</forename><surname>Kang-Wook Chon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Soo</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">439</biblScope>
			<biblScope unit="page" from="19" to="38" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Don&apos;t take the easy way out: Ensemble based methods for avoiding known dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Underspecification presents challenges for credibility in modern machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Alexander D&amp;apos;amour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Deaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.03395</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Human attention in visual question answering: Do humans and deep networks look at the same regions? Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="90" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interpretable explanations of black boxes by meaningful perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ruth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Removing bias in multi-modal classifiers: Regularization by maximizing functional entropies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itai</forename><surname>Gat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS), 2020. 3</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07780</idno>
		<title level="m">Shortcut learning in deep neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mutant: A training paradigm for out-of-distribution generalization in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratyay</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An analysis of visual question answering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Show, ask, attend, and answer: A strong baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Elqursh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03162</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Roses are red, violets are blue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigory</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moez</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05121</idno>
	</analytic>
	<monogr>
		<title level="m">but should vqa expect them to</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV)</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Explicit Bias Discovery in Visual Question Answering Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nirat</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<idno>Linguistics. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="3428" to="3448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning from failure: De-biasing classifier from biased classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyuntak</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Soo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Overcoming language priors in visual question answering with adversarial regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Sainandan Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">why should i trust you?&quot; explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cycle-consistency for robust visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL Short Papers)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL Short Papers)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A negative case analysis of visual grounding methods for vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robik</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE European Conference on Computer Vision (ECCV)</title>
		<meeting>the IEEE European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Ehsan Abbasnejad, and Anton van den Hengel. Unshuffling data for improved generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11894,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Actively seeking and learning from live data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1940" to="1949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">On the value of out-of-distribution testing: An example of goodhart&apos;s law</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robik</forename><surname>Shrestha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09241</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Ehsan Abbasnejad, Christopher Kanan, and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">We report accuracies on VQA v2 full validation set and on our two subsets: Counterexamples and Easy examples. We re-implemented all models and bias-reduction methods. ? VilBERT is pretrained on Conceptual Caption and fine-tuned on VQA v2 training set</title>
		<imprint/>
	</monogr>
	<note>Results of our VQA-CE evaluation protocol with ground-truth visual labels. Scores in (green) and (red) are relative to UpDown [3</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Approaches Overall Counterexamples (ours) Easy (ours) Number of examples 121</title>
		<imprint>
			<biblScope unit="volume">512</biblScope>
			<biblScope unit="page">539</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Results of our VQA-CE evaluation protocol on VQA v1 full validation set and on our two subsets: Counterexamples and Easy examples. We re-implemented all models and bias-reduction methods. Scores in (green) and (red) are relative to UpDown</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>Table 4.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
