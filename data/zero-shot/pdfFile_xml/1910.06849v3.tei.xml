<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TPAMI, SPECIAL ISSUE ON GRAPHS IN VISION AND PATTERN ANALYSIS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
							<email>guohao.li@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Visual Computing Center</orgName>
								<orgName type="institution" key="instit2">KAUST</orgName>
								<address>
									<region>Thuwal</region>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
							<email>matthias.mueller.2@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Visual Computing Center</orgName>
								<orgName type="institution" key="instit2">KAUST</orgName>
								<address>
									<region>Thuwal</region>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
							<email>guocheng.qian@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Visual Computing Center</orgName>
								<orgName type="institution" key="instit2">KAUST</orgName>
								<address>
									<region>Thuwal</region>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itzel</forename><forename type="middle">C</forename><surname>Delgadillo</surname></persName>
							<email>itzel.delgadilloperez@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Visual Computing Center</orgName>
								<orgName type="institution" key="instit2">KAUST</orgName>
								<address>
									<region>Thuwal</region>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulellah</forename><surname>Abualshour</surname></persName>
							<email>abdulellah.abualshour@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Visual Computing Center</orgName>
								<orgName type="institution" key="instit2">KAUST</orgName>
								<address>
									<region>Thuwal</region>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
							<email>ali.thabet@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Visual Computing Center</orgName>
								<orgName type="institution" key="instit2">KAUST</orgName>
								<address>
									<region>Thuwal</region>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Visual Computing Center</orgName>
								<orgName type="institution" key="instit2">KAUST</orgName>
								<address>
									<region>Thuwal</region>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IEEE TPAMI, SPECIAL ISSUE ON GRAPHS IN VISION AND PATTERN ANALYSIS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 DeepGCNs: Making GCNs Go as Deep as CNNs</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Graph Convolution Network</term>
					<term>Non-euclidean Data</term>
					<term>3D Semantic Segmentation</term>
					<term>Node classification</term>
					<term>Deep Learning !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (CNNs) have been very successful at solving a variety of computer vision tasks such as object classification and detection, semantic segmentation, activity understanding, to name just a few. One key enabling factor for their great performance has been the ability to train very deep networks. Despite their huge success in many tasks, CNNs do not work well with non-Euclidean data, which is prevalent in many real-world applications. Graph Convolutional Networks (GCNs) offer an alternative that allows for non-Eucledian data input to a neural network. While GCNs already achieve encouraging results, they are currently limited to architectures with a relatively small number of layers, primarily due to vanishing gradients during training. This work transfers concepts such as residual/dense connections and dilated convolutions from CNNs to GCNs in order to successfully train very deep GCNs. We show the benefit of using deep GCNs (with as many as 112 layers) experimentally across various datasets and tasks. Specifically, we achieve very promising performance in part segmentation and semantic segmentation on point clouds and in node classification of protein functions across biological protein-protein interaction (PPI) graphs. We believe that the insights in this work will open avenues for future research on GCNs and their application to further tasks not explored in this paper. The source code for this work is available at https://github.com/lightaime/deep gcns torch and https://github.com/lightaime/deep gcns for PyTorch and TensorFlow implementation respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>G CNS have become a prominent research topic in recent years. There are several reasons for this trend, but above all, GCNs promise a natural extension of CNNs to non-Euclidean data. While CNNs are very powerful when dealing with gridlike structured data, e.g. images, their performance on more irregular data, e.g. point clouds, graphs, etc., is sub-par. Since many real-world applications need to leverage such data, GCNs are a very natural fit. There has already been some success in using GCNs to predict individual relations in social networks <ref type="bibr" target="#b0">[1]</ref>, model proteins for drug discovery <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, enhance predictions of recommendation engines <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, and efficiently segment large point clouds <ref type="bibr" target="#b5">[6]</ref>. While these works show promising results, they rely on simple and shallow network architectures.</p><p>In the case of CNNs, the primary reason for their continued success and state-of-the-art performance on many computer vision tasks, is the ability to reliably train very deep network architectures. Surprisingly, it is not clear how to train deep GCN architectures and many existing works have investigated this limitation along with other shortcomings of GCNs <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Similar to CNNs, stacking multiple layers in GCNs leads to the vanishing gradient problem. The over-smoothing problem can also occurs when repeatedly applying many GCN layers <ref type="bibr" target="#b6">[7]</ref>. In this case, it was observed that the features of vertices within each connected component will converge to the same value and thus become indistinguishable from each other. As a result, most state-of-the-*Equal contribution *Corresponding author: Guohao Li -guohao.li@kaust.edu.sa art GCNs are limited to shallow network architectures, usually no deeper than 4 layers <ref type="bibr" target="#b8">[9]</ref>.</p><p>The vanishing gradient problem is well-known and wellstudied in the realm of CNNs. As a matter of fact, it was the key limitation for deep convolutional networks before ResNet <ref type="bibr" target="#b9">[10]</ref> proposed a simple, yet effective solution. The introduction of residual connections <ref type="bibr" target="#b9">[10]</ref> between consecutive layers addressed the vanishing gradient problem by providing additional paths for the gradient. This enabled deep residual networks with more than a hundred layers (e.g. ResNet-152) to be trained reliably. The idea was further extended by DenseNet <ref type="bibr" target="#b10">[11]</ref>, where additional connections are added across layers.</p><p>Training deep networks reveals another bottleneck, which is especially relevant for tasks that rely on the spatial composition of the input image, e.g. object detection, semantic segmentation, depth estimation, etc. With increased network depth, more spatial information can potentially be lost during pooling. Ideally, the receptive field should increase with network depth without loss of resolution. For CNNs, dilated convolutions <ref type="bibr" target="#b11">[12]</ref> were introduced to tackle this issue. The idea is again simple but effective. Essentially, the convolutions are performed across more distant neighbors as the network depth increases. In this way, multiple resolutions can be seamlessly encoded in deeper CNNs. Several innovations, in particular residual/dense connections and dilated convolutions, have enabled reliable training of very deep CNNs achieving state-of-the-art performance on many tasks. This yields the following question: do these innovations have a counterpart in the realm of GCNs?  <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b55">56</ref> and 112 layers, with and without residual connections for 100 epochs. We note that adding more layers without residual connections translates to a substantially higher loss and for very deep networks (e.g. 112 layers) even to divergence. (bottom) In contrast, training GCNs with residual connections results in consistent training stability across all depths. All training losses are obtained by training ResGCNs with varying depth but the same hyperparameters for the task of semantic segmentation on the S3DIS dataset.</p><p>In this work, we present an extensive study of methodologies that allow training very deep GCNs. We adapt concepts that were successful in training deep CNNs, in particular residual connections, dense connections, and dilated convolutions. We show how these concepts can be incorporated into a graph framework. In order to quantify the effect of these additions, we conduct an extensive analysis of each component and its impact on accuracy and stability of deep GCNs. To showcase the potential of these concepts in the context of GCNs, we apply them to the popular tasks of semantic segmentation and part segmentation of point clouds as well as node classification of biological graphs. Adding either residual or dense connections in combination with dilated convolutions, enables successful training of GCNs with a depth of 112 layers (refer to <ref type="figure" target="#fig_0">Figure 1</ref>). The proposed deep GCNs improve the baseline model on the challenging point cloud dataset S3DIS <ref type="bibr" target="#b12">[13]</ref> by 3.9% mIOU and outperform previous methods in many classes of PartNet <ref type="bibr" target="#b13">[14]</ref>. The same deep GCN architecture achieves an F1 score of 99.43 on the very different PPI dataset <ref type="bibr" target="#b1">[2]</ref>.</p><p>Contributions. The contributions of this work are three fold. <ref type="bibr" target="#b0">(1)</ref> We adapt residual connections, dense connections, and dilated convolutions, which were introduced for CNNs to enable deep GCN architectures, denoted DeepGCNs. <ref type="bibr" target="#b1">(2)</ref> We present extensive experiments on point cloud and biological graph data, showing the effect of each component to the stability and performance of training deep GCNs. We use semantic segmentation and part segmentation on point clouds, as well as, node classification of biological networks as our experimental testbeds. (3) We show how these new concepts enable successful training of a 112-layer GCN, the deepest GCN architecture by a large margin. With only 28 layers, we already improve the previous best performance by almost 4% in terms of mIOU on the S3DIS dataset <ref type="bibr" target="#b12">[13]</ref>; we also outperform previous methods in the task of part segmentation for many classes of PartNet <ref type="bibr" target="#b13">[14]</ref>. Similarly, we achieve superior results on the PPI dataset <ref type="bibr" target="#b1">[2]</ref> in the task of node classification in biological networks.</p><p>A preliminary version of this work was published in <ref type="bibr" target="#b14">[15]</ref>. This journal manuscript extends the initial version in several aspects. First, we investigate even deeper DeepGCN architectures with more than 100 layers. Interestingly, we find that divergence occurs when training a PlainGCN with 112 layers, while our proposed counterpart, ResGCN with skip connections and dilated convolutions, converges without problem (see <ref type="figure" target="#fig_0">Figure 1</ref>). Second, to investigate the generality of our DeepGCN framework, we perform extensive additional experiments on the tasks of part segmentation on PartNet and node classification on PPI. Third, we examine the performance and efficiency of MRGCN, a memoryefficient GCN aggregator we propose, with thorough experiments on the PPI dataset. Our results show that DeepMRGCN models are able to outperform previous methods. We also demonstrate that MRGCN is very memory-efficient compared to other GCN operators via GPU memory usage experiments. Finally, to ensure the reproducibility of our experiments and contribute to the graph learning research community, we have published code for training, testing, and visualization along with several pretrained models in both TensorFlow and PyTorch. To the best of our knowledge, our work is the first to successfully train deep GCNs beyond 100 layers and achieves superb results on both point cloud and biological graph data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>A large number of real-world applications deal with non-Euclidean data, which cannot be systematically and reliably processed by CNNs in general. To overcome the shortcomings of CNNs, GCNs provide well-suited solutions for non-Euclidean data processing, leading to greatly increasing interest in using GCNs for a variety of applications. In social networks <ref type="bibr" target="#b0">[1]</ref>, graphs represent connections between individuals based on mutual interests/relations. These connections are non-Euclidean and highly irregular. GCNs help better estimate edge strengths between the vertices of social network graphs, thus leading to more accurate connections between individuals. Graphs are also used to model chemical molecule structures <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Understanding the bioactivities of these molecules can have substantial impact on drug discovery. Another popular use of graphs is in recommendation engines <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, where accurate modelling of user interactions leads to improved product recommendations. Graphs are also popular modes of representation in natural language processing <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, where they are used to represent complex relations between large text units.</p><p>GCNs also find many applications in computer vision. In scene graph generation, semantic relations between objects are modelled using a graph. This graph is used to detect and segment objects in images, and also to predict semantic relations between object pairs <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Scene graphs facilitate the inverse process as well, where an image is reconstructed given a graph representation of the scene <ref type="bibr" target="#b21">[22]</ref>. Graphs are also used to model human joints for action recognition in video <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Moreover, GCNs are a perfect candidate for 3D point cloud processing, especially since the unstructured nature of point clouds poses a representational challenge for systematic research. Several attempts in creating structure from 3D data exist by either representing it with multiple 2D views <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, or by voxelization <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. More recent work focuses on directly processing unordered point cloud representations <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. The recent EdgeConv method by Wang et al. <ref type="bibr" target="#b5">[6]</ref> applies GCNs to point clouds. In particular, they propose a dynamic edge convolution algorithm for semantic segmentation of point clouds. The algorithm dynamically computes node adjacency at each graph layer using the distance between point features. This work demonstrates the potential of GCNs for point cloud related applications and beats the state-of-the-art in the task of point cloud segmentation. Unlike most other works, EdgeConv does not rely on RNNs or complex point aggregation methods.</p><p>Current GCN algorithms including EdgeConv are limited to shallow depths. Recent works have attempted to train deeper GCNs. For instance, Kipf et al. trained a semi-supervised GCN model for node classification and showed how performance degrades when using more than 3 layers <ref type="bibr" target="#b37">[38]</ref>. Pham et al. <ref type="bibr" target="#b38">[39]</ref> proposed Column Network (CLN) for collective classification in relational learning, where they showed peak performance at 10 layers and degrading performance for deeper graphs. Rahimi et al. <ref type="bibr" target="#b39">[40]</ref> developed a Highway GCN for user geo-location in social media graphs, where they add "highway" gates between layers to facilitate gradient flow. Even with these gates, the authors demonstrate performance degradation after 6 layers of depth. Xu et al. <ref type="bibr" target="#b40">[41]</ref> developed a Jump Knowledge Network for representation learning and devised an alternative strategy to select graph neighbors for each node based on graph structure. As with other works, their network is limited to a small number of layers <ref type="bibr" target="#b5">(6)</ref>. Recently, Li et al. <ref type="bibr" target="#b6">[7]</ref> studied the depth limitations of GCNs and showed that deep GCNs can cause over-smoothing, which results in features at vertices within each connected component converging to the same value. Other works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> also show the limitations of stacking GCN layers, specifically highly complex back-propagation and the common vanishing gradient problem.</p><p>Many difficulties facing GCNs nowadays (e.g. vanishing gradients and limited receptive field) were also present in the early days of CNNs <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>. We bridge this gap and show that the majority of these drawbacks can be remedied by borrowing several orthogonal tricks from CNNs. Deep CNNs achieved a huge boost in performance with the introduction of ResNet <ref type="bibr" target="#b9">[10]</ref>. By adding residual connections between inputs and outputs of layers, ResNet tends to alleviate the vanishing gradient problem. DenseNet <ref type="bibr" target="#b10">[11]</ref> takes this idea a step further and adds connections across layers as well. Dilated Convolutions <ref type="bibr" target="#b11">[12]</ref> are another recent approach that has lead to significant performance gains, specifically in imageto-image translation tasks such as semantic segmentation <ref type="bibr" target="#b11">[12]</ref>, by increasing the receptive field without loss of resolution. In this work, we show how one can benefit from concepts introduced for CNNs, mainly residual/dense connections and dilated convolutions, to train very deep GCNs. We support our claim by extending different GCN variants to deeper versions through adapting these concepts, and therefore significantly increasing their performance. Extensive experiments on the tasks of semantic segmentation and part segmentation of point clouds and node classification in biological graphs validate these ideas for general graph scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Representation Learning on Graphs</head><p>Graph Definition. A graph G is represented by a tuple G = (V, E) where V is the set of unordered vertices and E is the set of edges representing the connectivity between vertices v ? V. If e i,j ? E, then vertices v i and v j are connected to each other with an edge e i,j . Graph Convolution Networks. Inspired by CNNs, GCNs intend to extract richer features at a vertex by aggregating features of vertices from its neighborhood. GCNs represent vertices by associating each vertex v with a feature vector h v ? R D , where D is the feature dimension. Therefore, the graph G as a whole can be represented by concatenating the features of all the unordered vertices, i.e.</p><formula xml:id="formula_0">h G = [h v1 , h v2 , ..., h v N ] ? R N ?D ,</formula><p>where N is the cardinality of set V. A general graph convolution operation F at the l-th layer can be formulated as the following aggregation and update operations,</p><formula xml:id="formula_1">G l+1 = F(G l , W l ) = U pdate(Aggregate(G l , W agg l ), W update l ).</formula><p>(1)</p><formula xml:id="formula_2">G l = (V l , E l ) and G l+1 = (V l+1 , E l+1 )</formula><p>are the input and output graphs at the l-th layer, respectively. W agg l and W update l are the learnable weights of the aggregation and update functions respectively, and they are the essential components of GCNs. In most GCN frameworks, aggregation functions are used to compile information from the neighborhood of vertices, while update functions perform a non-linear transform on the aggregated information to compute new vertex representations. There are different variants of these two functions. For example, the aggregation function can be a mean aggregator <ref type="bibr" target="#b37">[38]</ref>, a max-pooling aggregator <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b41">[42]</ref>, an attention aggregator <ref type="bibr" target="#b42">[43]</ref>, or an LSTM aggregator <ref type="bibr" target="#b43">[44]</ref>. The update function can be a multi-layer perceptron <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b44">[45]</ref>, a gated network <ref type="bibr" target="#b45">[46]</ref>, etc. More concretely, the representation of vertices is computed at each layer by aggregating features of neighbor vertices for all v l+1 ? V l+1 as follows, <ref type="bibr" target="#b1">(2)</ref> where ? is a vertex feature aggregation function and ? is a vertex feature update function, h v l and h v l+1 are the vertex features at the l-th layer and (l + 1)-th layer, respectively. N (v l ) is the set of neighbor vertices of v at the l-th layer, and h u l is the feature of those neighbor vertices parametrized by W ? . W ? contains the learnable parameters of these functions. For simplicity and without loss of generality, we use a max-pooling vertex feature aggregator, without learnable parameters, to pool the difference of features between vertex v l and all of its neighbors:</p><formula xml:id="formula_3">h v l+1 = ? (h v l , ?({h u l |u l ? N (v l )}, h v l , W ? ), W ? ),</formula><formula xml:id="formula_4">?(.) = max(h u l ? h v l | u l ? N (v l )</formula><p>). We then model the vertex feature updater ? as a multi-layer perceptron (MLP) with batch normalization <ref type="bibr" target="#b46">[47]</ref> and a ReLU as an activation function. This MLP concatenates h v l with its aggregate features from ?(.) to form its input. Dynamic Edges. As mentioned earlier, most GCNs have fixed graph structures and only update the vertex features at each iteration. Recent work <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref> demonstrates that dynamic graph convolution, where the graph structure is allowed to change in each layer, can learn better graph representations compared to GCNs with fixed graph structure. For instance, ECC (Edge-Conditioned Convolution) <ref type="bibr" target="#b47">[48]</ref> uses dynamic edge-conditional filters to learn an edge-specific weight matrix. Moreover, EdgeConv <ref type="bibr" target="#b5">[6]</ref> finds the nearest neighbors in the current feature space to  reconstruct the graph after every EdgeConv layer. In order to learn to generate point clouds, Graph-Convolution GAN (Generative Adversarial Network) <ref type="bibr" target="#b48">[49]</ref> also applies k-NN graphs to construct the neighbourhood of each vertex in every layer. We find that dynamically changing neighbors in GCNs results in an effectively larger receptive field, when deeper GCNs are considered. In our framework, we propose to re-compute edges between vertices via a Dilated k-NN function in the feature space of each layer to further increase the receptive field.</p><p>Designing deep GCN architectures <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> is an open problem in the graph learning domain. Recent work <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> suggests that GCNs do not scale well to deep architectures, since stacking multiple layers of graph convolutions leads to high complexity in back-propagation. As such, most state-of-the-art GCN models are usually quite shallow <ref type="bibr" target="#b8">[9]</ref>. Inspired by the huge success of ResNet <ref type="bibr" target="#b9">[10]</ref>, DenseNet <ref type="bibr" target="#b10">[11]</ref>, and Dilated Convolutions <ref type="bibr" target="#b11">[12]</ref>, we transfer these ideas to GCNs to unleash their full potential. This enables much deeper GCNs that reliably converge in training and achieve superior performance in inference. In what follows, we provide a detailed description of three operations that can enable much deeper GCNs: residual connections, dense connections, and dilated aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Residual Connections for GCNs</head><p>In the original graph learning framework, the underlying mapping F, which takes a graph as an input and outputs a new graph representation (see Equation <ref type="formula">(1)</ref>), is learned. Here, we propose a graph residual learning framework that learns an underlying mapping H by fitting another mapping F. After G l is transformed by F, vertex-wise addition is performed to obtain G l+1 . The residual mapping F learns to take a graph as input and outputs a residual graph representation G res l+1 for the next layer. W l is the set of learnable parameters at layer l. In our experiments, we refer to our residual model as ResGCN.</p><formula xml:id="formula_5">G l+1 = H(G l , W l ) = F(G l , W l ) + G l = G res l+1 + G l .<label>(3)</label></formula><p>Dense Connections for GCNs. DenseNet <ref type="bibr" target="#b10">[11]</ref> was proposed to exploit dense connectivity among layers, which improves information flow in the network and enables efficient reuse of features among layers. Inspired by DenseNet, we adapt a similar idea to</p><p>GCNs so as to exploit information flow from different GCN layers.</p><p>In particular, we have:</p><formula xml:id="formula_6">G l+1 = H(G l , W l ) = T (F(G l , W l ), G l ) = T (F(G l , W l ), ..., F(G 0 , W 0 ), G 0 ).<label>(4)</label></formula><p>The operator T is a vertex-wise concatenation function that densely fuses the input graph G 0 with all the intermediate GCN layer outputs. To this end, G l+1 consists of all the GCN transitions from previous layers. Since we fuse GCN representations densely, we refer to our dense model as DenseGCN. The growth rate of DenseGCN is equal to the dimension D of the output graph (similar to DenseNet for CNNs <ref type="bibr" target="#b10">[11]</ref>). For example, if F produces a D dimensional vertex feature, where the vertices of the input graph G 0 are D 0 dimensional, the dimension of each vertex feature of G l+1 is D 0 + D ? (l + 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dilated Aggregation for GCNs</head><p>Dilated wavelet convolution is an algorithm originating from the wavelet processing domain <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>. To alleviate spatial information loss caused by pooling operations, Yu et al. <ref type="bibr" target="#b11">[12]</ref> propose dilated convolutions as an alternative to applying consecutive pooling layers for dense prediction tasks, e.g. semantic image segmentation. Their experiments demonstrate that aggregating multi-scale contextual information using dilated convolutions can significantly increase the accuracy on the semantic segmentation task. The reason behind this is the fact that dilation enlarges the receptive field without loss of resolution. We believe that dilation can also help with the receptive field of DeepGCNs. Therefore, we introduce dilated aggregation to GCNs. There are many possible ways to construct a dilated neighborhood. We use a Dilated k-NN to find dilated neighbors after every GCN layer and construct a Dilated Graph. In particular, for an input graph G = (V, E) with Dilated k-NN and d as the dilation rate, the Dilated k-NN operation returns the k nearest neighbors within the k ?d neighborhood region by skipping every d neighbors. The nearest neighbors are determined based on a pre-defined distance metric. In our experiments, we use the 2 distance in the feature space of the current layer. Let</p><formula xml:id="formula_7">N (d) (v) denote the d-dilated neigh- borhood of vertex v. If (u 1 , u 2 , .</formula><p>.., u k?d ) are the first sorted k ?d nearest neighbors, vertices (u 1 , u 1+d , u 1+2d , ..., u 1+(k?1)d ) are the d-dilated neighbors of vertex v (see <ref type="figure" target="#fig_2">Figure 3</ref>), i.e.</p><formula xml:id="formula_8">N (d) (v) = {u 1 , u 1+d , u 1+2d , ..., u 1+(k?1)d }.</formula><p>(5) Hence, the edges E (d) of the output graph are defined on</p><formula xml:id="formula_9">the set of d-dilated vertex neighbors N (d) (v). Specifically, there exists a directed edge e ? E (d) from vertex v to every vertex u ? N (d) (v).</formula><p>The GCN aggregation and update functions are applied, as in Equation <ref type="formula">(1)</ref>, by using the edges E (d) created by the Dilated k-NN, so as to generate the feature h</p><formula xml:id="formula_10">(d) v of each output vertex in V (d) .</formula><p>We denote this layer operation as a dilated graph convolution with dilation rate d, or more formally:</p><formula xml:id="formula_11">G (d) = (V (d) , E (d) )</formula><p>. We visualize and compare it to a conventional dilated convolution used in CNNs in <ref type="figure" target="#fig_2">Figure 3</ref>. To improve generalization, we use stochastic dilation in practice. During training, we perform the aforementioned dilated aggregations with a high probability (1 ? ) leaving a small probability to perform random aggregation by uniformly sampling k neighbors from the set of k ? d neighbors {u 1 , u 2 , ..., u k?d }. At inference time, we perform deterministic dilated aggregation without stochasticity, i.e. the dilated neighbors are sampled based on Equation 5 with probability 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Deep GCN Variants</head><p>In our experiments in the paper, we mostly work with a GCN based on EdgeConv <ref type="bibr" target="#b5">[6]</ref> to show how very deep GCNs can be trained. However, it is straightforward to build other deep GCNs with the same aforementioned concepts (e.g. residual/dense graph connections and dilated graph convolutions). To show that these concepts are universal operators and can be used for general GCNs, we perform additional experiments. In particular, we build ResGCNs based on GraphSAGE <ref type="bibr" target="#b41">[42]</ref> and Graph Isomorphism Network (GIN) <ref type="bibr" target="#b51">[52]</ref>. In practice, we find that EdgeConv learns a better representation than the other implementations. However, it is less efficient in terms of memory and computation. Therefore, we also propose a simple GCN operation combining the advantages of both, which we refer to as MRGCN (Max-Relative GCN). In the following, we discuss each GCN operator in detail.</p><p>EdgeConv. Instead of aggregating neighborhood features directly, EdgeConv <ref type="bibr" target="#b5">[6]</ref> proposes to first get local neighborhood information for each neighbor by subtracting the feature of the central vertex from its own feature. In order to train deeper GCNs, we add residual/dense graph connections and dilated graph convolutions to EdgeConv:</p><formula xml:id="formula_12">h res v l+1 = max {mlp(concat(h v l , h u l ? h v l ))|u l ? N (d) (v l )} , h v l+1 = h res v l+1 + h v l .<label>(6)</label></formula><p>GraphSAGE. GraphSAGE <ref type="bibr" target="#b41">[42]</ref> proposes different types of aggregator functions including a Mean aggregator, an LSTM aggregator, and a Pooling aggregator. Their experiments show that the Pooling aggregator outperforms the others. We adapt GraphSAGE with the max-pooling aggregator to obtain ResGraphSAGE:</p><formula xml:id="formula_13">h res N (d) (v l ) = max {mlp(h u l )|u l ? N (d) (v l )} , h res v l+1 = mlp concat h v l , h res N (d) (v l ) , h v l+1 = h res v l+1 + h v l ,<label>(7)</label></formula><p>In the original GraphSAGE paper, the vertex features are normalized after aggregation. We implement two variants, one without normalization (see Equation <ref type="formula" target="#formula_13">(7)</ref>), another with normal-</p><formula xml:id="formula_14">ization h res v l+1 = h res v l+1 / h res v l+1 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GIN. The main difference between GIN [52] and other GCNs is that an is learned at each GCN layer to give the central vertex and aggregated neighborhood features different weights. Hence</head><p>ResGIN is formulated as follows:</p><formula xml:id="formula_15">h res v l+1 = mlp (1 + ) ? h v l + sum({h u l |u l ? N (d) (v l )}) , h v l+1 = h res v l+1 + h v l .<label>(8)</label></formula><p>MRGCN. We find that first using a max aggregator to aggregate</p><formula xml:id="formula_16">neighborhood relative features (h u l ? h v l ), u l ? N (v l )</formula><p>is more effective and efficient than aggregating raw neighborhood features h v l , u l ? N (v l ) or aggregating features after nonlinear transforms. We refer to this simple GCN as MRGCN (Max-Relative GCN). The residual version of MRGCN is as such:</p><formula xml:id="formula_17">h res N (d) (v l ) = max {h u l ? h v l |u l ? N (d) (v l )} , h res v l+1 = mlp concat h v l , h res N (d) (v l )</formula><p>,</p><formula xml:id="formula_18">h v l+1 = h res v l+1 + h v l .<label>(9)</label></formula><p>Here, h v l+1 and h v l are the hidden states of vertex v at layers l and l + 1, and h res v l+1 is the hidden state of the residual graph. All the mlp (multilayer perceptron) functions use a ReLU as activation function; all the max and sum functions above are vertex-wise feature operators; concat functions concatenate features of two vertices into one feature vector. N (d) (v l ) denotes the neighborhood of vertex v l obtained from Dilated k-NN. MRGCN is more efficient than EdgeConv and GraphSAGE. MRGCN only applies an mlp transform once for each vertex, while EdgeConv needs to apply this transform for every neighbor of each vertex. In contrast to GraphSAGE, MRGCN does not need to apply mlp transforms to the neighborhood features before max aggregation and the additional computation of subtraction is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS ON 3D POINT CLOUDS</head><p>We propose ResGCN and DenseGCN to handle the vanishing gradient problem of GCNs. To enlarge the receptive field, we define a dilated graph convolution operator for GCNs. To evaluate our framework, we conduct extensive experiments on the tasks of semantic segmentation and part segmentation on largescale 3D point cloud datasets and demonstrate that our methods significantly improve performance. In addition, we perform a comprehensive ablation study to show the effect of different components of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph Learning on 3D Point Clouds</head><p>Point cloud segmentation is a challenging task because of the unordered and irregular structure of 3D point clouds. Normally, each point in a point cloud is represented by its 3D spatial coordinates and possibly auxiliary features such as color and/or surface normal. We treat each point as a vertex v in a directed graph G and we use k-NN to construct the directed dynamic edges between points at every GCN layer (refer to Section 3.1). In the first layer, we construct the input graph G 0 by applying a dilated k-NN search to find the nearest neighbors in 3D coordinate space. At subsequent layers, we dynamically build the edges using dilated k-NN in feature space. For the segmentation task, we predict the categories of all the vertices at the output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>We use the overall accuracy (OA) and mean intersection over union (mIoU) across all classes as evaluation metrics. For each class, the IoU is computed as T P T P +T ?P , where T P is the number of true positive points, T is the number of ground truth points of that class, and P is the number of predicted positive points. We perform the majority of our experiments on semantic segmentation of point clouds on the S3DIS dataset. To motivate the use of DeepGCNs, we do a thorough ablation study on area 5 of this dataset to analyze each component and provide insights. We then evaluate our proposed reference model ResGCN-28 (backbone of 28 layers with residual graph connections and stochastic dilated graph convolutions) on all 6 areas and compare it to the shallow DGCNN baseline <ref type="bibr" target="#b5">[6]</ref> and other state-of-the-art methods. In order to validate that our method is general and does not depend on a specific dataset, we also show results on PartNet for the task of part segmentation of point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Network Architectures</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, all the network architectures in our experiments have three blocks: a GCN backbone block, a fusion block and an MLP prediction block. The GCN backbone block is the only part that differs between experiments. For example, the only difference between PlainGCN and ResGCN is the use of residual skip connections for all GCN layers in ResGCN. Both have the same number of parameters. We linearly increase the dilation rate d of dilated k-NN with network depth. For fair comparison, we keep the fusion and MLP prediction blocks the same for all architectures. The GCN backbone block takes as input a point cloud with 4096 points, extracts features by applying consecutive GCN layers to aggregate local information, and outputs a learned graph representation with 4096 vertices. The fusion and MLP prediction blocks follow a similar architecture as PointNet <ref type="bibr" target="#b32">[33]</ref> and DGCNN <ref type="bibr" target="#b5">[6]</ref>. The fusion block is used to fuse the global and multi-scale local features. It takes as input the extracted vertex features from the GCN backbone block at every GCN layer and concatenates those features, then passes them through a 1?1 convolution layer followed by max pooling. The latter layer aggregates the vertex features of the whole graph into a single global feature vector, which in return is concatenated with the feature of each vertex from all previous GCN layers (fusion of global and local information). The MLP prediction block applies three MLP layers to the fused features of each vertex/point to predict its category. In practice, these layers are 1?1 convolutions.</p><p>PlainGCN. This baseline model consists of a PlainGCN backbone block, a fusion block, and an MLP prediction block. The backbone stacks EdgeConv <ref type="bibr" target="#b5">[6]</ref> layers with dynamic k-NN, each of which is similar to the one used in DGCNN <ref type="bibr" target="#b5">[6]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation</head><p>For semantic segmentation on S3DIS <ref type="bibr" target="#b12">[13]</ref>, we implement our models using TensorFlow, and for part segmentation on PartNet <ref type="bibr" target="#b13">[14]</ref>, we implement them using PyTorch. For fair comparison, we use the Adam optimizer with the same initial learning rate 0.001 and the same learning rate schedule for all experiments; the learning rate decays 50% every 3 ? 10 5 gradient decent steps. Batch normalization is applied to every layer. Dropout with a rate of 0.3 is used at the second MLP layer of the MLP prediction block. As mentioned in Section 3.3, we use dilated k-NN with a random uniform sampling probability = 0.2 for GCNs with dilations. In order to isolate the effect of the proposed DeepGCN architectures, we do not use any data augmentation or post processing techniques. We train our models end-to-end from scratch for 100 epochs. We evaluate every 10 th epoch on the test set and report the best result for each model. The networks are trained with two NVIDIA Tesla V100 GPUs using data parallelism in semantic segmentation on S3DIS, and the batch size is set to 8 for each GPU. For part segmentation on PartNet, we set the batch size to 7 and the networks are trained with one NVIDIA Tesla V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>For convenient referencing, we use the naming convention BackboneBlock-#Layers to denote the key models in our analysis. We focus on residual graph connections for our analysis, since ResGCN-28 is easier and faster to train, but we expect that our observations also hold for dense graph connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Semantic Segmentation on S3DIS</head><p>In order to thoroughly evaluate the ideas proposed in this paper, we conduct extensive experiments on the Stanford large-scale 3D Indoor Spaces Dataset (S3DIS), a large-scale indoor dataset for TABLE 1 Ablation study on area 5 of S3DIS. We compare our reference network (ResGCN-28) with 28 layers, residual graph connections, and dilated graph convolutions to several ablated variants. All models were trained with the same hyper-parameters for 100 epochs on all areas except for area 5, which is used for evaluation. We denote residual and dense connections with the ? and symbols respectively. ? denotes that residual connections are added between every two GCN layers. We highlight the most important results in bold. ?mIoU denotes the difference in mIoU with respect to the reference model ResGCN-28.</p><p>3D semantic segmentation of point clouds. S3DIS covers an area of more than 6, 000m 2 with semantically annotated 3D meshes and point clouds. In particular, the dataset contains 6 large-scale indoor areas represented as colored 3D point clouds with a total of 695,878,620 points. As is common practice, we train networks on 5 out of the 6 areas and evaluate them on the left out area.</p><p>We begin with the extensive ablation study, where we evaluate the trained models on area 5, after training them on the other areas. Our aim is to shed light on the contribution of each component of our novel network architectures. To this end, we investigate the performance of different ResGCN architectures, e.g. with dynamic dilated k-NN, with regular dynamic k-NN (without dilation), and with fixed edges. We also study the effect of different parameters, e.g. number of k-NN neighbors <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32)</ref>, number of filters <ref type="bibr" target="#b31">(32,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">128)</ref>, and number of layers <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b55">56)</ref>. To ensure that our contributions (residual/dense connections and dilated graph convolutions) are general, we apply them to multiple GCN variants that have been proposed in the literature. Overall, we conduct 25 experiments and report detailed results in <ref type="table" target="#tab_4">Table 1</ref>. We also summarize the most important insights of the ablation study in <ref type="figure">Figure 4</ref>. In the following, we discuss each block of experiments.  <ref type="figure">Fig. 4</ref>. Ablation study on area 5 of S3DIS. We compare our reference network (ResGCN-28) with 28 layers, residual graph connections, and dilated graph convolutions with several ablated variants. All models were trained with the same hyper-parameters for 100 epochs on all areas except for area 5 of the S3DIS dataset. ble 1 (Reference) show that residual graph connections play an essential role in training deeper networks, as they tend to result in more stable gradients. This is analogous to the insight from CNNs <ref type="bibr" target="#b9">[10]</ref>. ResNet <ref type="bibr" target="#b9">[10]</ref> adds skip connections between every two convolutional blocks; we conduct experiments with two skip connection variants, ResGCN-28 and ResGCN-28 ? . ResGCN-28 adds skip connections between every GCN layer while ResGCN-28 ? adds skip connections between every two GCN layers. We find that ResGCN-28 outperforms ResGCN-28 ? by 1.63% mIOU (refer to <ref type="table" target="#tab_4">Table 1</ref>). Thus, we adopt skip connections between every layer for all other ResGCN architectures. When the residual graph connections between layers are removed (i.e. in PlainGCN-28), performance dramatically degrades (-12% mIoU). <ref type="figure" target="#fig_3">Figure 5</ref> shows the importance of residual graph connections very clearly. As network depth increases, skip connections become critical for convergence. We also show similar performance gains by combining residual graph connections and dilated graph convolutions with other types of GCN layers. These results can be seen in the ablation study Table 1 (GCN Variants) and are further discussed later in this section. Effect of dilation. Results in <ref type="table" target="#tab_4">Table 1</ref> (Dilation) <ref type="bibr" target="#b11">[12]</ref> show that dilated graph convolutions account for a 2.85% improvement in mean IoU (row 3), justified primarily by the expansion of the network's receptive field. We find that adding stochasticity to the dilated k-NN does helps performance but not to a significant extent. We also investigate the effect of for stochastic dilation and report the results in <ref type="table" target="#tab_4">Table 1</ref> (sto. eps.). We find that models are robust to different values of . The mIOUs vary in the range of about 1% for different values. It is possible to achieve better performance by carefully tuning . For instance, ResGCN-28 with set to 0.4 increases the mIOU by 0.49% compared the Reference model with equal to 0.2. Interestingly, our results in <ref type="table" target="#tab_4">Table 1</ref> also indicate that dilation especially helps deep networks when combined with residual graph connections (rows 1,13). Without such connections, performance can actually degrade with dilated graph convolutions. The reason for this is probably that these varying neighbors result in 'worse' gradients, which further hinder convergence when residual graph connections are not used.</p><p>Dilation vs. Downsampling. Our proposed dilated graph convolution enlarges the receptive field without the need of downsampling. To show the advantage of dilated graph convolution compared to downsampling, we conduct experiments with U-Net-style <ref type="bibr" target="#b52">[53]</ref> ResGCN models and report the results in <ref type="table" target="#tab_4">Table 1</ref> (downsampling). For the U-Net variants, we insert downsampling modules into the backbone of our ResGCN-28. The number of vertices is downsampled by one half after every 7 layers. The final features are fed into the sequential interpolation layers to gradually upsample the features back to their original resolution. We experiment with two popular downsampling methods in the point cloud literature, i.e. random downsampling <ref type="bibr" target="#b53">[54]</ref> and farthest point sampling (FPS) <ref type="bibr" target="#b33">[34]</ref>. We denote the architectures as ResGCN-28-random and ResGCN-28-FPS respectively. We observe that ResGCN-28 with dilated graph convolutions outperforms ResGCN-28-random by 4.51% and ResGCN-28-FPS by 9.06% respectively, which clearly shows the advantage of dilation. However, it is worth mentioning that U-Net like structures with down-sampling are used in recent SOTA methods such as KPConv <ref type="bibr" target="#b54">[55]</ref> and RandLA-Net <ref type="bibr" target="#b53">[54]</ref> and achieve very promising results. The results of ResGCN-28-random and ResGCN-28-FPS could be sub-optimal. We conjecture that with more sophisticated architectural designs and hyper-parameter twists, U-Net like GCN architectures could reach more reasonable performance.</p><p>Effect of dynamic k-NN. While we observe an improvement when updating the k nearest neighbors after every layer, we would also like to point out that it comes at a relatively high computational cost. We show different variants without dynamic edges in <ref type="table" target="#tab_4">Table 1</ref> (Fixed k-NN).</p><p>Effect of dense graph connections. We observe similar performance gains with dense graph connections (DenseGCN-28) in <ref type="table" target="#tab_4">Table 1</ref> (Connections). However, with a naive implementation, the memory cost is prohibitive. Hence, the largest model we can fit into GPU memory uses only 32 filters and 8 nearest neighbors, as compared to 64 filters and 16 neighbors in the case of its residual counterpart ResGCN-28. Since the performance of these two deep GCN variants is similar, residual connections are more practical for most use cases and hence we focus on them in our ablation study. Yet, we do expect the same insights to transfer to the case of dense graph connections. <ref type="table" target="#tab_4">Table 1</ref> (Neighbors) show that a larger number of neighbors helps in general. As the number of neighbors is decreased by a factor of 2 and 4, the performance drops by 2.5% and 3.3% respectively. However, a large number of neighbors only results in a performance boost, if the network capacity is sufficiently large. This becomes apparent when we increase the number of neighbors by a factor of 2 and decrease the number of filters by a factor of 2 (refer to row 3 in Neighbors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of nearest neighbors. Results in</head><p>Effect of network depth.  the Instance Information Gain. We analyze the effect of residual graph connections and dilated graph convolutions on the over-smoothing issue of deep GCNs by measuring group distance ratio and instance information gain. All models were trained for 100 epochs on all areas except for area 5 with the same hyper-parameters. Residual connections and dilation are shown to be effective to alleviate the over-smoothing issue.</p><p>show that using normalization (i.e. GraphSAGE-N) is not essential. Interestingly, when using the GINoperator, the network converges well during the training phase and has a high training accuracy but fails to generalize to the test set. This phenomenon is also observed in the original paper <ref type="bibr" target="#b51">[52]</ref>, in which the best performance is achieved when is set to 0.  <ref type="figure">Figure 6</ref> show that the performance of ResGCN-28 and ResGCN-7 drops by 6.1% and 4% respectively when BN is removed. ResGCN-28 w/o BN is even outperformed by ResGCN-7 w/ BN. This indicates that BN is essential for training deep GCNs. Analysis of over-smoothing. To study the effect of residual graph connections and dilated graph convolutions on the oversmoothing issue, we adopt two quantitative metrics proposed in <ref type="bibr" target="#b55">[56]</ref>, i.e. Group Distance Ratio and Instance Information Gain, to measure the over-smoothness of the learned node representation of the last GCN layer. The group distance ratio measures the ratio of the inter-group distance and the intra-group distance in the Euclidean space of the final representation. The instance information gain measures the mutual information between a input node feature and the final representation. A higher group distance ratio or instance information gain implies less over-smoothing. We measure the group distance ratio and instance information gain of the trained reference model (ResGCN-28) and the corresponding ablated models without dilation (w/o dilation) or without residual connections (w/o connection). As shown in <ref type="table" target="#tab_5">Table 2</ref>, ResGCN-28 has a significantly higher group distance ratio (1.73 vs. 1.12) and instance information gain (0.46 vs. 0.01) than the non-residual counterpart. A similar observation can be made with respect to dilation albeit the impact is less pronounced. This indicates that ResGCN-28 learns sharper features and suffers less from oversmoothing. However, the current over-smoothing metrics can not isolate the effects caused by the difficulty of optimization. We believe disentangling the effects of over-smoothing and vanishing gradient is a promising direction to better understand deep GCN architectures.</p><p>Qualitative results. <ref type="figure">Figure 7</ref> shows qualitative results on area 5 of S3DIS <ref type="bibr" target="#b12">[13]</ref>. As expected from the results in Comparison to state-of-the-art. Finally, we compare our reference network (ResGCN-28), which incorporates the ideas put forward in the methodology, to several state-of-the-art baselines in <ref type="table" target="#tab_9">Table 3</ref>. The results clearly show the effectiveness of deeper models with residual graph connections and dilated graph convolutions. ResGCN-28 outperforms DGCNN [6] by 3.9% (absolute) in mean IoU. DGCNN has the same fusion and MLP prediction blocks as ResGCN-28 but a shallower PlainGCN-like backbone block. Furthermore, we outperform all baselines in 9 out of 13 classes. We perform particularly well in the difficult object classes such as board, where we achieve 51.1%, and sofa, where we improve state-of-the-art by about 10% mIOU. This significant performance improvement on the difficult classes is probably due to the increased network capacity, which allows the network to learn subtle details necessary to distinguish between a board and a wall for example. The first row in <ref type="figure">Figure 7</ref> is a representative example for this occurrence. Our performance gains are solely due to our innovation in the network architecture, since we use the same hyper-parameters and even learning rate schedule as the baseline DGCNN <ref type="bibr" target="#b5">[6]</ref> and only decrease the number of nearest neighbors from 20 to 16 and the batch size from 24 to 16 due to memory constraints. We outperform stateof-the art methods by a significant margin and expect further improvement from tweaking the hyper-parameters, especially the learning schedule. In <ref type="table" target="#tab_9">Table 3</ref>, we also include the most recent SOTA results from Deep LPN <ref type="bibr" target="#b56">[57]</ref>, ShellNet <ref type="bibr" target="#b57">[58]</ref>, RandLA-Net <ref type="bibr" target="#b53">[54]</ref> and KPConv <ref type="bibr" target="#b54">[55]</ref> for reference. These methods employ different data pre-processing and augmentation pipelines and were published after the short version of our paper. We color these results in gray.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Part Segmentation on PartNet</head><p>We further experiment with our architecture on the task of part segmentation and evaluate it on the recently proposed large-scale PartNet <ref type="bibr" target="#b13">[14]</ref> dataset. PartNet consists of over 26,671 3D models from 24 object categories with 573,585 annotated part instances. The dataset establishes three benchmarking tasks for part seg-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>Ground Truth PlainGCN-28 DenseGCN-28 ResGCN-28 mentation on 3D objects: fine-grained semantic segmentation, hierarchical semantic segmentation, and instance segmentation. For the following experiments, we focus on the fine-grained level of semantic segmentation, which includes 17 out of the 24 object categories present in the PartNet dataset.</p><p>We use the same reference architecture, ResGCN-28, that we used for the experiments on the S3IDS dataset. We compare its performance to the baseline architecture PlainGCN-28 to show the impact of our residual connections and stochastic dilated convolutions. We also compare against the state-of-the-art reported in the PartNet paper <ref type="bibr" target="#b13">[14]</ref>, namely PointNet <ref type="bibr" target="#b32">[33]</ref>, PointNet++ <ref type="bibr" target="#b33">[34]</ref>, SpiderCNN <ref type="bibr" target="#b58">[59]</ref>, and PointCNN <ref type="bibr" target="#b59">[60]</ref>. As suggested in PartNet <ref type="bibr" target="#b13">[14]</ref>, we use 10,000 sampled points as input. We train a separate network for each category and optimize them with Adam for 500 epochs without weight decay. The initial learning rate is 0.005 that is decayed by a factor of 0.9 every 50 epochs. We report our results using the best pretrained models on the validation sets.</p><p>Qualitative results. <ref type="figure">Figure 8</ref> shows qualitative results on 4 categories of PartNet <ref type="bibr" target="#b13">[14]</ref>: bottle, bed, microwave, and refrigerator. As expected from the results in <ref type="table">Table 4</ref>, ResGCN-28 performs very well compared to the baseline PlainGCN-28, where there are no residual connections between layers. Although ResGCN-28   <ref type="figure" target="#fig_1">-28</ref> with the state-of-the-art on S3DIS Semantic Segmentation. We report average per-class results across all areas for our reference model ResGCN-28 and state-of-the-art baselines. ResGCN-28 which has 28 GCN layers, residual graph connections, and dilated graph convolutions outperforms the previous state-of-the-art by almost 4%. It also outperforms all baselines in 9 out of 13 classes. The metrics shown are overall point accuracy (OA) and mean IoU (mIoU). '-' denotes not reported and bold denotes best performance.The most recent SOTA results published after the short version of this work are reported in gray. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><formula xml:id="formula_19">- - - - - - - - - - - - - - - - PosPool [61] 53.8 - - - - - - - - - - - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 4</head><p>Comparison of ResGCN-28 with other methods on PartNet Part Segmentation. We report the part-category mean IoU (mIoU) on the fine-grained level of segmentation. ResGCN-28 which has 28 GCN layers, residual graph connections, and dilated graph convolutions outperforms PlainGCN-28 and most previous methods w.r.t the average mIoU. The most recent SOTA results published after the short version of this work are reported in gray. '-' denotes that the results are not reported in the original papers.</p><p>produces some incorrect outputs compared to the ground truth in categories like microwave and bed, it still outperforms PlainGCN-28 and segments the important parts of the object. We provide more qualitative results in the supplementary material.</p><p>Comparison to state-of-the-art. We summarize the results of our ResGCN-28 and compare it to PlainGCN-28 and other state-ofthe-art methods in <ref type="table">Table 4</ref>.  <ref type="bibr" target="#b60">[61]</ref> for reference. These methods employ different data pre-processing and augmentation pipelines and were published after the short version of our paper. We color these results in gray.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Object Classification on ModelNet40</head><p>We experiment with our architecture on the task of object classification and evaluate it on the popular object classification dataset ModelNet40 <ref type="bibr" target="#b61">[62]</ref>. The dataset contains 12,311 CAD models from 40 different categories. Although saturated, Mod-elNet40 is considered an important benchmark for the task of 3D object classification. The results in <ref type="table" target="#tab_13">Table 5</ref> show the superiority of our ResGCN and DenseGCN architectures over the baseline PlainGCN and state-of-the-art methods. When compared to the baseline PlainGCN, one observes the positive effect of our residual connections, dense connections, and dilated convolutions. In addition, the ResGCN and DenseGCN architectures outperform all state-of-the-art approaches in overall test accuracy. Particularly, ResGCN-14 achieves 93.6% on ModelNet-40 in terms of overall accuracy. Here, we point out that the recent work of KPConv <ref type="bibr" target="#b54">[55]</ref> uses a much larger number of trainable parameters compared to our reference ResGCN-28, as shown in <ref type="table" target="#tab_13">Table 5</ref>. And yet, even our shallower variant ResGCN-14 can outperform KPConv, while using almost one order of magnitude less trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS ON BIOLOGICAL NETWORKS</head><p>In order to demonstrate the generality of our contributions and specifically our deep ResGCN architecture, we conduct further experiments on general graph data. We choose the popular task of node classification on biological graph data, which is quite different from point cloud data. In the following experiments, we mainly study the effects of skip connections, the number of GCN layers (i.e. depth), number of filters per layer (i.e. width) and different graph convolutional operators.   of our models and state-of-the-art models in addition to the to total number of parameters for each model. One can observe the superiority of our ResGCN and DenseGCN architectures over state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Graph Learning on Biological Networks</head><p>The main difference between biological networks and point cloud data is that biological networks have inherent edge information and high dimensional input features. For the graph learning task on biological networks, we use the PPI <ref type="bibr" target="#b1">[2]</ref> dataset to evaluate our architectures. PPI is a popular dataset for multi-label node classification, containing 24 graphs with 20 in the training set, 2 in the validation set, and 2 in the testing set. Each graph in PPI corresponds to a different human tissue, each node in a graph represents a protein and edges represent the interaction between proteins. Each node has positional gene sets, motif gene sets, and immunological signatures as input features (50 in total) and 121 gene ontology sets as labels. The input of the task is a graph that contains 2373 nodes on average, and the goal is to predict which labels are contained in each node. We use essentially the same reference architecture as for point cloud segmentation described in Section 4.1, but we predict multiple labels. The number of filters of the first and last layers are changed to adapt to this task. Instead of constructing edges by means of k-NN, we use the edges provided by PPI directly. If we were to construct edges dynamically, there is a chance to lose the rich information provided by the initial edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>Following common procedure, we use the micro-average F1 (m-F1) score as the evaluation metric. For each graph, the F1 score is computed as follows:</p><formula xml:id="formula_20">F1-score = 2 ? (precision ? recall) (precision + recall) ,<label>(10)</label></formula><p>where precision = T P P , recall = T P T , T P is the total number of true positive points of all the classes, T is the number of ground truth positive points, and P is the number of predicted positive points. We find the best model, i.e. the one with the highest accuracy (m-F1 score) on the validation set in the training phase, and then calculate the m-F1 score across all the graphs in the test set.</p><p>We show the performance and GPU memory usage of our proposed MRGCN and compare them with other graph convolutions, e.g. EdgeConv <ref type="bibr" target="#b5">[6]</ref>, GATConv <ref type="bibr" target="#b42">[43]</ref>, SemiGCN <ref type="bibr" target="#b37">[38]</ref> and GINConv <ref type="bibr" target="#b51">[52]</ref>. We conduct an extensive ablation study on the number of filters and the number of GCN layers to show their effect in the backbone network. Our ablation study also includes experiments to show the importance of residual graph connections and dense graph connections in our DeepGCN framework. To ensure a fair comparison, all networks in our ablation study share the same architecture. Finally, we compare our best models to several stateof-the-art methods for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation</head><p>For this biological network node classification task, we implement all our models based on PyTorch Geometric <ref type="bibr" target="#b67">[68]</ref>. We use the Adam optimizer with the same initial learning rate 0.0002 and learning rate schedule with learning rate decay of 80% every 2, 000 gradient decent steps for all the experiments. The networks are trained with one NVIDIA Tesla V100 GPU with a batch size of 1. Dropout with a rate of 0.2 is used at the first and second MLP layers of the prediction block. For fair comparison, we do not use any data augmentation or post processing techniques. Our models are trained end-to-end from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>We study the effect of residual and dense graph connections on multi-label node classification performance. We also investigate the influence of different parameters, e.g. the number of filters <ref type="bibr" target="#b31">(32,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256)</ref> and layers <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr">112)</ref>. To show the generality of our framework, we also apply the proposed residual connections to multiple GCN variants. Effect of graph connections. Results in <ref type="table" target="#tab_15">Table 6</ref> show that both residual and dense graph connections help train deeper networks. When the network is shallow, models with graph connections achieve similar performance as models without them. However, as the network grows deeper, the performance of models without graph connections drops dramatically, while the performance of models with graph connections is stable or even improves further. For example, when the number of filters is 32 and the depth is 112, the performance of ResMRGCN-112 is nearly 37.66% higher than PlainMRGCN-112 in terms of the m-F1 score. We note that DenseMRGCN achieves slightly better performance than ResMRGCN with the same network depth and width. Effect of network depth. Results in <ref type="table" target="#tab_15">Table 6</ref> show that increasing the number of layers improves network performance if residual or dense graph connections are used. Although ResMRGCN has a slight performance drop when the number of layers reaches 112, the m-F1 score is still much higher than the corresponding PlainMRGCN. The performance of DenseMRGCN increases reliably as the network grows deeper; however, DenseMRGCN consumes more memory than ResMRGCN due to concatenations of feature maps. Due to this memory issue, we are unable to train some models and denote them with '-' in <ref type="table" target="#tab_15">Table 6</ref>. Meanwhile, PlainMRGCN, which has no graph connections, only enjoys a slight performance gain as the network depth increases from 3 to 14. For depths beyond 14 layers, the performance drops significantly. Clearly, using graph connections improves performance, especially for deeper networks where it becomes essential. <ref type="figure">Fig. 9</ref>. Memory usage for different GCNs on PPI node classification. We keep the same parameters for different models and compare their m-F1 score with their total memory usage. All models in the comparison have 256 filters per layer and 56 layers. We notice that our model ResMRGCN uses approximately 1/6 of the total memory used by ResEdgeConv and gives a better m-F1 score.</p><p>Effect of network width. Results of each row in <ref type="table" target="#tab_15">Table 6</ref> show that increasing the number of filters can consistently increase performance. A higher number of filters can also help convergence for deeper networks. However, a large number of filters is very memory consuming. Hence, we only consider networks with up to 256 filters in our experiments.</p><p>Effect of GCN variants. <ref type="table">Table 7</ref> shows the effect of using different GCN operators with different model depths. Residual graph connections and the GCN operators are the only difference when the number of layers is kept the same. The results clearly show that residual graph connections in deep networks can help different GCN operators achieve better performance than PlainGCN. Interestingly, when the network grows deeper, the performance of PlainSemiGCN, PlainGAT, and PlainGIN decreases dramatically; meanwhile, PlainEdgeConv and PlainMRGCN only observe a relatively small performance drop. In comparison, our proposed MRGCN operator achieves the best performance among all models.</p><p>Memory usage of GCN variants. In <ref type="figure">Figure 9</ref>, we compare the total memory usage and performance of different GCN operators. All these models share the the same architecture except for the GCN operations. They all use residual graph connections with 56 layers and 256 filters. We implement all the models with PyTorch Geometric and train each using one NVIDIA Tesla V100. The GPU memory usage is measured when the memory usage is stable. Our proposed ResMRGCN achieves the best performance, while only using around 15% GPU memory compared to ResEdgeConv.</p><p>Comparison to state-of-the-art. Finally, we compare our DenseMRGCN-14 and ResMRGCN-28 to several state-of-theart baselines in <ref type="table" target="#tab_18">Table 8</ref>. Results clearly show the effectiveness of deeper models with residual and dense graph connections. DenseMRGCN-14 and ResMRGCN-28 outperform the previous state-of-the-art Cluster-GCN <ref type="bibr" target="#b68">[69]</ref> by 0.07% and 0.05% respectively. It is worth mentioning that a total of ten models in <ref type="table" target="#tab_15">Table 6</ref> surpass Cluster-GCN.  Ablation study on graph connections, network width, and network depth. The m-F1 score is used as the evaluation metric (in %). We find that residual and dense connections can help deep networks converge much better compared to the same model without any connections. Also, network width is positively correlated with network performance.</p><p>Note that '-' denotes models that are not applicable due to memory limitations. Bold highlights models that outperform all state-of-the-art baselines for this multi-label PPI node classification problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of layers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 7</head><p>Ablation study on network depth and GCN variants. m-F1 score is used as the evaluation metric (in %). We set the number of filters per layer in the backbone network to 256 and vary the number of layers. Residual graph connections can generally help different GCN operators achieve better performance than PlainGCN when the network grows deep. Bold highlights models that outperform all state-of-the-art baselines for this multi-label PPI node classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This work shows how proven concepts from CNNs (i.e. residual/dense connections and dilated convolutions) can be transferred to GCNs in order to make GCNs go as deep as CNNs. Adding skip connections and dilated convolutions to GCNs alleviates the training difficulty, which was impeding GCNs to go deeper and thus impeding further progress. We also encourage readers to read the recent work on benchmarking GNNs <ref type="bibr" target="#b72">[73]</ref> to further understand how different techniques can aid in training GNNs. A large number of experiments on semantic segmentation and part segmentation of 3D point clouds, as well as node classification on biological graphs show the benefit of deeper architectures, as they achieve state-of-the-art performance. We also show that our approach generalizes across several GCN operators. For the point cloud tasks, we achieve the best results using EdgeConv <ref type="bibr" target="#b73">[74]</ref> as GraphSAGE <ref type="bibr" target="#b41">[42]</ref> 61.20 GATConv <ref type="bibr" target="#b42">[43]</ref> 97.30 VR-GCN <ref type="bibr" target="#b69">[70]</ref> 97.80 GaAN <ref type="bibr" target="#b70">[71]</ref> 98.71 GeniePath <ref type="bibr" target="#b71">[72]</ref> 98.50 Cluster-GCN <ref type="bibr" target="#b68">[69]</ref> 99.36  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResMRGCN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">FUTURE WORK</head><p>Our results show that after solving the vanishing gradient problem plaguing deep GCNs, we can either make GCNs deeper or wider to get better performance. We expect GCNs to become a powerful tool for processing graph-structured data in computer vision, natural language processing, and data mining. We show successful cases for adapting concepts from CNNs to GCNs (i.e. skip connections and dilated convolutions). In the future, it will be worthwhile to explore how to transfer other operators (e.g. deformable convolutions <ref type="bibr" target="#b74">[75]</ref>), other architectures (e.g. feature pyramid architectures <ref type="bibr" target="#b75">[76]</ref>), etc.. It will also be interesting to study different distance measures to compute dilated k-NN, constructing graphs with different k at each layer, better dilation rate schedules <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b76">[77]</ref> for GCNs, and combining residual and dense connections. We also point out that, for the specific task of point cloud semantic segmentation, the common approach of processing the data in 1m ? 1m columns is sub-optimal for graph representation. A more suitable sampling approach should lead to further performance gains on this task. For the task of node classification, the existing datasets are relatively small. We expect that experimenting on larger datasets will further unleash the full potential of DeepGCNs. As evidence, a recent work <ref type="bibr" target="#b77">[78]</ref> shows that deep residual GNNs yield promising results on large-scale graph datasets <ref type="bibr" target="#b78">[79]</ref>. Ali Thabet is a Research Scientist at the Visual Computing Center (VCC) in King Abdullah University of Science and Technology (KAUST), working in the Image and Video Understanding Laboratory (IVUL). His research focuses on problems related to 3D computer vision. In general, he's interested in algorithmic applications of machine learning, and deep learning specifically, to understand the 3D world with the help of sensors like RGB-D cameras, LiDAR, and others. Also, Ali is interested in applying deep learning to image reconstruction, 3D object detection and generation, and autonomous vehicles.</p><p>Bernard Ghanem is currently an Associate Professor in the CEMSE division, a theme leader at the Visual Computing Center (VCC), and the Interim Lead of the AI Initiative at King Abdullah University of Science and Technology (KAUST). His research interests lie in computer vision and machine learning with emphasis on topics in video understanding, 3D recognition, and theoretical foundations of deep learning. He received his Bachelor's degree from the American University of Beirut (AUB) in 2005 and his MS/PhD from the University of Illinois at Urbana-Champaign (UIUC) in 2010. His work has received several awards and honors, including four Best Paper Awards for workshops in CVPR 2013&amp;2019 and ECCV 2018&amp;2020, a Google Faculty Research Award in 2015 (1st in MENA for Machine Perception), and a Abdul Hameed Shoman Arab Researchers Award for Big Data and Machine Learning in 2020. He has co-authored more than 120 peer reviewed conference and journal papers in his field as well as three issued patents. He serves as an Associate Editor for IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) and has served as Area Chair (AC) for CVPR 2018/2021, ICCV 2019/2021, ICLR 2021, and AAAI 2021. <ref type="bibr">Figures 10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14</ref> show qualitative results for DeepGCNs on S3DIS <ref type="bibr" target="#b12">[13]</ref> and <ref type="figure" target="#fig_0">Figure 15</ref> shows qualitative results for Deep-GCNs on PartNet <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">QUALITATIVE RESULTS FOR DEEPGCNS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">RUN-TIME OVERHEAD OF DYNAMIC K-NN</head><p>We conduct a run-time experiment comparing the inference time of the reference model ResGCN-28 (28 layers, k=16) with dynamic k-NN and fixed k-NN. The inference time with fixed k-NN is 45.63ms. Computing the dynamic k-NN increases the inference time by 150.88ms. It is possible to reduce computation by updating the k-NN less frequently (e.g. computing the dynamic k-NN every 3 layers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">COMPARISON WITH DGCNN OVER ALL CLASSES</head><p>To showcase the consistent improvement of our framework over the baseline DGCNN <ref type="bibr" target="#b5">[6]</ref>, we reproduce the results of DGCNN 1 in <ref type="table" target="#tab_21">Table 9</ref> and find our method outperforms DGCNN in all classes.   <ref type="table">Table  TrashCan</ref> Vase <ref type="figure" target="#fig_0">Fig. 15</ref>. Qualitative Results on PartNet Part Segmentation. We illustrate our performance compared to the ground truth on PartNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>arXiv:1910.06849v3 [cs.CV] 14 May 2021 Training DeepGCNs. (top) We show the square root of the training loss for GCNs with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Proposed GCN architecture for point cloud semantic segmentation. (left) Our framework consists of three blocks: a GCN Backbone Block (feature transformation of input point cloud), a Fusion Block (global feature generation and fusion), and an MLP Prediction Block (point-wise label prediction). (right) We study three types of GCN Backbone Block (PlainGCN, ResGCN and DenseGCN) and use two kinds of layer connection (vertex-wise addition used in ResGCN or vertex-wise concatenation used in DenseGCN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Dilated Convolution in GCNs. Visualization of dilated convolution on a structured graph arranged in a grid (e.g. 2D image) and on a general structured graph. (top) 2D convolution with kernel size 3 and dilation rate 1, 2, 4 (left to right). (bottom) Dynamic graph convolution with dilation rate 1, 2, 4 (left to right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>PlainGCN vs. ResGCN on area 5 of S3DIS. We compare networks of different depths with and without residual graph connections. All models were trained for 100 epochs on all areas except for area 5 with the same hyper-parameters. Only when residual graph connections are used do the results improve with increasing depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Qualitative Results on S3DIS Semantic Segmentation. We show here the effect of adding residual and dense graph connections to deep GCNs. PlainGCN-28, ResGCN-28, and DenseGCN-28 are identical except for the presence of residual graph connections in ResGCN-28 and dense graph connections in DenseGCN-28. We note how both residual and dense graph connections have a substantial effect on hard classes like board, bookcase, and sofa. These are lost in the results of PlainGCN-28. Qualitative Results on PartNet Part Segmentation. We show here the effect of adding residual connections to deep GCNs. PlainGCN-28 and ResGCN-28 are identical except for the presence of residual connections in ResGCN-28. We note how residual connections have a positive effect on part segmentation compared to PlainGCN-28. Many important parts of the objects are classified incorrectly using PlainGCN-28.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Model O.A. (%) C.A. (%) Param (M)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>No skip connections are used here. ResGCN. We construct ResGCN by adding dynamic dilated k-NN and residual graph connections to PlainGCN. The connections between all GCN layers in the GCN backbone block do not increase the number of parameters. DenseGCN. Similarly, DenseGCN is built by adding dynamic dilated k-NN and dense graph connections to the PlainGCN. As described in Section 3.2, dense graph connections are created by concatenating all the intermediate graph representations from previous layers. The dilation rate schedule of our DenseGCN is the same as for ResGCN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>NNs 35.00 37.50 40.00 42.50 45.00 47.50 50.00 52.50 55.00</figDesc><table><row><cell>reference</cell></row><row><cell>w/o stochastic</cell></row><row><cell>w/o dilation</cell></row><row><cell>w/o residual</cell></row><row><cell>1/2x NNs</cell></row><row><cell>1/4x NNs</cell></row><row><cell>1/2x layers</cell></row><row><cell>1/4x layers</cell></row><row><cell>1/2x filters</cell></row><row><cell>1/4x filters</cell></row><row><cell>2x layers, 1/2x NNs</cell></row><row><cell>2x filters, 1/2x</cell></row><row><cell>mIoU</cell></row></table><note>Effect of residual graph connections. Our experiments in Ta-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 (</head><label>1</label><figDesc>Depth) shows that increasing the number of layers improves network performance, but only if residual graph connections and dilated graph convolutions are used, as is clearly shown in Table 1 (Connections). of network width. Results in Table 1 (Width) show that increasing the number of filters leads to a similar increase in performance as increasing the number of layers. In general, a higher network capacity enables learning nuances necessary for succeeding in corner cases. GCN variants. Our experiments in Table 1 (GCN Variants) show the effect of using different GCN operators. The results clearly show that different deep GCN variants with residual graph connections and dilated graph convolutions converge better than PlainGCN. Using our proposed MRGCN operator achieves comparable performance to the ResGCN reference model, which relies on EdgeConv, while only using half the GPU memory. The GraphSAGE operator performs slightly worse and our results also</figDesc><table><row><cell>Effect</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 Analysis of over-smoothing using the Group Distance Ratio and</head><label>2</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1</head><label>1</label><figDesc>, our ResGCN-28 and DenseGCN-28 perform particularly well on difficult classes such as board, beam, bookcase and door. Rows 1-4 clearly show how ResGCN-28 and DenseGCN-28 are able to segment the board, beam, bookcase and door respectively, while PlainGCN-28 completely fails. Please refer to the supplementary material for more qualitative results and further results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>MethodOA mIOU ceiling floor wall beam column window door table chair sofa bookcase board clutter</figDesc><table><row><cell>PointNet [33]</cell><cell>78.5</cell><cell>47.6</cell><cell>88.0</cell><cell cols="3">88.7 69.3 42.4</cell><cell>23.1</cell><cell>47.5</cell><cell cols="3">51.6 54.1 42.0</cell><cell>9.6</cell><cell>38.2</cell><cell>29.4</cell><cell>35.2</cell></row><row><cell>MS+CU [35]</cell><cell>79.2</cell><cell>47.8</cell><cell>88.6</cell><cell cols="3">95.8 67.3 36.9</cell><cell>24.9</cell><cell>48.6</cell><cell cols="4">52.3 51.9 45.1 10.6</cell><cell>36.8</cell><cell>24.7</cell><cell>37.5</cell></row><row><cell>G+RCU [35]</cell><cell>81.1</cell><cell>49.7</cell><cell>90.3</cell><cell cols="3">92.1 67.9 44.7</cell><cell>24.2</cell><cell>52.3</cell><cell cols="3">51.2 58.1 47.4</cell><cell>6.9</cell><cell>39.0</cell><cell>30.0</cell><cell>41.9</cell></row><row><cell>PointNet++ [34]</cell><cell>-</cell><cell>53.2</cell><cell>90.2</cell><cell cols="3">91.7 73.1 42.7</cell><cell>21.2</cell><cell>49.7</cell><cell cols="4">42.3 62.7 59.0 19.6</cell><cell>45.8</cell><cell>48.2</cell><cell>45.6</cell></row><row><cell>3DRNN+CF [37]</cell><cell>86.9</cell><cell>56.3</cell><cell>92.9</cell><cell cols="3">93.8 73.1 42.5</cell><cell>25.9</cell><cell>47.6</cell><cell cols="4">59.2 60.4 66.7 24.8</cell><cell>57.0</cell><cell>36.7</cell><cell>51.6</cell></row><row><cell>DGCNN [6]</cell><cell>84.1</cell><cell>56.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ResGCN-28 (Ours) 85.9</cell><cell>60.0</cell><cell>93.1</cell><cell cols="3">95.3 78.2 33.9</cell><cell>37.4</cell><cell>56.1</cell><cell cols="4">68.2 64.9 61.0 34.6</cell><cell>51.5</cell><cell>51.1</cell><cell>54.4</cell></row><row><cell>Deep LPN [57]</cell><cell>85.7</cell><cell>60.0</cell><cell>91.0</cell><cell cols="3">95.6 76.1 50.3</cell><cell>25.9</cell><cell>55.1</cell><cell cols="4">56.8 66.3 74.3 25.8</cell><cell>54.0</cell><cell>52.3</cell><cell>55.3</cell></row><row><cell>ShellNet [58]</cell><cell>87.1</cell><cell>66.8</cell><cell>90.2</cell><cell cols="3">93.6 79.9 60.4</cell><cell>44.1</cell><cell>64.9</cell><cell cols="4">52.9 71.6 84.7 53.8</cell><cell>64.6</cell><cell>48.6</cell><cell>59.4</cell></row><row><cell>RandLA-Net [54]</cell><cell>88.0</cell><cell>70.0</cell><cell>93.1</cell><cell cols="3">96.1 80.6 62.4</cell><cell>48.0</cell><cell>64.4</cell><cell cols="4">69.4 69.4 76.4 60.0</cell><cell>64.2</cell><cell>65.9</cell><cell>60.1</cell></row><row><cell>KPConv [55]</cell><cell>-</cell><cell>70.6</cell><cell>93.6</cell><cell cols="3">92.4 83.1 63.9</cell><cell>54.3</cell><cell>66.1</cell><cell cols="4">76.6 64.0 57.8 74.9</cell><cell>69.3</cell><cell>61.3</cell><cell>60.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 3 Comparison of ResGCN</head><label>3</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Avg bed bottle chair clock dishw. disp. door earph. fauc. knife lamp micro. fridge st. furn. table tr. can vase</figDesc><table><row><cell>PointNet [33]</cell><cell>35.6 13.4 29.5 27.8 28.4 48.9 76.5 30.4 33.4 47.6 32.9 18.9 37.2</cell><cell>33.5</cell><cell>38.0</cell><cell>29.0 34.8 44.4</cell></row><row><cell>PointNet++ [34]</cell><cell>42.5 30.3 41.4 39.2 41.6 50.1 80.7 32.6 38.4 52.4 34.1 25.3 48.5</cell><cell>36.4</cell><cell>40.5</cell><cell>33.9 46.7 49.8</cell></row><row><cell>SpiderCNN [59]</cell><cell>37.0 36.2 32.2 30.0 24.8 50.0 80.1 30.5 37.2 44.1 22.2 19.6 43.9</cell><cell>39.1</cell><cell>44.6</cell><cell>20.1 42.4 32.4</cell></row><row><cell>PointCNN [60]</cell><cell>46.5 41.9 41.8 43.9 36.3 58.7 82.5 37.8 48.9 60.5 34.1 20.1 58.2</cell><cell>42.9</cell><cell>49.4</cell><cell>21.3 53.1 58.9</cell></row><row><cell cols="2">PlainGCN-28 (Ours) 31.3 20.1 29.4 24.6 21.4 38.2 73.2 24.2 36.4 42.2 32.5 16.7 35.6</cell><cell>29.8</cell><cell>30.1</cell><cell>15.1 33.8 28.2</cell></row><row><cell cols="2">ResGCN-28 (Ours) 45.1 35.9 49.3 41.1 33.8 56.2 81.0 31.1 45.8 52.8 44.5 23.1 51.8</cell><cell>34.9</cell><cell>47.2</cell><cell>33.6 50.8 54.2</cell></row><row><cell>Deep LPN [57]</cell><cell>38.6 -</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>ResGCN-28 is a general framework for any graph (e.g. point clouds, biological networks, citation networks, etc.). Despite this huge disadvantage, ResGCN-28 still achieves results on par with PointCNN [60] and even outperforms PointCNN on some categories including bottle, knife, lamp, and table. We also include the most recent SOTA results from Deep LPN [57] and PosPool</figDesc><table><row><cell>ResGCN-28 outperforms PlainGCN-</cell></row><row><cell>28 by a large margin, which illustrates how our proposed residual</cell></row><row><cell>connections and stochastic dilated convolutions enable training</cell></row><row><cell>deep GCN architectures. ResGCN-28 also substantially outper-</cell></row><row><cell>forms PointNet [33], PointNet++ [34], and SpiderCNN [59]. Note</cell></row><row><cell>that PointCNN designs a point convolution specialized for point</cell></row><row><cell>clouds, whereas</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 5 Comparison of our GCN variants with the state-of-the-art on ModelNet40 point cloud classification.</head><label>5</label><figDesc>We report the overall classification test accuracy (O.A.) and mean per-class accuracy (C.A.)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>97.35 98.69 99.22 99.38 PlainMRGCN-14 97.55 99.02 99.31 99.34 PlainMRGCN-28 98.09 99.00 99.02 99.31 PlainMRGCN-56 92.70 97.43 97.31 97.61 PlainMRGCN-112 60.75 71.97 89.69 91.50 ResMRGCN-3 96.04 97.60 98.53 99.09 ResMRGCN-7 97.00 98.43 99.19 99.30</figDesc><table><row><cell>Number of filters</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>256</cell></row><row><cell>PlainMRGCN-3</cell><cell cols="4">95.84 97.60 98.58 99.13</cell></row><row><cell>PlainMRGCN-7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResMRGCN-14</cell><cell cols="4">97.75 98.88 99.26 99.38</cell></row><row><cell>ResMRGCN-28</cell><cell cols="4">98.50 99.16 99.29 99.41</cell></row><row><cell>ResMRGCN-56</cell><cell cols="4">98.62 99.27 99.36 99.40</cell></row><row><cell>ResMRGCN-112</cell><cell cols="4">98.41 99.34 99.38 99.39</cell></row><row><cell>DenseMRGCN-3</cell><cell cols="4">95.96 97.85 98.66 99.11</cell></row><row><cell>DenseMRGCN-7</cell><cell cols="4">97.87 98.47 99.31 99.36</cell></row><row><cell cols="5">DenseMRGCN-14 98.93 99.00 99.01 99.43</cell></row><row><cell cols="4">DenseMRGCN-28 99.16 99.29 99.42</cell><cell>-</cell></row><row><cell cols="2">DenseMRGCN-56 99.22</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 6</head><label>6</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>PlainSemiGCN 97.82 90.40 80.55 41.00 50.75 ResSemiGCN 97.88 95.05 93.50 90.60 90.54 PlainGAT 98.52 80.92 56.88 42.40 48.95 ResGAT 98.63 97.86 98.99 99.06 63.25 PlainGIN 97.86 57.78 40.79 35.82 0.26 ResGIN 97.80 96.44 98.22 97.44 97.18 PlainEdgeConv 99.16 99.27 99.30 99.33 98.99 ResEdgeConv 99.03 99.19 99.26 99.30 99.04 PlainMRGCN 99.13 99.38 99.34 99.31 97.61 ResMRGCN 99.09 99.30 99.38 99.41 99.40</figDesc><table><row><cell>3</cell><cell>7</cell><cell>14</cell><cell>28</cell><cell>56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 8 Comparison of DenseMRGCN-14 with state-of-the-art on PPI node classification.</head><label>8</label><figDesc>We follow convention and compare models based on the m-F1 score. Our model DenseMRGCN-14, which has 14 MRGCN layers, 256 filters in the first layer, and dense graph connections outperforms all baselines. Our ResMRGCN-28, which has 28 MRGCN layers, 256 filters per layer, and residual graph connections also outperforms previous state-of-the-art.GCN operators for our backbone networks. Moreover, we find that dilated graph convolutions help to gain a larger receptive field without loss of resolution. Even with a small number of nearest neighbors, DeepGCNs can achieve high performance on point cloud semantic segmentation. ResGCN-112 and ResGCN-56 perform very well on this task, although they only use 4 and 8 nearest neighbors respectively compared to 16 for ResGCN-28. For the biological graph task, we achieve the best results using the MRGCN operator, which we propose as a novel memory-efficient alternative to EdgeConv. We successfully trained ResMRGCN-112 and DenseMRGCN-56; both networks converged very well and achieved very promising results on the PPI dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Guohao Li obtained his BEng degree in Communication Engineering from Harbin Institute of Technology in 2015. In 2018, he received his Master Degree in Communication and Information Systems from Chinese Academy of Science. He was a research intern at SenseTime and Intel ISL. He is currently a CS PhD student at King Abdullah University of Science and Technology. His primary research interests are Computer Vision, Robotics and Deep Learning. Matthias M?ller received his PhD in computer vision from KAUST in 2019. He now works as a research scientist at the Intelligent Systems Lab at Intel. His research interests lie in the fields of computer vision, robotics and machine learning where he has contributed to more than 10 publications in top tier conferences and journals. Matthias was recognized as an outstanding reviewer for CVPR'18 and won the best paper award at the ECCV'18 workshop UAVision. Qian received the BEng degree with first class honors from Xi'an Jiaotong University, China in 2018. He is working towards the MSc degree currently in the Department of Computer Science at King Abdullah University of Science and Technology. His research interests include computer vision, computational photography and neural architecture search. Itzel C. Delgadillo received the Bachelor in Artificial Intelligence from the Panamerican University, Mexico in 2019. She is currently a Visiting Student Research Intern in King Abdullah University of Science and Technology. Her research interests include computer vision and neural architecture search.</figDesc><table><row><cell>gree in computer science from Rutgers, The State University of New Jersey in 2018. He was a recipient of the KAUST Gifted Student Program (KGSP) scholarship award. He is currently an MS student and a member of the Image and Video Understanding Lab (IVUL) at the Visual Computing Center (VCC) at King Abdullah Uni-versity of Science and Technology (KAUST). His research interests include computer vision and Guocheng Abdulellah Abualshour received the BS de-deep learning.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 9 Comparison of ResGCN-28 with DGCNN. AveragePlainGCN-28 Bottle Ground Truth ResGCN-28 PlainGCN-28 Microwave Ground Truth ResGCN-28 PlainGCN-28 Chair Bed Refrigerator Clock Dishwasher Display Door Earphone Faucet Knife Lamp Storage Furniture</head><label>9</label><figDesc>per-class results across all areas for our reference network with 28 layers, residual graph connections and dilated graph convolutions compared to DGCNN baseline. ResGCN-28 outperforms DGCNN across all the classes. Metric shown is IoU. TPAMI, SPECIAL ISSUE ON GRAPHS IN VISION AND PATTERN ANALYSIS. TPAMI, SPECIAL ISSUE ON GRAPHS IN VISION AND PATTERN ANALYSIS.</figDesc><table><row><cell>Ground Truth</cell><cell>ResGCN-28</cell></row></table><note>1. The results across all classes were not provided in the DGCNN paper. Fig. 10. Qualitative Results for S3DIS Semantic Segmentation. We show the importance of stochastic dilated convolutions.IEEE19 Fig. 11. Qualitative Results for S3DIS Semantic Segmentation. We show the importance of the number of nearest neighbors used in the convolutions. Fig. 12. Qualitative Results for S3DIS Semantic Segmentation. We show the importance of network depth (number of layers).IEEE21 Fig. 13. Qualitative Results for S3DIS Semantic Segmentation. We show the importance of network width (number of filters per layer). Fig. 14. Qualitative Results for S3DIS Semantic Segmentation. We show the benefit of a wider and deeper network even with only half the number of nearest neighbors.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors thank Adel Bibi and Hani Itani for their help with the project. This work was supported by the King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research through the Visual Computing Center (VCC) funding. We also thank the editors and reviewers for their constructive suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Relational learning via latent social dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="817" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3697" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07829</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Joint 2D-3D-Semantic Data for Indoor Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PartNet: A large-scale benchmark for fine-grained and hierarchical partlevel 3D object understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1957" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="670" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Factorizable net: an efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="335" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Snapnet-r: Consistent 3d multi-view semantic labeling for robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>ICCVW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="669" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unstructured point cloud semantic labeling using deep segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lstm-cf: Unifying context modeling and fusion with lstms for rgb-d scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="541" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring spatial context for 3d semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, 3DRMS Workshop, ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2626" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3d recurrent neural networks with context fusion for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="415" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Column networks for collective classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Semi-supervised user geolocation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08049</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Crosssentence n-ary relation extraction with graph lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning localized generative models for 3d point clouds via graph convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Magli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A real-time algorithm for signal analysis with the help of the wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kronland-Martinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tchamitchian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wavelets</title>
		<imprint>
			<biblScope unit="page" from="286" to="297" />
			<date type="published" when="1990" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The discrete wavelet transform: wedding the a trous and mallat algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Shensa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2464" to="2482" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of largescale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6410" to="6419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks with differentiable group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Going deeper with lean point networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9500" to="9509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11527</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A closer look at local aggregation operators in point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">3dmfv: Threedimensional point cloud classification in real-time using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3145" to="3152" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
		<idno>abs/1803.05827</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Monte carlo convolution for learning on non-uniformly sampled point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-P</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Flex-convolutionmillion-scale point-cloud learning beyond grid-worlds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P A</forename><surname>Lensch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Clustergcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07953</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10568</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07294</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Geniepath: Graph neural networks with adaptive receptive paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4424" to="4431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Benchmarking graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international</title>
		<meeting>the IEEE international</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
