<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mask2Former for Video Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign (UIUC)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choudhuri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign (UIUC)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">A G</forename><surname>Schwing</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign (UIUC)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mask2Former for Video Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We find Mask2Former [4] also achieves state-of-the-art performance on video instance segmentation without modifying the architecture, the loss or even the training pipeline. In this report, we show universal image segmentation architectures trivially generalize to video segmentation by directly predicting 3D segmentation volumes. Specifically, Mask2Former sets a new state-of-the-art of 60.4 AP on YouTubeVIS-2019 and 52.6 AP on YouTubeVIS-2021. We believe Mask2Former is also capable of handling video semantic and panoptic segmentation, given its versatility in image segmentation. We hope this will make state-of-theart video segmentation research more accessible and bring more attention to designing universal image and video segmentation architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video instance segmentation <ref type="bibr" target="#b17">[18]</ref> differs from image segmentation in its goal to simultaneously segment and track objects in videos. Initial methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref> add "track embeddings" to per-pixel image segmentation architectures to track objects across time. Recent transformer-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> use "object queries" <ref type="bibr" target="#b1">[2]</ref> to connect objects across frames. However, all these methods are specifically designed to only process video data, disconnecting image and video segmentation research.</p><p>Universal image segmentation methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> show that mask classification is able to address common image segmentation tasks (i.e. panoptic, instance and semantic) using the same architecture, and achieve state-of-the-art results. A natural question emerges: does this universality extend to videos? In this report, we find Mask2Former <ref type="bibr" target="#b3">[4]</ref> achieves state-of-the-art results on video instance segmentation as well without modifying the architecture, the loss or even the training pipeline. To achieve this, we let Mask2Former directly attend to the 3D spatio-temporal features and pre-Work done while Bowen Cheng (bcheng9@illinois.edu) at FAIR. ? Equal advising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel Decoder</head><p>Transformer Decoder </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Video instance segmentation (VIS) <ref type="bibr" target="#b17">[18]</ref> requires tracking instances across time. In general, there are two ways to extend image segmentation models for the VIS task: Per-frame methods (a.k.a. online methods) treat a video clip as a sequence of frames. They run image segmentation models on each frame independently and associate predicted instance masks across frames with a post-processing step. MaskTrack R-CNN <ref type="bibr" target="#b17">[18]</ref>, TrackR-CNN <ref type="bibr" target="#b13">[14]</ref> and VP-SNet <ref type="bibr" target="#b8">[9]</ref> add per-instance track embedding prediction to Mask R-CNN <ref type="bibr" target="#b5">[6]</ref>. VIP-DeepLab <ref type="bibr" target="#b12">[13]</ref> extends Panoptic-DeepLab <ref type="bibr" target="#b2">[3]</ref> by predicting instance center offsets across frames. However, it is not always straightforward to modify an image segmentation model, as architectural changes and additional losses are needed. Per-clip methods (a.k.a. offline methods) treat a video clip as a 3D spatio-temporal volume and directly predict the 3D mask for each instance. STEm-Seg <ref type="bibr" target="#b0">[1]</ref> predicts and clusters spatio-temporal instance embeddings to generate 3D masks. More recently, inspired by the success of DETR <ref type="bibr" target="#b1">[2]</ref>, VisTR <ref type="bibr" target="#b14">[15]</ref>, IFC <ref type="bibr" target="#b7">[8]</ref> and SeqFormer <ref type="bibr" target="#b15">[16]</ref> design Transformer-based architectures to process the 3D volume via cross-attention. However, all these models are designed specifically for video instance segmentation, disconnecting image and video segmentation research. In this report, we show Mask2Former <ref type="bibr" target="#b3">[4]</ref>, a state-of-the-art universal image segmentation model, can also achieve state-of-the-art video instance segmentation without modifying the architecture, the loss or even the training pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Mask2Former for Videos</head><p>We treat a video sequence as a 3D spatio-temporal volume of dimension T ? H ? W , where T is the number of frames and H, W are height and width respectively. We make three changes to adapt Mask2Former to video segmentation: 1) we apply the masked attention to the spatiotemporal volume; 2) we add an extra positional encoding for the temporal dimension; and 3) we directly predict a 3D volume of an instance across time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Joint spatio-temporal masked attention</head><p>We apply masked attention to the 3D spatio-temporal features, i.e., we use</p><formula xml:id="formula_0">X l = softmax(M l?1 + Q l K T l )V l + X l?1 .<label>(1)</label></formula><p>Here, l is the layer index, X l ? R N ?C refers to N C-dimensional query features at the l th layer and Q l = f Q (X l?1 ) ? R N ?C . X 0 denotes input query features of the Transformer decoder. K l , V l ? R T H l W l ?C are the spatiotemporal features under transformation f K (?) and f V (?) respectively, T is the number of frames and H l and W l are the spatial resolution. f Q , f K and f V are linear functions. Moreover, the 3D attention mask M l?1 at feature location (t, x, y) is</p><formula xml:id="formula_1">M l?1 (t, x, y) = 0 if M l?1 (t, x, y) = 1 ?? otherwise .<label>(2)</label></formula><p>Here, M l?1 ? {0, 1} N ?T H l W l is the binarized output (thresholded at 0.5) of the resized 3D mask prediction of the previous (l ? 1)-th Transformer decoder layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal positional encoding</head><p>To obtain compatibility with image segmentation models we decouple the temporal positional encoding from the spatial encoding, i.e., we use the positional encoding e pos = e pos-t ? e pos-xy .</p><p>(</p><p>Here, e pos ? R T ?H l ?W l ?C is the final positional encoding, e pos-t ? R T ?1?1?C and e pos-xy ? R 1?H l ?W l ?C are the corresponding temporal and spatial positional encodings. Both e pos-t and e pos-xy are non-parametric sinusoidal positional encodings that can handle arbitrary length. ? denotes addition with numpy-style broadcasting.  <ref type="table">Table 2</ref>. YouTubeVIS-2021 val. Mask2Former outperforms all state-of-the-art models without using image data for augmentation. V: YouTubeVIS-2021 train set. C80k: 80k COCO train2017 images that contain YouTubeVIS categories. For Swin-L models, we evaluate with a lower resolution (shorter side to 360 pixels) due to memory constraints. We report the median of 5 runs together with the standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Joint spatio-temporal mask prediction</head><p>Similarly to the masked attention, we obtain the 3D mask of the n th query via a simple dot product, i.e., M(n, t, h, w) = sigmoid(E mask (:, n) T ? E pixel (:, t, h, w)).</p><p>Note, computation of the classification does not change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluation Mask2Former on YouTubeVIS-2019 and YouTubeVIS-2021 <ref type="bibr" target="#b17">[18]</ref>. We do not modify the architecture, the loss or even the training pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training</head><p>We use Detectron2 <ref type="bibr" target="#b16">[17]</ref> and follow the IFC <ref type="bibr" target="#b7">[8]</ref> settings for video instance segmentation. More specifically, we use the AdamW <ref type="bibr" target="#b11">[12]</ref> optimizer and the step learning rate schedule. We use an initial learning rate of 0.0001 and a weight decay of 0.05 for all backbones. A learning rate multiplier of 0.1 is applied to the backbone and we decay the learning rate at 2/3 fractions of the total number of training steps by a factor of 10. We train our models for 6k iterations with a batch size of 16 for YouTubeVIS-2019 and 8k iterations for YouTubeVIS-2021. During training, each video clip is composed of T = 2 frames, which makes the training much more efficient (2 hours per training with 8 V100 GPUs), and the shorter spatial side is resized to either 360 or 480. All models are initialized with COCO <ref type="bibr" target="#b9">[10]</ref> instance segmentation models from <ref type="bibr" target="#b3">[4]</ref>. Unlike <ref type="bibr" target="#b15">[16]</ref>, we only use YouTubeVIS training data and do not use COCO images for data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Inference</head><p>Our model is able to handle video sequences of various length. During inference, we provide the whole video sequence as input to the model and obtain 3D mask predictions without any post-processing. We keep the top 10 predictions for each video sequence. If not stated otherwise, we resize the shorter side to 360 pixels during inference for ResNet <ref type="bibr" target="#b6">[7]</ref> backbones and to 480 pixels for Swin Transformer <ref type="bibr" target="#b10">[11]</ref> backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>We compare Mask2Former with state-of-the-art models on the YouTubeVIS-2019 dataset in <ref type="table" target="#tab_0">Table 1</ref> and the YouTubeVIS-2021 dataset in <ref type="table">Table 2</ref>. Using the exact same training parameters, Mask2Former outperforms IFC <ref type="bibr" target="#b7">[8]</ref> by more than 6 AP. Mask2Former also outperforms concurrent SeqFormer <ref type="bibr" target="#b15">[16]</ref> without using extra COCO images for data augmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Mask2Former trivially generalizes to videos. For singleframe input data, it operates as a standard image segmentation architecture. For &gt; 1 frames, due to sharing of the queries across frames, it segments and tracks object instances across frames. dict a 3D volume to track each object instance across time(Fig. 1). This simple change results in state-of-the-art performance of 60.4 AP on the challenging YouTubeVIS-2019 data and 52.6 AP on YouTubeVIS-2021.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>YouTubeVIS-2019 val. Mask2Former outperforms all state-of-the-art models without using image data for augmentation. V: YouTubeVIS-2019 train set. C80k: 80k COCO train2017 images that contain YouTubeVIS categories. We report the median of 5 runs together with the standard deviation.</figDesc><table><row><cell></cell><cell>method</cell><cell cols="2">backbone data</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell></row><row><cell></cell><cell>VisTR [15]</cell><cell>R50 R101</cell><cell>V V</cell><cell>36.2 40.1</cell><cell>59.8 45.0</cell><cell>36.9 38.3</cell></row><row><cell>CNN</cell><cell>IFC [8]</cell><cell>R50 R101 R50</cell><cell>V V V</cell><cell>41.2 42.6 45.1</cell><cell>65.1 66.6 66.9</cell><cell>44.6 46.3 50.5</cell></row><row><cell></cell><cell>SeqFormer [16]</cell><cell>R50</cell><cell cols="2">V + C80k 47.4</cell><cell>69.8</cell><cell>51.8</cell></row><row><cell></cell><cell></cell><cell>R101</cell><cell cols="2">V + C80k 49.0</cell><cell>71.1</cell><cell>55.7</cell></row><row><cell></cell><cell>Mask2Former</cell><cell>R50 R101</cell><cell>V V</cell><cell>46.4 ?0.8 49.2 ?0.7</cell><cell>68.0 72.8</cell><cell>50.0 54.2</cell></row><row><cell></cell><cell cols="2">SeqFormer [16] Swin-L</cell><cell cols="2">V + C80k 59.3</cell><cell>82.1</cell><cell>66.4</cell></row><row><cell>Transformer</cell><cell>Mask2Former</cell><cell>Swin-T Swin-S Swin-B Swin-L</cell><cell>V V V V</cell><cell>51.5 ?0.7 54.3 ?0.7 59.5 ?0.7 60.4 ?0.5</cell><cell>75.0 79.0 84.3 84.4</cell><cell>56.5 58.8 67.2 67.0</cell></row><row><cell></cell><cell>best of 5 runs</cell><cell>Swin-L</cell><cell>V</cell><cell>60.7</cell><cell>84.4</cell><cell>66.7</cell></row><row><cell></cell><cell>method</cell><cell cols="2">backbone data</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell></row><row><cell></cell><cell>IFC [8]</cell><cell>R50</cell><cell>V</cell><cell>36.6</cell><cell>57.9</cell><cell>39.3</cell></row><row><cell>CNN</cell><cell cols="2">SeqFormer [16] R50 R50 Mask2Former R101</cell><cell cols="2">V + C80k 40.5 V 40.6 ?0.7 V 42.4 ?0.6</cell><cell>62.4 60.9 65.9</cell><cell>43.7 41.8 45.8</cell></row><row><cell></cell><cell cols="2">SeqFormer [16] Swin-L</cell><cell cols="2">V + C80k 51.8</cell><cell>74.6</cell><cell>58.2</cell></row><row><cell>Transformer</cell><cell>Mask2Former</cell><cell>Swin-T Swin-S Swin-B Swin-L</cell><cell>V V V V</cell><cell>45.9 ?0.6 48.6 ?0.4 52.0 ?0.6 52.6 ?0.7</cell><cell>68.7 77.2 76.5 76.4</cell><cell>50.7 52.0 54.2 57.2</cell></row><row><cell></cell><cell>best of 5 runs</cell><cell>Swin-L</cell><cell>V</cell><cell>53.0</cell><cell>75.9</cell><cell>58.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: B. Cheng, A. Choudhuri and A. Schwing were supported in part by NSF grants #1718221, 2008387, 2045586, 2106825, MRI #1725729, NIFA award 2020-67021-32799 and Cisco Systems Inc. (Gift Award CG 1377144 -thanks for access to Arcetri).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stem-seg: Spatio-temporal embeddings for instance segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabarinath</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Panoptic-DeepLab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Masked-attention mask transformer for universal image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01527</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Video instance segmentation using inter-frame communication transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukjun</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miran</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Video panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vip-deeplab: Learning visual perception with depth-aware video panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mots: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berin</forename><surname>Balachandar Gnana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Seqformer: a frustratingly simple model for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08275</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
