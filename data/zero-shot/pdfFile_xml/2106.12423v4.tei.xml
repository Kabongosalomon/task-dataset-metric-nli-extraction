<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Alias-Free Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>H?rk?nen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Aalto University and NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">NVIDIA and Aalto University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Alias-Free Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The resolution and quality of images produced by generative adversarial networks (GAN) <ref type="bibr" target="#b20">[21]</ref> have seen rapid improvement recently <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. They have been used for a variety of applications, including image editing <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b2">3]</ref>, domain translation <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b41">42]</ref>, and video generation <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24]</ref>. While several ways of controlling the generative process have been found <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b5">6]</ref>, the foundations of the synthesis process remain only partially understood.</p><p>In the real world, details of different scale tend to transform hierarchically. For instance, moving a head causes the nose to move, which in turn moves the skin pores on it. The structure of a typical GAN generator is analogous: coarse, low-resolution features are hierarchically refined by upsampling layers, locally mixed by convolutions, and new detail is introduced through nonlinearities. We observe that despite this superficial similarity, current GAN architectures do not synthesize images in a natural hierarchical manner: the coarse features mainly control the presence of finer features, but not their precise positions. Instead, much of the fine detail appears to be fixed in pixel coordinates. This disturbing "texture sticking" is clearly visible in latent interpolations (see <ref type="figure">Figure 1</ref> and our accompanying videos on the project page https://nvlabs.github.io/stylegan3), breaking the illusion of a solid and coherent object moving in space. Our goal is an architecture that exhibits a more natural transformation hierarchy, where the exact sub-pixel position of each feature is exclusively inherited from the underlying coarse features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StyleGAN2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours StyleGAN2</head><p>? latent interpolation ? Ours ? latent interpolation ? Averaged Central <ref type="figure">Figure 1</ref>: Examples of "texture sticking". Left: The average of images generated from a small neighborhood around a central latent (top row). The intended result is uniformly blurry because all details should move together. However, with StyleGAN2 many details (e.g., fur) stick to the same pixel coordinates, showing unwanted sharpness. Right: From a latent space interpolation (top row), we extract a short vertical segment of pixels from each generated image and stack them horizontally (bottom). The desired result is hairs moving in animation, creating a time-varying field. With StyleGAN2 the hairs mostly stick to the same coordinates, creating horizontal streaks instead.</p><p>It turns out that current networks can partially bypass the ideal hierarchical construction by drawing on unintentional positional references available to the intermediate layers through image borders <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b65">66]</ref>, per-pixel noise inputs <ref type="bibr" target="#b32">[33]</ref> and positional encodings, and aliasing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b68">69]</ref>. Aliasing, despite being a subtle and critical issue <ref type="bibr" target="#b43">[44]</ref>, has received little attention in the GAN literature. We identify two sources for it: 1) faint after-images of the pixel grid resulting from non-ideal upsampling filters 2 such as nearest, bilinear, or strided convolutions, and 2) the pointwise application of nonlinearities such as ReLU <ref type="bibr" target="#b59">[60]</ref> or swish <ref type="bibr" target="#b46">[47]</ref>. We find that the network has the means and motivation to amplify even the slightest amount of aliasing and combining it over multiple scales allows it to build a basis for texture motifs that are fixed in screen coordinates. This holds for all filters commonly used in deep learning <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b58">59]</ref>, and even high-quality filters used in image processing.</p><p>How, then, do we eliminate the unwanted side information and thereby stop the network from using it? While borders can be solved by simply operating on slightly larger images, aliasing is much harder. We begin by noting that aliasing is most naturally treated in the classical Shannon-Nyquist signal processing framework, and switch focus to bandlimited functions on a continuous domain that are merely represented by discrete sample grids. Now, successful elimination of all sources of positional references means that details can be generated equally well regardless of pixel coordinates, which in turn is equivalent to enforcing continuous equivariance to sub-pixel translation (and optionally rotation) in all layers. To achieve this, we describe a comprehensive overhaul of all signal processing aspects of the StyleGAN2 generator <ref type="bibr" target="#b33">[34]</ref>. Our contributions include the surprising finding that current upsampling filters are simply not aggressive enough in suppressing aliasing, and that extremely high-quality filters with over 100dB attenuation are required. Further, we present a principled solution to aliasing caused by pointwise nonlinearities <ref type="bibr" target="#b4">[5]</ref> by considering their effect in the continuous domain and appropriately low-pass filtering the results. We also show that after the overhaul, a model based on 1?1 convolutions yields a strong, rotation equivariant generator.</p><p>Once aliasing is adequately suppressed to force the model to implement more natural hierarchical refinement, its mode of operation changes drastically: the emergent internal representations now include coordinate systems that allow details to be correctly attached to the underlying surfaces. This promises significant improvements to models that generate video and animation. The new StyleGAN3 generator matches StyleGAN2 in terms of FID <ref type="bibr" target="#b25">[26]</ref>, while being slightly heavier computationally. Our implementation and pre-trained models are available at https://github.com/NVlabs/stylegan3</p><p>Several recent works have studied the lack of translation equivariance in CNNs, mainly in the context of classification <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b58">59]</ref>. We significantly expand upon the antialiasing measures in this literature and show that doing so induces a fundamentally altered image generation behavior. Group-equivariant CNNs aim to generalize the efficiency benefits of translational weight sharing to, e.g., rotation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b61">62]</ref> and scale <ref type="bibr" target="#b63">[64]</ref>. Our 1?1 convolutions can be seen an instance of a continuously E(2)-equivariant model <ref type="bibr" target="#b61">[62]</ref> that remains compatible with, e.g., channel-wise ReLU nonlinearities and modulation. Dey et al. <ref type="bibr" target="#b16">[17]</ref> apply 90 ? rotation-and-flip equivariant CNNs <ref type="bibr" target="#b15">[16]</ref> to GANs and show improved data efficiency. Our work is complementary, and not motivated by efficiency. Recent implicit network <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b12">13]</ref> based GANs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b53">54]</ref> generate each pixel independently via similar 1?1 convolutions. While equivariant, these models do not help with texture sticking, as they do not use an upsampling hierarchy or implement a shallow non-antialiased one. on a discrete feature map: Z = F(Z). The feature map has a corresponding continuous counterpart, so we also have a corresponding mapping in the continuous domain: z = f (z). Now, an operation specified in one domain can be seen to perform a corresponding operation in the other domain:</p><formula xml:id="formula_0">f (z) = ? s * F(X s z), F(Z) = X s f (? s * Z),<label>(1)</label></formula><p>where denotes pointwise multiplication and s and s are the input and output sampling rates. Note that in the latter case f must not introduce frequency content beyond the output bandlimit s /2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Equivariant network layers</head><p>Operation f is equivariant with respect to a spatial transformation t of the 2D plane if it commutes with it in the continuous domain: t ? f = f ? t. We note that when inputs are bandlimited to s/2, an equivariant operation must not generate frequency content above the output bandlimit of s /2, as otherwise no faithful discrete output representation exists.</p><p>We focus on two types of equivariance in this paper: translation and rotation. In the case of rotation the spectral constraint is somewhat stricter -rotating an image corresponds to rotating the spectrum, and in order to guarantee the bandlimit in both horizontal and vertical direction, the spectrum must be limited to a disc with radius s/2. This applies to both the initial network input as well as the bandlimiting filters used for downsampling, as will be described later.</p><p>We now consider the primitive operations in a typical generator network: convolution, upsampling, downsampling, and nonlinearity. Without loss of generality, we discuss the operations acting on a single feature map: pointwise linear combination of features has no effect on the analysis.</p><p>Convolution Consider a standard convolution with a discrete kernel K. We can interpret K as living in the same grid as the input feature map, with sampling rate s. The discrete-domain operation is simply F conv (Z) = K * Z, and we obtain the corresponding continuous operation from Eq. 1:</p><formula xml:id="formula_1">f conv (z) = ? s * K * (X s z) = K * ? s * (X s z) = K * z<label>(2)</label></formula><p>due to commutativity of convolution and the fact that discretization followed by convolution with ideal low-pass filter, both with same sampling rate s, is an identity operation, i.e., ? s * (X s z) = z.</p><p>In other words, the convolution operates by continuously sliding the discretized kernel over the continuous representation of the feature map. This convolution introduces no new frequencies, so the bandlimit requirements for both translation and rotation equivariance are trivially fulfilled.</p><p>Convolution also commutes with translation in the continuous domain, and thus the operation is equivariant to translation. For rotation equivariance, the discrete kernel K needs to be radially symmetric. We later show in Section 3.2 that trivially symmetric 1?1 convolution kernels are, despite their simplicity, a viable choice for rotation equivariant generative networks.</p><p>Upsampling and downsampling Ideal upsampling does not modify the continuous representation. Its only purpose is to increase the output sampling rate (s &gt; s) to add headroom in the spectrum where subsequent layers may introduce additional content. Translation and rotation equivariance follow directly from upsampling being an identity operation in the continuous domain. With f up (z) = z, the discrete operation according to Eq. 1 is F up (Z) = X s (? s * Z). If we choose s = ns with integer n, this operation can be implemented by first interleaving Z with zeros to increase its sampling rate and then convolving it with a discretized filter X s ? s .</p><p>In downsampling, we must low-pass filter z to remove frequencies above the output bandlimit, so that the signal can be represented faithfully in the coarser discretization. The operation in continuous domain is f down (z) = ? s * z, where an ideal low-pass filter ? s := s 2 ? ? s is simply the corresponding interpolation filter normalized to unit mass. The discrete counterpart is F down (Z) = X s ? s * (? s * Z) = 1/s 2 ? X s (? s * ? s * Z) = (s /s) 2 ? X s (? s * Z). The latter equality follows from ? s * ? s = ? min(s,s ) . Similar to upsampling, downsampling by an integer fraction can be implemented with a discrete convolution followed by dropping sample points. Translation equivariance follows automatically from the commutativity of f down (z) with translation, but for rotation equivariance we must replace ? s with a radially symmetric filter with disc-shaped frequency response. The ideal such filter <ref type="bibr" target="#b8">[9]</ref> is given by ? ? s (x) = jinc(s x ) = 2J 1 (?s x )/(?s x ), where J 1 is the first order Bessel function of the first kind.  <ref type="figure">Figure 3</ref>: Results for FFHQ-U (unaligned FFHQ) at 256 2 . Left: Training configurations. FID is computed between 50k generated images and all training images <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32]</ref>; lower is better. EQ-T and EQ-R are our equivariance metrics in decibels (dB); higher is better. Right: Parameter ablations using our final configuration (R) for the filter's support, magnification around nonlinearities, and the minimum stopband frequency at the first layer. * indicates our default choices.</p><p>Nonlinearity Applying a pointwise nonlinearity ? in the discrete domain does not commute with fractional translation or rotation. However, in the continuous domain, any pointwise function commutes trivially with geometric transformations and is thus equivariant to translation and rotation. Fulfilling the bandlimit constraint is another question -applying, e.g., ReLU in the continuous domain may introduce arbitrarily high frequencies that cannot be represented in the output.</p><p>A natural solution is to eliminate the offending high-frequency content by convolving the continuous result with the ideal low-pass filter ? s . Then, the continuous representation of the nonlinearity becomes f ? (z) = ? s * ?(z) = s 2 ? ? s * ?(z) and the discrete counterpart is F ? (Z) = s 2 ? X s (? s * ?(? s * Z)) (see <ref type="figure" target="#fig_0">Figure 2</ref>, right). This discrete operation cannot be realized without temporarily entering the continuous representation. We approximate this by upsampling the signal, applying the nonlinearity in the higher resolution, and downsampling it afterwards. Even though the nonlinearity is still performed in the discrete domain, we have found that only a 2? temporary resolution increase is sufficient for high-quality equivariance. For rotation equivariance, we must use the radially symmetric interpolation filter ? ? s in the downsampling step, as discussed above. Note that nonlinearity is the only operation capable of generating novel frequencies in our formulation, and that we can limit the range of these novel frequencies by applying a reconstruction filter with a lower cutoff than s/2 before the final discretization operation. This gives us precise control over how much new information is introduced by each layer of a generator network (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Practical application to generator network</head><p>We will now apply the theoretical ideas from the previous section in practice, by converting the well-established StyleGAN2 <ref type="bibr" target="#b33">[34]</ref> generator to be fully equivariant to translation and rotation. We will introduce the necessary changes step-by-step, evaluating their impact in <ref type="figure">Figure 3</ref>. The discriminator remains unchanged in our experiments.</p><p>The StyleGAN2 generator consists of two parts. First, a mapping network transforms an initial, normally distributed latent to an intermediate latent code w ? W. Then, a synthesis network G starts from a learned 4?4?512 constant Z 0 and applies a sequence of N layers -consisting of convolutions, nonlinearities, upsampling, and per-pixel noise -to produce an output image Z N = G(Z 0 ; w). The intermediate latent code w controls the modulation of the convolution kernels in G. The layers follow a rigid 2? upsampling schedule, where two layers are executed at each resolution and the number of feature maps is halved after each upsampling. Additionally, StyleGAN2 employs skip connections, mixing regularization <ref type="bibr" target="#b32">[33]</ref>, and path length regularization.</p><p>Our goal is to make every layer of G equivariant w.r.t. the continuous signal, so that all finer details transform together with the coarser features of a local neighborhood. If this succeeds, the entire network becomes similarly equivariant. In other words, we aim to make the continuous operation g of the synthesis network equivariant w.r.t. transformations t (translations and rotations) applied on the continuous input z 0 : g(t[z 0 ]; w) = t[g(z 0 ; w)]. To evaluate the impact of various architectural changes and practical approximations, we need a way to measure how well the network implements the equivariances. For translation equivariance, we report the peak signal-to-noise ratio (PSNR) in decibels (dB) between two sets of images, obtained by translating the input and output of the synthesis network by a random amount, resembling the definition by Zhang <ref type="bibr" target="#b68">[69]</ref>:</p><formula xml:id="formula_2">EQ-T = 10 ? log 10 I 2 max E w?W,x?X 2 ,p?V,c?C g(t x [z 0 ]; w) c (p) ? t x [g(z 0 ; w)] c (p) 2<label>(3)</label></formula><p>Each pair of images, corresponding to a different random choice of w, is sampled at integer pixel locations p within their mutually valid region V. Color channels c are processed independently, and the intended dynamic range of generated images ?1 . . . +1 gives I max = 2. Operator t x implements spatial translation with 2D offset x, here drawn from distribution X 2 of integer offsets. We define an analogous metric EQ-R for rotations, with the rotation angles drawn from U(0 ? , 360 ? ). Appendix E gives implementation details and our accompanying videos highlight the practical relevance of different dB values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fourier features and baseline simplifications (configs B-D)</head><p>To facilitate exact continuous translation and rotation of the input z 0 , we replace the learned input constant in StyleGAN2 with Fourier features <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b65">66]</ref>, which also has the advantage of naturally defining a spatially infinite map. We sample the frequencies uniformly within the circular frequency band f c = 2, matching the original 4?4 input resolution, and keep them fixed over the course of training. This change (configs A and B in <ref type="figure">Figure 3</ref>, left) slightly improves FID and, crucially, allows us to compute the equivariance metrics without having to approximate the operator t. This baseline architecture is far from being equivariant; our accompanying videos show that the output images deteriorate drastically when the input features are translated or rotated from their original position.</p><p>Next, we remove the per-pixel noise inputs because they are strongly at odds with our goal of a natural transformation hierarchy, i.e., that the exact sub-pixel position of each feature is exclusively inherited from the underlying coarse features. While this change (config C) is approximately FID-neutral, it fails to improve the equivariance metrics when considered in isolation.</p><p>To further simplify the setup, we decrease the mapping network depth as recommended by Karras et al. <ref type="bibr" target="#b31">[32]</ref> and disable mixing regularization and path length regularization <ref type="bibr" target="#b33">[34]</ref>. Finally, we also eliminate the output skip connections. We hypothesize that their benefit is mostly related to gradient magnitude dynamics during training and address the underlying issue more directly using a simple normalization before each convolution. We track the exponential moving average ? 2 = E[x 2 ] over all pixels and feature maps during training, and divide the feature maps by ? ? 2 . In practice, we bake the division into the convolution weights to improve efficiency. These changes (config D) bring FID back to the level of original StyleGAN2, while leading to a slight improvement in translation equivariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Step-by-step redesign motivated by continuous interpretation</head><p>Boundaries and upsampling (config E) Our theory assumes an infinite spatial extent for the feature maps, which we approximate by maintaining a fixed-size margin around the target canvas, cropping to this extended canvas after each layer. This explicit extension is necessary as border padding is known to leak absolute image coordinates into the internal representations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b65">66]</ref>. In practice, we have found a 10-pixel margin to be enough; further increase has no noticeable effect on the results.</p><p>Motivated by our theoretical model, we replace the bilinear 2? upsampling filter with a better approximation of the ideal low-pass filter. We use a windowed sinc filter with a relatively large Kaiser window <ref type="bibr" target="#b40">[41]</ref> of size n = 6, meaning that each output pixel is affected by 6 input pixels in upsampling and each input pixel affects 6 output pixels in downsampling. Kaiser window is a particularly good choice for our purposes, because it offers explicit control over the transition band and attenuation ( <ref type="figure" target="#fig_2">Figure 4a</ref>). In the remainder of this section, we specify the transition band explicitly and compute the remaining parameters using Kaiser's original formulas (Appendix C). For now, we choose to employ critical sampling and set the filter cutoff f c = s/2, i.e., exactly at the bandlimit, and transition band half-width f h = ( ? 2 ? 1)(s/2). Recall that sampling rate s equals the width of the canvas in pixels, given our definitions in Section 2.</p><p>The improved handling of boundaries and upsampling (config E) leads to better translation equivariance. However, FID is compromised by 16%, probably because we started to constrain what the feature maps can contain. In a further ablation <ref type="figure">(Figure 3</ref>, right), smaller resampling filters (n = 4) hurt translation equivariance, while larger filters (n = 8) mainly increase training time.</p><p>Filtered nonlinearities (config F) Our theoretical treatment of nonlinearities calls for wrapping each leaky ReLU (or any other commonly used non-linearity) between m? upsampling and m?   downsampling, for some magnification factor m. We further note that the order of upsampling and convolution can be switched by virtue of the signal being bandlimited, allowing us to fuse the regular 2? upsampling and a subsequent m? upsampling related to the nonlinearity into a single 2m? upsampling. In practice, we find m = 2 to be sufficient <ref type="figure">(Figure 3</ref>, right), again improving EQ-T (config F). Implementing the upsample-LReLU-downsample sequence is not efficient using the primitives available in current deep learning frameworks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">45]</ref>, and thus we implement a custom CUDA kernel (Appendix D) that combines these operations ( <ref type="figure" target="#fig_2">Figure 4b</ref>), leading to 10? faster training and considerable memory savings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-critical sampling (config G)</head><p>The critical sampling scheme -where filter cutoff is set exactly at the bandlimit -is ideal for many image processing applications as it strikes a good balance between antialiasing and the retention of high-frequency detail <ref type="bibr" target="#b57">[58]</ref>. However, our goals are markedly different because aliasing is highly detrimental for the equivariance of the generator. While highfrequency detail is important in the output image and thus in the highest-resolution layers, it is less important in the earlier ones given that their exact resolutions are somewhat arbitrary to begin with.</p><p>To suppress aliasing, we can simply lower the cutoff frequency to f c = s/2 ? f h , which ensures that all alias frequencies (above s/2) are in the stopband. <ref type="bibr" target="#b2">3</ref> For example, lowering the cutoff of the blue filter in <ref type="figure" target="#fig_2">Figure 4a</ref> would move its frequency response left so that the the worst-case attenuation of alias frequencies improves from 6 dB to 40 dB. This oversampling can be seen as a computational cost of better antialiasing, as we now use the same number of samples to express a slower-varying signal than before. In practice, we choose to lower f c on all layers except the highest-resolution ones, because in the end the generator must be able to produce crisp images to match the training data. As the signals now contain less spatial information, we modify the heuristic used for determining the number of feature maps to be inversely proportional to f c instead of the sampling rate s. These changes (config G) further improve translation equivariance and push FID below the original StyleGAN2.</p><p>Transformed Fourier features (config H) Equivariant generator layers are well suited for modeling unaligned and arbitrarily oriented datasets, because any geometric transformation introduced to the intermediate features z i will directly carry over to the final image z N . Due to the limited capability of the layers themselves to introduce global transformations, however, the input features z 0 play a crucial role in defining the global orientation of z N . To let the orientation vary on a per-image basis, the generator should have the ability to transform z 0 based on w. This motivates us to introduce a learned affine layer that outputs global translation and rotation parameters for the input Fourier features <ref type="figure" target="#fig_2">(Figure 4b</ref> and Appendix F). The layer is initialized to perform an identity transformation, but learns to use the mechanism over time when beneficial; in config H this improves the FID slightly.</p><p>Flexible layer specifications (config T) Our changes have improved the equivariance quality considerably, but some visible artifacts still remain as our accompanying videos demonstrate. On closer inspection, it turns out that the attenuation of our filters (as defined for config G) is still insufficient for the lowest-resolution layers. These layers tend to have rich frequency content near their bandlimit, which calls for extremely strong attenuation to completely eliminate aliasing.</p><p>So far, we have used the rigid sampling rate progression from StyleGAN2, coupled with simplistic choices for filter cutoff f c and half-width f h , but this need not be the case; we are free to specialize these parameters on a per-layer basis. In particular, we would like f h to be high in the lowestresolution layers to maximize attenuation in the stopband, but low in the highest-resolution layers to allow matching high-frequency details of the training data. <ref type="figure" target="#fig_2">Figure 4c</ref> illustrates an example progression of filter parameters in a 14-layer generator with two critically sampled full-resolution layers at the end. The cutoff frequency grows geometrically from f c = 2 in the first layer to f c = s N /2 in the first critically sampled layer. We choose the minimum acceptable stopband frequency to start at f t,0 = 2 2.1 , and it grows geometrically but slower than the cutoff frequency. In our tests, the stopband target at the last layer is f t = f c ? 2 0.3 , but the progression is halted at the first critically sampled layer. Next, we set the sampling rate s for each layer so that it accommodates frequencies up to f t , rounding up to the next power of two without exceeding the output resolution. Finally, to maximize the attenuation of aliasing frequencies, we set the transition band half-width to f h = max(s/2, f t ) ? f c , i.e., making it as wide as possible within the limits of the sampling rate, but at least wide enough to reach f t . The resulting improvement depends on how much slack is left between f t and s/2; as an extreme example, the first layer stopband attenuation improves from 42 dB to 480 dB using this scheme.</p><p>The new layer specifications again improve translation equivariance (config T), eliminating the remaining artifacts. A further ablation <ref type="figure">(Figure 3</ref>, right) shows that f t,0 provides an effective way to trade training speed for equivariance quality. Note that the number of layers is now a free parameter that does not directly depend on the output resolution. In fact, we have found that a fixed choice of N works consistently across multiple output resolutions and makes other hyperparameters such as learning rate behave more predictably. We use N = 14 in the remainder of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotation equivariance (config R)</head><p>We obtain a rotation equivariant version of the network with two changes. First, we replace the 3?3 convolutions with 1?1 on all layers and compensate for the reduced capacity by doubling the number of feature maps. Only the upsampling and downsampling operations spread information between pixels in this config. Second, we replace the sinc-based downsampling filter with a radially symmetric jinc-based one that we construct using the same Kaiser scheme (Appendix C). We do this for all layers except the two critically sampled ones, where it is important to match the potentially non-radial spectrum of the training data. These changes (config R) improve EQ-R without harming FID, even though each layer has 56% fewer trainable parameters.</p><p>We also employ an additional stabilization trick in this configuration. Early on in the training, we blur all images the discriminator sees using a Gaussian filter. We start with ? = 10 pixels, which we ramp to zero over the first 200k images. This prevents the discriminator from focusing too heavily on high frequencies early on. Without this trick, config R is prone to early collapses because the generator sometimes learns to produce high frequencies with a small delay, trivializing the discriminator's task. <ref type="figure">Figure 5</ref> gives results for six datasets using StyleGAN2 <ref type="bibr" target="#b33">[34]</ref> as well as our alias-free StyleGAN3-T and StyleGAN3-R generators. In addition to the standard FFHQ <ref type="bibr" target="#b32">[33]</ref> and METFACES <ref type="bibr" target="#b31">[32]</ref>, we created unaligned versions of them. We also created a properly resampled version of AFHQ <ref type="bibr" target="#b13">[14]</ref> and collected a new BEACHES dataset. Appendix B describes the datasets in detail. The results show that our FID remains competitive with StyleGAN2. StyleGAN3-T and StyleGAN3-R perform equally well in terms of FID, and both show a very high level of translation equivariance. As expected, only the latter provides rotation equivariance. In FFHQ (1024?1024) the three generators had <ref type="bibr" target="#b29">30</ref>  <ref type="figure">Figure 5</ref>: Left: Results for six datasets. We use adaptive discriminator augmentation (ADA) <ref type="bibr" target="#b31">[32]</ref> for the smaller datasets. "StyleGAN2" corresponds to our baseline config B with Fourier features. Right: Ablations and comparisons for FFHQ-U (unaligned FFHQ) at 256 2 . * indicates our default choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>(+103%) GPU hours. Our accompanying videos show side-by-side comparisons with StyleGAN2, demonstrating visually that the texture sticking problem has been solved. The resulting motion is much more natural, better sustaining an illusion that there is a coherent 3D scene being imaged.</p><p>Ablations and comparisons In Section 3.1 we disabled a number of StyleGAN2 features. We can now turn them on one by one to gauge their effect on our generators ( <ref type="figure">Figure 5</ref>, right). While mixing regularization can be re-enabled without any ill effects, we also find that styles can be mixed quite reliably even without this explicit regularization (Appendix A). Re-enabling noise inputs or relying on StyleGAN2's original layer specifications compromises equivariances significantly, and using fixed Fourier features or re-enabling path length regularization harms FID. Path length regularization is in principle at odds with translation equivariance, as it penalizes image changes upon latent space walk and thus encourages texture sticking. We suspect that the counterintuitive improvement in equivariance may come from slightly blurrier generated images, at a cost of poor FID.</p><p>In a scaling test we tried changing the number of feature maps, observing that equivariances remain at a high level, but FID suffers considerably when the capacity is halved. Doubling the capacity improves result quality in terms of FID, at the cost of almost 4? training time. Finally, we consider alternatives for our windowed Kaiser filter. Lanczos is competitive in terms of FID, but as a separable filter it compromises rotation equivariance in particular. Gaussian leads to clearly worse FIDs.</p><p>We compare StyleGAN3-R to an alternative where the rotation part is implemented using p4 symmetric G-CNN <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> on top of our StyleGAN3-T. This approach provides only modest rotation equivariance while being slower to train. Steerable filters <ref type="bibr" target="#b62">[63]</ref> could theoretically provide competitive EQ-R, but the memory and training time requirements proved infeasible with generator networks of this size.</p><p>Appendix A demonstrates that the spectral properties of generated images closely match training data, comparing favorably to several earlier architectures.</p><p>Internal representations <ref type="figure" target="#fig_3">Figure 6</ref> visualizes typical internal representations from the networks. While in StyleGAN2 all feature maps seem to encode signal magnitudes, in our networks some of the maps take a different role and encode phase information instead. Clearly this is something that is needed when the network synthesizes detail on the surfaces; it needs to invent a coordinate system. In StyleGAN3-R, the emergent positional encoding patterns appear to be somewhat more well-defined. We believe that the existence of a coordinate system that allows precise localization on the surfaces of objects will prove useful in various applications, including advanced image and video editing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations, discussion, and future work</head><p>In this work we modified only the generator, but it seems likely that further benefits would be available by making the discriminator equivariant as well. For example, in our FFHQ results the teeth do not move correctly when the head turns, and we suspect that this is caused by the discriminator accidentally preferring to see the front teeth at certain pixel locations. Concurrent work has identified that aliasing is detrimental for such generalization <ref type="bibr" target="#b58">[59]</ref>.</p><p>Our alias-free generator architecture contains implicit assumptions about the nature of the training data, and violating these may cause training difficulties. Let us consider an example. Suppose we have black-and-white cartoons as training data that we (incorrectly) pre-process using point sampling <ref type="bibr" target="#b43">[44]</ref>, leading to training images where almost all pixels are either black or white and the edges are jagged. This kind of badly aliased training data is difficult for GANs in general, but it is especially at odds with equivariance: on the one hand, we are asking the generator to be able to translate the output smoothly by subpixel amounts, but on the other hand, edges must still remain jagged and pixels only black/white, to remain faithful to the training data. The same issue can also arise with letterboxing of training images, low-quality JPEGs, or retro pixel graphics, where the jagged stair-step edges are a defining feature of the aesthetic. In such cases it may be beneficial for the generator to be aware of the pixel grid.</p><p>In future, it might be interesting to re-introduce noise inputs (stochastic variation) in a way that is consistent with hierarchical synthesis. A better path length regularization would encourage neighboring features to move together, not discourage them from moving at all. It might be beneficial to try to extend our approach to equivariance w.r.t. scaling, anisotropic scaling, or even arbitrary homeomorphisms. Finally, it is well known that antialiasing should be done before tone mapping. So far, all GANs -including ours -have operated in the sRGB color space (after tone mapping).</p><p>Attention layers in the middle of a generator <ref type="bibr" target="#b67">[68]</ref> could likely be dealt with similarly to non-linearities by temporarily switching to higher resolution -although the time complexity of attention layers may make this somewhat challenging in practice. Recent attention-based GANs that start with a tokenizing transformer (e.g., VQGAN <ref type="bibr" target="#b17">[18]</ref>) may be at odds with equivariance. Whether it is possible to make them equivariant is an important open question.</p><p>Potential negative societal impacts of (image-producing) GANs include many forms of disinformation, from fake portraits in social media <ref type="bibr" target="#b26">[27]</ref> to propaganda videos of world leaders <ref type="bibr" target="#b49">[50]</ref>. Our contribution eliminates certain characteristic artifacts from videos, potentially making them more convincing or deceiving, depending on the application. Viable solutions include model watermarking <ref type="bibr" target="#b66">[67]</ref> along with large-scale authenticity assessment in major social media sites. This entire project consumed 92 GPU years and 225 MWh of electricity on an in-house cluster of NVIDIA V100s. The new StyleGAN3 generator is only marginally costlier to train or use than that of StyleGAN2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>We thank David Luebke, Ming-Yu Liu, Koki Nagano, Tuomas Kynk??nniemi, and Timo Viitanen for reviewing early drafts and helpful suggestions. Fr?do Durand for early discussions. Tero Kuosmanen for maintaining our compute infrastructure. AFHQ authors for an updated version of their dataset. Getty Images for the training images in the BEACHES dataset. We did not receive external funding or additional revenues for this project. StyleGAN2 and our generators yield comparable FIDs in all of these datasets. Visual inspection did not reveal anything surprising in the first three datasets, but in BEACHES our new generators seem to generate a somewhat reduced set of possible scene layouts properly. We suspect that this is related to the lack of noise inputs, which forces the generators to waste capacity for what is essentially random number generation <ref type="bibr" target="#b33">[34]</ref>. Finding a way to reintroduce noise inputs without breaking equivariances is therefore an important avenue of future work.</p><p>The accompanying interpolation videos reveal major differences between StyleGAN2 and StyleGAN3-R. For example, in METFACES much of details such as brushstrokes or cracked paint seems to be glued to the pixel coordinates in StyleGAN2, whereas with StyleGAN3 all details move together with the depicted model. The same is evident in AFHQV2 with the fur moving credibly in StyleGAN3 interpolations, while mostly sticking to the image coordinates in StyleGAN2. In BEACHES we furthermore observe that StyleGAN2 tends to "fade in" details while retaining a mostly fixed viewing position, while StyleGAN3 creates plenty of apparent rotations and movement. The videos use hand-picked seeds to better showcase the relevant effects.</p><p>In a further test we created two example cinemagraphs that mimic small-scale head movement and facial animation in FFHQ. The geometric head motion was generated as a random latent space walk along hand-picked directions from GANSpace <ref type="bibr" target="#b24">[25]</ref> and SeFa <ref type="bibr" target="#b51">[52]</ref>. The changes in expression were realized by applying the "global directions" method of StyleCLIP <ref type="bibr" target="#b45">[46]</ref>, using the prompts "angry face", "laughing face", "kissing face", "sad face", "singing face", and "surprised face". The differences between StyleGAN2 and StyleGAN3 are again very prominent, with the former displaying jarring sticking of facial hair and skin texture, even under subtle movements.</p><p>The equivariance quality videos illustrate the practical relevance of the PSNR numbers in Figures 3 and 5 of the main paper. We observe that for EQ-T numbers over ?50 dB indicate high-quality results, and for EQ-R ?40 dB look good.</p><p>We also provide an animated version of the nonlinearity visualization in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>In style mixing <ref type="bibr" target="#b33">[34]</ref> two or more independently chosen latent codes are fed into different layers of the generator. Ideally all combinations would produce images that are not obviously broken, and furthermore, it would be desirable that specific layers end up controlling well-defined semantic aspects in the images. StyleGAN uses mixing regularization <ref type="bibr" target="#b33">[34]</ref> during training to achieve these goals. We observe that mixing regularization continues to work similarly in StyleGAN3, but we also wanted to know whether it is truly necessary because the regularization is known to be detrimental for many complex and multi-modal datasets <ref type="bibr" target="#b22">[23]</ref>. When we disable the regularization, obviously broken images remain rare, based on a visual inspection of a large number of images. The semantically meaningful controls are somewhat compromised, however, as <ref type="figure" target="#fig_9">Figure 11</ref> shows. <ref type="figure" target="#fig_0">Figure 12</ref> compares the convergence of our main configurations (config T and R) against the results of Karras et al. <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b31">32]</ref>. The overall shape of the curves is similar; introducing translation and rotation equivariance in the generator does not appear to significantly alter the training dynamics.</p><p>Following recent works that address signal processing issues in GANs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>, we show average power spectra of the generated and real images in <ref type="figure">Figure 13</ref>. The plots are computed from images that are whitened with the overall training dataset mean and standard deviation. Because FFT interprets the signal as periodic, we eliminate the sharp step edge across the image borders by windowing the pixel values prior to the transform. This eliminates the axis-aligned cross artifact which may obscure meaningful detail in the spectrum. We display the average 2D spectrum as a contour plot, which makes the orientation-dependent falloff apparent, and highlights detail like regularly spaced residuals of upsampling grids, and fixed noise patterns. We also plot 1D slices of the spectrum along the horizontal and diagonal angle without azimuthal integration, so as to not average out the detail. The code for reproducing these steps is included in the public release.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Datasets</head><p>In this section, we describe the new datasets and list the licenses of all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 FFHQ-U and MetFaces-U</head><p>We built unaligned variants of the existing FFHQ <ref type="bibr" target="#b32">[33]</ref> and METFACES <ref type="bibr" target="#b31">[32]</ref> datasets. The originals are available at https://github.com/NVlabs/ffhq-dataset and https://github.com/NVlabs/ metfaces-dataset, respectively. The datasets were rebuilt with a modification of the original procedure based on the original code, raw uncropped images, and facial landmark metadata. The code required to reproduce the modified datasets is included in the public release.</p><p>We use axis-aligned crop rectangles, and do not rotate them to match the orientation of the face. This retains the natural variation of camera and head tilt angles. Note that the images are still generally  <ref type="figure" target="#fig_0">Figure 12</ref>: Training convergence with three datasets using StyleGAN2 and our main configurations (config T and R). x-axis corresponds to the total number of real images shown to the discriminator and y-axis is the Fr?chet inception distance (FID), computed between 50k generated images and all training images <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32]</ref>; lower is better. The black dots indicate the best FID for each training run, matching the corresponding cases in <ref type="figure">Figures 3 and 5</ref>. METFACES was trained using adaptive discriminator augmentation (ADA) <ref type="bibr" target="#b31">[32]</ref>, starting from the corresponding FFHQ snapshot with the lowest FID.  <ref type="figure">Figure 13</ref>: Top: Average 2D power spectrum of the training images in FFHQ at 1024?1024 resolution, along with the corresponding spectra of random images generated using StyleGAN3-T, SWAGAN <ref type="bibr" target="#b18">[19]</ref>, and CIPS <ref type="bibr" target="#b3">[4]</ref>. Each plot represents the average power over 70k images, computed as follows. From each image, we subtract the training dataset mean, after which we divide it by the training dataset standard deviation. Note that these normalizing quantities represent the entire dataset reduced to two scalars, and do not vary by color channel or pixel coordinate. The image is then multiplied with a separable Kaiser window with ? = 8, and its power spectrum is computed as the absolute values of the FFT raised to the second power. This processing is applied to each color channel separately and the result is averaged over them. These spectra are then averaged over all the images in the dataset. The result is plotted on the decibel scale. Bottom: One-dimensional slices of the power spectra at 0 ? and 45 ? angles.</p><p>upright, i.e., never upside down or at 90 ? angle. The scale of the rectangle is determined as before.</p><p>For each image, the crop rectangle is randomly shifted from its original face-centered position, with the horizontal and vertical offset independently drawn from a normal distribution. The standard deviation is chosen as 20% of the crop rectangle dimension. If the crop rectangle falls partially outside the original image boundaries, we keep drawing new random offsets until we find one that does not. This removes the need to pad the images with fictional mirrored content, and we explicitly disabled this feature of the original build script.</p><p>Aside from the exact image content, the number of images and other specifications match the original dataset exactly. While FFHQ-U contains identifiable images of persons, it does not introduce new images beyond those already in the original FFHQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 AFHQv2</head><p>We used an updated version of the AFHQ dataset <ref type="bibr" target="#b13">[14]</ref> where the resampling filtering has been improved. The original dataset suffers from pixel-level artifacts caused by inadequate downsampling filters <ref type="bibr" target="#b43">[44]</ref>. This caused convergence problems with our models, as the sharp "stair-step" aliasing artifacts are difficult to reproduce without direct access to the pixel grid.</p><p>The dataset was rebuilt using the original uncropped images and crop rectangle metadata, using the PIL library implementation of Lanczos resampling as recommended by Parmar et al. <ref type="bibr" target="#b43">[44]</ref>. In a minority of cases, the crop rectangles were modified to remove non-isotropic scaling and other unnecessary transformations. A small amount (? 2%) of images were dropped for technical reasons, leaving a total of 15803 images. Aside from this, the specifications of the dataset match the original. We use all images of all the three classes (cats, dogs, and wild animals) as one training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Beaches</head><p>BEACHES is a new dataset of 20155 photographs of beaches at resolution 512?512. The training images were provided by Getty Images. BEACHES is a proprietary dataset that we are licensed to use, but not to redistribute. We are therefore unable to release the full training data or pre-trained models for this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Licenses</head><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Filter details</head><p>In this section, we review basic FIR filter design methodology and detail the recipe used to construct the upsampling and downsampling filters in our generator. We start with simple Kaiser filters in one dimension, discussing parameter selection and the necessary modifications needed for upsampling and downsampling. We then proceed to extend the filters to two dimensions and conclude by detailing the alternative filters evaluated in <ref type="figure">Figure 5</ref>, right. Our definitions are consistent with standard signal processing literature (e.g., Oppenheim <ref type="bibr" target="#b40">[41]</ref>) as well as widely used software packages (e.g., scipy.signal.firwin).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Kaiser low-pass filters</head><p>In one dimension, the ideal continuous-time low-pass filter with cutoff f c is given by ?(x) = 2f c ? sinc(2f c x), where sinc(x) = sin(?x)/(?x). The ideal filter has infinite attenuation in the stopband, i.e., it completely eliminates all frequencies above f c . However, its impulse response is also infinite, which makes it impractical for three reasons: implementation efficiency, border artifacts, and ringing caused by long-distance interactions. The most common way to overcome these issues is to limit the spatial extent of the filter using the window method <ref type="bibr" target="#b40">[41]</ref>:</p><formula xml:id="formula_3">h K (x) = 2f c ? sinc(2f c x) ? w K (x),<label>(4)</label></formula><p>where w K (x) is a window function and h K (x) is the resulting practical approximation of ?(x). Different window functions represent different tradeoffs between the frequency response and spatial extent; the smaller the spatial extent, the weaker the attenuation. In this paper we use the Kaiser window <ref type="bibr" target="#b29">[30]</ref>, also known as the Kaiser-Bessel window, that provides explicit control over this tradeoff. The Kaiser window is defined as</p><formula xml:id="formula_4">w K (x) = I 0 ? 1 ? (2x/L) 2 I 0 ? , if |x| ? L/2, 0, if |x| &gt; L/2,<label>(5)</label></formula><p>where L is the desired spatial extent, ? is a free parameter that controls the shape of the window, and I 0 is the zeroth-order modified Bessel function of the first kind. Note that the window has discontinuities at ?L/2; the value is strictly positive at x = L/2 but zero at x = L/2 + .</p><p>When operating on discretely sampled signals, it is necessary to discretize the filter as well:</p><formula xml:id="formula_5">h K [i] = h K i ? (n ? 1)/2 /s s, for i ? {0, 1, . . . , n ? 1},<label>(6)</label></formula><p>where h K [i] is the discretized version of h K (x) and s is the sampling rate. The filter is defined at n discrete spatial locations, i.e., taps, located 1/s units apart and placed symmetrically around zero.</p><p>Given the values of n and s, the spatial extent can be expressed as L = (n ? 1)/s. An odd value of n results in a zero-phase filter that preserves the original sample locations, whereas an even value shifts the sample locations by 1/(2s) units.</p><p>The filters considered in this paper are approximately normalized by construction, i.e.,</p><formula xml:id="formula_6">x h K (x) ? i h K [i] ? 1.</formula><p>Nevertheless, we have found it beneficial to explicitly normalize them after discretization. In other words, we strictly enforce i h K [i] = 1 by scaling the filter taps to reduce the risk of introducing cumulative scaling errors when the signal is passed through several consecutive layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Selecting window parameters</head><p>Kaiser <ref type="bibr" target="#b29">[30]</ref> provides convenient empirical formulas to connect the parameters of w K to the properties of h K . Given the number of taps and the desired transition band width, the maximum attenuation achievable with h K [i] is approximated by A = 2.285 ? (n ? 1) ? ? ? ?f + 7.95,</p><p>where A is the attenuation measured in decibels and ?f is the width of the transition band expressed as a fraction of s/2. We choose to define the transition band using half-width f h , which gives ?f = (2f h )/(s/2). Given the value of A, the optimal choice for the shape parameter ? is then approximated <ref type="bibr" target="#b29">[30]</ref> by</p><formula xml:id="formula_8">? = ? ? ? 0.1102 ? (A ? 8.7), if A &gt; 50, 0.5842 ? (A ? 21) 0.4 + 0.07886 ? (A ? 21), if 21 ? A ? 50, 0, if A &lt; 21,<label>(8)</label></formula><p>This leaves us with two free parameters: n controls the spatial extent while f h controls the transition band. The choice of these parameters directly influences the resulting attenuation; increasing either parameter yields a higher value for A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Upsampling and downsampling</head><p>When upsampling a signal, i.e., F up (Z) = X s (? s * Z) = 1/s 2 ? X s (? s * Z), we are concerned not only the with input sampling rate s, but also with the output sampling rate s . With an integer upsampling factor m, we can think of the upsampling operation as consisting of two steps: we first increase the sampling rate to s = s ? m by interleaving m ? 1 zeros between each input sample by and then low-pass filter the resulting signal to eliminate the alias frequencies above s/2. In order to keep the signal magnitude unchanged, we must also scale the result by m with one-dimensional signals, or by m 2 with two-dimensional signals. Since the filter now operates under s instead of s, we must adjust its parameters accordingly:</p><formula xml:id="formula_9">n = n ? m, L = (n ? 1)/s , ?f = (2f h )/(s /2),<label>(9)</label></formula><p>which gives us the final upsampling filter</p><formula xml:id="formula_10">h K [i] = h K i ? (n ? 1)/2 /s s , for i ? {0, 1, . . . , n ? 1}.<label>(10)</label></formula><p>Multiplying the number of taps by m keeps the spatial extent of the filter unchanged with respect to the input samples, and it also compensates for the reduced attenuation from ?f &lt; ?f . Note that if the upsampling factor is even, n will be even as well, meaning that h K shifts the sample locations by 1/(2s ). This is the desired behavior -if we consider sample i to represent the continuous interval [i ? s, (i + 1) ? s] in the input signal, the same interval will be represented by m consecutive samples m ? i, . . . , m ? i + m ? 1 in the output signal. Using a zero-phase upsampling filter, i.e., an odd value for n , would break this symmetry, leading to inconsistent behavior with respect to the boundaries. Note that our symmetric interpretation is common in many computer graphics APIs, such as OpenGL, and it is also reflected in our definition of the Dirac comb X in Section 2.</p><p>Upsampling and downsampling are adjoint operations with respect to each other, disregarding the scaling of the signal magnitude. This means that the above definitions are readily applicable to downsampling as well; to downsample a signal by factor m, we first filter it by h K and then discard the last m ? 1 samples within each group of m consecutive samples. The interpretation of all filter parameters, as well as the sample locations, is analogous to the upsampling case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Two-dimensional filters</head><p>Any one-dimensional filter, including h K , can be trivially extended to two dimensions by defining the corresponding separable filter</p><formula xml:id="formula_11">h + K (x) = h K (x 0 ) ? h K (x 1 ) = (2f c ) 2 ? sinc(2f c x 0 ) ? sinc(2f c x 1 ) ? w K (x 0 ) ? w K (x 1 ),<label>(11)</label></formula><p>where x = (x 0 , x 1 ). h + K has the same cutoff as h K along the coordinate axes, i.e., f c,x = (f c , 0) and f c,y = (0, f c ), and its frequency response forms a square shape over the 2D plane, implying that the cutoff frequency along the diagonal is f c,d = (f c , f c ). In practice, a separable filter can be implemented efficiently by first filtering each row of the two-dimensional signal independently with h K and then doing the same for each column. This makes h + K an ideal choice for all upsampling filters in our generator, as well as the downsampling filters in configs A-T <ref type="figure">(Figure 3, left)</ref>.</p><p>The fact that the spectrum of h + K is not radially symmetric, i.e., f c,d = f c,x , is problematic considering config R. If we rotate the input feature maps of a given layer, their frequency content will rotate as well. To enforce rotation equivariant behavior, we must ensure that the effective cutoff frequencies remain unchanged by this. The ideal radially symmetric low-pass filter <ref type="bibr" target="#b8">[9]</ref> is given by ? ? s (x) = (2f c ) 2 ? jinc(2f c x ). The jinc function, also known as besinc, sombrero function, or Airy disk, is defined as jinc(x) = 2J 1 (?x)/(?x), where J 1 is the first order Bessel function of the first kind. Using the same windowing scheme as before, we define the corresponding practical filter as</p><formula xml:id="formula_12">h ? K (x) = (2f c ) 2 ? jinc(2f c x ) ? w K (x 0 ) ? w K (x 1 ).<label>(12)</label></formula><p>Note that even though jinc is radially symmetric, we still treat the window function as separable in order to retain its spectral properties. In config R, we perform all downsampling operations using h ? K , except for the last two critically sampled layers where we revert to h + K .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Alternative filters</head><p>In <ref type="figure">Figure 5</ref>, right, we compare the effectiveness of Kaiser filters against two alternatives: Lanczos and Gaussian. These filters are typically defined using prototypical filter kernels k L and k G , respectively:</p><formula xml:id="formula_13">k L (x) = sinc(x) ? sinc(x/a), if |x| &lt; a, 0, if |x| ? a,<label>(13)</label></formula><formula xml:id="formula_14">k G (x) = exp ? 1 2 (x/?) 2 ? ? 2? ,<label>(14)</label></formula><p>where a is the spatial extent of the Lanczos kernel, typically set to 2 or 3, and ? is the standard deviation of the Gaussian kernel. In <ref type="figure">Figure 5</ref> of the main paper we set a = 2 and ? = 0.4; we tested several different values and found these choices to work reasonably well.</p><p>The main shortcoming of the prototypical kernels is that they do not provide an explicit way to control the cutoff frequency. In order to enable apples-to-apples comparison, we assume that the kernels have an implicit cutoff frequency at 0.5 and scale their impulse responses to account for the varying f c :</p><formula xml:id="formula_15">h L (x) = 2f c ? k L (2f c x), h G (x) = 2f c ? k G (2f c x).<label>(15)</label></formula><p>We limit the computational complexity of the Gaussian filter by enforcing h G (x) = 0 when |x| &gt; 8/s, with respect to the input sampling rate in the upsampling case. In practice, h G (x) is already very close to zero in this range, so the effect of this approximation is negligible. Finally, we extend the filters to two dimensions by defining the corresponding separable filters:</p><formula xml:id="formula_16">h + L (x) = (2f c ) 2 ? k L (2f c x 0 ) ? k L (2f c x 1 ), h + G (x) = (2f c ) 2 ? k G (2f c x 0 ) ? k G (2f c x 1 ). (16)</formula><p>Note that h + G is radially symmetric by construction, which makes it ideal for rotation equivariance. h + L , however, has no widely accepted radially symmetric counterpart, so we simply use the same separable filter in config R as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Custom CUDA kernel for filtered nonlinearity</head><p>Implementing the upsample-nonlinearity-downsample sequence is inefficient using the standard primitives available in modern deep learning frameworks. The intermediate feature maps have to be transferred between on-chip and off-chip GPU memory multiple times and retained for the backward pass. This is especially costly because the intermediate steps operate on upsampled, high-resolution data. To overcome this, we implement the entire sequence as a single operation using a custom CUDA kernel. This improves training performance by approximately an order of magnitude thanks to reduced memory traffic, and also decreases GPU memory usage significantly.</p><p>The combined kernel consists of four phases: input, upsampling, nonlinearity, and downsampling. The computation is parallelized by subdividing the output feature maps into non-overlapping tiles, and computing one output tile per CUDA thread block. First, in input phase, the corresponding input region is read into on-chip shared memory of the thread block. Note that the input regions for neighboring output tiles will overlap spatially due to the spatial extent of filters.</p><p>The execution of up-/downsampling phases depends on whether the corresponding filters are separable or not. For a separable filter, we perform vertical and horizontal 1D convolutions sequentially, whereas a non-separable filter requires a single 2D convolution. All these convolutions and the nonlinearity operate in on-chip shared memory, and only the final output of the downsampling phase is written to off-chip GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Gradient computation</head><p>To compute gradients of the combined operation, they need to propagate through each of the phases in reverse order. Fortunately, the combined upsample-nonlinearity-downsample operation is mostly self-adjoint with proper changes in parameters, e.g., swapping the up-/downsampling factors and the associated filters. The only problematic part is the nonlinearity that is performed in the upsampled resolution. A na?ve but general solution would be to store the intermediate high-resolution input to the nonlinearity, but the memory consumption would be infeasible for training large models.</p><p>Our kernel is specialized to use leaky ReLU as the nonlinearity, which offers a straightforward way to conserve memory: to propagate gradients, it is sufficient to know whether the corresponding input value to nonlinearity was positive or negative. When using 16-bit floating-point datatypes, there is an additional complication because the outputs of the nonlinearity need to be clamped <ref type="bibr" target="#b31">[32]</ref>, and when this occurs, the corresponding gradients must be zero. Therefore, in the forward pass we store two bits of auxiliary information per value to cover the three possible cases: positive, negative, or clamped. In the backward pass, reading these bits is sufficient for correct gradient computation -no other information from the forward pass is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Optimizations for common upsampling factors</head><p>Let us consider one-dimensional 2? upsampling where the input is (virtually) interleaved with zeros and convolved with an n -tap filter where n = 2n (cf. <ref type="bibr">Equation 9</ref>). There are n nonzero input values  under the n -tap kernel, so if each output pixel is computed separately, the convolution requires n multiply-add operations per pixel and equally many shared memory load instructions, for a total of 2n instructions per output pixel. <ref type="bibr" target="#b3">4</ref> However, note that the computation of two neighboring output pixels accesses only n + 1 input pixels in total. By computing two output pixels at a time and avoiding redundant shared memory load instructions, we obtain an average cost of 3 2 n + 1 2 instructions per pixel -close to 25% savings. For 4? upsampling, we can similarly reduce the instruction count by up to 37.5% by computing four output pixels at a time. We apply these optimizations in 2? and 4? upsampling for both separable and non-separable filters. <ref type="figure" target="#fig_2">Figure 14</ref> benchmarks the performance of our kernel with various up-/downsampling factors and with separable and non-separable filters. In network layers that keep the sampling rate fixed, both factors are 2?, whereas layers that increase the sampling rate by a factor of two, 4? upsampling is combined with 2? downsampling. The remaining combination of 2? upsampling and 4? downsampling is needed when computing gradients of the latter case. The speedup over native PyTorch operations varies between ?20-40?, which yields an overall training speedup of approximately 10?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Equivariance metrics</head><p>In this section, we describe our equivariance metrics, EQ-T and EQ-R, in detail. We also present additional results using an alternative translation metric, EQ-T frac , based on fractional sub-pixel translation.</p><p>We express each of our metrics as the peak signal-to-noise ratio (PSNR) between two sets of images, measured in decibels (dB). PSNR is a commonly used metric in image restoration literature. In the typical setting we have two signals, reference I and its noisy approximation K, defined over discrete domain D -usually a two-dimensional pixel grid. The PSNR between I and K is then defined via the mean squared error (MSE):</p><formula xml:id="formula_17">MSE D (I, K) = 1 D i?D I[i] ? K[i] 2 ,<label>(17)</label></formula><p>PSNR D (I, K) = 10 ? log 10</p><formula xml:id="formula_18">I 2 max MSE D (I, K) ,<label>(18)</label></formula><p>where MSE D (I, K) is the average squared difference between matching elements of I and K. I max is the expected dynamic range of the reference signal, i.e.,</p><formula xml:id="formula_19">I max ? max i?D (I[i]) ? min i?D (I[i]).</formula><p>The dynamic range is usually considered to be a global constant, e.g., the range of valid RGB values, as opposed to being dependent on the content of I. In our case, I and K represent desired and actual outputs of the synthesis network, respectively, with a dynamic range of [?1, 1]. This implies that I max = 2. High PSNR values indicate that K is close to I; in the extreme case, where K = I, we have PSNR D (I, K) = ? dB.</p><p>Since we are interested in sets of images, we use a slightly extended definition for MSE that allows I and K to be defined over an arbitrary, potentially uncountable domain:</p><formula xml:id="formula_20">MSE D (I, K) = E i?D I(i) ? K(i) 2 .<label>(19)</label></formula><p>Configuration FID  <ref type="figure">Figure 15</ref>: Results with our alternative translation equivariance metric EQ-T frac ; higher is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EQ-T EQ-Tfrac</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Integer translation</head><p>The goal of our integer translation metric, EQ-T, is to measure how closely, on average, the output the synthesis network G matches a translated reference image when we translate the input of G. In other words, EQ-T = PSNR W?X 2 ?V?C (I t , K t ),</p><formula xml:id="formula_21">I t (w, x, p, c) = T x G(z 0 ; w) [p, c], K t (w, x, p, c) = G(t x [z 0 ]; w)[p, c],<label>(20)</label></formula><p>where w ? W is a random intermediate latent code produced by the mapping network, x = (x 0 , x 1 ) ? X 2 is a random translation offset, p enumerates pixel locations in the mutually valid region V, c ? C is the color channel, and z 0 represents the input Fourier features. For integer translations, we sample the translation offsets x 0 and x 1 from X = U[?s N /8, s N /8], where s N is the width of the image in pixels.</p><p>In practice, we estimate the expectation in <ref type="bibr">Equation 20</ref> as an average over 50,000 random samples of (w, x) ? W ? X 2 . For given w and x, we generate the reference image I t by running the synthesis network and translating the resulting image by x pixels (operator T x ). We then obtain the approximate result image K t by translating the input Fourier features by the corresponding amount (operator t x ), as discussed in Appendix F.1, and running the synthesis network again. The mutually valid region of I t (translated by (x 0 , x 1 )) and K t (translated by (0, 0)) is given by</p><formula xml:id="formula_22">V = {max(x 0 , 0), . . . , s N + min(x 0 , 0) ? 1}? {max(x 1 , 0), . . . , s N + min(x 1 , 0) ? 1}.<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Fractional translation</head><p>Our translation equivariance metric has the nice property that, for a perfectly equivariant generator, the value of EQ-T converges to ? dB when the number of samples tends to infinity. However, this comes at the cost of completely ignoring subpixel effects. In fact, it is easy to imagine a generator that is perfectly equivariant to integer translation but fails with subpixel translation; in principle, this is true for any generator whose output is not properly bandlimited, including, e.g., implicit coordinate-based MLPs <ref type="bibr" target="#b3">[4]</ref>.</p><p>To verify that our generators are able to handle subpixel translation, we define an alternative translation equivariance metric, EQ-T frac , where the translation offsets x 0 and x 1 are sampled from a continuous distribution X = U(?s N /8, s N /8). While the continuous operator t x readily supports this new definition with fractional offsets, extending the discrete T x is slightly more tricky.</p><p>In practice, we define T x via standard Lanczos resampling, by filtering the image produced by G using the prototypical Lanczos filter (Equation 15) with a = 3, evaluated at integer tap locations offset by x. We explicitly normalize the resulting discretized filter to enforce the partition of unity property. We also shrink the mutually valid region to account for the spatial extent a by redefining V = {max(x 0 + a, 0), . . . , s N + min(x 0 ? a, ?1)}? {max(x 1 + a, 0), . . . , s N + min(x 1 ? a, ?1)}. <ref type="figure">Figure 15</ref> compares the results of the two metrics, EQ-T and EQ-T frac , using the same training configurations as <ref type="figure">Figure 3</ref> in the main paper. The metrics agree reasonably well up until ?40 dB, after which the fractional metric starts to saturate; it consistently fails to rise above 50 dB in our tests. This is due to the fact that the definition of subpixel translation is inherently ambiguous. The choice of the resampling filter represents a tradeoff between aliasing, ringing, and retention of high frequencies; there is no reason to assume that the generator would necessarily have to make the same tradeoff as the metric. Based on the results, we conclude that our configs G-R are essentially perfectly equivariant to subpixel translation within the limits of Lanczos resampling's accuracy. However, due to its inherent limitations, we refrain from choosing EQ-T frac as our primary metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Rotation</head><p>Measuring equivariance with respect to arbitrary rotations has the same fundamental limitation as our EQ-T frac metric: the resampling operation is inherently ambiguous, so we cannot except the results to be perfectly accurate beyond ?40 dB. Arbitrary rotations also have the additional complication that the bandlimit of a discretely sampled image is not radially symmetric.</p><p>Consider rotating the continuous representation of a discretely sampled image by 45 ? . The original frequency content of the image is constrained within the rectangular bandlimit f ? [?s N /2, +s N /2] 2 . The frequency content of the rotated image, however, forms a diamond shape that extends all the way to f = ? 2s N /2 along the main axes but only to f = s N /2 along the diagonals. In other words, it simultaneously has too much frequency content, but also too little. This has two implications. First, in order to obtain a valid discretized result image, we have to low-pass filter the image both before and after the rotation to completely eliminate aliasing. Second, even if we are successful in eliminating the aliasing, the rotated image will still lack the highest representable diagonal frequencies. The second point further implies that when computing PSNR, our reference image I will inevitably lack some frequencies that are present in the output of G. To obtain the correct result, we must eliminate these extraneous frequencies -without modifying the output image in any other way.</p><p>Based on the above reasoning, we define our EQ-R metric as follows:</p><formula xml:id="formula_24">EQ-R = PSNR W?A?V?C (I r , K r ), I r (w, ?, p, c) = R ? G(z 0 ; w) [p, c], K r (w, ?, p, c) = R * ? G(r ? [z 0 ]; w) [p, c],<label>(23)</label></formula><p>where the random rotation angle ? is drawn from A = U(0 ? , 360 ? ) and operator r ? corresponds to continuous rotation of the input Fourier features by ? with respect to the center of the canvas [0, 1] 2 . R ? corresponds to high-quality rotation of the reference image, and R * ? represents a pseudo-rotation operator that modifies the frequency content of the image as if it had undergone R ? -but without actually rotating it.</p><p>The ideal rotation operatorR is easily defined under our theoretical framework presented in Section 2.1:</p><formula xml:id="formula_25">R ? [Z] = X ? * r ? [? * Z] = 1/s 2 ? X ? * r ? [? * Z] .<label>(24)</label></formula><p>In other words, we first convolve the discretely sampled input image Z with ? to obtain the corresponding continuous representation. We then rotate this continuous representation using r ? , bandlimit the result by convolving with ?, and finally extract the corresponding discrete representation by multiplying with X. To reduce notational clutter, we omit the subscripts denoting the sampling rate s. We can swap the order of the rotation and a convolution in the above formula by rotating the kernel in the opposite direction to compensate:</p><formula xml:id="formula_26">R ? [Z] = 1/s 2 ? X r ? [? R * Z],? R = r ?? [?] * ?,<label>(25)</label></formula><p>where? R represents an ideal "rotation filter" that bandlimits the signal with respect to both the input and the output. Its spectrum is the eight-sided polygonal intersection of the original and the rotated rectangle.</p><p>In order to obtain a practical approximation R ? , we must replace? R with an approximate filter h R that has finite support. Given such a filter, we get R ? [Z] = 1/s 2 ? X r ? [h R * Z]. In practice, we implement this operation using two additional approximations. First, we approximate 1/s 2 ? h R * Z by an upsampling operation to a higher temporary resolution, using h R as the upsampling filter and m = 4. Second, we approximate X r ? by performing a set of bilinear lookups from the temporary high-resolution image.</p><p>To obtain h R , we again utilize the standard Lanczos window with a = 3:</p><formula xml:id="formula_27">h R = r ?? [?] * ? (r ?? [w + L ] * w + L ),<label>(26)</label></formula><p>where we apply the same rotation-convolution to both the filter and the window function. w + L corresponds the canonical separable Lanczos window, similar to the one used in Equation <ref type="bibr" target="#b14">15</ref>:</p><formula xml:id="formula_28">w + L (x) = sinc(x 0 /a) ? sinc(x 1 /a), if max(|x 0 |, |x 1 |) &lt; a, 0, if max(|x 0 |, |x 1 |) ? a,<label>(27)</label></formula><p>We can now define the pseudo-rotation operator R * ? [Z] as a simple convolution with another filter that resembles h R :</p><formula xml:id="formula_29">R * ? [Z] = 1/s 2 ? X (h * R * Z) = H * R * Z, h * R = ? * r ? [?] (w + L * r ? [w + L ]),<label>(28)</label></formula><p>where the discrete version H * R is obtained from h * R using Equation <ref type="bibr" target="#b5">6</ref>. Finally, we define the valid region V the same way as in Appendix E.2: the set of pixels for which both filter footprints fall within the bounds of the corresponding original images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Implementation details</head><p>We implemented our alias-free generator on top of the official PyTorch implementation of StyleGAN2-ADA, available at https://github.com/NVlabs/stylegan2-ada-pytorch. We kept most of the details unchanged, including discriminator architecture <ref type="bibr" target="#b33">[34]</ref>, weight demodulation <ref type="bibr" target="#b33">[34]</ref>, equalized learning rate for all trainable parameters <ref type="bibr" target="#b30">[31]</ref>, minibatch standard deviation layer at the end of the discriminator <ref type="bibr" target="#b30">[31]</ref>, exponential moving average of generator weights <ref type="bibr" target="#b30">[31]</ref>, mixed-precision FP16/FP32 training <ref type="bibr" target="#b31">[32]</ref>, non-saturating logistic loss <ref type="bibr" target="#b20">[21]</ref>, R 1 regularization <ref type="bibr" target="#b39">[40]</ref>, lazy regularization <ref type="bibr" target="#b33">[34]</ref>, and Adam optimizer <ref type="bibr" target="#b35">[36]</ref> with ? 1 = 0, ? 2 = 0.99, and = 10 ?8 .</p><p>We ran all experiments on NVIDIA DGX-1 with 8 Tesla V100 GPUs using PyTorch 1.7.1, CUDA 11.0, and cuDNN 8.0.5. We computed FID between 50k generated images and all training images using the official pre-trained Inception network, available at http://download.tensorflow.org/ models/image/imagenet/inception-2015-12-05.tgz</p><p>Our implementation and pre-trained models are available at https://github.com/NVlabs/stylegan3 F.1 Generator architecture Normalization (configs D-R) We have observed that eliminating the output skip connections in StyleGAN2 <ref type="bibr" target="#b33">[34]</ref> results in uncontrolled drift of signal magnitudes over the generator layers. This does not necessarily lead to lower-quality results, but it generally increases the amount of random variation between training runs and may occasionally lead to numerical issues with mixed-precision training. We eliminate the drift by tracking a long-term exponential moving average of the input signal magnitude on each layer and normalizing the feature maps accordingly. We update the moving average once per training iteration, based on the mean of squares over the entire input tensor, and freeze its value after training. We initialize the moving average to 1 and decay it at a constant rate, resulting in 50% decay per 20k real images shown to the discriminator. With this explicit normalization in place, we have found it beneficial to slightly adjust the dynamic range of the output RGB colors. StyleGAN2 uses ?1 and +1 to represent black and white, respectively; we change these values to ?4 and +4 starting from config D and, for consistency with the original generator, divide the color channels by 4 afterwards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformed Fourier features (configs H-R)</head><p>We enable the orientation of the input features z 0 to vary on a per-image basis by introducing an additional affine layer <ref type="figure" target="#fig_2">(Figure 4b</ref>) and applying a geometric transformation based on its output. The affine layer produces a four-dimensional vector t = (r c , r s , t x , t y ) based on w. We initialize its weights so that t = (1, 0, 0, 0) at the beginning, but allow them to change freely over the course of training. To interpret t as a geometric transformation, we first normalize its value based on the first two components, i.e., t = (r c , r s , t x , t y ) = t r 2 c + r 2 s . This makes the transformation independent of the magnitude of w, similar to the weight modulation and  <ref type="table">Batch size  32  32  32  64  64  64  Moving average  10k  10k  10k  20k  20k  20k  Mapping net depth  8  2  2  8  2  2  Minibatch stddev  4  4  4  8  4  4  G layers  15/17  14  14  13</ref>   demodulation <ref type="bibr" target="#b33">[34]</ref> on the other layers. We then interpret the first two components as rotation around the center of the canvas [0, 1] 2 , with the rotation angle ? defined by r c = cos ? and r s = sin ?. Finally, we interpret the remaining two components as translation by (t x , t y ) units, so that the translation is performed after the rotation. In practice, we implement the resulting geometric transformation by modifying the phases and two-dimensional frequencies of the Fourier features, which is equivalent to applying the same transformation to the continuous representation of z 0 analytically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flexible layer specifications</head><p>In configs T and R, we define the per-layer filter parameters <ref type="figure" target="#fig_2">(Figure 4c</ref>) as follows. The cutoff frequency f c and the minimum acceptable stopband frequency f t obey geometric progression until the first critically sampled layer:</p><formula xml:id="formula_30">f c [i] = f c,0 ? (f c,N /f c,0 ) min(i/(N ?Ncrit),1) , f t [i] = f t,0 ? (f t,N /f t,0 ) min(i/(N ?Ncrit),1) ,<label>(29)</label></formula><p>where N = 14 is the total number of layers, N crit = 2 is the number of critically sampled layers at the end, f c,0 = 2 corresponds to the frequency content of the input Fourier features, and f c,N = s N /2 is defined by the output resolution. f t,0 and f t,N are free parameters; we use f t,0  <ref type="figure">1, 0)</ref>]) ? m, where m is the upsampling parameter discussed in Section 3.2 that we set to 2 in most of our tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Hyperparameters and training configurations</head><p>We used 8 GPUs for all our training runs and continued the training until the discriminator had seen a total of 25M real images when training from scratch, or 5M images when using transfer learning. <ref type="figure" target="#fig_3">Figure 16</ref> shows the hyperparameters used in each experiment. We performed the baseline runs (configs A-C) using the corresponding standard configurations: StyleGAN2 config F <ref type="bibr" target="#b33">[34]</ref> for the high-resolution datasets in <ref type="figure">Figure 5</ref>, left, and ADA 256?256 baseline config <ref type="bibr" target="#b31">[32]</ref> for the ablations in <ref type="figure">Figure 3</ref> and <ref type="figure">Figure 5</ref>, right.</p><p>Many of our hyperparameters, including discriminator capacity and learning rate, batch size, and generator moving average decay, are inherited directly from the baseline configurations, and kept unchanged in all experiments. In configs C and D, we disable noise inputs <ref type="bibr" target="#b32">[33]</ref>, path length regularization <ref type="bibr" target="#b33">[34]</ref>, and mixing regularization <ref type="bibr" target="#b32">[33]</ref>. In config D, we also decrease the mapping network depth to 2 and set the minibatch standard deviation group size to 4 as recommended in the StyleGAN2-ADA documentation. The introduction of explicit normalization in config D allows us to use the same generator learning rate, 0.0025, for all output resolutions. In <ref type="figure">Figure 5</ref>, right, we show results for path length regularization with weight 0.5 and mixing regularization with probability 0.5.</p><p>Augmentation Since our datasets are horizontally symmetric in nature, we enable dataset xflip augmentation in all our experiments. To prevent the discriminator from overfitting, we enable adaptive discriminator augmentation (ADA) <ref type="bibr" target="#b31">[32]</ref> with default settings for METFACES, METFACES-U, AFHQV2, and BEACHES, but disable it for FFHQ and FFHQ-U. Furthermore, we train METFACES and METFACES-U using transfer learning from the corresponding FFHQ or FFHQ-U snapshot with the lowest FID, similar to Karras et al. <ref type="bibr" target="#b31">[32]</ref>, but start the training from scratch in all other experiments.</p><p>Generator capacity StyleGAN2 defines the number of feature maps on a given layer to be inversely proportional to its resolution, i.e., C[i] = C(s[i]) = min(round(C base /s[i]), C max ), where s[i] is the output resolution of layer i. Parameters C base and C max control the overall capacity of the generator; our baseline configurations use C max = 512 and C base = 2 14 or 2 15 depending on the output resolution. Since StyleGAN2 can be considered to employ critical sampling on all layers, i.e., f c [i] = s[i]/2, we can equally well define the number of feature maps as C[i] = C(2f c [i]). These two definitions are equivalent for configs A-F, but in configs G-R we explicitly set f c [i] ? s[i]/2, which necessitates using the latter definition. In config R, we double the value of both C base and C max to compensate for the reduced capacity of the 1?1 convolutions. In <ref type="figure">Figure 5</ref>, right, we sweep the capacity by multiplying both parameters by 0.5, 1.0, and 2.0.</p><p>R 1 regularization The optimal choice for the R 1 regularization weight ? is highly dependent on the dataset, necessitating a grid search <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b31">32]</ref>. For the baseline config B, we tested ? ? {1, 2, 5, 10, 20} and selected the value that gave the best FID for each dataset. For our configs T and R, we followed the recommendation of Karras et al. <ref type="bibr" target="#b31">[32]</ref> to define ? = ? 0 ? N/M , where N = s 2 N is the number of output pixels and M is the batch size, and performed a grid search over ? 0 ? {0.0002, 0.0005, 0.0010, 0.0020, 0.0050}. For the low-resolution ablations, we chose to use a fixed value ? = 1 for simplicity. The resulting values of ? are shown in <ref type="figure" target="#fig_3">Figure 16</ref>, right.</p><p>Training of config R In this configuration, we blur all images the discriminator sees in the beginning of the training. This Gaussian blur is executed just before the ADA augmentation. We start with ? = 10 pixels, which we ramp to zero over the first 200k images. This prevents the discriminator from focusing too heavily on high frequencies early on. It seems that in this configuration the generator sometimes learns to produce high frequencies with a small delay, allowing the discriminator to trivially tell training data from the generated images without providing useful feedback to the generator. As such, config R is prone to random training failures in the beginning of the training without this trick. The other configurations do not have this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 G-CNN comparison</head><p>In <ref type="figure">Figure 5</ref>, bottom, we compare our config R with config T extended with p4-symmetric group convolutions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. p4 symmetry makes the generator equivariant to 0 ? , 90 ? , 180 ? , and 270 ? rotations, but not to arbitrary rotation angles. In practice, we implement the group convolutions by extending all intermediate activation tensors in the synthesis network with an additional group dimension of size 4 and introducing appropriate redundancy in the convolution weights. We keep the input layer unchanged and introduce the group dimension by replicating each element of z 0 four times. Similarly, we eliminate the group dimension after the last layer by computing an average of the four elements. p4-symmetric group convolutions have 4? as many trainable parameters as the corresponding regular convolutions. To enable an apples-to-apples comparison, we compensate for this increase by halving the values of C base and C max , which brings the number of parameters back to the original level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Energy consumption</head><p>Computation is an essential resource in machine learning projects: its availability and cost, as well as the associated energy consumption, are key factors in both choosing research directions and practical adoption. We provide a detailed breakdown for our entire project in <ref type="table">Table 17</ref> in terms of both GPU time and electricity consumption. We report expended computational effort as single-GPU years (Volta class GPU). We used a varying number of NVIDIA DGX-1s for different stages of the project, and converted each run to single-GPU equivalents by simply scaling by the number of GPUs used.   <ref type="figure">Figure 5</ref>. Results intentionally left out includes additional results that were initially planned, but then left out to improve focus and clarity.</p><p>We followed the Green500 power measurements guidelines <ref type="bibr" target="#b19">[20]</ref>. The entire project consumed approximately 225 megawatt hours (MWh) of electricity. Approximately 70% of it was used for exploratory runs, where we gradually built the new configurations; first in an unstructured manner and then specifically ironing out the new StyleGAN3-T and StyleGAN3-R configurations. Setting up the intermediate configurations between StyleGAN2 and our generators, as well as, the key parameter ablations was also quite expensive at ?15%. Training a single instance of StyleGAN3-R at 1024?1024 is only slightly more expensive (0.9MWh) than training StyleGAN2 (0.7MWh) <ref type="bibr" target="#b33">[34]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Left: Discrete representation Z and continuous representation z are related to each other via convolution with ideal interpolation filter ? s and pointwise multiplication with Dirac comb X s . Right: Nonlinearity ?, ReLU in this example, may produce arbitrarily high frequencies in the continuous-domain ?(z). Low-pass filtering via ? s is necessary to ensure that Z captures the result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>(a) 1D example of a 2? upsampling filter with n = 6, s = 2, f c = 1, and f h = 0.4 (blue). Setting f h = 0.6 makes the transition band wider (green), which reduces the unwanted stopband ripple and thus leads to stronger attenuation. (b) Our alias-free generator, corresponding to configs T and R in Figure 3. The main datapath consists of Fourier features and normalization (Section 3.1), modulated convolutions [34], and filtered nonlinearities (Section 3.2). (c) Flexible layer specifications (config T) with N = 14 and s N = 1024. Cutoff f c (blue) and minimum acceptable stopband frequency f t (orange) obey geometric progression over the layers; sampling rate s (red) and actual stopband f c + f h (green) are computed according to our design constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Example internal representations (3 feature maps as RGB) in StyleGAN2 and our generators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Uncurated sets of samples for StyleGAN2 (baseline config B with Fourier features) and our alias-free generators StyleGAN3-T and StyleGAN3-R are shown in Figures 7 (FFHQ-U), 8 (METFACES-U), 9 (AFHQV2), and 10 (BEACHES). Truncation trick was not used when generating the images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>StyleGAN3-T (ours), FID 3.67StyleGAN3-R (ours), FID 3.66</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Uncurated samples for unaligned FFHQ (FFHQ-U). Truncation was not used. StyleGAN2, FID 18.98 StyleGAN3-T (ours), FID 18.75 StyleGAN3-R (ours), FID 18.75 Uncurated samples for unaligned METFACES (METFACES-U). Truncation was not used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Uncurated samples for AFHQV2. Truncation was not used. Real images from the training set StyleGAN2, FID 5.03 StyleGAN3-T (ours), FID 4.32 StyleGAN3-R (ours), FID 4.57</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Uncurated samples for BEACHES. Truncation was not used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Hand-picked style mixing examples where the coarse (0-6) and fine<ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref><ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref> layers use a different latent code. Mixing regularization was not used during training. Head pose, coarse facial shape, hair length and glasses seem to get inherited from the coarse layers (top row), while coloring and finer facial features are mostly inherited from the fine layers (leftmost column). The control is not quite perfect: e.g., feminine/masculine features are not reliably copied from exactly one of the sources. Moving the fine/coarse boundary fixes this particular issue, but other similar problems persist.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>StyleGAN3, 0 ? slice (f) StyleGAN3, 45 ? slice (g) Comparison methods, 45 ? slice</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>FFHQ dataset is available under Creative Commons BY-NC-SA 4.0 license by NVIDIA Corporation, and consist of images published by respective authors under Creative Commons BY 2.0, Creative Commons BY-NC 2.0, Public Domain Mark 1.0, Public Domain CC0 1.0, and U.S. Government Works license. The METFACES dataset is available under Creative Commons BY-NC 2.0 license by NVIDIA Corporation, and consists of images available under the Creative Commons Zero (CC0) license by the Metropolitan Museum of Art. The original AFHQ dataset is available at https://github.com/clovaai/stargan-v2 under Creative Commons BY-NC 4.0 license by NAVER Corporation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Upsample-nonlinearity-downsample timings in milliseconds using native PyTorch operations vs our optimized CUDA kernel. The benchmarks were run on NVIDIA Titan V GPU, using input size 512?512?32 and filter size n = 6, i.e., n = 12 and n = 24 for up-/downsampling rates of 2 and 4, respectively. Sep. up and Sep. down indicate the use of separable up-/downsampling filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 :</head><label>16</label><figDesc>Left: Hyperparameters used in each experiment. Right: R 1 regularization weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 17 :</head><label>17</label><figDesc>Computational effort expenditure and electricity consumption data for this project. The unit for computation is GPU-years on a single NVIDIA V100 GPU -it would have taken approximately 92 years to execute this project using a single GPU. See the text for additional details about the computation and energy consumption estimates. Early exploration includes early training runs that affected our decision to start this project. Project exploration includes training runs that were done specifically for this project, leading to the final StyleGAN3-T and StyleGAN3-R configurations. These runs were not intended to be used in the paper as-is. Setting up ablations includes hyperparameter tuning for the intermediate configurations and ablation experiments inFigures 3 and 5. Per-dataset tuning includes hyperparameter tuning for individual datasets, mainly the grid search for R 1 regularization weight. Config R at 1024?1024 corresponds to one training run inFigure 5, left, and Other runs in the dataset table includes the remaining runs. Ablation tables includes the low-resolution ablations inFigures 3 and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ConfigurationFID ? EQ-T ? EQ-R ?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Parameter</cell><cell cols="2">FID ? EQ-T ? EQ-R ?</cell><cell>Time Mem.</cell></row><row><cell>A StyleGAN2</cell><cell>5.14</cell><cell>-</cell><cell>-</cell><cell>Filter size n = 4</cell><cell>4.72 57.49</cell><cell>39.70</cell><cell>0.84? 0.99?</cell></row><row><cell>B + Fourier features</cell><cell cols="2">4.79 16.23</cell><cell>10.81</cell><cell>* Filter size n = 6</cell><cell>4.50 66.65</cell><cell>40.48</cell><cell>1.00? 1.00?</cell></row><row><cell>C + No noise inputs</cell><cell cols="2">4.54 15.81</cell><cell>10.84</cell><cell>Filter size n = 8</cell><cell>4.66 65.57</cell><cell>42.09</cell><cell>1.18? 1.01?</cell></row><row><cell>D + Simplified generator</cell><cell cols="2">5.21 19.47</cell><cell>10.41</cell><cell>Upsampling m = 1</cell><cell>4.38 39.96</cell><cell>36.42</cell><cell>0.65? 0.87?</cell></row><row><cell>E + Boundaries &amp; upsampling</cell><cell cols="2">6.02 24.62</cell><cell>10.97</cell><cell>* Upsampling m = 2</cell><cell>4.50 66.65</cell><cell>40.48</cell><cell>1.00? 1.00?</cell></row><row><cell>F + Filtered nonlinearities</cell><cell cols="2">6.35 30.60</cell><cell>10.81</cell><cell>Upsampling m = 4</cell><cell>4.57 74.21</cell><cell>40.97</cell><cell>2.31? 1.62?</cell></row><row><cell>G + Non-critical sampling H + Transformed Fourier features T + Flexible layers (StyleGAN3-T)</cell><cell cols="2">4.78 43.90 4.64 45.20 4.62 63.01</cell><cell>10.84 10.61 13.12</cell><cell>Stopband ft,0 = 2 1.5 * Stopband ft,0 = 2 2.1</cell><cell>4.62 51.10 4.50 66.65</cell><cell>29.14 40.48</cell><cell>0.86? 0.90? 1.00? 1.00?</cell></row><row><cell>R + Rotation equiv. (StyleGAN3-R)</cell><cell cols="2">4.50 66.65</cell><cell>40.48</cell><cell>Stopband ft,0 = 2 3.1</cell><cell>4.68 73.13</cell><cell>41.63</cell><cell>1.36? 1.25?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Colors Impulse response Frequency response Gain (dB)</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Latent</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L2</cell><cell>w 2</cell></row><row><cell></cell><cell></cell><cell cols="2">Mapping</cell><cell>A</cell><cell>Fourier feat.</cell></row><row><cell></cell><cell></cell><cell cols="2">network</cell><cell></cell><cell cols="2">A</cell><cell>Mod EMA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv 1?1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Demod</cell><cell>Conv 3?3 or 1?1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L1</cell><cell>b 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Upsample 2? or 4?</cell></row><row><cell></cell><cell>Half-width</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Custom</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Leaky ReLU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L4</cell><cell>CUDA</cell></row><row><cell cols="2">Cutoff</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L5</cell><cell>kernel</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Downsample 2?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L6</cell></row><row><cell>Passband</cell><cell>Transition band</cell><cell>Stopband</cell><cell></cell><cell></cell><cell></cell><cell>Crop</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ToRGB</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Fixed</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L11</cell></row><row><cell>Attenuation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Learned</cell><cell></cell><cell>L12</cell><cell>w</cell><cell>14</cell><cell>EMA</cell><cell>Cutoff</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L13</cell><cell>Min. stopband</cell></row><row><cell></cell><cell></cell><cell>A</cell><cell>Affine</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">A</cell><cell>Mod</cell><cell>Conv 1?1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sampling rate</cell></row><row><cell></cell><cell>Ripple</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ToRGB</cell><cell>Stopband</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>b</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>14</cell></row><row><cell cols="7">(a) Filter design concepts (b) Our alias-free StyleGAN3 generator architecture</cell><cell>(c) Flexible layers</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.0M, 22.3M and 15.8M parameters, while the training times were 1106, 1576 (+42%) and 2248</figDesc><table><row><cell>Dataset FFHQ-U</cell><cell>Config StyleGAN2</cell><cell>FID ? EQ-T ? EQ-R ? 3.79 15.89 10.79</cell><cell>Ablation</cell><cell>Translation eq. FID ? EQ-T ?</cell><cell cols="3">+ Rotation eq. FID ? EQ-T ? EQ-R ?</cell></row><row><cell>70000 img, 1024 2</cell><cell>StyleGAN3-T (ours)</cell><cell>3.67 61.69 13.95</cell><cell>* Main configuration</cell><cell>4.62 63.01</cell><cell cols="2">4.50 66.65</cell><cell>40.48</cell></row><row><cell>Train from scratch</cell><cell>StyleGAN3-R (ours)</cell><cell>3.66 64.78 47.64</cell><cell>With mixing reg.</cell><cell>4.60 63.48</cell><cell cols="2">4.67 63.59</cell><cell>40.90</cell></row><row><cell>FFHQ</cell><cell>StyleGAN2</cell><cell>2.70 13.58 10.22</cell><cell>With noise inputs</cell><cell>4.96 24.46</cell><cell cols="2">5.79 26.71</cell><cell>26.80</cell></row><row><cell>70000 img, 1024 2</cell><cell>StyleGAN3-T (ours)</cell><cell>2.79 61.21 13.82</cell><cell>Without flexible layers</cell><cell>4.64 45.20</cell><cell cols="2">4.65 44.74</cell><cell>22.52</cell></row><row><cell>Train from scratch</cell><cell>StyleGAN3-R (ours)</cell><cell>3.07 64.76 46.62</cell><cell>Fixed Fourier features</cell><cell>5.93 64.57</cell><cell cols="2">6.48 66.20</cell><cell>41.77</cell></row><row><cell>METFACES-U</cell><cell>StyleGAN2</cell><cell>18.98 18.77 13.19</cell><cell>With path length reg.</cell><cell>5.00 68.36</cell><cell cols="2">5.98 71.64</cell><cell>42.18</cell></row><row><cell>1336 img, 1024 2</cell><cell>StyleGAN3-T (ours)</cell><cell>18.75 64.11 16.63</cell><cell>0.5? capacity</cell><cell>7.43 63.14</cell><cell cols="2">6.52 63.08</cell><cell>39.89</cell></row><row><cell>ADA, from FFHQ-U</cell><cell>StyleGAN3-R (ours)</cell><cell>18.75 66.34 48.57</cell><cell>* 1.0? capacity</cell><cell>4.62 63.01</cell><cell cols="2">4.50 66.65</cell><cell>40.48</cell></row><row><cell>METFACES</cell><cell>StyleGAN2</cell><cell>15.22 16.39 12.89</cell><cell>2.0? capacity</cell><cell>3.80 66.61</cell><cell cols="2">4.18 70.06</cell><cell>42.51</cell></row><row><cell>1336 img, 1024 2 ADA, from FFHQ</cell><cell>StyleGAN3-T (ours) StyleGAN3-R (ours)</cell><cell>15.11 65.23 16.82 15.33 64.86 46.81</cell><cell>* Kaiser filter, n = 6 Lanczos filter, a = 2</cell><cell>4.62 63.01 4.69 51.93</cell><cell cols="2">4.50 66.65 4.44 57.70</cell><cell>40.48 25.25</cell></row><row><cell>AFHQV2 15803 img, 512 2</cell><cell>StyleGAN3-T (ours) StyleGAN2</cell><cell>4.04 60.15 13.51 4.62 13.83 11.50</cell><cell>Gaussian filter, ? = 0.4</cell><cell>5.91 56.89</cell><cell cols="2">5.73 59.53</cell><cell>39.43</cell></row><row><cell>ADA, from scratch</cell><cell>StyleGAN3-R (ours)</cell><cell>4.40 64.89 40.34</cell><cell>G-CNN comparison</cell><cell cols="4">FID ? EQ-T ? EQ-R ? Params Time</cell></row><row><cell>BEACHES</cell><cell>StyleGAN2</cell><cell>5.03 15.73 12.69</cell><cell>* StyleGAN3-T (ours)</cell><cell>4.62 63.01</cell><cell>13.12</cell><cell cols="2">23.3M 1.00?</cell></row><row><cell>20155 img, 512 2</cell><cell>StyleGAN3-T (ours)</cell><cell>4.32 59.33 15.88</cell><cell>+ p4 symmetry [16]</cell><cell>4.69 61.90</cell><cell>17.07</cell><cell cols="2">21.8M 2.48?</cell></row><row><cell>ADA, from scratch</cell><cell>StyleGAN3-R (ours)</cell><cell>4.57 63.66 37.42</cell><cell>* StyleGAN3-R (ours)</cell><cell>4.50 66.65</cell><cell>40.48</cell><cell cols="2">15.8M 1.37?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>2 2.1 and f t,N = f c,N ? 2 0.3 in most of our tests. Given the values of f c [i] and f t [i], the sampling rate s[i] and transition band half-width f h [i] are then determined bys[i] = exp 2 log 2 min(2 ? f t [i], s N ) , f h [i] = max(f t [i], s[i]/2) ? f c [i].(30)The sampling rate is rounded up to the nearest power of two that satisfies s[i] ? 2f t [i], but it is not allowed to exceed the output resolution. The transition band half-width is selected to satisfy eitherf c [i] + f h [i] = f t [i] or f c [i] + fh [i] = s[i]/2, whichever yields a higher value. We consider f c [i] to represent the output frequency content of layer i, for i ? {0, 1, . . . , N ? 1}, whereas the input is represented by f c [max(i ? 1, 0)]. Thus, we construct the corresponding upsampling filter according to f c [max(i ? 1, 0)] and f h [max(i ? 1, 0)] and the downsampling filter according to f c [i] and f h [i]. The nonlinearity is evaluated at a temporary sampling rate</figDesc><table /><note>s = max(s[i], s[max(i ?</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Consider nearest neighbor upsampling. If we upsample a 4?4 image to 8?8, the original pixels will be clearly visible, allowing one to reliably distinguish between even and odd pixels. Since the same is true on all scales, this (leaked) information makes it possible to reconstruct even the absolute pixel coordinates. With better filters such as bilinear or bicubic, the clues get less pronounced, but are nevertheless evident for the generator.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Here, fc and f h correspond to the output (downsampling) filter of each layer. The input (upsampling) filters are based on the properties of the incoming signal, i.e., the output filter parameters of the previous layer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Real images from the training setStyleGAN2, FID 3.79</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Input of the upsampling is stored in shared memory, but the filter weights can be stored in CUDA constant memory where they can be accessed without a separate load instruction.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;16</title>
		<meeting>12th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">StyleFlow: Attribute-conditioned exploration of StyleGANgenerated images using conditional continuous normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Only a matter of style: Age transformation using a style-based regression model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno>abs/2102.02754</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image generators with conditionally-independent pixel synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Anokhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Demochkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khakhulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sterkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Korzhenkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Why do deep convolutional networks generalize so poorly to small image transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Azulay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">184</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>abs/2103.10951</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rewriting a deep generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">GAN dissection: Visualizing and understanding generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Theory of remote image formation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Blahut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Network bending: Expressive manipulation of deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Broad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Leymarie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grierson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EvoMUSART</title>
		<meeting>EvoMUSART</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Truly shift-invariant convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dokmani?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning continuous image representation with local implicit image function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">StarGAN v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning temporal coherence via self-supervision for GAN-based video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Group equivariant generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghafurian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">SWAGAN: A style-based wavelet-driven generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bermano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno>abs/2102.06108</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Power measurement tutorial for the Green500 list</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pyla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<ptr target="https://www.top500.org/green500/resources/tutorials/" />
		<imprint>
			<date type="published" when="2020-03-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image processing using multi-code GAN prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Making anime faces with stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gwern</surname></persName>
		</author>
		<ptr target="https://www.gwern.net/Faces#stylegan2-ext-modifications" />
		<imprint>
			<date type="published" when="2021-06-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gancraft</surname></persName>
		</author>
		<idno>abs/2104.07659</idno>
		<title level="m">Unsupervised 3D neural rendering of minecraft worlds. CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GANSpace: Discovering interpretable GAN controls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Designed to deceive: Do these people look real to you? The New York Times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>White</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How much position information do convolutional neural networks encode?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D B</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the &quot;steerability&quot; of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nonrecursive digital filter design using the I0-sinh window function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1974 IEEE International Symposium on Circuits &amp; Systems</title>
		<meeting>1974 IEEE International Symposium on Circuits &amp; Systems</meeting>
		<imprint>
			<date type="published" when="1974" />
			<biblScope unit="page" from="20" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On translation invariance in CNNs: Convolutional layers can exploit absolute spatial location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">S</forename><surname>Kayhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shift equivariance in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Manfredi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV 2020 Workshops</title>
		<meeting>ECCV 2020 Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PULSE: Self-supervised photo upsampling via latent space exploration of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Which training methods for GANs do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Discrete-Time Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Schafer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Prentice Hall Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Swapping autoencoder for deep image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On buggy resizing libraries and surprising subtleties in FID calculation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno>abs/2104.11222</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">StyleCLIP: Text-driven manipulation of StyleGAN imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<idno>abs/2103.17249</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Swish: a self-gated activation function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1710.05941</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/2102.12092</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Encoding in style: A StyleGAN encoder for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nitzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Canny AI: Imagine world leaders singing. fxguide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seymour</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Communication in the presence of noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Institute of Radio Engineers</title>
		<meeting>Institute of Radio Engineers</meeting>
		<imprint>
			<date type="published" when="1949" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Closed-form factorization of latent semantics in GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adversarial generation of continuous images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Skorokhodov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ignatyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Spatially controllable image synthesis with internal representation collaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yonetsuji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<idno>abs/1811.10153</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">MoCoGAN: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Filters for Common Resampling Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Turkowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Academic Press Professional, Inc., USA</publisher>
			<biblScope unit="page" from="147" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Impact of aliasing on generalization in deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Romijnders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Self-organization of orientation sensitive cells in striate cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Der Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="100" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">General E(2)-equivariant steerable CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cesa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning steerable filters for rotation equivariant CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Storath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep scale-spaces: Equivariance over scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Positional encoding as spatial inductive bias in GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Artificial fingerprinting for generative models: Rooting deepfake attribution in training data. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Skripniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdelnabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Delving deeper into anti-aliasing in ConvNets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
