<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Densely connected normalizing flows</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Grci?</surname></persName>
							<email>matej.grcic@fer.hrivan.grubisic@fer.hrsinisa.segvic@fer.hr</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Grubi?i?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sini?a</forename><surname>?egvi?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Densely connected normalizing flows</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Normalizing flows are bijective mappings between inputs and latent representations with a fully factorized distribution. They are very attractive due to exact likelihood evaluation and efficient sampling. However, their effective capacity is often insufficient since the bijectivity constraint limits the model width. We address this issue by incrementally padding intermediate representations with noise. We precondition the noise in accordance with previous invertible units, which we describe as crossunit coupling. Our invertible glow-like modules increase the model expressivity by fusing a densely connected block with Nystr?m self-attention. We refer to our architecture as DenseFlow since both cross-unit and intra-module couplings rely on dense connectivity. Experiments show significant improvements due to the proposed contributions and reveal state-of-the-art density estimation under moderate computing budgets. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the main tasks of modern artificial intelligence is to generate images, audio waveforms, and natural-language symbols. To achieve the desired goal, the current state of the art uses deep compositions of non-linear transformations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> known as deep generative models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7]</ref>. Formally, deep generative models estimate an unknown data distribution p D given by a set of i.i.d. samples D = {x 1 , ..., x n }. The data distribution is approximated with a model distribution p ? defined by the architecture of the model and a set of parameters ?. While the architecture is usually handcrafted, the set of parameters ? is obtained by optimizing the likelihood across the training distribution p D :</p><formula xml:id="formula_0">? * = argmin ??? E x?p D [? ln p ? (x)].<label>(1)</label></formula><p>The learned joint distribution p(z i , e i ) approximates the product of the target distributions p * (z i ) and p * (e i ), which is explained in more detail in Appendix D. We transform the introduced noise e i with element-wise affine transformation. Parameters of this transformation are computed by a learned non-linear transformation g i (z &lt;i ) of previous representations z &lt;i = [z 0 , ..., z i?1 ]. The resulting layer h i can be defined as:</p><formula xml:id="formula_1">z (aug) i = h i (z i , e i , z &lt;i ) = [z i , ? e i + ?], (?, ?) = g i (z &lt;i ).<label>(6)</label></formula><p>Square brackets [?, ?] denote concatenation along the features dimension. In order to compute the likelihood for (z i , e i ), we need the determinant of the jacobian</p><formula xml:id="formula_2">?z (aug) i ?[z i , e i ] = I 0 0 diag(?) .<label>(7)</label></formula><p>Now we can express p(z i , e i ) in terms of p(z (aug) i ) according to <ref type="bibr" target="#b3">(4)</ref>:</p><formula xml:id="formula_3">ln p(z i , e i ) = ln p(z (aug) i ) + ln | det diag(?)|.<label>(8)</label></formula><p>We join equations <ref type="bibr" target="#b4">(5)</ref> and <ref type="formula" target="#formula_3">(8)</ref> into a single step:</p><formula xml:id="formula_4">ln p(z i ) ? E ei?p * (ei) [ln p(z (aug) i ) ? ln p * (e i ) + ln | det diag(?)|].<label>(9)</label></formula><p>We refer the transformation h i as cross-unit coupling since it acts as an affine coupling layer <ref type="bibr" target="#b17">[17]</ref> over a group of previous invertible units. The latent part of the input tensor is propagated without change, while the noise part is linearly transformed. The noise transformation can be viewed as reparametrization of the distribution from which we sample the noise <ref type="bibr" target="#b3">[4]</ref>. Note that we can conveniently recover z i from z (aug) i by removing the noise dimensions. This step is performed during model sampling. <ref type="figure">Fig. 1</ref> compares the standard normalizing flow (a) normalizing flow with input augmentation <ref type="bibr" target="#b24">[24]</ref> (b) and the proposed densely connected incremental augmentation with cross-unit coupling (c). Each flow unit f DF i consists of several invertible modules m i,j and cross-unit coupling h i . The main novelty of our architecture is that each flow unit f DF i+1 increases the dimensionality with respect to its predecessor f DF i . Cross-unit coupling h i augments the latent variable z i with affinely transformed noise e i . Parameters of the affine noise transformation are obtained by an arbitrary function g i which accepts all previous variables z &lt;i . Note that reversing the direction does not require evaluating g i since we are only interested in the value of z i . For further clarification, we show the likelihood computation for the extended framework.  <ref type="figure">Figure 1</ref>: Standard normalizing flow <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref> (a), normalizing flow with augmented input <ref type="bibr" target="#b24">[24]</ref> (b), and the proposed incremental augmentation with cross-unit coupling (c). Unlike (b) which adds noise only to the input, (c) adds noise to the output of every unit except the last.</p><formula xml:id="formula_5">Invertible module m i,N z i-1 (aug) z i e i~N (0, I) [z i ,e i ??+?] g i (?) (?,?) [z i ,e i ]<label>(c)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1 (Likelihood computation)</head><p>Let m 1 and m 2 be the bijective mappings from z 0 to z 1 and z (aug) 1 to z 2 , respectively. Let h 1 be the cross-unit coupling from z 1 to z (aug)</p><formula xml:id="formula_6">1 , z (aug) 1 = [z 1 , ? e 1 +?].</formula><p>Assume ? and ? are computed by any non-invertible neural network g 1 . The network accepts z 0 as the input. We calculate log likelihood of the input z 0 according to the following sequence of equations: [transformation, cross-unit coupling, transformation, termination].</p><p>ln p(z 0 ) = ln p(z 1 ) + ln | det J f1 |,</p><formula xml:id="formula_7">ln p(z 1 ) ? E e1?p * (e1) [ln p(z (aug) 1 ) ? ln p(e 1 ) + ln | det diag(?)|], (?, ?) = g 1 (z 0 ),<label>(10)</label></formula><formula xml:id="formula_8">ln p(z (aug) 1 ) = ln p(z 2 ) + ln | det J f2 |,<label>(11)</label></formula><p>ln p(z 2 ) = ln N (z 2 ; 0, I).</p><p>We approximate the expectation using MC sampling with a single sample during training and a few hundreds of samples during evaluation to reduce the variance of the likelihood. Note however that our architecture generates samples with a single pass since the inverse does not require MC sampling.</p><p>We repeatedly apply the cross-unit coupling h i throughout the architecture to achieve incremental augmentation of intermediate latent representations. Consequently, the data distribution is modeled in a latent space of higher dimensionality than the input space <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b26">26]</ref>. This enables better alignment of the final latent representation with the NF prior. We materialize the proposed expansion of the normalizing flow framework by developing an image-oriented architecture which we call DenseFlow.  <ref type="figure">Figure 2</ref>: A glow-like module m i,j consist of ActNorm, 1x1 convolution and intra-module affine coupling. The proposed intra-module coupling fuses the global context recovered by fast self-attention <ref type="bibr" target="#b28">[28]</ref> and local correlations extracted by densely connected convolutions <ref type="bibr" target="#b29">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image-oriented invertible module</head><p>We propose a glow-like invertible module (also known as step of flow <ref type="bibr" target="#b19">[19]</ref>) consisting of activation normalization, 1 ? 1 convolution and intra-module affine coupling layer. The attribute "intra-module" emphasizes distinction with respect to cross-unit coupling. Different than in the original glow design, our coupling network leverages advanced transformations based on dense connectivity and fast selfattention. All three layers are designed to capture complex data dependencies while keeping tractable Jacobians and efficient inverse computation. For completeness, we start by reviewing elements of the original glow module <ref type="bibr" target="#b19">[19]</ref>.</p><p>ActNorm <ref type="bibr" target="#b19">[19]</ref> is an invertible substitute for batch normalization <ref type="bibr" target="#b30">[30]</ref>. It performs affine transformation with per-channel scale and bias parameters:</p><formula xml:id="formula_11">y i,j = s x i,j + b.<label>(14)</label></formula><p>Scale and bias are calculated as the variance and mean of the initial minibatch.</p><p>Invertible 1 ? 1 Convolution is a generalization of channel permutation <ref type="bibr" target="#b19">[19]</ref>. Convolutions with 1 ? 1 kernel are not invertible by construction. Instead, a combination of orthogonal initialization and the loss function keeps the kernel inverse numerically stable. The normalizing flow loss maximizes ln | det J f | which is equivalent to maximizing i ln |? i |, where ? i are eigenvalues of the Jacobian. Maintaining a relatively large amplitude of the eigenvalues ensures a stable inversion. The Jacobian of this transformation can be efficiently computed by LU-decomposition <ref type="bibr" target="#b19">[19]</ref>.</p><p>Affine Coupling <ref type="bibr" target="#b18">[18]</ref> splits the input tensor x channel-wise into two halves x 1 and x 2 . The first half is propagated without changes, while the second half is linearly transformed <ref type="bibr" target="#b15">(15)</ref>. The parameters of the linear transformation are calculated from the first half. Finally, the two results are concatenated as shown in <ref type="figure">Fig. 2</ref>.</p><formula xml:id="formula_12">y 1 = x 1 , y 2 = s x 2 + t, (s, t) = coupling_net(x 1 ).<label>(15)</label></formula><p>Parameters s and t are calculated using a trainable network which is typically implemented as a residual block <ref type="bibr" target="#b18">[18]</ref>. However, this setting can only capture local correlations. Motivated by recent advances in discriminative architectures <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32]</ref>, we design our coupling network to fuse both global context and local correlations as shown in <ref type="figure">Fig. 2</ref>: First, we project the input into a low-dimensional manifold. Next, we feed the projected tensor to a densely-connected block <ref type="bibr" target="#b29">[29]</ref> and self-attention module <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b33">33]</ref>. The densely connected block captures the local correlations <ref type="bibr" target="#b34">[34]</ref>, while the self-attention module captures the global spatial context. Outputs of these two branches are concatenated and blended through a BN-ReLU-Conv unit. As usual, the obtained output parameterizes the affine coupling transformation <ref type="bibr" target="#b15">(15)</ref>. Differences between the proposed coupling network and other network designs are detailed in related work.</p><p>It is well known that full-fledged self-attention layers have a very large computational complexity. This is especially true in the case of normalizing flows which require many coupling layers and large latent dimensionalities. We alleviate this issue by approximating the keys and queries with their low-rank approximations according to the Nystrom method <ref type="bibr" target="#b28">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-scale architecture</head><p>We propose an image-oriented architecture which extends multi-scale Glow <ref type="bibr" target="#b19">[19]</ref> with incremental augmentation through cross-unit coupling. Each DenseFlow block consists of several DenseFlow units and resolves a portion of the latent representation according to a decoupled normal distribution <ref type="bibr" target="#b18">[18]</ref>. Each DenseFlow unit f DF i consists of N glow-like modules (m i = m i,N ? ? ? ? ? m i,1 ) and cross-unit coupling (h i ). Recall that each invertible module m i,j contains the affine coupling network from <ref type="figure">Fig. 2</ref> as described Section 2.2.</p><p>The input to each DenseFlow unit is the output of the previous unit augmented with the noise and transformed in the cross-unit coupling fashion. The number of introduced noise channels is defined as the growth-rate hyperparameter. Generally, the number of invertible modules in latter DenseFlow units should increase due to enlarged latent representation. We stack M DenseFlow units to form a DenseFlow block. The last invertible unit in the block does not have the corresponding cross-unit coupling. We stack multiple DenseFlow blocks to form a normalizing flow with large capacity. We decrease the spatial resolution and compress the latent representation by introducing a squeeze-and-drop modules <ref type="bibr" target="#b18">[18]</ref> between each two blocks. A squeeze-and-drop module applies space-to-channel reshaping and resolves half of the dimensions according to the prior distribution. We denote the developed architecture as DenseF low-L-k, where L is the total number of invertible modules while k denotes the growth rate. The developed architecture uses two independent levels of skip connections. The first level (intra-module) is formed of skip connections inside every coupling network. The second level (cross-unit) connects DenseFlow units at the top level of the architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Our experiments compare the proposed DenseFlow architecture with the state of the art. Quantitative experiments measure the accuracy of density estimation and quality of generated samples, analyze the computational complexity of model training, as well as ablate the proposed contributions. Qualitative experiments present generated samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Density estimation</head><p>We study the accuracy of density estimation on CIFAR-10 <ref type="bibr" target="#b35">[35]</ref>, ImageNet <ref type="bibr" target="#b36">[36]</ref> resized to 32 ? 32 and 64 ? 64 pixels and CelebA <ref type="bibr" target="#b37">[37]</ref>. Tab. 1 compares generative performance of various contemporary models. Models are grouped into four categories based on factorization of the probability density. Among these, autoregressive models have been achieving the best performance. Image Transformer <ref type="bibr" target="#b38">[38]</ref> has been the best on ImageNet32, while Routing transformer <ref type="bibr" target="#b39">[39]</ref> has been the best on Ima-geNet64. The fifth category contains hybrid architectures which combine multiple approaches into a single model. Hybrid models have succeeded to outperform many factorization-specific architectures.</p><p>The bottom row of the table presents the proposed DenseFlow architecture. We use the same DenseFlow-74-10 model in all experiments except ablations in order to illustrate the general applicability of our concepts. The first block of DenseFlow-74-10 uses 6 units with 5 glow-like modules in each DenseFlow unit, the second block uses 4 units with 6 modules, while the third block uses a single unit with 20 modules. We use the growth rate of 10 in all units. Each intra-module coupling starts with a projection to 48 channels. Subsequently, it includes a dense block with 7 densely connected layers, and the Nystr?m self-attention module with a single head. Since the natural images are discretized, we apply variational dequantization <ref type="bibr" target="#b27">[27]</ref> to obtain continuous data which is suitable for normalizing flows.</p><p>On CIFAR-10, DenseFlow reaches the best recognition performance among normalizing flows, which equals to 2.98 bpd. Models trained on ImageNet32 and ImageNet64 achieve state-of-the-art density estimation corresponding to 3.63 and 3.35 bpd respectively. The obtained recognition performance is significantly better than the previous state of the art (3.77 and 3.43 bpd). Finally, our model achieves competetive results on the CelebA dataset, which corresponds to 1.99 bpd. The likelihood is computed using 1000 MC samples for CIFAR-10 and 200 samples for CelebA and ImageNet. The reported results are averaged over three runs with different random seeds. One MC sample is enough for accurate log-likelihood estimation since the per-example standard deviation is already about 0.01 bpd and a validation dataset size N additionally divides it by ? N . The reported results are averaged over seven runs with different random seeds. Training details are available in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Computational complexity</head><p>Deep generative models require an extraordinary amount of time and computation to reach state-ofthe-art performance. Moreover, contemporary architectures have scaling issues. For example, VFlow <ref type="bibr" target="#b24">[24]</ref> requires 16 GPUs and two months to be trained on the ImageNet32 dataset, while the NVAE <ref type="bibr" target="#b56">[56]</ref> requires 24 GPUs and about 3 days. This limits downstream applications of developed models and slows down the rate of innovation in the field. In contrast, the proposed DenseFlow design places a great emphasis on the efficiency and scalability.</p><p>Tab. 2 compares the time and memory consumption of the proposed model with respect to competing architectures. We compare our model with VFlow <ref type="bibr" target="#b24">[24]</ref> and NVAE <ref type="bibr" target="#b56">[56]</ref> due to similar generative performance on CIFAR-10 and CelebA, respectively. We note that RTX 3090 and Tesla V100 deliver similar performance, while RTX2080Ti has a slightly lower performance compared to the previous two. However, since we model relatively small images, GPU utilization is limited by I/O performance. In our experiments, training the model for one epoch on any of the aforementioned GPUs had similar duration. Therefore, we can still make a fair comparison. Please note that we are unable to include approaches based on transformers <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b58">58]</ref> since they do not report the computational effort for model training.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image generation</head><p>Normalizing flows can efficiently generate samples. The generation is performed in two steps. We first sample from the latent distribution and then transform the obtained latent tensor through the inverse mapping. <ref type="figure" target="#fig_3">Fig. 4</ref> shows unconditionally generated images with the model trained on ImageNet64. <ref type="figure" target="#fig_4">Fig. 5</ref> shows generated images using the model trained on CelebA. In this case, we modify the latent distribution by temperature scaling with factor 0.8 <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b56">56]</ref>. Generated images show diverse hairstyles, skin tones and backgrounds. More generated samples can be found in Appendix G. The   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Visual quality</head><p>The ability to generate high fidelity samples is crucial for real-world applications of generative models.</p><p>We measure the quality of generated samples using the FID score <ref type="bibr" target="#b59">[59]</ref>. The FID score requires a large corpus of generated samples in order to provide an unbiased estimate. Hence, we generate 50k samples for CIFAR-10, and CelebA, and 200k samples for ImageNet. The samples are generated using the model described in Sec. 3.1. The generated ImageNet32 samples achieve a FID score of 38.8, the CelebA samples achieve 17.1 and CIFAR-10 samples achieve 34.9 when compared to the corresponding training dataset. When compared with corresponding validation datasets, we achieve 37.1 on CIFAR10 and 38.5 on ImageNet32.</p><p>Tab. 3 shows a comparison with FID scores of other generative models. Our model outperforms contemporary autoregressive models <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b60">60]</ref> and the majority of normalizing flows <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b19">19]</ref>. Our FID score is comparable with the first generation of GANs. Similar to other NF models, the achieved FID score is still an order of magnitude higher than current state of the art <ref type="bibr" target="#b62">[62]</ref>. The results for PixelCNN, DCGAN, and WGAN-GP are taken from <ref type="bibr" target="#b60">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablations</head><p>Tab. 4 explores the contributions of incremental augmentation and dense connectivity in cross-unit and intra-module coupling transforms. We decompose cross-unit coupling into incremental augmentation of the flow dimensionality (column 1) and affine noise transformation (column 2). Column 3 ablates the proposed intra-module coupling network based on fusion of fast self-attention and a densely connected convolutional block with the original Glow coupling <ref type="bibr" target="#b19">[19]</ref>.</p><p>The bottom row of the All models are trained on CIFAR-10 for 300 epochs and then fine-tuned for 10 epochs. We use the same training hyperparameters for all models. The proposed cross-unit coupling improves the density estimation from 3.42 bpd (row 1) to 3.37 bpd (row 3) starting from a model with the standard glow modules. When a model is equipped with our intra-module coupling, cross-unit coupling leads to improvement from 3.14 bpd (row 4) to 3.07 bpd (row 6). Hence, the proposed cross-unit coupling <ref type="table">Table 3</ref>: Evaluation of FID score on CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model FID ? Autoregressive Models</head><p>PixelCNN <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b60">60]</ref> 65.93 PixelIQN <ref type="bibr" target="#b60">[60]</ref> 49.46</p><p>Normalizing Flows i-ResNet <ref type="bibr" target="#b61">[61]</ref> 65.01 Glow <ref type="bibr" target="#b19">[19]</ref> 46.90 Residual flow <ref type="bibr" target="#b52">[52]</ref> 46.37 ANF <ref type="bibr" target="#b26">[26]</ref> 30.60</p><p>GANs DCGAN <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b60">60]</ref> 37.11 WGAN-GP <ref type="bibr" target="#b63">[63,</ref><ref type="bibr" target="#b60">60]</ref> 36.40 DA-StyleGAN V2 <ref type="bibr" target="#b62">[62]</ref> 5.79</p><p>Diffusion models VDM <ref type="bibr" target="#b47">[47]</ref> 4.00 DDPM <ref type="bibr" target="#b13">[13]</ref> 3.17 UDM (RVE) + ST <ref type="bibr" target="#b45">[45]</ref> 2.33</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid Architectures</head><p>SurVAE-flow <ref type="bibr" target="#b34">[34]</ref> 49.03 mAR-SCF <ref type="bibr" target="#b54">[54]</ref> 33.06 VAEBM <ref type="bibr" target="#b64">[64]</ref> 12.19 DenseFlow-74-10 (ours) 34.90 improves the density estimation in all experiments. Both components of cross-unit coupling are important. Models with preconditioned noise outperform models with simple white noise (row 2 vs row 3, and row 5 vs row 6). A comparison of rows 1-3 with rows 4-6 reveals that the proposed intra-module coupling network also yields significant improvements. We have performed two further ablation experiments with the same model. Densely connected cross-coupling contributes 0.01 bpd in comparison to preconditioning noise with respect to the previous representation only. Self-attention module contributes 0.01 bpd with respect to the model with only DenseBlock coupling on ImageNet 32 ? 32. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>VFlow <ref type="bibr" target="#b24">[24]</ref> increases the dimensionality of a normalizing flow by concatenating input with a random variable drawn from p(e|x). The resulting optimization maximizes the lower bound E e?p * (e|x) [ln p(x, e) ? ln p * (e|x)], where each term is implemented by a separate normalizing flow. Similarly, ANF <ref type="bibr" target="#b26">[26]</ref> draws a connection between maximizing the joint density p(x, e) and lower-bound optimization <ref type="bibr" target="#b3">[4]</ref>. Both approaches augment only the input variable x while we augment latent representations many times throughout our models.</p><p>Surjective flows <ref type="bibr" target="#b34">[34]</ref> decrease the computational complexity of the flow by reducing the dimensionality of deep layers. However, this also reduces the generative capacity. Our approach achieves a better generative performance under affordable computational budget due to gradual increase of the latent dimensionality and efficient coupling.</p><p>Invertible DenseNets <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b53">53]</ref> apply skip connections within invertible residual blocks <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b52">52]</ref>. However, this approach lacks a closed-form inverse, and therefore can generate data only through slow iterative algorithms. Our approach leverages skip connections both in cross-unit and intramodule couplings, and supports fast analytical inverse by construction.</p><p>Models with an analytical inverse allocate most of their capacity to coupling networks <ref type="bibr" target="#b17">[17]</ref>. Early coupling networks were implemented as residual blocks <ref type="bibr" target="#b18">[18]</ref>. Recent work <ref type="bibr" target="#b27">[27]</ref> increases the coupling capacity by stacking convolutional and multihead self-attention layers into a gated residual <ref type="bibr" target="#b66">[66,</ref><ref type="bibr" target="#b49">49]</ref>. However, heavy usage of self-attention radically increases the computational complexity. Contrary to stacking convolutional and self-attention layers in alternating fashion, the design of our network uses two parallel modules. Outputs of the these two modules are fused into a single output. SurVAE <ref type="bibr" target="#b34">[34]</ref> expresses the coupling network as a densely connected block <ref type="bibr" target="#b29">[29]</ref> with residual connection. In comparison with <ref type="bibr" target="#b34">[34]</ref>, our intra-module coupling omits residual connectivity, decreases the number of densely connected layers and introduces a parallel branch with Nystr?m self-attention. Thus, our intra-module coupling fuses local cues with the global context.</p><p>Normalizing flow capacity can be further increased by adding complexity to the latent prior p(z).</p><p>Autoregressive prior <ref type="bibr" target="#b54">[54]</ref> may deliver better density estimation and improved visual quality of the generated samples. However, the computational cost of sample generation grows linearly with spatial dimensionality. Joining this approach with the proposed incremental latent variable augmentation could be a suitable direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Normalizing flows allow principled recovery of the likelihood by evaluating factorized latent activations. However, their efficiency is hampered by the bijectivity constraint since it determines the model width. We propose to address this issue by incremental augmentation of intermediate latent representations. The introduced noise is preconditioned with respect to preceding representations throughout cross-unit affine coupling. We also propose an improved design of intra-module coupling transformations within glow-like invertible modules. We express these transformations as a fusion of local correlations and the global context captured by self-attention. The resulting DenseFlow architecture sets the new state-of-the-art in likelihood evaluation on ImageNet while requiring a relatively small computational budget. Our results imply that the expressiveness of a NF does not only depend on latent dimensionality but also on its distribution across the model depth. Moreover, expressiveness of a NF can be further improved by conditioning the introduced noise with the proposed densely connected cross-unit coupling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Broader impact</head><p>This paper introduces a new generative model called DenseFlow, which can be trained to achieve stateof-the-art density evaluation under moderate computational budget. Fast convergence and modest memory footprint lead to relatively small environmental impact of training and favor applicability to many downstream tasks. Technical contributions of this paper do not raise any particular ethical challenges. However, image generation has known issues related to bias and fairness <ref type="bibr" target="#b67">[67]</ref>. In particular, samples generated by our method will reflect any kind of bias from the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Limitations</head><p>The proposed DenseFlow model is based on the extended NF framework. However, since it uses Monte Carlo sampling by construction to estimate the likelihood, its characteristics slightly deviate from standard NF models.</p><p>Aggressive application of the cross-unit coupling can lead to overgrowth of the latent tensor. Therefore, the developed architecture can become computationally intractable. This problem can be alleviated by using an appropriate growth rate and careful application of the cross-unit coupling step.</p><p>Replacing standard glow modules <ref type="bibr" target="#b19">[19]</ref> with the proposed glow-like modules which fuse local correlations and global context increases the expressiveness of the resulting normalizing flow at cost of an increased number of trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Environmental impact</head><p>Our DenseFlow model achieves competitive results with significantly less computation. Our experiments were conducted using private infrastructure powered by public grid, which has a carbon efficiency of 0.329 kgCO2eq/kWh. A cumulative of 1120 hours of computation was performed for the main experiments. According to <ref type="bibr" target="#b68">[68]</ref>, total emissions are estimated to be 110.54 kgCO2eq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Training details</head><p>We train the proposed DenseFlow-74-10 architecture on ImageNet32 for 20 epoch using Adamax optimizer with learning rate set to 10 ?3 and batch size 64. We augment the training data by applying random horizontal flip with the probability of 0.5. We apply linear warm-up of the learning rate in the first 5000 epochs. During training, learning rate is exponentially decayed by a factor of 0.95 after every epoch. The model is fine-tuned using a learning rate of 2 ? 10 ?5 for 2 epochs. Similarly, the model is trained for 10 epoch on ImageNet64, 50 epochs on CelebA and 580 epochs on CIFAR-10. In the case of CIFAR-10, we decay the learning rate by a factor of 0.9975. The model is fine-tuned for 1 epoch on ImageNet64, 5 epochs on CelebA and 70 epochs on CIFAR-10. We use batch size of 64 for CIFAR-10 and 32 for CelebA and ImageNet64. Other hyperparameters are the same as in ImageNet32 training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proofs</head><p>D.1 Proof of Equation <ref type="formula">(5)</ref> We denote target distributions by p * and learned distributions by p. Let zi denote the input, which we consider to be distributed according to p * (zi). Let ei be noise independent of zi with a known distribution p * (ei). Let p(hi) be a Gaussian distribution and f a function representing a normalizing flow: hi = f (zi, ei). The normalizing flow distribution p(zi, ei) = p(hi) det ?hi ?(zi, ei)</p><p>approximates the true distribution p * (zi, ei), which is factorized as the product of p * (zi) and p * (ei). We do not have a guarantee that p(zi) = p(zi, ei)/p * (ei).</p><p>To get the density p(zi), we have to marginalize p(zi, ei):</p><formula xml:id="formula_14">p(zi) = p(zi, ei) dei .<label>(17)</label></formula><p>We can efficiently estimate the integral using importance sampling: p(zi) = p(zi, ei) p * (ei) p * (ei) dei</p><p>= E e i ?p * (e i ) p(zi, ei) p * (ei) .</p><p>Log-likelihood can be computed as:</p><p>ln p(zi) = ln E e i ?p * (e i ) p(zi, ei) p * (ei) .</p><p>By applying Jensen's inequality, we obtain a lower bound on the log-likelihood, ln p(zi) ? E e i ?p * (e i ) [ln p(zi, ei) ? ln p * (ei)] ,</p><p>which corresponds to Eq. (5).</p><p>It can be argued that increasing the dimensionality of the latent representation can lead to training set memorization. We show that this is not the case when using DenseFlow. Following <ref type="bibr" target="#b56">[56]</ref>, we first generate faces using the model trained on CelebA, and then compute the L2 distance between the generated and the training images. The L2 distance is computed over 42 ? 42 center crop, so it captures only face pixels. <ref type="figure">Fig. 6</ref> shows generated images in the first column, followed by the five most similar training images. <ref type="figure">Figure 6</ref>: The generated faces and the five most similar samples from training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Implementation</head><p>Significant part of our experimental implementation benefited from <ref type="bibr" target="#b34">[34]</ref> whose code was publicly released under the MIT license. Our code can be found in supplementary material together with the link to model parameters.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G More samples</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 Figure 3 :</head><label>33</label><figDesc>shows the final architecture of the proposed model. Gray squares represent DenseFlow units. Cross-unit coupling is represented with blue dots and dashed skip connections. Finally, squeezeand-drop operations between successive DenseFlow blocks are represented by dotted squares. The proposed DenseFlow design applies invertible but less powerful transformations (e.g. convolution 1 ? 1) on tensors of larger dimensionality. On the other hand, powerful non-invertible transformations The proposed DenseFlow architecture. DenseFlow blocks consist of DenseFlow units (f DF i ) and a Squeeze-and-Drop module [18]. DenseFlow units are densely connected through cross-unit coupling (h i ). Each DenseFlow unit includes multiple invertible modules (m i,j ) from Fig. 2. such as coupling networks perform most of their operations on lower-dimensional tensors. This leads to resource-efficient training and inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>developed</head><label></label><figDesc>DenseFlow-74-10 model generates minibatch of 128 CIFAR-10 samples for 0.96 sec. The result is averaged over 10 runs on RTX 3090.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Samples from DenseFlow-74-10 trained on ImageNet 64 ? 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Samples from DenseFlow-74-10 trained on CelebA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>ImageNet 32 ? 32 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>CIFAR-10 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>CelebA samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>ImageNet 64 ? 64 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1 z i-2 Flow unit f DF i z 0</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>e ~ p(e|x)</cell><cell></cell></row><row><cell cols="2">x Invertible module m 1,N Invertible module m 1,1 ??? Flow unit f 1 Invertible unit (m 1 )</cell><cell cols="4">[x,e] Invertible module m 1,1 ??? z i-Invertible module m 1,N Invertible module m i,1 ??? Invertible unit (m i ) Flow unit f 1 Invertible unit (m 1 )</cell><cell>???</cell></row><row><cell></cell><cell>z 1</cell><cell></cell><cell>z 1</cell><cell></cell></row><row><cell>Flow unit f 2</cell><cell></cell><cell>Flow unit f 2</cell><cell></cell><cell>Cross-unit</cell></row><row><cell>Invertible unit (m 2 )</cell><cell>Invertible module m 2,1</cell><cell>Invertible unit (m 2 )</cell><cell>Invertible module m 2,1</cell><cell>coupling (h i )</cell></row><row><cell></cell><cell>???</cell><cell></cell><cell>???</cell><cell></cell></row><row><cell cols="2">Invertible module m 2,N</cell><cell cols="2">Invertible module m 2,N</cell><cell>z i</cell><cell>(aug)</cell></row><row><cell></cell><cell>??? z 2</cell><cell></cell><cell>??? z 2</cell><cell cols="2">???</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Likelihood evaluation (in bits/dim) on standard datasets.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="4">CIFAR-10 ImageNet CelebA ImageNet 32x32 32x32 64x64 64x64</cell></row><row><cell></cell><cell>Conv Draw [40]</cell><cell>3.58</cell><cell>4.40</cell><cell>-</cell><cell>4.10</cell></row><row><cell>Variational Autoencoders</cell><cell>DVAE++ [41] IAF-VAE [42] BIVA [43]</cell><cell>3.38 3.11 3.08</cell><cell>--3.96</cell><cell>--2.48</cell><cell>---</cell></row><row><cell></cell><cell>CR-NVAE [44]</cell><cell>2.51</cell><cell>-</cell><cell>1.86</cell><cell>-</cell></row><row><cell></cell><cell>DDPM [13]</cell><cell>3.70</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Diffusion</cell><cell>UDM (RVE) + ST [45]</cell><cell>3.04</cell><cell>-</cell><cell>1.93</cell><cell>-</cell></row><row><cell>models</cell><cell>Imp. DDPM [46]</cell><cell>2.94</cell><cell>-</cell><cell>-</cell><cell>3.53</cell></row><row><cell></cell><cell>VDM [47]</cell><cell>2.65</cell><cell>3.72</cell><cell>-</cell><cell>3.40</cell></row><row><cell></cell><cell>Gated PixelCNN [48]</cell><cell>3.03</cell><cell>3.83</cell><cell>-</cell><cell>3.57</cell></row><row><cell></cell><cell>PixelRNN [7]</cell><cell>3.00</cell><cell>3.86</cell><cell>-</cell><cell>3.63</cell></row><row><cell>Autoregressive Models</cell><cell>PixelCNN++ [11] Image Transformer [38] PixelSNAIL [49]</cell><cell>2.92 2.90 2.85</cell><cell>-3.77 3.80</cell><cell>-2.61 -</cell><cell>---</cell></row><row><cell></cell><cell>SPN [50]</cell><cell>-</cell><cell>3.85</cell><cell>-</cell><cell>3.53</cell></row><row><cell></cell><cell>Routing transformer [39]</cell><cell>2.95</cell><cell>-</cell><cell>-</cell><cell>3.43</cell></row><row><cell></cell><cell>Real NVP [18]</cell><cell>3.49</cell><cell>4.28</cell><cell>3.02</cell><cell>3.98</cell></row><row><cell></cell><cell>GLOW [19]</cell><cell>3.35</cell><cell>4.09</cell><cell>-</cell><cell>3.81</cell></row><row><cell></cell><cell>Wavelet Flow [51]</cell><cell>-</cell><cell>4.08</cell><cell>-</cell><cell>3.78</cell></row><row><cell>Normalizing</cell><cell>Residual Flow [52]</cell><cell>3.28</cell><cell>4.01</cell><cell>-</cell><cell>3.78</cell></row><row><cell>Flows</cell><cell>i-DenseNet [53]</cell><cell>3.25</cell><cell>3.98</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Flow++ [27]</cell><cell>3.08</cell><cell>3.86</cell><cell>-</cell><cell>3.69</cell></row><row><cell></cell><cell>ANF [26]</cell><cell>3.05</cell><cell>3.92</cell><cell>-</cell><cell>3.66</cell></row><row><cell></cell><cell>VFlow [24]</cell><cell>2.98</cell><cell>3.83</cell><cell>-</cell><cell>3.66</cell></row><row><cell></cell><cell>mAR-SCF [54]</cell><cell>3.22</cell><cell>3.99</cell><cell>-</cell><cell>3.80</cell></row><row><cell></cell><cell>MaCow [55]</cell><cell>3.16</cell><cell>-</cell><cell>-</cell><cell>3.69</cell></row><row><cell>Hybrid</cell><cell>SurVAE Flow [34]</cell><cell>3.08</cell><cell>4.00</cell><cell>-</cell><cell>3.70</cell></row><row><cell>Architectures</cell><cell>NVAE [56]</cell><cell>2.91</cell><cell>3.92</cell><cell>2.03</cell><cell>-</cell></row><row><cell></cell><cell>PixelVAE++ [57]</cell><cell>2.90</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>?-VAE [58]</cell><cell>2.83</cell><cell>3.77</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DenseFlow-74-10 (ours)</cell><cell>2.98</cell><cell>3.63</cell><cell>1.99</cell><cell>3.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparative analysis of the computational budget for training contemporary methods. DenseFlow decreases the training complexity by an order of magnitude.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Params</cell><cell>GPU type</cell><cell cols="3">GPUs Duration (h) BPD</cell></row><row><cell></cell><cell>VFlow [24]</cell><cell>38M</cell><cell>RTX 2080Ti</cell><cell>16</cell><cell>?500</cell><cell>2.98</cell></row><row><cell>CIFAR-10</cell><cell>NVAE [56]</cell><cell>257M</cell><cell>Tesla V100</cell><cell>8</cell><cell>55</cell><cell>2.91</cell></row><row><cell></cell><cell>DenseFlow-74-10</cell><cell>130M</cell><cell>RTX 3090</cell><cell>1</cell><cell>250</cell><cell>2.98</cell></row><row><cell></cell><cell>VFlow [24]</cell><cell>38M</cell><cell>Tesla V100</cell><cell>16</cell><cell>?1440</cell><cell>3.83</cell></row><row><cell>ImageNet32</cell><cell>NVAE [56]</cell><cell>-</cell><cell>Tesla V100</cell><cell>24</cell><cell>70</cell><cell>3.92</cell></row><row><cell></cell><cell>DenseFlow-74-10</cell><cell>130M</cell><cell>Tesla V100</cell><cell>1</cell><cell>310</cell><cell>3.63</cell></row><row><cell></cell><cell>VFlow [24]</cell><cell>-</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>-</cell></row><row><cell>CelebA</cell><cell>NVAE [56]</cell><cell>153M</cell><cell>Tesla V100</cell><cell>8</cell><cell>92</cell><cell>2.03</cell></row><row><cell></cell><cell>DenseFlow-74-10</cell><cell>130M</cell><cell>Tesla V100</cell><cell>1</cell><cell>224</cell><cell>1.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>table corresponds to a DenseFlow-45-6 model. The first DenseFlow block has 5 DenseFlow units with 3 invertible modules per unit. The second DenseFlow block has 3 units with 5 modules, while the final block has 15 modules in a single unit. We use the growth rate of 6. The top row of the table corresponds to the standard normalized flow [18, 19] with three blocks and 15 modules per block. Consequently, all models have the same number of invertible glow-like modules.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablations on the CIFAR-10 dataset with DenseFlow-45-6.</figDesc><table><row><cell>#</cell><cell>Latent variable augmentation</cell><cell>Pre-conditioned noise</cell><cell>Intra-module coupling with two-way fusion</cell><cell>BPD</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>3.42</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>3.40</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell>3.37</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell>3.14</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell>3.08</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell><cell>3.07</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been supported by Croatian Science Foundation, grant IP-2020-02-5851 ADEPT. The first two authors have been employed on research projects KK.01.2.1.02.0119 DATACROSS and KK.01.2.1.02.0119 A-Unit funded by European Regional Development Fund and Gideon Brothers ltd. This work has also been supported by VSITE -College for Information Technologies who provided access to 2 GPU Tesla-V100 32GB. We thank Marin Or?i?, Julije O?egovi?, Josip ?ari? as well as Jakob Verbeek for insightful discussions during early stages of this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Twelth International Conference on Artificial Intelligence and Statistics<address><addrLine>Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sherjil Ozair, Aaron C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On markov chain monte carlo methods for tall data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">C</forename><surname>Holmes</surname></persName>
		</author>
		<idno>18:47:1-47:43</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Denoising diffusion implicit models. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive density estimation for generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="11993" to="12003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">NICE: non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03" />
			<biblScope unit="page" from="10236" to="10245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">i-revnet: Deep invertible networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Likelihood ratios for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="14680" to="14691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hybrid models with deep and invertible features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">T</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilan</forename><surname>G?r?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4723" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Rousselle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Nov?k</surname></persName>
		</author>
		<idno>145:1-145:19</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vflow: More expressive generative flows with variational data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqi</forename><surname>Chenli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1660" to="1669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07101</idno>
		<title level="m">Augmented normalizing flows: Bridging the gap between generative flows and latent variable models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Flow++: Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Nystr?mformer: A nystr?m-based algorithm for approximating self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrasis</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
		<idno>abs/2102.03902</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Survae flows: Surjections to bridge the gap between vaes and flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didrik</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyank</forename><surname>Jaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-05" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4052" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards conceptual compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="3549" to="3557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DVAE++: discrete variational autoencoders with overlapping transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbing</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Khoshaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Andriyash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5042" to="5051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04934</idno>
		<title level="m">Improving variational inference with inverse autoregressive flow</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">BIVA: A very deep hierarchy of latent variables for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Li?vin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="6548" to="6558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Consistency regularization for variational auto-encoders. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adji</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Score matching model for unbounded data score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjae</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungwoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il-Chul</forename><surname>Moon</surname></persName>
		</author>
		<idno>abs/2106.05527</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno>abs/2102.09672</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Variational diffusion models. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
		<idno>abs/2107.00630</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
	<note>Oriol Vinyals, and Alex Graves</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pixelsnail: An improved autoregressive generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="863" to="871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Generating high fidelity images with subscale pixel networks and multidimensional upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Wavelet flow: Fast training of high resolution normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Residual flows for invertible generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="9913" to="9923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yura</forename><surname>Perugachi-Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandjai</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhulai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02694</idno>
		<title level="m">Invertible densenets with concatenated lipswish</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Normalizing flows with multi-scale autoregressive priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apratim</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shweta</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Macow: Masked convolutional generative flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="5891" to="5900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">NVAE: A deep hierarchical variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Pixelvae++: Improved pixelvae with discrete prior. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Andriyash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Vinci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Buffoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Amin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Preventing posterior collapse with delta-vaes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Autoregressive quantile networks for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="3933" to="3942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Invertible residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Differentiable augmentation for dataefficient GAN training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">VAEBM: A symbiosis between variational autoencoders and energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<idno>abs/2010.00654</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Invertible densenets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yura</forename><surname>Perugachi-Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandjai</forename><surname>Bhulai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Symposium on Advances in Approximate Bayesian Inference</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Biases in AI systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramya</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Chander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="44" to="49" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Quantifying the carbon emissions of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dandres</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09700</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
