<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FDFLOWNET: FAST OPTICAL FLOW ESTIMATION USING A DEEP LIGHTWEIGHT NETWORK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingtong</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Image Processing and Pattern Recognition</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Image Processing and Pattern Recognition</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Medical Robotics</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FDFLOWNET: FAST OPTICAL FLOW ESTIMATION USING A DEEP LIGHTWEIGHT NETWORK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Optical Flow</term>
					<term>Convolutional Neural Net- works (CNNs)</term>
					<term>U-shape Network</term>
					<term>Partial Fully Connected Structure</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Significant progress has been made for estimating optical flow using deep neural networks. Advanced deep models achieve accurate flow estimation often with a considerable computation complexity and time-consuming training processes. In this work, we present a lightweight yet effective model for real-time optical flow estimation, termed FDFlowNet (fast deep flownet). We achieve better or similar accuracy on the challenging KITTI and Sintel benchmarks while being about 2 times faster than PWC-Net. This is achieved by a carefullydesigned structure and newly proposed components. We first introduce an U-shape network for constructing multi-scale feature which benefits upper levels with global receptive field compared with pyramid network. In each scale, a partial fully connected structure with dilated convolution is proposed for flow estimation that obtains a good balance among speed, accuracy and number of parameters compared with sequential connected and dense connected structures. Experiments demonstrate that our model achieves state-of-the-art performance while being fast and lightweight.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Optical flow estimation is a fundamental problem in computer vision which plays an important role in many vision applications such as action recognition <ref type="bibr" target="#b0">[1]</ref>, video understanding <ref type="bibr" target="#b1">[2]</ref> and self-driving cars <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. With decades of research, approaches based on energy minimization <ref type="bibr" target="#b4">[5]</ref> perform best among all methods. Optimization of an energy function in a coarse-to-fine manner is typically computationally heavy, hampering it from real-time applications.</p><p>Recently deep convolutional neural networks (CNNs) have dominated many fields of computer vision for being end-to-end trainable and powerful feature extraction ability. With advanced parallel computing equipments, CNNs can be inferred in real time. <ref type="bibr" target="#b5">[6]</ref> firstly apply CNNs to optical flow estimation and put forward two models namely FlowNetS and FlowNetC. Although the accuracy is close to state-ofthe-art energy minimization approaches, FlowNet's inference speed is orders of magnitude faster. By stacking multiple diverse Networks (FlowNetC, FlowNetS) and exploring different training schedules on multiple datasets, FlowNet2 <ref type="bibr" target="#b6">[7]</ref> improves both estimation accuracy and inference speed considerably. A drawback of FlowNet2 is the model size (over 160M parameters), making it difficult for deployment to embedded devices. SPyNet <ref type="bibr" target="#b7">[8]</ref> adopts a spatial pyramid network and warps the second image to the first one to estimate the residual flow in a coarse-to-fine manner. Compared with FlowNet, SPyNet has a much smaller-sized model (1.2M parameters) with the price of decreasing performance and running speed.</p><p>Compared with these previous works, recent works of PWC-Net <ref type="bibr" target="#b8">[9]</ref> and LiteFlowNet <ref type="bibr" target="#b9">[10]</ref> use a similar design paradigm which can both increase performance and reduce model sizes. First, both models use feature pyramids to replace image pyramids for a better representation. Warping and correlation operations are also adopted at each pyramid level to reduce error as soon as possible. Both networks are top performing on several optical flow benchmarks, which confirms the superiority of these structures in optical flow estimation. Our work is mainly inspired by the success of PWC-Net and LiteFlowNet, but we go into the overall architecture and explore the design of several key elements to reduce parameters and computation while obtaining high performance. In summary our contributions are:</p><p>? We propose a compact and effecient U-shape network as a improvement of pyramid network for optical flow estimation that can efficiently fuse multi-scale information while saving computing resources.</p><p>? We present a new partial fully connected structure of flow estimation network as an integration of existed dense connected and sequential connected structures. It provides a tradeoff among model size, computation cost and network performance.</p><p>? We show a new fast and lightweight deep neural network for optical flow estimation named FDFlowNet that achieves state-of-the-art performance on the challenging KITTI and Sintel benchmarks. It runs about 2 times faster than PWC-Net <ref type="bibr" target="#b8">[9]</ref> and about 3.2 times faster than LiteFlowNet <ref type="bibr" target="#b9">[10]</ref> on The left is traditional pyramid network for optical flow estimation adopted in PWC-Net and LiteFlowNet, the right is our proposed U-shape network. Only three levels are shown for brevity.</p><p>Sintel resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD</head><p>Our goal is to estimated the dense optical flow field F given two time adjacent frames I 1 and I 2 . Working in a coarseto-fine manner, FDFlowNet consists of one U-shape network for constructing multi-scale feature and five flow estimation networks in different levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">U-shape Network</head><p>The U-shape network architecture plays an important role in FDFlowNet as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Two frames I 1 and I 2 are imported into the same pyramid network from level 1 to level 6. When passing through level k, it generates two image feature f k 1 and f k 2 . Different from previous works of PWC-Net <ref type="bibr" target="#b8">[9]</ref> and LiteFlowNet <ref type="bibr" target="#b9">[10]</ref> that directly use pyramid feature, we build a symmetrical part for fusing low-level and high-level information that forms the U-shape network.</p><p>A drawback of pyramid network is that the higher resolution where flow estimator locates, the less semantic information corresponding pyramid feature contains. So it takes more convolution layers for the flow estimator to rebuild it. On the other hand, this semantic information is similar among different resolutions, which means that computation on multiple levels can be redudant.</p><p>To keep local property for dense matching, original pyramid feature f k 1 and f k 2 are used to calculate cost volume. Differently, semantic information is now offered by the fused featuref k 1 in U-shape network instead of f k 1 . From the first to the sixth level, the number of feature maps are 32, 64, 80, 96, 112 and 128. Transposed convolution which outputs half of input channels and one following convolution layer for feature fusing are employed in each level that constitute the U-shape network. Note that number of channels in pyramid feature f k 1 is the same as that in fused featuref k 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2:</head><p>Comparison of three different modes of flow estimator. Partial fully connected structure used in our FDFlowNet provides a balance and tradeoff between the other two types in model size, computation cost and network performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Warping, Correlation and Aggregation</head><p>One challenge in optical flow estimation is the large displacement of objects in two neighboring frames. To solve the problem, an effective method is to use the initially estimated flow to warp <ref type="bibr" target="#b10">[11]</ref> the second feature map. Given two feature map f k 1 and f k 2 , we use upsampled flow in level (k + 1) to resample f k 2 with bilinear interpolation that generates the warped second pyramid feature f k warp . To get better correspondence comparison between two feature of f k 1 and f k warp , we employ the correlation layer in <ref type="bibr" target="#b5">[6]</ref> to build a 3D cost volume which can be formulated as</p><formula xml:id="formula_0">c k (x, d) = f k 1 (x) ? f k warp (x + d)/N.</formula><p>x and d represent spatial and offset two-dimension coordinates respectively. N is the number of channels in cost volume. We set search radius to 4 in all levels as <ref type="bibr" target="#b8">[9]</ref>, and we have found that adding a convolution layer after rigid cost volume for feature aggregation can improve performance. The aggregated cost volume c k aggr has 126 channels in each scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Partial Fully Connected Flow Estimator</head><p>As mention above, fused feature of the first imagef k 1 , aggregated cost volume c k aggr and upsampled flow in previous level up 2 (F k+1 ) are used for estimating flow field F k . The work in <ref type="bibr" target="#b8">[9]</ref> has tested two types of flow estimation network: sequential connected structure and dense connected structure. Their experiments have shown that dense connected structure can get a better result after fine-tuning. However, it takes more parameters and computational cost. <ref type="bibr" target="#b9">[10]</ref> use a sequential connected structure for flow estimation and also get relatively good results.</p><p>Inspired by the above two connection manners, we proposed a new partial fully connected structure that provides a  tradeoff between the other two types in model size, computation cost and network performance. These three structures are depicted in <ref type="figure">Fig. 2</ref>. Output feature maps of flow estimation network in FDFlowNet are 128, 128, 128, 96, 64, 32 and 2 where the dense connection is only adopted in the second last convolution. Follwing <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, we change the convolution layers which output 64 and 32 channels to dilated convolution with dilation rate 2 for enlarging receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training Details</head><p>We use the same multi-scale training loss as PWC-Net <ref type="bibr" target="#b8">[9]</ref>. All experiments are conducted on one NVIDIA 1080Ti GPU. We implement codes in PyTorch and adopt Adam <ref type="bibr" target="#b13">[14]</ref> optimizer. Weight decay is set to 1e?4 for regularization. For fair comparison, we first train FDFlowNet on FlyingChairs <ref type="bibr" target="#b5">[6]</ref> dataset and then fine-tune it on FlyingThings3D <ref type="bibr" target="#b14">[15]</ref> dataset. S short and S f ine <ref type="bibr" target="#b6">[7]</ref> schedules are employed for the two stage training respectively. We use the same data augmentation method including mirror, translate, zoom, rotate, squeeze, color jitter and random noise. We call the model FDFlowNet after this sequential training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head><p>We test proposed FDFlowNet on benchmarks of Sintel <ref type="bibr" target="#b15">[16]</ref>, KITTI2012 <ref type="bibr" target="#b17">[17]</ref> and KITTI2015 <ref type="bibr" target="#b18">[18]</ref>. Detailed results are listed in <ref type="table">Table 1</ref>. Default criterion is end point error. MPI Sintel When fine-tuning on Sinel, we crop 384 ? 768 patches and remove noise like <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. We adopt batch size of 4 where 2 images from clean part and the others from final part. Training schedule is the same as <ref type="bibr" target="#b8">[9]</ref>. FDFlowNet outperforms PWC-Net and LiteFlowNet by a large margin on Sintel Clean test set. On the Sintel Final test benchmark, our method is only a little worse than PWC-Net and outperforms all the other approaches. Some visual results are depicted in <ref type="figure" target="#fig_1">Fig. 3</ref>. We can see that FDFlowNet keeps better semantic border in flow fields.   <ref type="figure">Fig. 4</ref>. It can be seen that our model predicts better flow fields in low and repetitive texture areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ablation Study</head><p>In this section, we conduct ablation study to evaluate the effectiveness of proposed approaches. Results of end point error are listed in <ref type="table" target="#tab_2">Table 2</ref>, "FDFlowNet-U" means that U-shape network is removed. "FDFlowNet-PFC" represents substituting sequential connected structure for partial fully connected structure with dilated convolution.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Runtime and Parameters</head><p>It is important for optical flow network to be running fast in real-time and lightweight. This is especially significant in embedded and mobile devices. Here we measure running speed and number of parameters of different optical flow networks as displayed in <ref type="table" target="#tab_4">Table 3</ref>. Experiments are conducted on a machine equipped with one NVIDIA GTX 1080Ti GPU. We use the PyTorch implement of all networks for fair comparison. Running time is obtained on Sintel resolution (436 ? 1024) averaged over 1000 times. FDFlowNet runs fastest among all the well-behaved models. It is about 2 times faster than PWC-Net and about 3.2 times faster than LiteFlowNet. It also outperforms PWC-Net-small in both running speed and benchmark performance which demonstrates the effectiveness of proposed FDFlowNet and related contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>This paper has presented a new fast and lightweight deep network for optical flow. By replacing previous pyramid feature with fused feature of U-shape network, our model gets better results on challenging benchmarks. We have proposed a new partial fully connected structure that provides a tradeoff between dense and sequential connected structures in model size, computation cost and network performance. It makes our FDFlowNet run at about 60 fps on Sintel resolution with relatively good performance. We hope our model and related contributions can help vast computer vision applications such as action recognition, video processing and automatic driving.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The left is traditional pyramid network for optical flow estimation adopted in PWC-Net and LiteFlowNet, the right is our proposed U-shape network. Only three levels are shown for brevity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Visualized optical flow field on the Sinel Final test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Flow estimation error on the KITTI2015 test set. Red region means large error while blue region denotes small error.</figDesc><table><row><cell cols="2">Fused Image</cell><cell></cell><cell>FDFlowNet</cell><cell>PWC-Net</cell><cell>LiteFlowNet</cell></row><row><cell>Fig. 4: Variants</cell><cell cols="3">FDFlowNet FDFlowNet-U FDFlowNet-PFC</cell></row><row><cell>Chairs</cell><cell>1.92</cell><cell>2.14</cell><cell>2.02</cell></row><row><cell>Sintel Clean</cell><cell>3.06</cell><cell>3.24</cell><cell>3.18</cell></row><row><cell>Sintel Final</cell><cell>4.23</cell><cell>4.46</cell><cell>4.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of different variants of FDFlowNet.</figDesc><table><row><cell>KITTI When fine-tuning on KITTI, we crop 320 ? 896</cell></row><row><cell>patches and reduce magnitude of rotation, zoom and squeeze</cell></row><row><cell>in data augmentation. Learning schedule is the same as fine-</cell></row><row><cell>tuning on Sintel. On KITTI2012 test benchmark, FDFlowNet</cell></row><row><cell>excels all the others. Our method gets the same good results</cell></row><row><cell>on KITTI2015 test set as LiteFlowNet. Note that LiteFlowNet</cell></row><row><cell>has lower end point error on the training datasets of Sintel</cell></row><row><cell>and KITTI which indicates good generalization ability of</cell></row><row><cell>proposed deep network. Comparison of flow estimation er-</cell></row><row><cell>rors among several competitive methods on KITTI2015 test</cell></row><row><cell>dataset is shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>All models are trained on FlyingChairs using the same learning schedule and evaluated on Chairs test, Sintel training datasets. Experiments show that U-shape network can provide better feature representation with fused multi-scale information that obtains an obvious improvement. It is about 5.6% improvement on Sintel Clean and about 5.2% improvement on Sintel Final. Partial fully connected structure with dilated convolution also surpass traditional sequential topology.</figDesc><table><row><cell>Model</cell><cell>FlowNetC</cell><cell>FlowNet2</cell><cell>SPyNet</cell></row><row><cell>parameters (M)</cell><cell>39.18</cell><cell>162.49</cell><cell>1.20</cell></row><row><cell>runtime (ms)</cell><cell>24.6</cell><cell>115.7</cell><cell>47.4</cell></row><row><cell>Model</cell><cell cols="3">PWC-Net LiteFlowNet FDFlowNet</cell></row><row><cell>parameters (M)</cell><cell>8.75</cell><cell>5.37</cell><cell>5.79</cell></row><row><cell>runtime (ms)</cell><cell>32.2</cell><cell>53.2</cell><cell>16.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of model size and running time.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Twostream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simonyan</forename><surname>Karen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zisserman</forename><surname>Andrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning on-road visual control for selfdriving vehicles with auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Praveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019-01" />
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised learning optical flow by robust reconstruction and edge-aware smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingtong</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing (ICONIP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
	<note>Flownet: Learning optical flow with convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2720" to="2729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2004, Tom?s Pajdla and Ji?? Matas</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ad-net: Attention guided network for optical flow estimation using dilated convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="2207" to="2211" />
		</imprint>
	</monogr>
	<note>Saddik</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning optical flow via dilated networks and occlusion reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="3333" to="3337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Part IV</title>
		<editor>A. Fitzgibbon et al.</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012-10" />
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
