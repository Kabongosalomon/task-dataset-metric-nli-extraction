<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Unsupervised Domain Adaptation Scheme for Single-Stage Artwork Recognition in Cultural Sites</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="202021-12-23">December 23, 2020 21 Dec 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Pasqualino</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Catania</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Catania</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Signorello</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CUTGANA</orgName>
								<orgName type="institution" key="instit2">University of Catania</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
							<email>gfarinella@dmi.unict.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Catania</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CUTGANA</orgName>
								<orgName type="institution" key="instit2">University of Catania</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">ICAR-CNR</orgName>
								<orgName type="institution" key="instit2">National Research Council</orgName>
								<address>
									<settlement>Palermo</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Unsupervised Domain Adaptation Scheme for Single-Stage Artwork Recognition in Cultural Sites</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="202021-12-23">December 23, 2020 21 Dec 2020</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Elsevier</note>
					<note>* Corresponding author:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object Detection</term>
					<term>Cultural Sites</term>
					<term>First Person Vision</term>
					<term>Unsupervised Domain Adaptation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing artworks in a cultural site using images acquired from the user's point of view (First Person Vision) allows to build interesting applications for both the visitors and the site managers. However, current object detection algorithms working in fully supervised settings need to be trained with large quantities of labeled data, whose collection requires a lot of times and high costs in order to achieve good performance. Using synthetic data generated from the 3D model of the cultural site to train the algorithms can reduce these costs. On the other hand, when these models are tested with real images, a significant drop in performance is observed due to the differences between real and synthetic images. In this study we consider the problem of Unsupervised Domain Adaptation for object detection in cultural sites. To address this problem, we created a new dataset containing both synthetic and real images of 16 different artworks. We hence investigated different domain adaptation techniques based on one-stage and two-stage object detector, image-to-image translation and feature alignment. Based on the observation that single-stage detectors are more robust to the domain shift in the considered settings, we proposed a new method which builds on RetinaNet and feature alignment that we called DA-RetinaNet. The proposed approach achieves better results than compared methods on the proposed dataset. It also obtains better results on Cityscapes. To support research in this field we release the dataset at the following link https://iplab.dmi.unict.it/EGO-CH-OBJ-UDA/ and the code of the proposed architecture at https://github.com/fpv-iplab/DA-RetinaNet. (Giovanni Maria Farinella) 1 These authors are co-first authors and contributed equally to this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recognizing artworks in a cultural site is a key feature for many applications aimed either to provide additional services to the users or to obtain insights into the behavior of the visitors, and hence measure the performance of the cultural site <ref type="bibr" target="#b0">[1]</ref>. For example, artwork recognition allows to automatically show additional information about an artwork observed by the visitor through augmented reality <ref type="bibr" target="#b1">[2]</ref>, or monitor visitor behavior to understand where people spend more time during their visit, as well as to infer which artworks attract their interest <ref type="bibr" target="#b2">[3]</ref>. Artwork recognition can be obtained fine-tuning standard object detector architectures (e.g. Faster-RCNN <ref type="bibr" target="#b3">[4]</ref>, YOLO <ref type="bibr" target="#b4">[5]</ref>, RetinaNet <ref type="bibr" target="#b5">[6]</ref>) on labeled data. However, in order to achieve good performance, object detection algorithms need to be trained on large datasets of manually labeled images. Depending on the cultural site, collecting and labeling visual data can be difficult especially when many artworks are present whose images should be acquired from different points of view. Moreover, labeling these data with bounding box annotations for each artwork is expensive and, since objects must be recognized at the instance level, the collection and labeling efforts must be repeated for each cultural site. To mitigate the aforementioned problems, a recent work <ref type="bibr" target="#b6">[7]</ref> proposed an approach to generate large quantities of synthetic images from the 3D model of a cultural site simulating a visitor navigating the site. Since the position of artworks can been labeled in the 3D model (i.e, one 3D bounding box per artwork), all images during the simulated navigation can be automatically labeled with 2D bounding box annotations. This approach allows to easily generate labeled datasets of arbitrary size which can be used to train an object detection algorithm. Nonetheless, there is a domain gap between the generated and real visual data which the object detector models must deal with at test time. <ref type="figure" target="#fig_0">Figure 1</ref> shows the results of a standard Faster-RCNN model trained on the labeled synthetic images. Due to the domain gap, the model successfully detects artworks on synthetic images, whereas it fails on real images. Infact, these algorithms assume that the images used for training and those on which the algorithm will be tested belong to the same domain distribution. In this context, for example, if an object detector is trained with a dataset of synthetic images and tested on a dataset containing their real counterparts, the performance will drastically drop and, in many cases, the algorithm will not be able to recognize the artworks, as it shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In this experimental scenario, the set of training data is generally referred to as "source domain", whereas the set of test data is called "target domain". The drop in performance due to domain gap represents a significant limitation since it requires the creation of a dataset of annotated images belonging to the target domain in order to re-train or fine-tune the algorithms. The labeling process, in particular, imposes additional costs in terms of money and time. For this reason, many works have focused on reducing the domain gap by leveraging labeled images belonging to a source domain and only unlabeled images from the target domain. This research area is referred to as "Unsupervised Domain Adaptation" <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. In this paper, we investigate the use of unsupervised domain adaptation techniques for artwork detection. Specifically, we consider a scenario in which large quantities of labeled synthetic images are available, whereas only unlabeled real images can be used at training time. The synthetic images can be easily obtained starting from a 3D model of the cultural site acquired with a 3D scanner such as Matterport 2 and using the tool proposed in <ref type="bibr" target="#b6">[7]</ref> to automatically generate the labeled data. The real unlabeled images can be easly collected visiting the cultural site acquiring videos with a wearable camera. Note that, since no manual labeling is required for the real images in the unsupervised settings, this procedure has a low cost. We hence aim to train the object detection models using labeled synthetic images and real unlabeled images. To the best of our knowledge, there are not publicly available datasets to study domain adaptation for artwork detection in cultural sites. Therefore we collect and publicly release a suitable one which we name UDA-CH (Unsupervised Domain Adaptation on Cultural Heritage). We hence study the main unsupervised domain adaptation techniques for object detection on UDA-CH: 1) image-to-image translation and 2) feature alignment. We compare the performance of two popular object detection approaches, Faster R-CNN <ref type="bibr" target="#b3">[4]</ref> and RetinaNet <ref type="bibr" target="#b5">[6]</ref>. Since in our study RetinaNet obtained results more robust to the domain gap than Faster-RCNN, we propose a novel approach which combines feature alignment techniques based on adversarial learning <ref type="bibr" target="#b8">[9]</ref> for unsupervised domain adaptation with the Reti-naNet architecture. Our experiments show that the proposed approach greatly outperforms prior art. When combined with image to image translation, our method achieves a mAP of 58.01% on real data without seeing a single labeled real images at training time. To better demonstrate the effectiveness of the proposed method, we have also tested the generalization of the approach in urban scenario exploiting the popular Cityscapes dataset <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. In sum, the contributions of this paper are as follows: 1) we introduce a new dataset to study synthetic to real unsupervised domain adaptation for artwork detection in cultural sites. The dataset has been acquired from a first person point of view on a real cultural site with 16 artworks; 2) we benchmark different solutions to address unsupervised domain adaptation for artwork detection; 3) we propose a novel architecture based on RetinaNet which obtains better results than similar approaches based on Faster-RCNN. The code of our approach is publicly available at the following link https://github.com/fpv-iplab/ DA-RetinaNet; 4) We demonstrate the generalization of the proposed approach considering also a popular dataset of a different domain (i.e urban domain); 5) we analyze the limits of the investigated techniques and discuss future research directions. The remainder of the paper is organized as follows. In Section 2, we discuss related work. Section 3 presents the compared methods. Section 4 reports the experimental settings and discusses results. Section 5 concludes the paper and summarises the main findings of our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Our work is related to different lines of research: egocentric vision in cultural sites, object detection, image to image translation, feature alignment for domain adaptation, and unsupervised domain adaptation for object detection. The following sections discuss the relevant works belonging to these research lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Egocentric vision in cultural sites</head><p>Wearable devices can be used to improve the fruition of artworks and the user experience in cultural sites <ref type="bibr" target="#b11">[12]</ref>. The authors of <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref> propose a smart audio guide that, based on the actions and interests of museum visitors, interacts with the visitors improving their experience and the fruition of multimedia materials. An important ability for these systems is related to the detection of artworks, which can be achieved using object detectors <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Unfortunately, object detectors need to be trained with big datasets of labeled images which can be expensive to collect in cultural sites. Consequently, few datasets are available to study the problem in this context. One of the few has been proposed by the authors of <ref type="bibr" target="#b0">[1]</ref>, who collected and labeled a dataset of first person images with Microsoft Hololens in two cultural sites located in Italy. However, since artwork recognition needs to be performed at the instance level, the data collection process and the training of the algorithms has to be repeated for every cultural site. To reduce data collection and annotation costs, the authors of <ref type="bibr" target="#b6">[7]</ref> proposed a tool to generate synthetic labeled images from a 3D reconstruction of a real cultural site. However, the generated data are not as photorealistic as the real images on which the object detection algorithm has to work at inference time, which induces a significant domain gap. Our work focuses on filling this domain gap by designing algorithms to detect objects in a cultural site considering only labeled synthetic images and unlabeled real images for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Object Detection</head><p>The detection of objects is one of the most important challenges in computer vision with impact on many aplications <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Modern object detection algorithms are based on deep learning and can be divided into two main categories according to their architecture: the algorithms belonging to the "two-stage" category, whose main representative are Faster R-CNN <ref type="bibr" target="#b3">[4]</ref>, Cascade R-CNN <ref type="bibr" target="#b16">[17]</ref> and Mask-RCNN <ref type="bibr" target="#b17">[18]</ref> and those belonging to the "single-stage" category such as RetinaNet <ref type="bibr" target="#b5">[6]</ref>, SSD <ref type="bibr" target="#b18">[19]</ref> and YOLO <ref type="bibr" target="#b4">[5]</ref>. The former address object detection by first extracting a set of object proposal and then processing them to determine the object class and refine its position in the image. These algorithms are generally characterized by a higher accuracy in the recognition and classification of objects, but also involve higher computational costs. The latter perform object detection in a single forward pass and are characterized by a higher computational efficiency generally obtained at the expense of detection precision. In our work we compare the main representatives of the two categories, Faster-RCNN (two-stage) and RetinaNet (single-stage). We find that RetinaNet is less sensitive to the domain gap and obtains better performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Domain Adaptation</head><p>Domain adaptation is a branch of machine learning that studies solutions to adapt a model trained on a set of images following a certain distribution (source distribution), to work on a set of test images following a different distribution (target distribution). When the adaptation is done using labeled images from the source domain and unlabeled images from the target domain, the task is referred as "Unsupervised Domain Adaptation". The authors of <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> proposed to minimize divergence quantities that can be measured between source and target distributions. Minimizing these quantities allows the model to extract features that are invariant with respect to the two domain distributions. In particular, the authors of <ref type="bibr" target="#b19">[20]</ref> exploited the use of the MMD metric <ref type="bibr" target="#b22">[23]</ref> in a CNN to reduce the distribution mismatch. The authors of <ref type="bibr" target="#b20">[21]</ref> used the CORAL metrics <ref type="bibr" target="#b23">[24]</ref> inside a CNN to align the covariances of the source and target distributions. The authors of <ref type="bibr" target="#b21">[22]</ref> proposed a method that aims to minimize the intra-class discrepancy and maximize the inter-class discrepancy. Other works used adversarial learning to align the distributions of the features extracted by the models of the source and target domains. The authors of <ref type="bibr" target="#b8">[9]</ref> introduce a gradient reversal layer into a standard CNN to align the distributions of source and target features using adversarial learning. Specifically, the model they propose includes two components. The first one processes the input samples to solve the supervised task (e.g., classification). The second one is devoted to discriminate if the features extracted from the input sample belong to the source or target domain. The network is trained to minimize the supervised loss of the first component and the discriminator loss of the second one. The gradient reversal layer is used to invert the gradients of the discriminator when they are used to update the parameters of the first component, which implements a minmax game similar to the one described in <ref type="bibr" target="#b24">[25]</ref>. The authors of <ref type="bibr" target="#b7">[8]</ref> propose a method based on two stages: in the first stage a CNN is trained on the source dataset. In the second stage the weights of the CNN are adapted to extract domain-invariant features. During the test phase, the weights obtained during the second stage are used to extract the features, whereas the classification layers are obtained from the network trained on the source domain. In our work, we consider adversarial learning for domain adaptation with gradient reversal layer <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Image-to-image translation</head><p>When the images of the source and target domains are visually different (e.g., images acquired with different light conditions), a way to reduce the domain gap between the two domains is to use image-to-image translation techniques <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. The goal of these techniques is to translate an image belonging to a source domain into an image belonging to the target domain without changing its content but adapting only its style and colors. When pairs of images belonging to the source and target domains are available, a mapping between the two domains can be learned exploiting a conditional adversarial network <ref type="bibr" target="#b28">[29]</ref>. The authors of <ref type="bibr" target="#b29">[30]</ref> note that paired datasets are difficult to obtain in practice and introduce a method that translates images from a source domain X to a target domain Y in the absence of paired examples. As proposed in <ref type="bibr" target="#b29">[30]</ref>, the goal is to learn a function G : X ?? Y such that the distribution of the transformed images G (X) is indistinguishable from the distribution of Y . Since the translation between the two domains should be consistent, an inverse</p><formula xml:id="formula_0">mapping F : Y ?? X is introduced such that F (G(X)) ? X.</formula><p>As discussed in previous works <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, the algorithms described in this section can be used in combination with the domain adaptation techniques of the previous subsections to deal with the domain gap. The images belonging to the source domain can be translated into the target domain and subsequently used as training images. The resulting model can be used directly on the target domain at test time. Vice versa, it is possible to train the model on the source domain and translate the test images to the source domain at inference time. In our work, we explore the benefits of image to image translation techniques and their combination with the feature alignment methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Unsupervised Domain Adaptation for object detection</head><p>Previous works have investigated the application of unsupervised domain adaptation to the problem of object detection. Many of these works use adversarial learning with the gradient reversal layer. The authors of <ref type="bibr" target="#b33">[34]</ref> present a custom version of a Faster RCNN <ref type="bibr" target="#b3">[4]</ref> that includes two modules: the first one aligns the features of the entire input (i.e., at the image level), the second module aligns the features before they are used for classification and regression (i.e. at the instance level). The authors of <ref type="bibr" target="#b31">[32]</ref> propose to adapt source and target domains exploiting both high-and low-level features. The authors of <ref type="bibr" target="#b34">[35]</ref> propose an architecture similar to the one presented in <ref type="bibr" target="#b33">[34]</ref>, but they add more discriminators with a gradient reversal layer to the Faster-RCNN backbone. The authors of <ref type="bibr" target="#b35">[36]</ref> propose a framework to align the source and target domains at the level of image regions extracted from the "region proposal network" of a Faster-RCNN. This architecture has two main components: 1) region mining, which extracts the regions of interest from the source and target images, groups them and selects the most important regions containing the objects; 2) the region level alignment, which learns to align the patches of the reconstructed images starting from the features selected by the previous module through adversarial learning. Recent methods address domain adaptation employing Faster-RCNN as baseline object detection architecture, whereas few approaches have investigated the use of single-stage object detectors such as RetinaNet <ref type="bibr" target="#b5">[6]</ref>. In our work, we compare the performance of the two methods and introduce an object detector based on RetinaNet which includes a domain adaptation component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>We compare several approaches to unsupervised domain adaptation for object detection. Specifically we considered the following: 1) a baseline object detector without adaptation, 2) domain adaptation through image-to-image translation, 3) domain adaptation through feature alignment, 4) the proposed method based on RetinaNet and feature alignment and 5) approaches combining feature alignment and image-to-image translation. In the following section, we give details on all the compared approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Baseline approaches without adaptation</head><p>To assess performance in the absence of domain shift, we train and test Faster RCNN and RetinaNet on the same domain (either synthetic or real images), as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(a) and <ref type="figure" target="#fig_1">Figure 2(b)</ref>. We also consider a model trained on synthetic images and tested directly on real test images, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(c). These methods allow to assess the gap between the two domains.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Domain adaptation through image-to-image translation</head><p>Transforming images from synthetic to real and vice versa is a common way to reduce the domain gap. In particular, we use CycleGAN <ref type="bibr" target="#b29">[30]</ref> to transform images from one domain to another. We compare two approaches: 1) translating synthetic images to real, training Faster RCNN and RetinaNet on the transformed images and testing the two detectors with real images. This approach is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>(a); 2) translating real test images to synthetic, testing the two models that were previously trained on synthetic images as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Domain adaptation through feature alignment</head><p>We consider DA-Faster-RCNN <ref type="bibr" target="#b33">[34]</ref> and Strong-Weak <ref type="bibr" target="#b31">[32]</ref> and compare their results with our method DA-RetinaNet described in the next subsection. All these methods use synthetic labeled images and unlabeled real images for training as shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Proposed Method: DA-RetinaNet</head><p>The proposed method is based on RetinaNet architecture <ref type="bibr" target="#b5">[6]</ref> and it is illustrated in <ref type="figure" target="#fig_4">Figure 5</ref>. At each level of the feature pyramid map (C 3 , C 4 and C 5 ) in the ResNet backbone, we add a discriminator (D 3 , D 4 , D 5 ) with a Gradient Reversal Layer. The three discriminators have different architectures: D 3 has 3 convolutional layers with a kernel size of 1 and ReLU as activation function; D 4 has 3 convolutional layers with kernel size of 3 followed by batch normalization, ReLU and Dropout. At the end of the last convolutional layer there is a fully connected layer; D 5 has 3 convolutional layers with kernel size of 3 followed by batch normalization, ReLU and Dropout. After the convolutional layer there are 2 fully connected layers. Our idea follows <ref type="bibr" target="#b8">[9]</ref>, thus we train our model to minimize the cost function:</p><formula xml:id="formula_1">L = L class + L box ? ?(L D3 + L D4 + L D5 )<label>(1)</label></formula><p>where L class is the sum of the losses of each classification subnet module, L box is the sum of the losses of each regression subnet module. Their sum represent the standard RetinaNet loss. L D3 , L D4 , L D5 are the losses of each discriminator module and each of them is given by L Di = 1 2 (L Ds,i + L Dt,i ) where L Ds,i and L Dt,i are respectively the losses computed by the discriminators when receive in input respectively synthetic and real images and defined using the Focal loss <ref type="bibr" target="#b5">[6]</ref>. ? is the hyperparameter that balances RetinaNet and discriminators losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Domain adaptation through feature alignment and image to image translation</head><p>We combine the feature alignment techniques presented in Section 3.3 and Section 3.4 with image-to-image translation. This approach is similar to Cy-CADA proposed in <ref type="bibr" target="#b30">[31]</ref> with the difference that we consider state-of-art feature alignment methods to perform the adaptation. We combine these techniques in two ways: 1) transforming synthetic labeled images to real, then training feature-alignment-based architectures using transformed labeled and real unlabeled images ( <ref type="figure" target="#fig_5">Figure 6)</ref>; 2) transforming real unlabeled images to synthetic, then training feature alignment based architecture using synthetic labeled and  transformed unlabeled images and testing on real images transformed to synthetic ( <ref type="figure" target="#fig_6">Figure 7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Settings and Results</head><p>This section presents the proposed dataset, reports and analyze the results of the methods presented in the previous section and discusses the computational resources required to train all the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>The proposed dataset <ref type="bibr" target="#b36">[37]</ref> contains 16 objects that cover a variety of artworks which can be found in a museum like sculptures, paintings and books. Specifically, the dataset has been collected inside the cultural site "Galleria Regionale di Palazzo Bellomo" located in Siracusa, Italy 3 . We generated 75244 synthetic labeled images, we have used a 3D model of the museum acquired using Matterport 4 , of the 16 artworks using the public tool proposed by the authors of <ref type="bibr" target="#b6">[7]</ref> (see <ref type="figure" target="#fig_7">Figure 8</ref>). The tool proposed in <ref type="bibr" target="#b6">[7]</ref> allows generate automatic labeled synthetic images, simulating a visitor who walks around the site while observing the artworks. Each image acquired during the simulation is associated to a semantic mask which allows to obtain bounding box annotations for each image. Real images of the same 16 artworks are taken from the EGO-CH dataset proposed in <ref type="bibr" target="#b0">[1]</ref> (see <ref type="figure" target="#fig_8">Figure 9</ref>), which contains videos of 70 subjects who visited two cultural sites which have been captured using a Microsoft HoloLens device. EGO-CH includes 176999 images manually annotated  with bounding boxes. For the experiments, a subset of EGO-CH was taken into account. In particular, we considered 2190 images which contain the 16 artworks present in the synthetic dataset. To perform the experiments, we split both sets of synthetic and real images to training and a test set. We used 51284 synthetic and 1502 real images as training set and 23960 synthetic and 688 images as test set. The proposed dataset is available at the following URL: https://iplab.dmi.unict.it/EGO-CH-OBJ-UDA/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Settings</head><p>We trained all the object detectors for 62K iterations starting from Ima-geNet <ref type="bibr" target="#b37">[38]</ref> pre-trained weights. We used Faster-RCNN and RetinaNet Detec-   <ref type="bibr" target="#b39">[40]</ref> as backbone. The batch size has been set to 4 and the learning rate to 0.0002 for the first 30K iterations and multiplied by 0.1 for the remaining iterations. CycleGAN was trained for 60 epochs using the default parameters. For DA-Faster RCNN 6 and Strong-Weak 7 we used the settings proposed by the authors in their respective works <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b31">[32]</ref>. DA-RetinaNet 8 was implemented using Detectron2 and it was trained with a batch size of 6 and learning rate of 0.0002 for the first 30K iterations. Also in this case, the learning rate has been then multiplied by 0.1 for the remaining iterations. <ref type="table" target="#tab_0">Table 1</ref> reports the results of the two models when they are trained and tested on the same domain. As can be noted, when images came from the same distribution, these algorithms achieve good performance. <ref type="table" target="#tab_1">Table 2</ref> shows the performance achieved by Faster RCNN and RetinaNet when trained on synthetic images and tested on real images. The results highlight that models trained for few iterations generalize better than models trained for more iterations. RetinaNet is in general more robust to domain shift than Faster RCNN. In particular, RetinaNet trained for 12K iterations achieves an mAP of 14.44% vs 9.67% obtained by Faster RCNN and 11.97% vs 3.62% considering 62K iterations. This suggests that training for more iterations both models increases the domain gap between the two distributions because the models learn to extract features specific to the source domain that do not generalize to the target domain. It is worth noting that, even the best RetinaNet model (14.44%) exhibits a drastic drop in performances if compared with the results of Table 1 (92.15%).  This is due to the domain shift between synthetic images used for training and real images used for test. <ref type="table" target="#tab_2">Table 3</ref> shows the results of Faster RCNN and RetinaNet tested on real images transformed to synthetic using CycleGAN. We analyzed the performances of both models trained for 12K and 62K iterations to explore the impact of overfitting. As shown in the <ref type="table" target="#tab_2">Table 3</ref>, CycleGAN improves the performance of both models. RetinaNet performs better than Faster RCNN. Indeed Faster RCNN achieves performance similar to RetinaNet only when tested on images transformed using a CycleGAN model trained for 50 epochs. <ref type="table" target="#tab_3">Table 4</ref> reports the results of both models trained using synthetic images transformed to real. In this case, the performance of Faster RCNN (26.03%) are lower than the previous method which uses images translated from real to synthetic (28.25%). RetinaNet increases its performance by ?20% from 35.76% to 55.54%. Even in this case, the results seem to confirm that RetinaNet is more robust to domain shift. While training CycleGAN for more epochs may allow for minor improvements, it should be noted that training CycleGAN for 60 epochs required about 61 days. A detailed discussion on the training times of all methods is provided in Section 4.8. <ref type="figure" target="#fig_0">Figure 10</ref> shows qualitative example obtained translating images from real to synthetic and vice versa. The first row of <ref type="figure" target="#fig_0">Figure 10 (a)</ref> shows an example of successful translation. In the second row the image is not correctly transformed due to light reflection, whereas in the third row the texture is destroyed during the transformation. First two rows of <ref type="figure" target="#fig_0">Figure 10 (b)</ref> show an example of successful translation while the last rows show a bad translation example where the background contains many artifacts.   <ref type="table" target="#tab_4">Table 5</ref> reports the results of the methods based only on feature alignment and combined with CycleGAN. As can be seen from <ref type="table" target="#tab_4">Table 5</ref>, the proposed DA-RetinaNet achieves better performances when compared to other methods. Without image-to-image translation, DA-RetinaNet obtains an mAP of 31.04% which is an increase in performance of about 6% as compared to Strong-Weak (25.12%). The improvement is about 11% when the models are combined with SynToReal CycleGAN (58.01% vs 47.70%). Furthermore, it is worth noting that all models benefit from a performance improvement which varies between 21% and 27% when combined with CycleGAN. <ref type="table" target="#tab_5">Table 6</ref> reports the results of DA-RetinaNet considering one, two or three Discriminators D i without any image-to-image translation technique. As shown in the table, aligning features using the paradigm of adversarial learning improves in each case the performance of standard RetinaNet. D 3 , the discriminator that aligns low level features, doubles the performance with respect to the standard RetinaNet model achieving a mAP of 28.61% vs 14.44%. The use of D 4 and D 5 allows to achieve similar performance (16.38% and 15.84%) improving the baseline results by about 1.5%. This is probably due to the design of the RetinaNet architecture. Indeed between the feature map C 4 and C 5 there are few convolutive layers. Combining the two discriminators which achieve the best performance allows to improve the mAP of about 2% (28.61% vs 30.52%).  The best model is obtained by using all the discriminators (31.04%), which is our suggested design, as shown in <ref type="figure" target="#fig_4">Figure 5</ref>    increases performance by about 5% when compared to Faster RCNN with Cy-cleGAN, and by 6% compared to Strong-Weak. Again, our method increases the performance of a standard RetinaNet with CycleGAN by about 2.5% (55.54% vs 58.01%). <ref type="figure" target="#fig_0">Figure 11</ref> shows qualitative result of the baseline and the models based on feature alignment. Faster RCNN does not detect any object and in some cases its predictions are false positive. DA-Faster RCNN and RetinaNet correctly detect objects in the "easy" examples (first two rows), with some misclassification problems when there are more objects and occlusions (last three rows). Strong-Weak and DA-RetinaNet are more accurate in detection but they still produce some false positive and false negative predictions. <ref type="figure" target="#fig_0">Figure 12</ref> reports the qualitative results of the previous five methods combined with CycleGAN to translate images from synthetic to real. Faster RCNN and DA-Faster RCNN have similar results to DA-RetinaNet but they have much more false positive detections. Strong-Weak and RetinaNet combined with CycleGAN correctly detect the objects of the first three rows. Strong-Weak is less accurate than RetinaNet but has less false positive detections. DA-RetinaNet combined with CycleGAN perfectly detects artworks in the first four rows with only a misclassification in the fourth rows behind the statue. As can be noted, even these models are not able to detect object in the last two rows. Possible reasons are: 1) bad translation results from synthetic to real, 2) few synthetic object are not  similar to their real counterpart, 3) some synthetic objects are similar to each other (e.g. some books). <ref type="table" target="#tab_10">Table 8</ref> shows the training times required by the algorithms using a single NVIDIA Tesla K80. We use the same batch size for each object detector to evaluate the training times. Training CycleGAN for 60 epochs required 61 days in the considered settings. Methods based on feature alignment require from 3 to 6 days depending on the considered object detector. In particular, DA-Faster RCNN, Strong-Weak and DA-RetinaNet have only a small computational overhead given by the presence of the discriminators. However, even if these methods required less time when compared to CycleGAN, they have limited performance when compared to their counterparts who make use of image-toimage translation (e.g. DA-RetinaNet: <ref type="bibr" target="#b30">31</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Baseline Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Image-to-Image translation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Analysis of Computational Resources</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Results on Cityscapes Dataset</head><p>To better asses the performance of the proposed method and to understand generalization capability over datasets, we have performed experiments on the Cityscapes dataset [10] <ref type="bibr" target="#b10">[11]</ref>. To this aim, we trained RetinaNet and DA-RetinaNet for 50K iteration with a learning rate of 0.0002, batch size of 4 and starting from weights pre-trained on ImageNet. Following <ref type="bibr" target="#b31">[32]</ref>, we used Cityscapes <ref type="bibr" target="#b9">[10]</ref> as source domain and Foggy-Cityscapes <ref type="bibr" target="#b10">[11]</ref> as target domain. Both dataset have 2975 images in the training set. We reported results on the 500 images of the validation set. <ref type="table">Table 9</ref> reports the results obtained by standard object detector architectures and domain adaptation methods based on feature alignment. The table highlights that standard RetinaNet achieves better performance than Strong-Weak and Diversify and Match by about 6%. The proposed DA-RetinaNet increases performance by 4%, 10%, and 24% if compared respectively with standard RetinaNet, Strong Weak and Diversify and Match, and DA-Faster RCNN. However there is still a gap between the best results obtained by the proposed architecture and the result of the Oracle which is obtained training and testing RetinaNet on the Foggy Cityscapes dataset, which suggests that there is still room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We considered the problem of Unsupervised Domain Adaptation for object detection in cultural. To conduct our study, we created a new dataset consisting of 75244 synthetic images and 2190 real images of 16 artworks, which we publicly release. To better assess generalization of the compared approaches, we have also performed experiment with a dataset related to urban environment. Experiments showed that the proposed DA-RetinaNet method achieves better performance compared to DA-Faster RCNN and Strong-Weak. At the same time, the results obtained by these methods based on feature alignment <ref type="table">Table 9</ref>: Results adaptation between Cityscapes and Foggy Cityscapes dataset. The performance scores of the methods marked with the "*" symbol are reported from the authors of their respective papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>mAP Faster RCNN* <ref type="bibr" target="#b31">[32]</ref> 20.30% DA-Faster RCNN* <ref type="bibr" target="#b33">[34]</ref> 27.60% Strong-Weak* <ref type="bibr" target="#b31">[32]</ref> 34.30% Diversify and Match* <ref type="bibr" target="#b32">[33]</ref>  achieved very poor performance if compared to their counterparts combined with image-to-image translation techniques. DA-RetinaNet performed better than others also when combined with CycleGAN. However, using CycleGAN with this dataset required a high computational training cost. We hope that the proposed dataset will encourage research on this challenging topic and that the proposed DA-RetinaNet will serve as a strong baseline for future works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example of a synthetic image generated using the tool proposed in<ref type="bibr" target="#b6">[7]</ref> (left) and a real image of the same artwork (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>We used 3 different pipelines: (a) training and testing on the synthetic domain, (b) training and testing on the real domain, (c) training using synthetic images and testing on real images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Pipeline used to train models on synthetic images transformed to real with test performed on real images. (b) Pipeline used to train models on synthetic images with test performed on real images transformed to synthetic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Pipeline used to train models based on feature alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Architecture of the proposed DA-RetinaNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Pipeline used to combine feature alignment and image to image translation from synthetic to real techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Pipeline used to combine feature alignment and image to image translation from real to synthetic techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Sample synthetic images of the 16 artworks of our dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Sample real images of the 16 artworks of our dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative CycleGAN results. We show the source domain (real synthetic), the transformed image, and a reference image for visual comparison. Left: transformation from real to synthetic. Right: transformation from synthetic to real.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>4. 7 .</head><label>7</label><figDesc>Qualitative Results and Summary table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Qualitative results of baseline and feature alignment approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Qualitative results of the baseline and feature alignment combined with CycleGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of Faster RCNN and RetinaNet trained and tested on images from the same domain.</figDesc><table><row><cell></cell><cell cols="2">mAP</cell></row><row><cell>Model</cell><cell>Synthetic</cell><cell>Real</cell></row><row><cell>Faster RCNN</cell><cell>93.08%</cell><cell>92.04%</cell></row><row><cell>RetinaNet</cell><cell>91.67%</cell><cell>92.15%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of Faster RCNN and RetinaNet trained on synthetic images for a different amounts of iterations and tested on real images. RCNN 2.27% 9.67% 5.79% 3.58% 3.33% 3.81% 3.62% RetinaNet 9.83% 14.44% 13.22% 12.31% 12.09% 12.44% 11.97%</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Training Iterations</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>6K</cell><cell>12K</cell><cell>22K</cell><cell>32K</cell><cell>42K</cell><cell>52K</cell><cell>62K</cell></row><row><cell>F.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>tron2 [39] architectures 5 with ResNet101</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results obtained transforming real images to synthetic at test time. The models have been trained on synthetic images. N.A. stands for No Adaptation. 62K) 3.62% 25.16% 25.49% 25.51% 26.68% 27.65% 28.25% RetinaNet (62K) 11.97% 27.30% 32.14% 34.15% 32.66% 32.79% 32.82% F. RCNN (12K) 9.67% 29.93% 32.84% 33.95% 31.45% 34.19% 31.58% RetinaNet (12K) 14.44% 34.51% 35.45% 34.84% 35.34% 35.76% 35.74%</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">Training epochs for CycleGAN</cell><cell></cell></row><row><cell>Model (iter)</cell><cell>N.A.</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell></row><row><cell>F. RCNN (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results obtained training the models on synthetic images transformed to real and tested on real images. N.A. stands for No Adaptation. 67% 18.76% 20.92% 21.22% 23.17% 24.45% 26.03% RetinaNet 14.44% 40.13% 44.29% 46.05% 47.89% 49.96% 55.54%</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">Training epochs for CycleGAN</cell><cell></cell></row><row><cell>Model</cell><cell>N.A.</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell></row><row><cell cols="2">F. RCNN 9.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results of DA-Faster RCNN, Strong-Weak and the proposed DA-RetinaNet combined with two different image-to-image translation approaches.</figDesc><table><row><cell></cell><cell cols="3">image-to-image translation</cell></row><row><cell>Model</cell><cell>None</cell><cell cols="2">Real2Syn Syn2Real</cell></row><row><cell>DA-Faster RCNN</cell><cell>12.94%</cell><cell>19.88%</cell><cell>33.20%</cell></row><row><cell>Strong-Weak</cell><cell>25.12%</cell><cell>33.33%</cell><cell>47.70%</cell></row><row><cell>DA-RetinaNet</cell><cell>31.04%</cell><cell>37.49%</cell><cell>58.01%</cell></row><row><cell cols="4">4.5. Feature Alignment and Image-to-Image translation Results</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation study about the impact of each discriminator D i .</figDesc><table><row><cell>Model</cell><cell>D 3 D 4 D 5</cell><cell>mAP</cell></row><row><cell>RetinaNet (62K)</cell><cell></cell><cell>11.97%</cell></row><row><cell>RetinaNet (12K)</cell><cell></cell><cell>14.44%</cell></row><row><cell>DA-RetinaNet</cell><cell></cell><cell>15.84%</cell></row><row><cell>DA-RetinaNet</cell><cell></cell><cell>16.38%</cell></row><row><cell>DA-RetinaNet</cell><cell></cell><cell>28.61%</cell></row><row><cell>DA-RetinaNet</cell><cell></cell><cell>30.52%</cell></row><row><cell>DA-RetinaNet</cell><cell></cell><cell>31.04%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Summary table of the analyzed methods.</figDesc><table><row><cell>Object Detector</cell><cell>Adaptation</cell><cell>mAP</cell></row><row><cell>Faster RCNN</cell><cell>None</cell><cell>9.67%</cell></row><row><cell>RetinaNet</cell><cell>None</cell><cell>14.44%</cell></row><row><cell>Faster RCNN</cell><cell>Real2Syn (Test set)</cell><cell>34.19%</cell></row><row><cell>RetinaNet</cell><cell>Real2Syn (Test set)</cell><cell>35.76%</cell></row><row><cell>Faster RCNN</cell><cell>Syn2Real (labeled Training set)</cell><cell>26.03%</cell></row><row><cell>RetinaNet</cell><cell>Syn2Real (labeled Training set)</cell><cell>55.54%</cell></row><row><cell>DA-Faster RCNN</cell><cell>Feat.Align.</cell><cell>12.94%</cell></row><row><cell></cell><cell>Feat.Align.+Real2Syn</cell><cell></cell></row><row><cell>DA-Faster RCNN</cell><cell>(Test set and unlabeled Training set)</cell><cell>19.88%</cell></row><row><cell></cell><cell>Feat.Align.+Syn2Real</cell><cell></cell></row><row><cell>DA-Faster RCNN</cell><cell>(labeled Training set)</cell><cell>33.20%</cell></row><row><cell>Strong-Weak</cell><cell>Feat.Align.</cell><cell>25.12%</cell></row><row><cell></cell><cell>Feat.Align.+Real2Syn</cell><cell></cell></row><row><cell>Strong-Weak</cell><cell>(Test set and unlabeled Training set)</cell><cell>33.33%</cell></row><row><cell></cell><cell>Feat.Align.+Syn2Real</cell><cell></cell></row><row><cell>Strong-Weak</cell><cell>(labeled Training set)</cell><cell>47.70%</cell></row><row><cell>DA-RetinaNet</cell><cell>Feat.Align.</cell><cell>31.04%</cell></row><row><cell></cell><cell>Feat.Align.+Real2Syn</cell><cell></cell></row><row><cell>DA-RetinaNet</cell><cell>(Test set and unlabeled Training set)</cell><cell>37.49%</cell></row><row><cell></cell><cell>Feat.Align.+Syn2Real</cell><cell></cell></row><row><cell>DA-RetinaNet</cell><cell>(labeled Training set)</cell><cell>58.01%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc>summarizes all the performances of the analyzed methods with respect to the considered adaptation techniques. The table confirms that the proposed DA-RetinaNet achieves better performance than the compared methods. In particular, considering only feature alignment techniques, our architecture</figDesc><table><row><cell>Faster RCNN</cell><cell>DA-Faster RCNN</cell><cell>RetinaNet</cell><cell>Strong-Weak DA-RetinaNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>.04 % vs 58.01 %, Strong-Weak: 25.12 % vs 47.70 %, DA-Faster RCNN: 12.94 % vs 33.20 %). We argue that more attention should be devoted to such approaches in order to minimize training times.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Training times required by the models.</figDesc><table><row><cell>Model</cell><cell>Hours (Days)</cell></row><row><cell>RetinaNet (12K iterations)</cell><cell>? 10 (? 0.5)</cell></row><row><cell>RetinaNet (62K iterations)</cell><cell>? 65 (? 3)</cell></row><row><cell>DA-RetinaNet</cell><cell>? 67 (? 3)</cell></row><row><cell>Faster RCNN (62K iterations)</cell><cell>? 131 (? 5.5)</cell></row><row><cell>DA-Faster RCNN</cell><cell>? 142 (? 6)</cell></row><row><cell>Strong-Weak</cell><cell>? 147 (? 6)</cell></row><row><cell>CycleGAN</cell><cell>? 1470 (? 61)</cell></row><row><cell>CycleGAN + RetinaNet</cell><cell>? 1535 (? 64)</cell></row><row><cell>CycleGAN + DA-RetinaNet</cell><cell>? 1537 (? 64)</cell></row><row><cell>CycleGAN + Faster RCNN</cell><cell>? 1601 (? 66)</cell></row><row><cell cols="2">CycleGAN + DA-Faster RCNN ? 1612 (? 67)</cell></row><row><cell>CycleGAN + Strong-Weak</cell><cell>? 1617 (? 67)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://matterport.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.regione.sicilia.it/beniculturali/palazzobellomo/ 4 https://matterport.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/facebookresearch/detectron2 6 https://github.com/krumo/Detectron-DA-Faster-RCNN 7 https://github.com/VisionLearningGroup/DA_Detection 8 https://github.com/fpv-iplab/DA-RetinaNet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ego-ch: Dataset and fundamental tasks for visitors behavioral understanding using egocentric vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ragusa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Signorello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="150" to="157" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep artwork detection and retrieval for automatic context-aware audio guides</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baecchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Uricchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferracani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Egocentric point of interest recognition in cultural sites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ragusa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Signorello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VISIGRAPP (5: VISAPP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="381" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Egocentric visitor localization and artwork detection in cultural sites using synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="17" to="24" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="973" to="992" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visions for augmented cultural heritage experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="82" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully convolutional network and region proposal for instance identification with egocentric vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Portaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qu?not</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Chevallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2383" to="2391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object detection and classification in surveillance system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sreeraj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Recent Advances in Intelligent Computational Systems (RAICS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="299" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in firstperson camera views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
	<note>Ssd: Single shot multibox detector</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond sharing weights for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="801" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4893" to="4902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Correlation alignment for unsupervised domain adaptation, in: Domain Adaptation in Computer Vision Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="153" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
	<note>Generative adversarial nets</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Night-today image translation for retrieval-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anoosheh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5958" to="5964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4500" to="4509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image-to-image translation for cross-domain disentanglement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1287" to="1298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Strong-weak distribution alignment for adaptive object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6956" to="6965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Diversify and match: A domain adaptive representation learning paradigm for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12456" to="12465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3339" to="3348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-level domain adaptive learning for cross-domain detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adapting object detectors via selective cross-domain alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for object detection in cultural sites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M F</forename><surname>Giovanni Pasqualino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>Imagenet: A largescale hierarchical image database</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detectron2</forename></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
