<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Fast Knowledge Distillation Framework for Visual Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
							<email>zhiqians@andrew.cmu.eduepxing@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">CMU &amp; MBZUAI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CMU &amp; MBZUAI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Fast Knowledge Distillation Framework for Visual Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While Knowledge Distillation (KD) has been recognized as a useful tool in many visual tasks, such as supervised classification and self-supervised representation learning, the main drawback of a vanilla KD framework is its mechanism, which consumes the majority of the computational overhead on forwarding through the giant teacher networks, making the entire learning procedure inefficient and costly. ReLabel [52], a recently proposed solution, suggests creating a label map for the entire image. During training, it receives the cropped region-level label by RoI aligning on a pre-generated entire label map, allowing for efficient supervision generation without having to pass through the teachers many times. However, as the KD teachers are from conventional multi-crop training, there are various mismatches between the global label-map and region-level label in this technique, resulting in performance deterioration. In this study, we present a Fast Knowledge Distillation (FKD) framework that replicates the distillation training phase and generates soft labels using the multi-crop KD approach, while training faster than ReLabel since no postprocesses such as RoI align and softmax operations are used. When conducting multi-crop in the same image for data loading, our FKD is even more efficient than the traditional image classification framework. On ImageNet-1K, we obtain 79.8% with ResNet-50, outperforming ReLabel by ?1.0% while being faster. On the self-supervised learning task, we also show that FKD has an efficiency advantage. Our project page is here, source code and models are available at:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Knowledge Distillation (KD) <ref type="bibr" target="#b15">[16]</ref> has been a widely used technique in various visual domains, such as the supervised recognition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48]</ref> and self-supervised representation learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b33">33]</ref>. The mechanism of knowledge distillation is to force the student to imitate the output of a teacher network or ensemble teachers, as well as converging on the ground-truth labels. Given the param- <ref type="table">Table 1</ref>. A feature-by-feature comparison between ReLabel <ref type="bibr" target="#b52">[52]</ref> and our FKD framework on various elements and properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Generating label Label storage Info. loss Training Vanilla KD Implicit None No Slow ReLabel <ref type="bibr" target="#b52">[52]</ref> Fast Efficient Yes Fast FKD (Ours) Slow Efficient No Faster eters ? of the target student at iteration (t), we can learn the next iteration parameters ? (t+1) by minimizing the following objective which contains two terms: ? (t+1) student = arg min ???</p><formula xml:id="formula_0">1 N N n=1 (1 ? ?)H (y n , S ? (x n )) +?H T (t) (x n ), S ? (x n )</formula><p>(1) where y n is the ground-truth label for n-th sample. T (t) is the teacher's output at iteration (t) and S ? (x n ) is the student's prediction for the input sample x n . H is the crossentropy loss function. ? is coefficient for balancing the two objectives. The first term aims to minimize the entropy between one-hot ground-truth label and student's prediction while the second term is to minimize between teacher and student's predictions. The teacher T can be pre-trained in either a supervised or self-supervised manner. Many literature <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b52">52]</ref> have empirically shown that the first term of true hard label in Eq. 1 is not required on largerscale datasets like ImageNet <ref type="bibr" target="#b7">[8]</ref> under the circumstance that the teacher or ensembled teachers are accurate enough. In this work, we simply minimize the soft predictions between teacher and student models for the fast distillation design.</p><p>The fundamental disadvantage in such a paradigm, according to KD's definition, is that a considerable proportion of computing resources is consumed on passing training data through large teacher networks to produce the supervision T (t) in each iteration, rather than updating or training the target student parameters. Intuitively, the forward propagation through teachers can be shared across epochs since the parameters of them are frozen for the entire training. Considering that the vanilla distillation framework itself is basically inefficient, how to reduce or share the forward computing of teacher networks across different epochs becomes the core for accelerating KD frameworks. A natural solution to overcome this drawback is to generate one probability vector as the soft label of input data correspond- ing to each image in advance, and then reuse them circularly for different training epochs. However, in modern network training, we usually impose various data augmentation strategies, particularly the random crop technique, which causes the inconsistency in which the simple globallevel soft vector for the entire image can no longer accurately reflect the true probability distribution of the local input region after these augmentations.</p><p>To address the data augmentation, specially random-crop caused inconsistency issue in generating one global vector to the region-level input, while preserving the advantage of soft label property, ReLabel <ref type="bibr" target="#b52">[52]</ref> is proposed to store the global label map annotations from a pre-trained strong teacher for reutilization by RoI align <ref type="bibr" target="#b12">[13]</ref> without passing through the teacher networks repeatedly. <ref type="figure" target="#fig_0">Fig. 1</ref> (left) shows a full explanation of this mechanism. However, due to the different input processes on teachers, this strategy is essentially not equivalent to the vanilla KD procedure. The mismatches are primarily due to two factors: (i) the teacher network is usually trained with a random-crop-resize scheme, whereas in ReLabel, the global label map is obtained by feeding into the global image, which cannot exactly reflect the soft distribution as distillation process whose randomcrop-resize operation is employed in the input space; (ii) RoI align cannot guarantee the distribution is completely identical to that from the teachers' forwarding.</p><p>In this work, we introduce a Fast Knowledge Distillation (FKD) framework to overcome the mismatching drawback and further avoid information loss on soft labels. Our strategy is straightforward: As shown in <ref type="figure" target="#fig_0">Fig 1 (right)</ref>, in the label generation phase, we directly store the soft probability from multiple random-crops into the label files, together with the coordinates and other data augmentation status like flipping. During training, we assign these stored coordinates back to the input image to generate the crop-resized input for passing through the networks, and computing the loss with the corresponding soft labels. The advantages of such a strategy are two folders: (i) Our region-based generating process and obtained soft label for each input region are identical to the vanilla KD's output, implying that no information is lost during the label creation phase; (ii) Our training phase enjoys a faster pace since no post-process is required, such as RoI align, softmax, etc. We can further assign multiple regions from the same image in a mini-batch to facilitate the burden of data loading.</p><p>We demonstrate the advantages of our FKD in terms of accuracy and training speed on supervised and selfsupervised learning tasks. In the supervised learning scheme, we compare the baseline ReLabel and vanilla KD (Oracle) from scratch across a variety of backbone network architectures, such as CNNs, vision transformers, and the competitive MEAL V2 framework with pre-trained initialization. Our FKD is ?1% higher and slightly faster than ReLabel on ImageNet-1K, and 3?5? faster than oracle KD with similar performance. On the self-supervised learning manner, we employ S 2 -BNN as the baseline for verifying the speed advantage of our proposed framework.</p><p>Our contributions of this work: ? We present a fast knowledge distillation (FKD) framework that achieves the same high level of performance as vanilla KD, while keeping the same training speed and efficiency as non-KD training without information loss. ? We reveal a discovery that in image classification frameworks, one image can be sampled with multiple crops within a mini-batch to facilitate data loading and speed up training, meanwhile without sacrificing performance. ? To prove the effectiveness and versatility of our approach, we demonstrate FKD on a variety of tasks and distillation frameworks, including supervised classification and selfsupervised representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Knowledge Distillation. The principle behind Knowledge Distillation <ref type="bibr" target="#b15">[16]</ref> is that a student is encouraged to emulate or mimic the teachers' prediction, which helps the student generalize better on unseen data. One core advantage of distillation is that the teacher can provide softened distribution which contains richer information about the input data compared to the traditional one-hot labels, especially when the data augmentation such as random cropping is used on the input space. Distillation can avoid incorrect labels by predicting them from the strong teachers in each iteration, which reflects the real situation of the transformed input data. Conventionally, we can impose a temperature on the logits to re-scale the output distributions from teacher and student models to amplify the inter-class relationship on supervisions and allow for improved distillation. Recently, many variants and extensions are proposed <ref type="bibr">[6, 18, 24-26, 35, 37, 44, 47, 49, 54]</ref>, such as employing internal feature representations <ref type="bibr" target="#b29">[30]</ref>, adversarial training with discriminators <ref type="bibr" target="#b32">[32]</ref>, transfer flow <ref type="bibr" target="#b48">[48]</ref>, contrastive distillation <ref type="bibr" target="#b40">[40]</ref>, patient and consistent <ref type="bibr" target="#b1">[2]</ref> etc. For the broader overviews of related methods for knowledge distillation, please refer to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b43">43]</ref>. Efficient Knowledge Distillation. Improving training efficiency for knowledge distillation is crucial for pushing this technique to a wider usage scope in real-world applications. Previous efforts on this direction are generally not sufficient. ReLabel <ref type="bibr" target="#b52">[52]</ref> is a recently proposed solution that addresses this inefficient issue of KD surpassingly. In particular, it generates the global label map for the strong teacher and then reuses them through RoI align across different epochs. Our proposed FKD approach in this paper lies in an essentially different consideration and solution. We consider the property of vanilla KD to generate the randomly cropped region-level soft labels from the strong teachers and store them in advance, then reuse them by allocating them to different epochs in training. Our approach enjoys the same accuracy as vanilla KD and same training speed as regular non-KD classification frameworks, making it superior than ReLabel in both performance and training speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we begin by introducing some observations and properties on ReLabel's global-level soft label and FKD's region-level soft label distributions. Then, we present the detailed workflow of our FKD framework and elaborately discuss the generated label quality, training speed and the applicability on supervised and selfsupervised learning. Finally, we analyze the strategies of label compression and storage for practical usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries: Limitations of Previous Solution</head><p>The mechanism of ReLabel through RoI align operation (an approximation solution) is naturally different from the  <ref type="figure">Figure 2</ref>. Illustration of label distributions of ReLabel <ref type="bibr" target="#b52">[52]</ref>, our FKD full label and our quantized label (Top-5). "MS" denotes the marginal smoothed labels, more details can be referred in Sec. 3.5. Grey numbers in each block are the corresponding partial probabilities/labels (limited by space) from different frameworks.</p><p>vanilla KD when generating region-level soft labels. In <ref type="figure">Fig. 2</ref>, we visualize the region-level label distributions of ReLabel and FKD on ImageNet-1K, and several empirical observations are noticed: (i) ReLabel is more confident in many cases of the regions, so the soft information is weaker than our FKD. We conjecture this is because ReLabel feeds the global images into the network instead of local regions, which makes the generated global label map encode more global category information and ignores the backgrounds, as shown in <ref type="figure">Fig. 2</ref> (row 1). Though sometimes the maximal probabilities are similar between ReLabel and FKD, FKD still contains more informative subordinate probabilities in the label distribution, as shown in <ref type="figure">Fig. 2</ref> (row 2); (ii) for some outlier regions, our strategy is substantially more robust than ReLabel, such as the loose bounding boxes of objects, partial object, etc., as shown in <ref type="figure">Fig. 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(row 3); (iii)</head><p>In some particular circumstance, ReLabel is unexpectedly collapsed with nearly uniform distribution, while our FKD still works well, as shown in the bottom row of <ref type="figure">Fig. 2</ref>. Moreover, there are existing mismatches between the soft label from ReLabel and oracle teacher prediction in KD when employing more data augmentations such as Flip, Color jittering, etc., since these augmentations are randomly applied during training. In ReLabel design, we cannot take them into account and prepare in advance when generating the global label map. In contrast, FKD is adequate to handle this situation: it is effortless to involve extra augmentations and record all information (ratio, degree, coefficient, etc.) for individual region from same or different images, and generate corresponding soft label by feeding the transformed image regions into the pre-trained teacher networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fast Knowledge Distillation</head><p>In a conventional visual training system, the bottleneck is usually from the network passing and data loader, while in  <ref type="formula">(1)</ref>. We illustrate the first 50 classes in ImageNet-1K dataset. a distillation framework, besides these computational consumptions, giant teachers have been the biggest burden for training. Our FKD aims to solve this intractable drawback. Label Generation Phase. Following the regular randomcrop resize training strategy, we randomly crop M regions from one image and employ other augmentations like flipping on them, then input these regions into the teachers to generate the corresponding soft label vectors P i , i.e., P i = T (R i ) where R is the transformed region by transformations F and T is the pre-trained teacher network, i is the region index. We store all the region coordinates and augmentation hyper-parameters F with the soft label P for the following training phase, as shown in <ref type="figure" target="#fig_0">Fig. 1 (upper right)</ref>. A detailed analysis of how to store these required values is provided in the following section. Training Phase. In the training stage, instead of randomly generating crops as the conventional image classification strategy, we directly load the label file, and assign our stored crop coordinates and data augmentations for this particular image to prepare the transformed region-level inputs. The corresponding soft label will be used as the supervision of these regions for training. With the cross-entropy loss, the objective is:</p><formula xml:id="formula_1">Gap Gap (1) (3) (2) (4) 1 #$ log ( $%&amp;&amp; 1 '() log 1 *+,-.+, 1 #$ log ( *+/-.+, 1 #$ log ( '() 1 #$ log ( 01$2345 1 #$ log 1 *+,-.+, 1 #$ log 1 '()</formula><formula xml:id="formula_2">L = ? i P i logS ? (R i ), where S ? (R i )</formula><p>is the student's prediction for the input region R i , ? is the parameter of the student model that we need to learn. The detailed training procedure is shown in <ref type="figure" target="#fig_0">Fig. 1</ref> (bottom right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Higher Label Quality</head><p>Distance Analysis. We analyze the quality of various formulations of labels through the entropy distance with measures on their mutual cross-entropy matrix. We consider three types of labels: (1) human-annotated one-hot label, ReLabel, and our FKD. We also calculate the distance on the predictions of four pre-trained models with different accuracies, including: PyTorch pre-trained model (weakest), Timm pre-trained model <ref type="bibr" target="#b46">[46]</ref> (strongest), ReLabel trained model and FKD trained model. An overview of our illustration is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. The upper curves, as well in <ref type="formula">(2)</ref>, are averaged cross-entropy across 50 classes of (ReLabel?FKD), (ReLabel?One-hot) and (FKD?Onehot). Here, we derive an important observation:</p><formula xml:id="formula_3">(D CE R?F = ?P F KD logP ReLabel ) &gt; (D CE R?O OR D CE F ?O ) (2) where D CE</formula><p>R?F is the cross-entropy value of ReLabel ? FKD. Essentially, FKD soft label can be regarded as the oracle KD label and D CE R?F is the distance to such "KD ground truth". From <ref type="figure" target="#fig_1">Fig. 3</ref> (2) we can see its distance is even large than ReLabel and FKD to the one-hot label. Since ReLabel (global-map soft label) and FKD (region-level soft label) are greatly discrepant from the one-hot hard label, the gap between ReLabel and FKD ("KD ground truth") is fairly significant and considerable. If we shift attention to the curves of D CE R?O and D CE F ?O , they are highly aligned across different classes with similar values. In some particular classes, D CE F ?O are slightly larger. This is sensible as one-hot label is basically not the "optimal label" we desired.</p><p>In the bottom group, i.e., <ref type="figure" target="#fig_1">Fig. 3 (3)</ref>, the entropy values are comparatively small. This is because they are from the pre-trained models and they have the decent performance under the criterion metric of one-hot label. Among them, M T imm has the minimal cross-entropy to the one-hot label, this is expected since the timm model is optimized thoroughly to fit the one-hot label with the highest accuracy. In <ref type="figure" target="#fig_1">Fig. 3 (4)</ref>, D CE T imm?F and D CE P T ?F lie in the middle of D CE T imm?R and D CE P T ?R with smaller variances. This reflects that FKD is more stable to the pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Faster Training Speed</head><p>Multi-crop sampling within a mini-batch. As illustrated in <ref type="figure" target="#fig_0">Fig. 1 (right)</ref>, we can use multiple crops in the same image to facilitate loading image and label files. Intuitively, it will reduce the diversity of training samples in a mini-batch since some of samples are from the same image. However, our experimental results indicate that it will not hurt the model performance, in contrast, it even boosts the accuracy when the number of crops from the same image is within a reasonable range (e.g., 4?8). Serrated learning rate scheduler. Since FKD samples multiple crops (#crop) from one image, when iterating over the entire dataset once, we actually train the dataset #crop epochs with the same learning rate. It has no effect while using milestone/step lr scheduler, but it will change the lr curve to be serrated if applying continuous cosine or linear learning rate strategies. This is also the potential reason that multi-crop training can improve the accuracy. Training Time Analysis: 1. Data Load Data loading strategy in FKD is efficient. For instance, when training with a mini-batch of 256, traditional image classification framework requires to load 256 images and ReLabel will load 256 images + 256 label files, while in our method, FKD only needs to load 256 #crop images + 256 #crop label files, even faster than traditional training if we choose a slightly larger value for #crop (when #crop &gt;2) 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Label Preparation</head><p>We assign #crop regions in an image to the current minibatch for training. Since we store the label probability after softmax (in supervised learning), we can use assigned soft labels for the mini-batch samples directly without any postprocess. This assignment is fast and efficient in implementation with a randperm function in PyTorch <ref type="bibr" target="#b26">[27]</ref>. If the label <ref type="bibr" target="#b0">1</ref> We assume that loading each image and label file will consume the similar time by CPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normal KD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ReLabel</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FKD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Load Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Load Labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generate Random Coordinates Crop-Resize Other Augs (Options)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generate Soft Labels Forward Network</head><p>(we randomly select from a set) <ref type="figure">Figure 5</ref>. Training flow analysis for normal KD, ReLabel <ref type="bibr" target="#b52">[52]</ref> and our fast knowledge distillation (FKD) framework. Maroon dashed boxes indicate that the processes are required by ReLabel only while not existing in our FKD. Note that "generate soft labels" indicates RoI align + softmax in ReLabel. We both have the recovering process from the compressed label to full soft label as discussed in Sec. <ref type="bibr">3.3.</ref> is compressed using the following strategies, we will operate with an additional simple recovering process (as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>) to obtain D-way soft label distributions. Note that ReLabel also has this process so the time consumption on this part will be similar to ReLabel. A detailed workflow and item-by-item comparison is shown in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Self-supervised Model with Supervised Scheme</head><p>In this section, we introduce how to apply our FKD for extending to the self-supervised learning (SSL) with faster training speed, comparing to the widely-used Siamese SSL frameworks. The label generation (from the self-supervised strong teachers), label preparation and training procedure are similar to the supervised scheme. However, we keep the projection head in original SSL teachers and store the soft labels before softmax for operating temperature 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Label Compression and Storage Analysis</head><p>We consider and introduce the following four strategies for compressing soft label for storage, an elaborated comparison of them can be referred to <ref type="table">Table 2.</ref> ? Hardening. In hardening quantization strategy, the hard label Y H is generated using the index of the maximum logits from the teacher predictions of regions. In general, label hardening is the one-hot label with correction by strong teacher models in region-level space.</p><formula xml:id="formula_4">Y H = argmax c z FKD (c)<label>(3)</label></formula><p>where z FKD is the logits for each randomly cropped region produced by our FKD process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Smoothing. Smoothing quantization replaces one-hot</head><p>hard label Y H with a mixture of soft y c and a uniform distribution same as label smoothing <ref type="bibr" target="#b38">[38]</ref>:</p><formula xml:id="formula_5">y S c = p c if c = hardening label, (1 ? p c )/(C ? 1) otherwise.</formula><p>(4) where p c is the probability after softmax at c-th class and C is the number of total classes. (1 ? p c )/(C ? 1) a <ref type="table">Table 2</ref>. A detailed comparison of different label quantization/compression strategies on ImageNet-1K. M is the number of crops within an image and here we choose 200 crops as an example to calculate the space consumption. Nimage is the number of images, i.e., 1.2M for ImageNet-1K. SLM is the size of label map. Cclass is the number of classes. DDA is the parameter dimension of data augmentations to store.</p><p>ReLabel (Full) <ref type="bibr" target="#b52">[52]</ref> ReLabel (Top-5) <ref type="bibr">[</ref> </p><formula xml:id="formula_6">y MS c = ? ? ? ? ? p c if c ? {Top?K}, 1? c?{Top?K} pc C?K otherwise.<label>(5)</label></formula><p>where y MS c ? Y MS is the marginally smoothed label at c-th class.</p><p>? Marginal Re-Norm with Top-K (MR). Marginal renormalization will re-normalize Top-K predictions to c?{Top?K} p c = 1 and maintain other logits to be zero (this strategy is spiritually similar to ReLabel <ref type="bibr" target="#b52">[52]</ref> but slightly different in implementation as its input is logits before softmax so it used softmax while we use normalize, resulting in that our values outside Top-K remain zero.):</p><formula xml:id="formula_7">y M c = p c if c ? {Top?K}, 0 otherwise. (6) y MR c = Normalize(y M c ) = y M c C c=1 (y M c )<label>(7)</label></formula><p>where y MR c ?Y MR is the re-normalized label at c-th class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Experimental Settings and Datasets. Detailed lists of our hyper-parameter choices are shown in <ref type="table">Table 10</ref>, <ref type="table">Table 11</ref> of Appendix. For the transparency and reproducibility of our framework, unless noted otherwise, we did not involve extra data augmentations (beyond the basic random crop and random horizontal flip) such as RandomAug <ref type="bibr" target="#b6">[7]</ref>, MixUp <ref type="bibr" target="#b53">[53]</ref>, CutMix <ref type="bibr" target="#b51">[51]</ref>, etc., in all of our experiments.</p><p>Except for experiments on MEAL V2, we use EfficientNet-L2-ns-475 <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b47">47]</ref> as the teacher model, we also tried weaker teacher but the performance in our experiment is slightly worse. For MEAL V2, we follow its original design by using SENet154 + ResNet152 v1s (gluon version) ensemble as the soft label. ImageNet-1K <ref type="bibr" target="#b7">[8]</ref> is used for the supervised classification and self-supervised representation learning. COCO <ref type="bibr" target="#b20">[21]</ref> is used for the transfer learning experiments in this work. <ref type="table">Table 3</ref>. Comparison between ReLabel <ref type="bibr" target="#b52">[52]</ref> and our FKD on ImageNet-1K. "?" denotes our training following the same protocol in <ref type="table">Table 10</ref>    <ref type="bibr" target="#b18">[19]</ref>, such as ResNet <ref type="bibr" target="#b13">[14]</ref>, MobileNet <ref type="bibr" target="#b16">[17]</ref> and Vision Transformers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">42]</ref>, such as DeiT <ref type="bibr" target="#b41">[41]</ref>, SReT <ref type="bibr" target="#b34">[34]</ref>. For binary backbone, we use ReAct-Net <ref type="bibr" target="#b21">[22]</ref> in the self-supervised experiments. Learning Schemes. We consider three training manners in vision tasks: (i) conventional supervised training from scratch; (ii) supervised fine-tuning from pre-trained parameters; and (iii) self-supervised distillation from scratch. Baseline Knowledge Distillation Methods.</p><p>ReLabel <ref type="bibr" target="#b52">[52]</ref> (Label Map Distillation). ReLabel used the pre-generated global label maps from the pre-trained teacher for reducing the cost on the teacher branch when conducting distillation. MEAL V2 <ref type="bibr" target="#b36">[36]</ref> (Fine-tuning Distillation). MEAL V2 proposed to distill student network from the pre-trained parameters <ref type="bibr" target="#b2">3</ref> and giant teacher ensemble for fast convergence and better accuracy. FunMatch <ref type="bibr" target="#b52">[52]</ref> (Oracle Distillation). FunMatch is a standard knowledge distillation framework with strong teacher models and augmentations. We consider it as the strong baseline approach for efficient KD when using the same or similar teacher supervisors.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Supervised Learning</head><p>CNNs.</p><p>(i) ReLabel <ref type="bibr" target="#b52">[52]</ref>. The comparison with ReLabel is shown in <ref type="table">Table 3</ref>, using the training settings introduced in our Appendix, which is nearly the same as ReLabel, our accuracies on ResNet-50/101 both outperform ReLabel by ?1% with slightly faster training speed. These significant and consistent improvements of FKD show great potential for practical usages in real-world applications.</p><p>(ii) MEAL V2 <ref type="bibr" target="#b36">[36]</ref>. We use FKD framework to train the MEAL V2 models. The results are shown in Tabel 4. When employing the same hyper-parameters and teacher networks, FKD can speed up 2?4? for MEAL V2 without compromising accuracy. Using cosine lr and more epochs in training further improves the accuracy.</p><p>(iii) FunMatch <ref type="bibr" target="#b1">[2]</ref> (Oracle). We consider FunMatch as the oracle/strong KD baseline, our plain FKD achieves 79.8% w/o extra augmentations, which is 0.7% lower than FunMatch (80.5%). The result of FKD with more complex optimizers <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b50">50]</ref> and more augmentations <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b53">53]</ref> (similar to FunMatch) will be presented after being explored. Vision Transformers.</p><p>(i) ViT/DeiT <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b41">41]</ref>. The results are shown in Tabel 5 of first group. Our non-extra augmentation result (75.2%) using ViT-T backbone is better than DeiT-T with distillation (74.5%), while we only require 0.15? resources than DeiT distillation protocol in training.</p><p>(ii) SReT <ref type="bibr" target="#b34">[34]</ref>. We also conduct our FKD using SReT-LT, our result (78.7%) is consistently better than its original KD design (77.7%) with fewer training resources. Ablations: (i) Effects of Crop Number in Each Image. We explore the effect of different numbers of crops sampled from the same image within a mini-batch to the final performance. For the conventional data preparation strategy, on each image we solely sample one crop for a mini-batch to train the model. Here, we evaluate the m from 1 crop to 32 crops as shown in <ref type="table" target="#tab_5">Table 7</ref>. Surprisingly, using a few crops from the same image leads to better performance than the single crop solution with a non-negligible margin, especially on the traditional image classification system. This indicates that the internal diversity of samples in a minibatch has a limit for tolerance, properly reducing such diversity can boost the accuracy, while we can also observe that after m&gt;8, the performance decreases substantially, thus the diversity is basically still critical for learning good status of the model. Nevertheless, this is a good observation for us to speedup data loading in our FKD framework.</p><p>(ii) Different Label Compression Strategies. We evaluate the performance for different label compression strategies. We use m=8 for this ablation and the results are shown in Tabel 6. On MEAL V2 w/ FKD, we obtain the highest accuracy 80.65% when using the full soft labels, while on the pure FKD, the best performance is from Marginal Smoothing (K=5) with 79.51%. Increasing K both decrease the accuracies in these two scenarios, we conjecture that larger K will involve more noise or unnecessary minor information on the soft labels. While, they are still better than the Hard and Smoothing strategies. <ref type="bibr" target="#b33">[33]</ref> is a pure distillation-based framework for self-supervised learning, thus the proposed FKD approach is sufficient to train S 2 -BNN <ref type="bibr" target="#b33">[33]</ref> smoothly and more efficiently. We employ SwAV <ref type="bibr" target="#b2">[3]</ref> and MoCo V2 <ref type="bibr" target="#b4">[5]</ref> pre-trained <ref type="table">Table 8</ref>. Linear evaluation results of FKD with self-supervised Binary CNN (ReActNet <ref type="bibr" target="#b21">[22]</ref>), Real-valued CNN (ResNet-50 <ref type="bibr" target="#b13">[14]</ref> models as the teacher networks. Consider the more flattening distribution from the SSL learned teachers than the supervised teacher predictions (indicating that the subordinate classes from SSL trained teachers also carry crucial information), we still use the full soft label for now, and label compression strategies on SSL task will be verified further. We employ ReActNet <ref type="bibr" target="#b21">[22]</ref> and ResNet-50 <ref type="bibr" target="#b13">[14]</ref> as the target/student backbones in these experiments. The results are shown in <ref type="table">Table 8</ref>, our FKD trained models achieve slightly better performance than S 2 -BNN with roughly 3? acceleration in training since we only use a single branch of network and no explicitly teacher forwarding existing. The slight boosts are from our liter data augmentation for FKD when producing SSL soft labels. This is interesting in the FKD equipped SSL methods, i.e., data augmentation strategies for distillation-based SSL, and is worth exploring further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Self-Supervised Learning</head><formula xml:id="formula_8">S 2 -BNN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transfer Learning</head><p>Here, we further examine whether the FKD obtained improvements on ImageNet-1K can be transferred to various downstream tasks. As shown in Tabel 9, we present the results of object detection and instance segmentation tasks on COCO dataset <ref type="bibr" target="#b20">[21]</ref> with models pretrained on ImageNet-1K with FKD. We also employ Faster RCNN <ref type="bibr" target="#b28">[29]</ref> and Mask RCNN <ref type="bibr" target="#b12">[13]</ref> with FPN <ref type="bibr" target="#b19">[20]</ref> following ReLabel <ref type="bibr" target="#b52">[52]</ref>. Over the regular baseline and ReLabel, our FKD pre-trained parameters show constant gains on the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization, Analysis and Discussion</head><p>To investigate the learned differences of information between ReLabel and FKD, we depict the intermediate attention maps using gradient-based localization <ref type="bibr" target="#b30">[31]</ref>. There are three important observations that align our aforementioned analyses in <ref type="figure">Fig. 6</ref>.</p><p>(i) FKD's predictions are less confident than ReLabel with more surrounding context; This is reasonable since in random-crop training, many crops are basically back-ReLabel FKD Base Input <ref type="figure">Figure 6</ref>. Visualizations of learned attention map using Grad-CAM <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31]</ref>. "Base" indicates the pre-trained PyTorch model. In each group of ReLabel and FKD, left is Grad-CAM and right is Guided Backprop. grounds (context), the soft predicted label from the teacher model might be completely different from the ground-truth one-hot label and the training mechanism of FKD can leverage the additional information from context.</p><p>(ii) FKD's attention maps have a larger active area on the object regions, which indicates that FKD trained model utilizes more cues for prediction and also captures more subtle and fine-grained information. However, it is interesting to see that the guided backprop is more focusing than ReLabel.</p><p>(iii) ReLabel's attention is more aligned with PyTorch pre-trained model, while FKD's results are substantially unique to them. It implies that FKD's learned attention differs significantly from one-hot and global label map learned models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Given its widespread use and superior performance in training compact and efficient networks, it is worthwhile investigating ways to increase the efficiency and speed of vanilla knowledge distillation. In this paper, we have presented a fast distillation framework through the pregenerated region-level soft label scheme. We have elaborately discussed the strategies of compressing soft label for practical storage and their performance comparison. We identified the observation that the input training samples within a mini-batch can be sampled from the same input image to facilitate the overhead of data loading process. We exhibit the effectiveness and adaptability of our framework by demonstrating it on supervised image classification and self-supervised representation learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Details and Experimental Settings</head><p>Training details used in <ref type="table">Table 3</ref> of the main text. When comparing our FKD with ReLabel <ref type="bibr" target="#b52">[52]</ref>  <ref type="table">(Table 3</ref> of the main text), we use the training settings and hyper-parameters following <ref type="table">Table 10</ref>, which is nearly the same as ReLabel <ref type="bibr" target="#b52">[52]</ref> while without Warmup and Color jittering. <ref type="table">Table 10</ref>. Training hyper-parameters and details between ReLabel <ref type="bibr" target="#b52">[52]</ref> and FKD used for the comparison in <ref type="table">Table 3</ref> of main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>ReLabel <ref type="bibr">[</ref> Training details used in <ref type="table">Table 5</ref> of the main text. When comparing our FKD with ViT [9]/DeiT <ref type="bibr" target="#b41">[41]</ref>/SReT <ref type="bibr" target="#b34">[34]</ref> ( <ref type="table">Table 5</ref> of the main text), we use the training settings and hyper-parameters following <ref type="table">Table 11</ref>. <ref type="table">Table 11</ref>. Training hyper-parameters and details for the comparison in <ref type="table">Table 5</ref> of the main text when employing ViT <ref type="bibr" target="#b8">[9]</ref> and its variants as the backbone networks. In this section, we provide more results on Ima-geNet ReaL <ref type="bibr" target="#b0">[1]</ref> and ImageNetV2 <ref type="bibr" target="#b27">[28]</ref> datasets. On Ima-geNetV2 <ref type="bibr" target="#b27">[28]</ref>, we verify our FKD models on three metrics "Top-Images", "Matched Frequency", and "Threshold 0.7" as ReLabel <ref type="bibr" target="#b52">[52]</ref>. We conduct experiments on two network structures: ResNet-50 and ResNet-101. The results are shown in <ref type="table" target="#tab_9">Table 12</ref>, we achieve consistent improvement over baseline ReLabel on both ResNet-50 and ResNet-101.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Visualizations</head><p>We provide more visualizations of the intermediate attention maps from ResNet-50 to explore the learned differences of information between ReLabel and FKD, as shown in <ref type="figure" target="#fig_3">Fig. 7</ref>. The observation is consistent to our main text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of ReLabel<ref type="bibr" target="#b52">[52]</ref> and our fast knowledge distillation (FKD) framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Entropy distance analysis between different pairs of soft/one-hot labels and different labels trained model predictions. (1) is the overall distance visualization. (2), (3), (4) represent each detailed group in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Different label compression strategies and storage analysis for our fast knowledge distillation (FKD) framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>More visualizations of response/attention maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>-04, 3.2259e-04, 3.2259e-04, 3.2259e-04, 1.7468e-03, 3.4675e-04, 3.2259e-04, 6.8171e-04, 3.8466e-04, 3.2259e-04, 3.8022e-04, 6.1787e-04, 3.2259e-04, 6.7311e-01, 3.2259e-04, 3.2356e-04, 3.3983e-04, 3.2259e-04, 9.2554e-05, 1.1788e-04, 8.0115e-05, 1.1138e-04, 6.4849e-05, 4.7843e-05, 6.1574e-05, 5.1641e-05, 2.2732e-04, 4.3680e-05, 3.5955e-02, 1.8559e-02, 1.0231e-02, 9.1087e-02, 4.6381e-04, 1.2917e-03, 2.7748e-02, 4.0997e-02, 7.3948e-04, 6.9020e-01, 2.3188e-03, 2.3724e-04, 4.7438e-05, 7.7372e-05, 1.1458e-04, 1.1458e-04, 1.1458e-04, 1.1458e-04, 1.1458e-04, 1.1458e-04, 1.1458e-04, 1.1458e-04, 1.1458e-04, 1.1458e-04, 3.5955e-02, 1.1458e-04, 1.1458e-04, 9.1087e-02, 1.1458e-04, 1.1458e-04, 2.7748e-02, 4.0997e-02,</figDesc><table><row><cell>Crop ID 4</cell><cell>2.3047e-04, 2.3454e-04, 2.3047e-04, 2.3047e-04, 2.3047e-04, 2.3047e-04, 2.3047e-04, 2.3047e-04, 2.3047e-04, 2.3557e-04, 7.9685e-04, 2.3047e-04, 3.0858e-04, 7.6529e-01, 2.3047e-04, 2.9043e-04, 5.5709e-04, 5.8863e-04,</cell><cell>6.8742e-05, 6.4029e-05, 9.0348e-05, 1.1750e-04, 8.8943e-05, 8.5020e-05, 1.2789e-04, 9.2288e-05, 1.1462e-04, 7.1049e-05, 9.8732e-02, 1.4937e-02, 4.5940e-02, 2.0252e-01, 1.4085e-02, 2.9823e-02, 5.8928e-03, 1.1526e-01,</cell><cell>2.0375e-04, 2.0375e-04, 2.0375e-04, 2.0375e-04, 2.0375e-04, 2.0375e-04, 2.0375e-04, 2.0375e-04, 2.0375e-04, 2.0375e-04, 9.8732e-02, 2.0375e-04, 4.5940e-02, 2.0252e-01, 2.0375e-04, 2.0375e-04, 2.0375e-04, 1.1526e-01,</cell></row><row><cell></cell><cell>3.2259e-04, 3.4571e-04, 3.2259e-04,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>3.2259e-04, 3.2259e-04, 3.2259e-04,</cell><cell></cell><cell></cell></row><row><cell>Crop ID 5</cell><cell cols="3">3.2259e1.1458e-04, 6.9020e-01, 1.1458e-04, 1.1458e-04, 1.1458e-04, 1.1458e-04,</cell></row><row><cell></cell><cell>0.0007, 0.0007, 0.2120,</cell><cell>2.0470e-04, 9.9457e-05, 6.1459e-01,</cell><cell>1.6104e-04, 1.6104e-04, 6.1459e-01,</cell></row><row><cell></cell><cell>0.1068, 0.0032, 0.0007,</cell><cell>1.0494e-01, 1.2174e-02, 1.4359e-03,</cell><cell>1.0494e-01, 1.6104e-04, 1.6104e-04,</cell></row><row><cell></cell><cell>0.0007, 0.0007, 0.0007,</cell><cell>5.9240e-04, 8.6308e-05, 6.8236e-05,</cell><cell>1.6104e-04, 1.6104e-04, 1.6104e-04,</cell></row><row><cell></cell><cell>0.0007, 0.0007, 0.0007,</cell><cell>7.5624e-05, 9.7924e-05, 1.0270e-04,</cell><cell>1.6104e-04, 1.6104e-04, 1.6104e-04,</cell></row><row><cell></cell><cell>0.0007, 0.0007, 0.0007,</cell><cell>1.0274e-04, 6.8960e-05, 1.9348e-04,</cell><cell>1.6104e-04, 1.6104e-04, 1.6104e-04,</cell></row><row><cell></cell><cell>0.0007, 0.0007, 0.0007,</cell><cell>9.1110e-05, 1.3632e-04, 8.7275e-05,</cell><cell>1.6104e-04, 1.6104e-04, 1.6104e-04,</cell></row><row><cell>Crop ID 3</cell><cell>0.0007, 0.0007, 0.0007,</cell><cell>7.9049e-05, 1.0957e-04, 1.5789e-04,</cell><cell>1.6104e-04, 1.6104e-04, 1.6104e-04,</cell></row><row><cell></cell><cell>0.0019, 0.0026, 0.0010,</cell><cell>3.0239e-03, 9.0165e-01, 4.1147e-05,</cell><cell>3.0239e-03, 9.0165e-01, 9.2765e-05,</cell></row><row><cell></cell><cell>0.0010, 0.0010, 0.0010,</cell><cell>5.7362e-05, 4.8461e-05, 5.9309e-04,</cell><cell>9.2765e-05, 9.2765e-05, 5.9309e-04,</cell></row><row><cell></cell><cell>0.0010, 0.0010, 0.0010,</cell><cell>6.9991e-05, 1.0403e-04, 7.3024e-05,</cell><cell>9.2765e-05, 9.2765e-05, 9.2765e-05,</cell></row><row><cell></cell><cell>0.0010, 0.0010, 0.0010,</cell><cell>7.8669e-05, 9.2071e-05, 5.7395e-05,</cell><cell>9.2765e-05, 9.2765e-05, 9.2765e-05,</cell></row><row><cell></cell><cell>0.0010, 0.0010, 0.0010,</cell><cell>1.7510e-04, 5.2435e-05, 7.0081e-05,</cell><cell>9.2765e-05, 9.2765e-05, 9.2765e-05,</cell></row><row><cell>Crop ID 3</cell><cell>0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, collapse</cell><cell>6.3078e-05, 5.8547e-05, 6.0871e-05, 5.5401e-05, 5.1361e-05, 9.6294e-05,</cell><cell>9.2765e-05, 9.2765e-05, 9.2765e-05, 9.2765e-05, 9.2765e-05, 9.2765e-05,</cell></row><row><cell>Random Crops</cell><cell>ReLabel</cell><cell>Our Full</cell><cell>Our MS (K=5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Calculation Nimage ?SLM ?Cclass Nimage ?SLM ?2CTop-5 Nimage ?(Cclass +DDA) Nimage ?(1+DDA) Nimage ?(2+DDA) Nimage ?(2K +DDA) Nimage ?(2K +DDA) Nimage ?(2K +DDA) Dim. of Soft Label 15 ? 15 ? 1, 000 15 ? 15 ? 10</figDesc><table><row><cell></cell><cell></cell><cell>52]</cell><cell>Full</cell><cell>Hard</cell><cell>Smoothing</cell><cell>M Re-Norm (K=5)</cell><cell>MS (K=5)</cell><cell>MS (K=10)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>M ?1,000</cell><cell>M ?1</cell><cell>M ?2</cell><cell>M ?10</cell><cell>M ?10</cell><cell>M ?20</cell></row><row><cell>+ Coordinate &amp; Flip</cell><cell>-</cell><cell>-</cell><cell>M ?1,005</cell><cell>M ?6</cell><cell>M ?7</cell><cell>M ?15</cell><cell>M ?15</cell><cell>M ?25</cell></row><row><cell>Real Cons. on Disk</cell><cell>?1TB</cell><cell>10GB</cell><cell>?0.9TB</cell><cell>5.3GB</cell><cell>6.2GB</cell><cell>13.3GB</cell><cell>13.3GB</cell><cell>22.2GB</cell></row><row><cell cols="4">small value for flattening the one-hot labels. y S c ? Y S is</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">the smoothed label at c-th class.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">? Marginal Smoothing with Top-K (MS). Marginal</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">smoothing quantization reserves more soft information</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">(Top-K) of teacher prediction than the single smoothing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>label Y S :</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>w/o distillation. Models are trained from scratch.</figDesc><table><row><cell>Method</cell><cell>Network</cell><cell>Top-1 (%)</cell><cell cols="2">Top-5 (%) Training Time</cell></row><row><cell>Vanilla?</cell><cell>ResNet-50</cell><cell>78.1</cell><cell>94.0</cell><cell>1.0</cell></row><row><cell cols="2">ReLabel [52] ResNet-50</cell><cell>78.9</cell><cell>-</cell><cell>?0.5% [52]</cell></row><row><cell>FKD (Ours)</cell><cell>ResNet-50</cell><cell>79.8 +0.9</cell><cell>94.6</cell><cell>?0.5%</cell></row><row><cell>Vanilla?</cell><cell>ResNet-101</cell><cell>79.7</cell><cell>94.6</cell><cell>1.0</cell></row><row><cell cols="2">ReLabel [52] ResNet-101</cell><cell>80.7</cell><cell>-</cell><cell>?0.5% [52]</cell></row><row><cell>FKD (Ours)</cell><cell>ResNet-101</cell><cell>81.7 +1.0</cell><cell>95.6</cell><cell>?0.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of MEAL V2<ref type="bibr" target="#b36">[36]</ref> and our FKD on ImageNet-1K. "w/ FKD" denotes the model is trained using the same protocol as original MEAL V2, i.e., all the same hyperparameters. "?" represents the training using cosine lr and 1.5? epochs. Models are trained from the pre-trained initialization.</figDesc><table><row><cell>Method</cell><cell>Network</cell><cell cols="3">#Params Top-1 Top-5 Speedup</cell></row><row><cell>MEAL V2 [36]</cell><cell>ResNet-50</cell><cell>25.6M</cell><cell>80.67 95.09</cell><cell>1.0</cell></row><row><cell>MEAL V2 w/ FKD</cell><cell>ResNet-50</cell><cell>25.6M</cell><cell>80.70 95.13</cell><cell>0.3?</cell></row><row><cell>MEAL V2 w/ ?FKD</cell><cell>ResNet-50</cell><cell>25.6M</cell><cell>80.91 95.39</cell><cell>0.5?</cell></row><row><cell>MEAL V2 [36]</cell><cell>MobileNet V3-S0.75</cell><cell>2.04M</cell><cell>67.60 87.23</cell><cell>1.0</cell></row><row><cell cols="2">MEAL V2 w/ ?FKD MobileNet V3-S0.75</cell><cell>2.04M</cell><cell>67.83 87.35</cell><cell>0.4?</cell></row><row><cell>MEAL V2 [36]</cell><cell>MobileNet V3-S1.0</cell><cell>2.54M</cell><cell>69.65 88.71</cell><cell>1.0</cell></row><row><cell cols="2">MEAL V2 w/ ?FKD MobileNet V3-S1.0</cell><cell>2.54M</cell><cell>69.94 88.82</cell><cell>0.4?</cell></row><row><cell cols="5">Network Architectures. Experiments are conducted on</cell></row><row><cell cols="2">Convolutional Neural Networks</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>FKD with supervised Vision Transformer<ref type="bibr" target="#b8">[9]</ref> variants (224?224 input size) on ImageNet-1K. Models are trained from scratch. Ablation results (Top-1) on ImageNet-1K of different label quantization strategies. m = 8 is used in this ablation.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="5">Network Epochs #Params (M) FLOPs (B)</cell><cell>Extra Data Aug.</cell><cell>Top-1 (%)</cell><cell>Speedup</cell></row><row><cell cols="2">DeiT [41] w/o KD</cell><cell></cell><cell>ViT-T</cell><cell>300</cell><cell>5.7</cell><cell>1.3</cell><cell>MixUp+CutMix+RA</cell><cell>72.2</cell><cell>-</cell></row><row><cell cols="2">DeiT [41] w/ KD</cell><cell></cell><cell>ViT-T</cell><cell>300</cell><cell>5.7</cell><cell>1.3</cell><cell>MixUp+CutMix+RA</cell><cell>74.5</cell><cell>1.0</cell></row><row><cell cols="2">ViT [9] (Vanilla)</cell><cell></cell><cell>ViT-T</cell><cell>300</cell><cell>5.7</cell><cell>1.3</cell><cell>None</cell><cell>68.7 [15]</cell><cell>-</cell></row><row><cell cols="2">ViT w/ FKD (Ours)</cell><cell></cell><cell>ViT-T</cell><cell>300</cell><cell>5.7</cell><cell>1.3</cell><cell>None</cell><cell>75.2</cell><cell>0.15?</cell></row><row><cell cols="2">SReT [34] w/o KD</cell><cell cols="2">SReT-LT</cell><cell>300</cell><cell>5.0</cell><cell>1.2</cell><cell>MixUp+CutMix+RA</cell><cell>76.7</cell><cell>-</cell></row><row><cell cols="2">SReT [34] w/ KD</cell><cell cols="2">SReT-LT</cell><cell>300</cell><cell>5.0</cell><cell>1.2</cell><cell>MixUp+CutMix+RA</cell><cell>77.7</cell><cell>1.0</cell></row><row><cell cols="2">SReT [34] (Vanilla)</cell><cell cols="2">SReT-LT</cell><cell>300</cell><cell>5.0</cell><cell>1.2</cell><cell>None</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">SReT w/ FKD (Ours) SReT-LT</cell><cell>300</cell><cell>5.0</cell><cell>1.2</cell><cell>None</cell><cell>78.7</cell><cell>0.14?</cell></row><row><cell>Method</cell><cell>Network</cell><cell>Full</cell><cell cols="6">Hard Smoothing Marginal Re-Norm (K=5) Marginal Smoothing (K=5) Marginal Smoothing (K=10)</cell></row><row><cell cols="4">MEAL V2 w/ FKD ResNet-50 80.65 80.20</cell><cell>80.23</cell><cell></cell><cell>80.40</cell><cell>80.58</cell><cell>80.52</cell></row><row><cell cols="4">FKD (from scratch) ResNet-50 79.48 79.09</cell><cell>79.37</cell><cell></cell><cell>79.23</cell><cell>79.51</cell><cell>79.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Ablation results (Top-1) on ImageNet-1K of different numbers (m) of cropping regions within an image used in a mini-batch.</figDesc><table><row><cell>Method</cell><cell>Network</cell><cell cols="6">m = 1 m = 2 m = 4 m = 8 m = 16 m = 32</cell></row><row><cell>Vanilla</cell><cell cols="2">ResNet-50 77.18</cell><cell>77.91</cell><cell>78.14</cell><cell>77.89</cell><cell>75.89</cell><cell>70.09</cell></row><row><cell cols="3">MEAL V2 w/ FKD ResNet-50 80.67</cell><cell>80.70</cell><cell>80.66</cell><cell>80.58</cell><cell>80.36</cell><cell>80.17</cell></row><row><cell cols="3">FKD (from scratch) ResNet-50 79.59</cell><cell>79.62</cell><cell>79.76</cell><cell>79.51</cell><cell>78.12</cell><cell>74.61</cell></row><row><cell cols="3">S 2 -BNN [52] (Self-supervised Distillation). A distil-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">lation solution for self-supervised learning task. The</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">teacher is pre-learned from the self-supervised learning</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">methods, such as MoCo V2 [5], SwAV [3], etc.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Table is adapted from<ref type="bibr" target="#b41">[41]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="2">ViT-B [9] DeiT [41]/SReT [34]</cell><cell>FKD</cell></row><row><cell>Epoch</cell><cell>300</cell><cell>300</cell><cell>300</cell></row><row><cell>Batch size</cell><cell>4096</cell><cell>1024</cell><cell>1024</cell></row><row><cell>Optimizer</cell><cell>AdamW</cell><cell>AdamW</cell><cell>AdamW</cell></row><row><cell>Init. lr</cell><cell>0.003</cell><cell>0.001</cell><cell>0.002</cell></row><row><cell>lr scheduler</cell><cell>cosine</cell><cell>cosine</cell><cell>cosine</cell></row><row><cell>Weight decay</cell><cell>0.3</cell><cell>0.05</cell><cell>0.05</cell></row><row><cell>Warmup epochs</cell><cell>3.4</cell><cell>5</cell><cell>5</cell></row><row><cell>Label smoothing</cell><cell>None</cell><cell>0.1</cell><cell>None</cell></row><row><cell>Dropout</cell><cell>0.1</cell><cell>None</cell><cell>None</cell></row><row><cell>Stoch. Depth</cell><cell>None</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Repeated Aug</cell><cell>None</cell><cell>Yes</cell><cell>None</cell></row><row><cell>Gradient Clip.</cell><cell>Yes</cell><cell>None</cell><cell>None</cell></row><row><cell>Rand Augment</cell><cell>None</cell><cell>9/0.5</cell><cell>None</cell></row><row><cell>Mixup prob.</cell><cell>None</cell><cell>0.8</cell><cell>None</cell></row><row><cell>Cutmix prob.</cell><cell>None</cell><cell>1.0</cell><cell>None</cell></row><row><cell>Erasing prob.</cell><cell>None</cell><cell>0.25</cell><cell>None</cell></row><row><cell cols="4">B. More Comparison and Results on ImageNet</cell></row><row><cell cols="4">ReaL [1] and ImageNetV2 [28] Datasets</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 12 .</head><label>12</label><figDesc>Results of FKD on ImageNet ReaL<ref type="bibr" target="#b0">[1]</ref> and Ima-geNetV2<ref type="bibr" target="#b27">[28]</ref> with ResNet-{50, 101}. * indicates that the results are tested using their provided pre-trained model.</figDesc><table><row><cell>Method</cell><cell cols="3">ImageNet ReaL ImageNetV2</cell><cell>ImageNetV2</cell><cell>ImageNetV2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Top-images Matched-frequency Threshold-0.7</cell></row><row><cell>ResNet-50:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ReLabel [52]</cell><cell>78.9</cell><cell>85.0</cell><cell>80.5</cell><cell>67.3</cell><cell>76.0</cell></row><row><cell>FKD</cell><cell>79.8</cell><cell>85.5</cell><cell>81.0</cell><cell>68.1</cell><cell>76.7</cell></row><row><cell>ResNet-101:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ReLabel [52]  *</cell><cell>80.7</cell><cell>86.5</cell><cell>82.4</cell><cell>69.7</cell><cell>78.2</cell></row><row><cell>FKD</cell><cell>81.7</cell><cell>87.0</cell><cell>83.1</cell><cell>70.5</cell><cell>78.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The temperature ? is applied on the logits before the softmax operation for self-supervised distillation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The pre-trained parameter is from timm<ref type="bibr" target="#b45">[45]</ref> with version&lt;=0.4.12.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Xiaohua Zhai, and A?ron van den Oord</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolesnikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159,2020.11</idno>
	</analytic>
	<monogr>
		<title level="m">Are we done with imagenet? arXiv preprint</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Knowledge distillation: A good teacher is patient and consistent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Am?lie</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larisa</forename><surname>Markeeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05237</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature-map-level online adversarial knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inseop</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
		<idno>PMLR, 2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Seed: Self-supervised distillation for visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pytorch library for cam methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Gildenblat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/jacobgil/pytorch-grad-cam" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge distillation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1789" to="1819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Like what you like: Knowledge distill via neuron selectivity transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01219</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reactnet: Towards precise binary neural network with generalized activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="143" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE symposium on security and privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<idno>PMLR, 2019. 11</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page" from="5389" to="5400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grad-Cam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Meal: Multi-model ensemble via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhankui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4886" to="4893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">S2-bnn: Bridging the gap between self-supervised real and 1-bit neural networks via guided distribution calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2165" to="2174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05297</idno>
		<title level="m">Sliced recursive transformer</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kwang-Ting Cheng, and Marios Savvides. Is label smoothing truly incompatible with knowledge distillation: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08453</idno>
		<title level="m">Meal v2: Boosting vanilla resnet-50 to 80%+ top-1 accuracy on imagenet without tricks</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Kirichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05945</idno>
		<title level="m">Does knowledge distillation really work</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR, 2021. 6</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10959</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Dataset distillation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00476,2021.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dreaming to distill: Data-free knowledge transfer via deepinversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Hongxu Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Niraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8715" to="8724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Re-labeling imagenet: from single to multi-labels, from global to localized labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
