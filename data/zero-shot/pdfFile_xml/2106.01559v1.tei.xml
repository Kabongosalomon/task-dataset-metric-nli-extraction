<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adjacency List Oriented Relational Fact Extraction via Adaptive Multi-task Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fubang</forename><surname>Zhao</surname></persName>
							<email>fubang.zfb@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoren</forename><surname>Jiang</surname></persName>
							<email>jiangzhuoren@zju.edu.cnyangyang.kangyy@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Public Affairs</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlong</forename><surname>Sun</surname></persName>
							<email>changlong.scl@taobao.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhong</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Informatics, Computing and Engineering</orgName>
								<orgName type="institution">IUB</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adjacency List Oriented Relational Fact Extraction via Adaptive Multi-task Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relational fact extraction aims to extract semantic triplets from unstructured text. In this work, we show that all of the relational fact extraction models can be organized according to a graph-oriented analytical perspective. An efficient model, aDjacency lIst oRiented rElational faCT (DIRECT), is proposed based on this analytical framework. To alleviate challenges of error propagation and sub-task loss equilibrium, DIRECT employs a novel adaptive multi-task learning strategy with dynamic sub-task loss balancing. Extensive experiments are conducted on two benchmark datasets, and results prove that the proposed model outperforms a series of state-of-the-art (SoTA) models for relational triplet extraction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relational fact extraction, as an essential NLP task, is playing an increasingly important role in knowledge graph construction <ref type="bibr" target="#b14">(Han et al., 2019;</ref><ref type="bibr" target="#b7">Distiawan et al., 2019)</ref>. It aims to extract relational triplet from the text. A relational triplet is in the form of (subject, relation, object) or (s, r, o) <ref type="bibr" target="#b33">(Zeng et al., 2019)</ref>. While various prior models proposed for relational fact extraction, few of them analyze this task from the perspective of output data structure.</p><p>As shown in <ref type="figure">Figure 1</ref>, the relational fact extraction can be characterized as a directed graph construction task, where graph representation flexibility and heterogeneity accompany additional benefaction. In practice, there are three common ways to represent graphs <ref type="bibr" target="#b12">(Gross and Yellen, 2005)</ref>:</p><p>Edge List is utilized to predict a sequence of triplets (edges). The recent sequence-to-sequence based models, such as NovelTagging <ref type="bibr" target="#b36">(Zheng et al., 2017)</ref>, CopyRE <ref type="bibr" target="#b34">(Zeng et al., 2018)</ref>, CopyRL (Zeng * These two authors contributed equally to this research. ? Zhuoren Jiang is the corresponding author <ref type="figure">Figure 1</ref>: Example of exploring the relational fact extraction task from the perspective of directed graph representation method as output data structure. <ref type="bibr">et al., 2019)</ref>, and PNDec <ref type="bibr" target="#b22">(Nayak and Ng, 2020)</ref>, fall into this category. Edge list is a simple and space-efficient way to represent a graph <ref type="bibr" target="#b0">(Arifuzzaman and Khan, 2015)</ref>. However, there are three problems. First, the triplet overlapping problem <ref type="bibr" target="#b34">(Zeng et al., 2018)</ref>. For instance, as shown in <ref type="figure">Figure 1</ref>, for triplets (Obama, nationality, USA) and (Obama, president of, USA), there are two types of relations between the "Obama" and "USA". If the model only generates one sequence from the text <ref type="bibr" target="#b36">(Zheng et al., 2017)</ref>, it may fail to identify the multi-relation between entities. Second, to overcome the triplet overlapping problem, the model may have to extract the triplet element repeatedly <ref type="bibr" target="#b34">(Zeng et al., 2018)</ref>, which will increase the extraction cost. Third, there could be an ordering problem <ref type="bibr" target="#b33">(Zeng et al., 2019)</ref>: for multiple triplets, the extraction order could influence the model performance.</p><p>Adjacency Matrices are used to predict matrices that represent exactly which entities (vertices) have semantic relations (edges) between them. Most early works, which take a pipeline ap-proach <ref type="bibr" target="#b31">(Zelenko et al., 2003;</ref><ref type="bibr" target="#b37">Zhou et al., 2005)</ref>, belong to this category. These models first recognize all entities in text and then perform relation classification for each entity pair. The subsequent neural network-based models <ref type="bibr" target="#b2">(Bekoulis et al., 2018;</ref><ref type="bibr" target="#b5">Dai et al., 2019)</ref>, that attempt to extract entities and relations jointly, can also be classified into this category.</p><p>Compared to edge list, adjacency matrices have better relation (edge) searching efficiency <ref type="bibr" target="#b0">(Arifuzzaman and Khan, 2015)</ref>. Furthermore, adjacency matrices oriented models is able to cover different overlapping cases <ref type="bibr" target="#b34">(Zeng et al., 2018)</ref> for relational fact extraction task. But the space cost of this approach can be expensive. For most cases, the output matrices are very sparse. For instance, for a sentence with n tokens, if there are m kinds of relations, the output space is n ? n ? m, which can be costly for graph representation efficiency. This phenomenon is also illustrated in <ref type="figure">Figure 1</ref>.</p><p>Adjacency List is designed to predict an array of linked lists that serves as a representation of a graph. As depicted in <ref type="figure">Figure 1</ref>, in the adjacency list, each vertex v (key) points to a list (value) containing all other vertices connected to v by several edges. Adjacency list is a hybrid graph representation between edge list and adjacency matrices <ref type="bibr" target="#b12">(Gross and Yellen, 2005)</ref>, which can balance space and searching efficiency 1 . Due to the structural characteristic of the adjacency list, this type of model usually adopts a cascade fashion to identify subject, object, and relation sequentially. For instance, the recent state-of-the-art model Cas-Rel <ref type="bibr" target="#b30">(Wei et al., 2020)</ref> can be considered as an exemplar. It utilizes a two-step framework to recognize the possible object(s) of a given subject under a specific relation. However, CasRel is not fully adjacency list oriented: in the first step, it use subject as the key; while in the second step, it predicts (relation, object) pairs using adjacency matrix representation.</p><p>Despite its considerable potential, the cascade fashion of adjacency list oriented model may cause problems of sub-task error propagation <ref type="bibr" target="#b26">(Shen et al., 2019)</ref>, i.e., errors from ancestor sub-tasks may accumulate to threaten downstream ones, and subtasks can hardly share supervision signals. Multitask learning <ref type="bibr" target="#b3">(Caruana, 1997)</ref> can alleviate this problem, however, the sub-task loss balancing prob-1 More detailed complexity analyses of different graph representations are provided in Appendix section 6.3. lem <ref type="bibr" target="#b4">(Chen et al., 2018;</ref><ref type="bibr" target="#b25">Sener and Koltun, 2018)</ref> could compromise its performance.</p><p>Based on the analysis from the perspective of output data structure, we propose a novel solution, aDjacency lIst oRiented rElational faCT extraction model (DIRECT), with the following advantages:</p><p>? For efficiency, DIRECT is a fully adjacency list oriented model, which consists of a shared BERT encoder, the Pointer-Network based subject and object extractors, and a relation classification module. In Section 3.4, we provide a detailed comparative analysis 2 to demonstrate the efficiency of the proposed method.</p><p>? From the performance viewpoint, to address sub-task error propagation and sub-task loss balancing problems, DIRECT employs a novel adaptive multi-task learning strategy with the dynamic subtask loss balancing approach. In Section 3.2 and 3.3, the empirical experimental results demonstrate DIRECT can achieve the state-of-the-art performance of relational fact extraction task, and the adaptive multi-task learning strategy did play a positive role in improving the task performance.</p><p>The major contributions of this paper can be summarized as follows:</p><p>1. We refurbish the relational fact extraction problem by leveraging an analytical framework of graph-oriented output structure. To the best of our knowledge, this is a pioneer investigation to explore the output data structure of relational fact extractions.</p><p>2. We propose a novel solution, DIRECT 3 , which is a fully adjacency list oriented model with a novel adaptive multi-task learning strategy.</p><p>3. Through extensive experiments on two benchmark datasets 3 , we demonstrate the efficiency and efficacy of DIRECT. The proposed DIRECT outperforms the state-of-the-art baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The DIRECT Framework</head><p>In this section, we will introduce the framework of the proposed DIRECT model, which includes a shared BERT encoder and three output layers: subject extraction, object extraction, and relation classification. As shown in <ref type="figure">Figure 2</ref>, DIRECT is fully adjacency list oriented. The input sentence is firstly fed into the subject extraction module to <ref type="figure">Figure 2</ref>: An overview of the proposed DIRECT framework extract all subjects. Then each extracted subject is concatenated with the sentence, and fed into the object extraction module to extract all objects, which can form a set of subject-object pairs. Finally, the subject-object pair is concatenated with sentence, and fed into the relation classification module to get the relations between them. For balancing the weights of sub-task losses and to improve the global task performance, three modules share the BERT encoder layer and are trained with an adaptive multi-task learning strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Shared BERT Encoder</head><p>In the DIRECT framework, the encoder is used to extract the semantic features from the inputs for three modules. As aforementioned, we employ the BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> as the shared encoder to make use of its pre-trained knowledge and attention mechanism.</p><p>The architecture of the shared method is shown in <ref type="figure">Figure 2</ref>. The lower embedding layer and transformers <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> are shared across all the three modules, while the top layers represent the task-specific outputs.</p><p>The encoding process is as follows:</p><formula xml:id="formula_0">h t = BERT(x t )<label>(1)</label></formula><p>where x t = [w 1 , ..., w n ] is the input text of task t and h t is the hidden vector sequence of the input. Due to the limited space, the detailed architecture of BERT please refer to the original paper <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Subject and Object Extraction</head><p>The subject and object extraction modules are motivated by the Pointer-Network <ref type="bibr" target="#b29">(Vinyals et al., 2015)</ref> architecture, which are widely used in Machine Reading Comprehension (MRC) <ref type="bibr" target="#b23">(Rajpurkar et al., 2016)</ref> task. Different from MRC task that only needs to extract a single span, the subject and object extractions need to extract multiple spans. Therefore, in the training phase, we replace sof tmax function with sigmoid function for the activation function of the output layer, and replace cross entropy (CE) <ref type="bibr" target="#b11">(Goodfellow et al., 2016)</ref> with binary cross entropy (BCE) <ref type="bibr" target="#b20">(Luc et al., 2016)</ref> for the loss function. Specifically, we will perform independent binary classifications for each token twice to indicate whether the current token is the start or the end of a span. The probability of a token to be start or end is as follows:</p><formula xml:id="formula_1">p t i,start = ?(W t start ? h i + b t start ) (2) p t i,end = ?(W t end ? h i + b t end )<label>(3)</label></formula><p>where h i represents the hidden vector of the i th token, t ? [s, o] represents subject and object extraction respectively, W t ? R h?1 represents the trainable weight, b t ? R 1 is the bias and ? is sigmoid function. During inference, we first recognize all the start positions by checking if the probability p t i,start &gt; ?, where ? is the threshold of extraction. Then, we identify the corresponding end position with the largest probability p t i,end between two neighboring start positions. Concretely, assuming pos j,start is the start position of the j th span, the corresponding end position is: pos j,end = argmax pos j,start &lt;=i&lt;pos j+1,start</p><formula xml:id="formula_2">p t i,end<label>(4)</label></formula><p>Though the overall structure is similar, the inputs for subject and object extraction are different. When extracting the subject, only the original sentence needs to be input:</p><formula xml:id="formula_3">x = [w 1 , ..., w n ] (5) input s = [[cls], x, [sep]]<label>(6)</label></formula><p>where w i represents the i th token of the original sentence.</p><p>Meanwhile, the object extraction is based on the corresponding subject. To form the input, the subject s and the original sentence x are concatenated with [sep] as follows:</p><formula xml:id="formula_4">input o = [[cls], s, [sep], x, [sep]]<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relation classification</head><p>The output layer of relation classification is relatively simple, which is a normal multi-label classification model. The [cls] vector obtained by BERT encoder is used as the sentence embedding. A fully connected layer is used for the nonlinear transformation, and perform multi-label classification to predict relations of the input subject-object pair. The detailed operations of relation classification are as follows:</p><formula xml:id="formula_5">P r = ?(W r ? h [cls] + b r )<label>(8)</label></formula><p>where P r ? R c is the predicted probability vector of relations, ? is sigmoid function, W r ? R h?c and b r ? R c are the trainable weights and bias, h is the hidden size of encoder, c is the number of relations, and h <ref type="bibr">[cls]</ref> denotes the hidden vector of the first token [cls]. The input for relation classification task is as follows:</p><formula xml:id="formula_6">input r = [[cls], s, [sep], o, [sep], x, [sep]] (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Adaptive Multi-task Learning</head><p>In DIRECT, subject extraction module, object extraction module, and relation classification module can be considered as three sub-tasks. As aforementioned, if we train each module directly and separately, the error propagation problem would Algorithm 1: Adaptive Multi-task Learning with Dynamic Loss Balancing Initialize model parameters ? randomly; Load pre-trained BERT parameters for shared encoder; Prepare the data for each task t and pack them into mini-batch: D t , t ? [s, o, r] ; Get the number of batch for each task: n t ; Set the number of epoch for training:</p><formula xml:id="formula_7">epoch max ; for epoch in 1, 2, ..., epoch max do 1. Merge all the datasets: D = D s ? D o ? D r ; 2. Shuffle D; 3. Initialize EMA for each task v t = 1</formula><p>and its decay = 0.99 ;</p><formula xml:id="formula_8">for b t in D do // b t is a mini-batch of D t ; 4. Compute loss: l t (?) ; 5. Update EMA: v t = (1 ? ) ? (l t ) + ? v t ; 6. Calculate and normalize the weights: w t = (v t /n t )/(v r /n r ) ; 7. Update model ? with gradient: ?(w t ?l t ) ; end end</formula><p>reduce the task performance. Meanwhile, three independent encoders would consume more memory. Therefore, we use multi-task learning to alleviate this problem, and the encoder layer is shared across three modules.</p><p>However, applying multi-task learning could be challenging in DIRECT, due to the following problems:</p><p>? The input and output of the three modules are different, which means we cannot simply sum up the loss of each task.</p><p>? How should we balance the weights of losses for three sub-task modules?</p><p>These issues can affect the final results of multitask training <ref type="bibr" target="#b26">(Shen et al., 2019;</ref><ref type="bibr" target="#b25">Sener and Koltun, 2018)</ref>.</p><p>In this work, based on the architecture of MT-DNN <ref type="bibr" target="#b19">(Liu et al., 2019b)</ref>, we propose a novel adaptive multi-task learning strategy to address the above problems. The algorithm is shown as Algorithm 1. Basically, the datasets are firstly split into mini-batches. A batch is then randomly sampled to calculate the loss. The parameters of the shared encoder and its task-specific layer are updated accordingly. Especially, the learning effect of each task t is different and dynamically changing during training. Therefore, an approach of adaptively adjusting the weights of task losses is applied. The sum of sub-task's loss l t is utilized to approximate its optimization effect. The adaptive weight adjusting strategy ensures that the more room a sub-task has to be optimized, the more weight its loss will receive. Furthermore, an exponential moving average (EMA) (Lawrance and Lewis, 1977) is maintained to avoid the drastic fluctuations of loss weights. Last but not least, to make sure that each task has enough influence on the shared encoder, the weight of the sub-task will be penalized according to the training data amount of each sub-task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Experiment Setting</head><p>Datasets. Two public datasets are used for evaluation: NYT <ref type="bibr" target="#b24">(Riedel et al., 2010)</ref> is originally produced by the distant supervision approach. There are 1.18M sentences with 24 predefined relation types in NYT. WebNLG <ref type="bibr" target="#b10">(Gardent et al., 2017)</ref> is originally created for Natural Language Generation (NLG) tasks. <ref type="bibr" target="#b34">(Zeng et al., 2018)</ref> adopts this dataset for relational triplet extraction task. It contains 246 predefined relation types. There are different versions of these two datasets. To facilitate comparison evaluation, we use the datasets released by <ref type="bibr" target="#b34">(Zeng et al., 2018)</ref> and follow their data split rules.</p><p>Besides the basic relational triplet extraction, recent studies are focusing on the relational triplet overlapping problem <ref type="bibr" target="#b34">(Zeng et al., 2018;</ref><ref type="bibr" target="#b30">Wei et al., 2020)</ref>. Follow the overlapping pattern definition of relational triplets <ref type="bibr" target="#b34">(Zeng et al., 2018)</ref>, the sentences in both datasets are divided into three categories, namely, Normal, EntityPairOverlap (EPO), and SingleEntityOverlap (SEO). The statistics of the two datasets are described in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Baselines: the following strong state-of-the-art (SoTA) models have been compared in the experiments.</p><p>? NovelTagging <ref type="bibr" target="#b36">(Zheng et al., 2017)</ref> introduces a tagging scheme that transforms the joint entity and relation extraction task into a sequence labeling problem. It can be considered as edge list oriented.</p><p>? CopyRE <ref type="bibr" target="#b34">(Zeng et al., 2018)</ref> is a seq2seq based model with the copy mechanism, which Category</p><p>NYT <ref type="table" target="#tab_0">WebNLG  Train Test Train Test  Normal 37013 3266 1596 246  EPO  9782  978  227  26  SEO  14735 1297 3406 457  ALL</ref> 56195 5000 5019 703 can effectively extract overlapping triplets. It has two variants: CopyRE one employs one decoder; CopyRE mul employs multiple decoders. CopyRE is also edge list oriented.</p><p>? GraphRel <ref type="bibr" target="#b9">(Fu et al., 2019</ref>) is a GCN (graph convolutional networks) (Kipf and Welling, 2017) based model, where a relation-weighted GCN is utilized to learn the interaction between entities and relations. It is a two phases model: GraphRel 1p denotes 1st-phase extraction model; GraphRel 2p denotes full extraction model. GraphRel is adjacency matrices oriented.</p><p>? CopyRL <ref type="bibr" target="#b33">(Zeng et al., 2019)</ref> combines the reinforcement learning with a seq2seq model to automatically learn the extraction order of triplets. CopyRL is edge list oriented.</p><p>? CasRel <ref type="bibr" target="#b30">(Wei et al., 2020</ref>) is a cascade binary tagging framework, where all possible subjects are identified in the first stage, and then for each identified subject, all possible relations and the corresponding objects are simultaneously identified by a relation specific tagger. This work recently achieves the SoTA results. As aforementioned, Cas-Rel is partially adjacency list oriented.</p><p>Evaluation Metrics: following the previous work <ref type="bibr" target="#b34">(Zeng et al., 2018;</ref><ref type="bibr" target="#b30">Wei et al., 2020)</ref>, different models are compared by using standard micro Precision (Prec.), Recall (Rec.), and F1-score 4 . An extracted relational triplet (subject, relation, object) is regarded as correct only if the relation and the heads of both subject and object are all correct.</p><p>Implementation Details.</p><p>The hyperparameters are determined on the validation set. To avoid the evaluation bias, all reported results from our method are averaged results for 5 runs. More implementation details are described in Appendix section 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Analysis</head><p>Relational Triplet Extraction Performance. The task performances on two datasets are summarized in <ref type="table" target="#tab_2">Table 2</ref>. Based on the experiment results, we have the following observations and discussions:</p><p>? The proposed DIRECT model outperformed all baseline models in terms of all evaluation metrics on both datasets, which proved DIRECT model can effectively address the relational triplet extraction task.</p><p>? The best-performed model (DIRECT) and runner-up model (CasRel) were both adjacency list oriented model. These two models overwhelmingly outperformed other models, which indicated the considerable potential of adjacency list (as the output data structure) for improving the task performance.</p><p>? To further compare the relation extraction ability of DIRECT and CasRel, we took a closer look at the extraction performance of relational triplet elements from these two models. As shown in <ref type="table" target="#tab_3">Table 3</ref> 5 , DIRECT outperformed CasRel in terms of all relational triplet elements on both datasets. These empirical results suggested that, for relational triplet extraction, a fully adjacency list oriented model (DIRECT) may have advantages over a partially oriented one (CasRel). we conducted further experiments on NYT dataset. <ref type="figure" target="#fig_0">Figure 3</ref> illustrated of F1 scores of extracting relational triplets from sentences with different overlapping patterns. DIRECT outperformed all baseline models in terms of all overlapping patterns. These results demonstrated the effectiveness of the proposed model in solving the overlapping problem.</p><p>Ability in Handling Multiple Relation Extraction. We further compared the model's ability of extracting relations from sentences that contain multiple triplets. The sentences in NYT and WebNLG were divided into 5 categories. Each category contained sentences that had 1,2,3,4 or ? 5 triplets. The triplet number was denoted as N . As shown in <ref type="table" target="#tab_5">Table 4</ref>:</p><p>? DIRECT achieved the best performance for all triplet categories on both datasets. These experimental results demonstrated our model had an excellent ability in handling multiple relation extraction.</p><p>? In both NYT and WebNLG datasets, when the sentences contained more triplets, the leading advantage of DIRECT became greater. This observation indicated that DIRECT was good at solving complex relational fact extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>To validate the effectiveness of components in DI-RECT, We implemented several model variants for ablation tests 6 . The results of the comparison on NYT dataset are shown in <ref type="table" target="#tab_6">Table 5</ref>. In particular, we aim to address the following two research questions:</p><p>RQ1: Is it possible to improve the model performance by sharing the parameters of extraction layers?</p><p>RQ2: Did the proposed adaptive multi-task learning strategy improve the task performance?</p><p>Effects of Sharing Extraction Layer Parameters (RQ1). As described in Section 2, the structures of subject extraction and object extraction output layers are exactly the same. To answer RQ1, we merged the subject extraction and object extraction layers into one entity extraction layer by sharing the parameters of output layers of these two modules, denoted as DIRECT shared . From the results of <ref type="table" target="#tab_6">Table 5</ref>, we can observe that, sharing the parameters of output layers of two extraction modules would reduce the performance of the model.   A possible explanation is that, although the output of these two modules is similar, the semantics of subject and object are different. Hence, directly sharing the output parameters of two modules could lead to an unsatisfactory performance. Effects of Adaptive Multi-task Learning (RQ2). As described in Section 2, the adaptive multi-task learning strategy with the dynamic subtask loss balancing approach is proposed for improving the task performance. To answer RQ2, we replaced the adaptive multi-task learning strategy with an ordinary learning strategy. In this strategy, the losses of three sub-tasks were computed with equal weights, denoted as DIRECT equal . From the results of <ref type="table" target="#tab_6">Table 5</ref>, we can observe that, by using adaptive multi-task learning, DIRECT was able to get a 1.5 percentage improvement on the F1-score. This significant improvement indicated that adaptive multi-task learning played a positive role in the balance of sub-task learning and can improve the global task performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Graph Representation Efficiency Analysis</head><p>Based on the amount estimation of predicted logits 7 , we conduct a graph representation efficiency 7 Numeric output (0/1) of the last layer analysis to demonstrate the efficiency of the proposed method 8 . For each graph representation category, we choose one representative algorithms. Edge List: CopyRE <ref type="bibr" target="#b34">(Zeng et al., 2018)</ref>; Adjacency Matrices: MHS <ref type="bibr" target="#b2">(Bekoulis et al., 2018)</ref>; Adjacency List: Cas-Rel (partially) <ref type="bibr" target="#b30">(Wei et al., 2020)</ref> and the proposed DIRECT (fully).</p><p>The averaged predicted logits estimation for one sample 9 of different models on two datasets are shown in <ref type="table" target="#tab_7">Table 6</ref>. MHS is adjacency matrices oriented, it has the most logits that need to be predicted. Since CasRel is partially adjacency list oriented, it needs to predict more logits than DI-RECT. Theoretically, as an edge list oriented, the predicted logits of CopyRE should be the least. But, as described in Section 1, it needs to extract the entities repeatedly to handle the overlapping problem. Hence, its graph representation efficiency could be worse than our model. The structure of our model is simple and fully adjacency list oriented. Therefore, from the viewpoint of predicted logits estimation, DIRECT is the most representative-efficient model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Relation Fact Extraction. In this work, we show that all of the relational fact extraction models can be unified into a graph-oriented output structure analytical framework. From the perspective of graph representation, the prior models can be divided into three categories. Edge List, this type of model usually employs sequence-to-sequence fashion, such as NovelTagging <ref type="bibr" target="#b36">(Zheng et al., 2017)</ref>, CopyRE <ref type="bibr" target="#b34">(Zeng et al., 2018)</ref>  <ref type="table" target="#tab_0">Count  3244  1045  312  291  108  268  174  128  89</ref>     2019), and PNDec <ref type="bibr" target="#b22">(Nayak and Ng, 2020)</ref>. Some models of this category may suffer from the triplet overlapping problem and expensive extraction cost. Adjacency Matrices, many early pipeline approaches <ref type="bibr" target="#b31">(Zelenko et al., 2003;</ref><ref type="bibr" target="#b37">Zhou et al., 2005;</ref><ref type="bibr" target="#b21">Mintz et al., 2009</ref>) and recent neural network-based models <ref type="bibr" target="#b2">(Bekoulis et al., 2018;</ref><ref type="bibr" target="#b5">Dai et al., 2019;</ref><ref type="bibr" target="#b9">Fu et al., 2019)</ref>, can be classified into this category. The main problem for this type of model is the graph representation efficiency. Adjacency List, the recent state-of-the-art model CasRel <ref type="bibr" target="#b30">(Wei et al., 2020</ref>) is a partially adjacency list oriented model. In this work, we propose DIRECT that is a fully adjacency list oriented relational fact extraction model. To the best of our knowledge, few previous works analyze this task from the output data structure perspective. GraphRel <ref type="bibr" target="#b9">(Fu et al., 2019)</ref> employs a graph-based approach, but it is utilized from an encoding perspective, while we analyze it from the perspective of output structure. Our work is a pioneer investigation to analyze the output data structure of relational fact extraction.</p><p>Multi-task Learning. Multi-task Learning (MTL) can improve the model performance. <ref type="bibr" target="#b3">(Caruana, 1997</ref>) summarizes the goal succinctly: "it improves generalization by leveraging the domainspecific information contained in the training signals of related task." It has two benefits (Vandenhende et al.): (1) multiple tasks share a single model, which can save memory. (2) Associated tasks complement and constrain each other by sharing information, which can reduce overfitting and improve global performance. There are two main types of MTL: hard parameter sharing (Baxter, 1997) and soft parameter sharing <ref type="bibr" target="#b8">(Duong et al., 2015)</ref>. Most of the multi-task learning is done by summing the loses directly, this approach is not suitable for our case. When the input and output are different, it is impossible to get two losses in one forward propagation. MT-DNN <ref type="bibr" target="#b19">(Liu et al., 2019b)</ref> is proposed for this problem. Furthermore, MTL is difficult for training, the magnitudes of different task-losses are different, and the direct summation of losses may lead to a bias for a particular task. There are already some studies proposed to address this problem <ref type="bibr" target="#b4">(Chen et al., 2018;</ref><ref type="bibr" target="#b13">Guo et al., 2018;</ref><ref type="bibr" target="#b18">Liu et al., 2019a)</ref>. They all try to dynamically adjust the weight of the loss according to the magnitude of the loss, the difficulty of the problem, the speed of learning, etc. In this study, we adopt MT-DNN's framework, and propose an adaptive multi-task learning strategy that can dynamically adjust the loss weight based on the averaged EMA (Lawrance and Lewis, 1977) of the training data amount, task difficulty, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduce a new analytical perspective to organize the relational fact extraction models and propose DIRECT model for this task. Unlike existing methods, DIRECT is fully adja-cency list oriented, which employs a novel adaptive multi-task learning strategy with dynamic sub-task loss balancing. Extensive experiments on two public datasets, prove the efficiency and efficacy of the proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implementation Details</head><p>We adopted the pre-trained BERT model [BERT-Base-Cased] 10 as our encoder, where the number of Transformer layers was 12 and the hidden size was 768. The token types of input were always set to 0.</p><p>We used Adam as our optimizer and applied a triangular learning rate schedule as suggested by original BERT paper. In addition, we adopted a lazy mechanism for optimization. Different from the momentum mechanism of ordinary Adam optimizer <ref type="bibr" target="#b15">(Kingma and Ba, 2015)</ref> that updated the output layer parameters for all tasks, this lazy-Adam mechanism wouldn't update the parameters of noncurrent tasks.</p><p>The dacay rate of EMA was set to 0.99 as default. The max sequence length was 128.</p><p>The other hyper-parameters were determined on the validation set. Notably, considering our special decoding strategy, we raised the threshold of extraction to 0.9 to balance the precision and the recall. The threshold of relation classification was set to 0.5 as default. The hyper-parameter setting was listed in <ref type="table" target="#tab_8">Table 7</ref>.</p><p>Our mthod were implemented by Pytorch 11 and run on a server configured with a Tesla V100 GPU, 16 CPU, and 64G memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameter NYT WebNLG</head><p>Learning Rate 8e-5 1e-4 Epoch Num.</p><p>15 60 Batch Size 32 16 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Supplementary Experimental Results</head><p>Ablation Study. To validate the effectiveness of components in DIRECT, We implemented several model variants for ablation tests respectively. For experimental fairness, we kept the other components in the same settings when modifying one module.</p><p>? DIRECT shared , we merged the subject extraction and object extraction layers into one 10 Available at: https://storage.googleapis.com/bert models/ 2018 10 18/cased L-12 H-768 A-12.zip 11 https://pytorch.org/ entity extraction layer by sharing the parameters of output layers of these two modules.</p><p>? DIRECT equal , we replaced the adaptive multi-task learning strategy with an ordinary learning strategy. In this strategy, the losses of three sub-tasks were computed with equal weights, denoted as DIRECT equal .</p><p>? DIRECT threshold , we simply recognized all the start and end positions of entities by checking if the probability p t i,start/end &gt; ?, where ? was the threshold of extraction.</p><p>? DIRECT adam , we used ordinary Adam as optimizer.  From the results of <ref type="table" target="#tab_10">Table 8</ref>, we can observe that:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>1. Sharing the parameters of output layers of subject and object extraction modules would reduce the performance of the model.</p><p>2. Compared to ordinary multi-task learning strategy, by using adaptive multi-task learning, DIRECT was able to get a 1.5 percentage point improvement on F1-score.</p><p>3. There would be a slight drop in performance, if we just used a simple threshold policy to recognize the start and end positions of an entity.</p><p>4. Despite the difference in precision and recall, there was no significant difference between these two optimizers (ordinary-Adam &amp; lazy-Adam ) for the task.</p><p>Results on Extracting Elements of Relational Triplets. The complete extraction performance of relational triplet elements from DIRECT and CaslRel are listed in   suggest that, for relational triplet extraction, a fully adjacency list oriented model (DIRECT) may have advantages over a partially oriented one (CasRel).</p><p>Results of Different Methods under Exact-Match Metrics. In experiment section, we followed the match metric from <ref type="bibr" target="#b34">(Zeng et al., 2018)</ref>, which only required to match the first token of entity span. Many previous works adopted this match metric <ref type="bibr" target="#b9">(Fu et al., 2019;</ref><ref type="bibr" target="#b33">Zeng et al., 2019;</ref><ref type="bibr" target="#b30">Wei et al., 2020)</ref>.</p><p>In fact, our model is capable of extracting the complete entities. Therefore, we collected papers that reported the results of exact-match metrics (requiring to match the complete entity span). The following strong state-of-the-art (SoTA) models have been compared:</p><p>? CopyMTL ) is a multi-task learning framework, where conditional random field is used to identify entities, and a seq2seq model is adopted to extract relational triplets.</p><p>? WDec <ref type="bibr" target="#b22">(Nayak and Ng, 2020</ref>) fuses a seq2seq model with a new representation scheme, which enables the decoder to generate one word at a and can handle full entity names of different length and overlapping entities.</p><p>? PNDec <ref type="bibr" target="#b22">(Nayak and Ng, 2020</ref>) is a modification of seq2seq model. Pointer networks are used in the decoding framework to identify the entities in the sentence using their start and end locations.</p><p>? Seq2UMTree (Zhang et al., 2020) is a modification of seq2seq model, which employs an unordered-multi-tree decoder to to minimize exposure bias.</p><p>The task performances on NYT dataset are summarized in <ref type="table" target="#tab_0">Table 10</ref>. The proposed DIRECT model outperformed all baseline models in terms of all evaluation metrics. This experimental results further confirmed the efficacy of DIRECT for relational fact extraction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Complexity Analysis of Graph Representations</head><p>For a graph G = (V, E), |V | denotes the number of nodes/entities and |E| denotes the number of edges/relations. Suppose there are m kinds of relations, d(v) denotes the number of edges from node v.</p><p>? Edge List Complexity  DIRECT 2l + 2sl + or 238 542 <ref type="table" target="#tab_0">Table 11</ref>: Graph representation efficiency based on the theoretical logits amount and the estimated logits amount on two benchmark datasets.</p><p>? Find all edges/relations from a node: O(|V | ? m)</p><p>? Adjacency List Complexity </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Graph Representation Efficiency Analysis</head><p>Based on the amount estimation of predicted logits 12 (0/1), we conduct a graph representation efficiency analysis to demonstrate the efficiency of proposed method 13 . For each graph representation category, we choose one representative model algorithms. Edge List: CopyRE <ref type="bibr" target="#b34">(Zeng et al., 2018)</ref>; Adjacency Matrices: MHS <ref type="bibr" target="#b2">(Bekoulis et al., 2018)</ref>; Adjacency List: CasRel (partially) <ref type="bibr" target="#b30">(Wei et al., 2020)</ref> and DI-RECT (fully).</p><p>Formally, for a sentence whose length is l (l tokens), there are r types of relations, k denotes the number of triplets. Suppose there are s keys (subjects) and o values (corresponding amount of object-based lists) in adjacency list. The theoretical logits amount and the estimated logits amount on two benchmark datasets (NYT and WebNLG) are shown in <ref type="table" target="#tab_0">Table 11</ref>. From the viewpoint of predicted logits estimation, DIRECT is the most representative-efficient model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>F1 score of extracting relational triples from sentences with different overlapping patterns on NYT dataset.Ability in Handling TheOverlapping Problem. The relational facts in sentences are often complicated. Different relational triplets may have overlaps in a sentence. To verify the ability of our models in handling the overlapping problem, 5 More detailed results with Precision and Recall are provided in Appendix section 6.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, CopyRL (Zeng et al., Method NYT WebNLG N = 1 N = 2 N = 3 N = 4 N ? 5 N = 1 N = 2 N = 3 N = 4 N ? 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?</head><label></label><figDesc>Space: O(|E|) ? Find all edges/relations from a node: O(|E|) ? Adjacency Matrices Complexity ? Space: O(|V | ? |V | ? m)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?</head><label></label><figDesc>Space: O(|V | + |E|) ? Find all edges/relations from a node: O(d(v))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of Dataset NYT and WebNLG. Note that a sentence can belong to both EPO class and SEO class.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of different methods on NYT and WebNLG datasets. EL: Edge List; AM: Adjacency Matrices; AL P : Adjacency List (Partially); AL F : Adjacency List (Fully).</figDesc><table><row><cell>Method</cell><cell cols="3">Element NYT WebNLG</cell></row><row><cell></cell><cell>s</cell><cell>93.5</cell><cell>95.7</cell></row><row><cell>CasRel</cell><cell>o</cell><cell>93.5</cell><cell>95.3</cell></row><row><cell></cell><cell>r</cell><cell>94.9</cell><cell>94.0</cell></row><row><cell></cell><cell>s</cell><cell>95.4</cell><cell>97.3</cell></row><row><cell>DIRECT(Ours)</cell><cell>o</cell><cell>96.4</cell><cell>96.4</cell></row><row><cell></cell><cell>r</cell><cell>97.8</cell><cell>97.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>F1-score for extracting elements of relational triplets on NYT and WebNLG datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>F1-score of extracting relational triplets from sentences with different number (denoted as N) of triplets.</figDesc><table><row><cell>Method</cell><cell>NYT Prec. Rec. F1</cell></row><row><cell cols="2">DIRECT shared 92.1 91.6 91.9</cell></row><row><cell cols="2">DIRECT equal 90.6 91.3 91.0</cell></row><row><cell>DIRECT</cell><cell>92.3 92.8 92.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results of model variants for ablation tests.</figDesc><table><row><cell cols="4">Method Category NYT WebNLG</cell></row><row><cell>CopyRe</cell><cell>EL</cell><cell>329</cell><cell>712</cell></row><row><cell>MHS</cell><cell>AM</cell><cell>57369</cell><cell>26518</cell></row><row><cell>CasRel</cell><cell>AL P</cell><cell>3084</cell><cell>15836</cell></row><row><cell>DIRECT</cell><cell>AL F</cell><cell>238</cell><cell>542</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Graph representation efficiency estimation</cell></row><row><cell>based on the predicted logits amount. EL: Edge List;</cell></row><row><cell>AM: Adjacency Matrices; AL P : Adjacency List (Par-</cell></row><row><cell>tially); AL</cell></row></table><note>F : Adjacency List (Fully).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Hyper-parameter setting for NYT and WebNLG datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Results of model variants for ablation tests.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>DIRECT outperformed CasRel in terms of all relational triplet elements on both datasets. These empirical results</figDesc><table><row><cell cols="2">Method Element</cell><cell>NYT Prec. Rec. F1 Prec. Rec. F1 WebNLG</cell></row><row><cell></cell><cell>s</cell><cell>94.6 92.4 93.5 98.7 92.8 95.7</cell></row><row><cell>CasRel</cell><cell>o</cell><cell>94.1 93.0 93.5 97.7 93.0 95.3</cell></row><row><cell></cell><cell>r</cell><cell>96.0 93.8 94.9 96.6 91.5 94.0</cell></row><row><cell></cell><cell>s</cell><cell>95.1 95.1 95.1 97.1 96.8 96.9</cell></row><row><cell>Ours</cell><cell>o</cell><cell>97.2 96.3 96.7 96.4 96.3 96.3</cell></row><row><cell></cell><cell>r</cell><cell>98.6 98.3 98.5 97.6 97.3 97.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Results on extracting elements of relational triplets</figDesc><table><row><cell>Method</cell><cell>NYT Prec. Rec. F1</cell></row><row><cell>MHS  *  (Bekoulis et al., 2018)</cell><cell>60.7 58.6 59.6</cell></row><row><cell>CopyMTL one (Zeng et al., 2020)</cell><cell>72.7 69.2 70.9</cell></row><row><cell cols="2">CopyMTL mul (Zeng et al., 2020) 75.7 68.7 72.0</cell></row><row><cell>WDec (Nayak and Ng, 2020)</cell><cell>88.1 76.1 81.7</cell></row><row><cell>PNDec (Nayak and Ng, 2020)</cell><cell>80.6 77.3 78.9</cell></row><row><cell cols="2">Seq2UMTree (Zhang et al., 2020) 79.1 75.1 77.1</cell></row><row><cell>DIRECT(ours)</cell><cell>90.2 90.2 90.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Results of different methods under Exact-Match Metrics. * marks results reproduced by official implementation.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Theoretical representation efficiency analysis of graph representative models are described in Appendix section 6.4.3  To help other scholars reproduce the experiment outcome, we will release the code and datasets via GitHub: https://github.com/fyubang/direct-ie.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In this study, the results of baseline models are all selfreported results from their original papers. Meanwhile, the experimental results of our proposed model are the average of five runs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Due to the length limitation, we list two main ablation experiments, the rest will be provided in the Appendix section 6.2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">From the graph representation perspective, when a method requires fewer logits to represent the graph (set of triples), it will reduce the model fitting difficulty.9  The theoretical analysis of predicted logits for different models are described in Appendix section 6.4.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Numeric output of the last layer 13 As aforementioned, from the graph representation perspective, when a method requires fewer logits to represent the graph (set of triples), it will reduce the model fitting difficulty.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are thankful to the anonymous reviewers for their helpful comments. This work is supported by Alibaba Group through Alibaba Research Fellowship Program, the National Natural Science Foundation of China <ref type="formula">(61876003)</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast parallel conversion of edge list to adjacency list for large-scale graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaikh</forename><surname>Arifuzzaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maleq</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on High Performance Computing</title>
		<meeting>the Symposium on High Performance Computing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A bayesian/information theoretic model of learning to learn via multiple task sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="7" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint entity recognition and relation extraction as a multi-head selection problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="794" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and overlapping relations using position-attentive sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6300" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural relation extraction for knowledge base enrichment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Bayu Distiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="229" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Low resource dependency parsing: Crosslingual parameter sharing in a neural network parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd annual meeting of the Association for Computational Linguistics and the 7th international joint conference on natural language processing</title>
		<meeting>the 53rd annual meeting of the Association for Computational Linguistics and the 7th international joint conference on natural language processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="845" to="850" />
		</imprint>
	</monogr>
	<note>short papers</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graphrel: Modeling text as relational graphs for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Hsuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1409" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Creating training corpora for nlg micro-planners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Deep learning</title>
		<imprint>
			<publisher>MIT press Cambridge</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Graph theory and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yellen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic task prioritization for multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="282" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Opennre: An open and extensible toolkit for neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="169" to="174" />
		</imprint>
	</monogr>
	<note>System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An exponential moving-average sequence and point process (ema1)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aj Lawrance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Probability</title>
		<imprint>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end multi-task learning with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1871" to="1880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4487" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semantic segmentation using adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08408</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effective modeling of encoder-decoder architecture for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapas</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8528" to="8535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-task learning as multi-objective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="525" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-task learning for conversational question answering over a large-scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiubo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2442" to="2451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multi-task learning for dense prediction tasks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A novel cascade binary tagging framework for relational triple extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1476" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Copymtl: Copy mechanism for joint extraction of entities and relations with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9507" to="9514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning the extraction order of multiple relational facts in a sentence with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Minimize exposure bias of seq2seq models in joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianying</forename><surname>Ranran Haoran Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Xuemo Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="236" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel tagging scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting of the association for computational linguistics (acl&apos;05)</title>
		<meeting>the 43rd annual meeting of the association for computational linguistics (acl&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
