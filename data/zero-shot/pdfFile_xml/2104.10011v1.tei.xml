<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Perceptual Loss for Robust Unsupervised Homography Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Koguciuk</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Advanced Research Lab</orgName>
								<orgName type="institution">NavInfo Europe</orgName>
								<address>
									<settlement>Eindhoven</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elahe</forename><surname>Arani</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Advanced Research Lab</orgName>
								<orgName type="institution">NavInfo Europe</orgName>
								<address>
									<settlement>Eindhoven</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahram</forename><surname>Zonooz</surname></persName>
							<email>bahram.zonooz@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Advanced Research Lab</orgName>
								<orgName type="institution">NavInfo Europe</orgName>
								<address>
									<settlement>Eindhoven</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Perceptual Loss for Robust Unsupervised Homography Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Homography estimation is often an indispensable step in many computer vision tasks. The existing approaches, however, are not robust to illumination and/or larger viewpoint changes. In this paper, we propose bidirectional implicit Homography Estimation (biHomE) loss for unsupervised homography estimation. biHomE minimizes the distance in the feature space between the warped image from the source viewpoint and the corresponding image from the target viewpoint. Since we use a fixed pre-trained feature extractor and the only learnable component of our framework is the homography network, we effectively decouple the homography estimation from representation learning. We use an additional photometric distortion step in the synthetic COCO dataset generation to better represent the illumination variation of the real-world scenarios. We show that biHomE achieves state-of-the-art performance on synthetic COCO dataset, which is also comparable or better compared to supervised approaches. Furthermore, the empirical results demonstrate the robustness of our approach to illumination variation compared to existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Given a pinhole camera model assumption, a homography relates any two images of the same planar surface in space or any two images produced by pure rotational movement of the camera <ref type="bibr" target="#b12">[13]</ref>. One image, called a source, can be transformed by a 3 ? 3 homography matrix H as if viewed from the viewpoint of the other image, called target. Even if the primary assumptions are violated, the homography can be applied as an initial alignment step in other tasks such as mesh flow <ref type="bibr" target="#b20">[21]</ref> and optical flow <ref type="bibr" target="#b14">[15]</ref>. Therefore, homography has been widely used in many computer vision applications such as SLAM <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref>, image stitching <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b11">12]</ref>, and change detection and description <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">40]</ref>.</p><p>Traditionally, homography estimation was performed using non-learnable approaches either in the pixel-space (direct methods) or in hand-crafted feature space (featurebased methods) <ref type="bibr" target="#b39">[39]</ref>. Recently, with the advancement in S-COCO </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.51</head><p>Reproduced Not Converged Ours unsupervised supervised <ref type="figure">Figure 1</ref>. Homography estimation comparison on synthetic COCO (S-COCO) <ref type="bibr" target="#b3">[4]</ref> on the left and a photometrically distorted version of it (PDS-COCO) on the right. On PSD-COCO, our bidirectional implicit Homography Estimation (biHomE) loss is the only unsupervised method able to converge, while still being on par with the performance of supervised approaches. The performance of methods <ref type="bibr">(Nguyen [27]</ref>, DeTone <ref type="bibr" target="#b3">[4]</ref>, Zeng <ref type="bibr" target="#b43">[43]</ref>, and Zhang <ref type="bibr" target="#b44">[44]</ref>) is reported in Mean Absolute Corner Error (MACE).</p><p>Deep Neural Networks (DNNs), DeTone et al. <ref type="bibr" target="#b3">[4]</ref> proposed a simple CNN architecture trained in an end-to-end fashion. The idea was to directly regress the parameters of a homography and it achieved similar performance to traditional feature-based methods. A more effective approach is presented by Zeng et al. <ref type="bibr" target="#b43">[43]</ref>, where the problem was formulated as per-pixel offset regression. However, supervised methods are often unlikely to be used in real-life scenarios, where ground truth homography labeling is prohibitively expensive.</p><p>To mitigate this issue, Nguyen et al. <ref type="bibr" target="#b26">[27]</ref> introduced an end-to-end unsupervised approach. First, a homography is estimated using both input images and then homography estimation is learned by comparing the per-pixel intensity of warped source image and target image. Models trained using this formulation perform surprisingly well even for images with big viewpoint differences. However, they are not robust to big illumination changes and cannot be used in real-life scenarios <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>. In contrast, Zhang et al. <ref type="bibr" target="#b44">[44]</ref>, by learning the feature representation used for both homography estimation and image comparison, achieved robustness to different lighting conditions, but not for images with big viewpoint changes. Therefore, robust unsupervised homography estimation for both big illumination and viewpoint changes at the same time is still an open problem.</p><p>Instead of learning a shared representation for homography estimation and image comparison <ref type="bibr" target="#b44">[44]</ref>, we propose to decouple those two tasks by using a dedicated Loss Network to compare warped source image and target image. It has been shown that a pre-trained and fixed convolutional neural network as a Loss Network can capture perceptual differences <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">41]</ref>. The homography is learned implicitly by comparing the images in feature space produced by the Loss Network. Since the homography should be invertible, we compare warped source and target images and swap their roles. Therefore, we call our loss function bidirectional implicit Homography Estimation (biHomE) loss.</p><p>We exhibit the simplicity and effectiveness of biHomE loss by applying it on top of three homography estimation architectures on the Synthetic COCO (S-COCO) dataset <ref type="bibr" target="#b3">[4]</ref>. biHomE not only outperforms other unsupervised methods but is also on par with its supervised counterpart. Next, we use an additional photometric distortion augmentation on synthetic COCO (denoted as PDS-COCO) to mimic the illumination changes in real-life scenarios. We show that biHomE is the only unsupervised method that can converge on PDS-COCO, while still maintaining comparable or better performance compared to supervised approaches. To further demonstrate the effectiveness of our loss, we provide an experimental study on the influence of image alignment on the quality of generated change captions by the Dual Dynamic Attention Model (DUDA) <ref type="bibr" target="#b27">[28]</ref> method. We achieve a new state-of-the-art performance on the change captioning task. Our contributions are as follows:</p><p>? We introduce a new perceptual loss (biHomE) to be used in an unsupervised setting, which decouples homography estimation from representation learning for image comparison.</p><p>? We propose to use an additional photometric distortion step in the synthetic COCO dataset generation (PDS-COCO) to evaluate the robustness of homography estimation to big illumination and viewpoint changes, which is more aligned with real-life scenarios.</p><p>? We achieve state-of-the-art performance using bi-HomE loss on both S-COCO and PDS-COCO datasets for unsupervised homography estimation.</p><p>? We apply biHomE for unsupervised image alignment to achieve a state-of-the-art change captioning quality on the CLEVR Change dataset <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Traditional Approaches</head><p>There are two families of traditional approaches for homography estimation: direct methods and feature-based methods <ref type="bibr" target="#b39">[39]</ref>. Direct methods work in pixel intensity space, where most of the studies follow the Lucas-Kanade algorithm <ref type="bibr" target="#b23">[24]</ref>. There are many improved approaches, including utilizing different error metrics <ref type="bibr" target="#b5">[6]</ref> or Fourier domain representation <ref type="bibr" target="#b24">[25]</ref>. Feature-based approaches mostly consist of three components: keypoint detection (such as SIFT <ref type="bibr" target="#b22">[23]</ref> and ORB <ref type="bibr" target="#b34">[34]</ref>), correspondence matching (Euclidean distance, correlation), and homography estimation using Direct Linear Transform (DLT) <ref type="bibr" target="#b12">[13]</ref> with RANSAC <ref type="bibr" target="#b7">[8]</ref> outlier rejection. Feature-based methods usually perform better than direct methods, however, their success depends on the quality of the hand-crafted stages and the content of the image itself. Poorly distributed features (i.e. repetitive pattern or texture), illumination variations, or large viewpoint differences <ref type="bibr" target="#b42">[42]</ref> are challenging for both direct and featurebased methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Supervised Approaches</head><p>DeTone et al. <ref type="bibr" target="#b3">[4]</ref> uses a deep learning model to estimate homography, which comes on par with or better than traditional methods. Input images are concatenated channelwise and passed through 10-layer VGG-style <ref type="bibr" target="#b36">[36]</ref> CNN with a fully connected layer with eight outputs (x-y coordinates for 4 points) on top. The problem is then formulated as a 4-point homography regression given the ground truth transformation. On the other hand, Zeng et al. <ref type="bibr" target="#b43">[43]</ref> estimates pixel-to-pixel bijection between two images using a U-Net <ref type="bibr" target="#b33">[33]</ref> architecture. The final homography during inference is produced using RANSAC and DLT in the postprocessing stage. Their method achieves a performance boost compared to DeTone's et al. <ref type="bibr" target="#b3">[4]</ref> approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Unsupervised Approaches</head><p>Nguyen et al. <ref type="bibr" target="#b26">[27]</ref> formulated the 4-point homography regression as an unsupervised approach. The idea is to minimize pixel-wise intensity error between the warped source image and the target image. They achieve comparable performance to the supervised DeTone et al. <ref type="bibr" target="#b3">[4]</ref> method on the S-COCO dataset. Their method, however, cannot compensate for larger illumination changes between images as the learning is performed using pixel intensity comparison. Zhang et al. <ref type="bibr" target="#b44">[44]</ref>, applies the loss function to the feature space instead of the pixel-space. The authors propose to minimize the feature distance between the warped source and the target images and maximize the feature difference between the source and warped source images. Although presented triple loss is efficient for images with small viewpoint shifts, it is not robust to big viewpoint changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>There are two key insights of our method. The first one is that splitting the architecture into two separate components effectively decouples the homography estimation from representation learning. The second insight of our method is that we want to leverage the fact that convolutional neural networks pre-trained for image classification have already learned to encode meaningful information, which can be used in downstream tasks like image comparison for homography estimation assessment. Therefore, as shown in <ref type="figure">Figure 2</ref>, the system is composed of two components: The Homography Estimation Network (HEN) f , which can be realized by any architecture able to produce a transformation matrix H and a fixed Loss Network g(I; ?) that is used to define loss function L. Homography estimation is learned implicitly by minimizing the loss function defined in the feature space produced by Loss Network g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Homography Estimation Networks</head><p>The goal of HEN is to estimate a 3 ? 3 homography matrix between two images I S and I T using the learnable parameters ?:</p><formula xml:id="formula_0">H ST = f (I S , I T ; ?)<label>(1)</label></formula><p>Modern HENs can be divided into two categories: architectures that directly produce homography (DeTone et al. <ref type="bibr" target="#b3">[4]</ref> and Zhang et al. <ref type="bibr" target="#b44">[44]</ref>) and models that produce per pixel offset (Zeng et al. <ref type="bibr" target="#b43">[43]</ref>). The former can be directly applied into our framework, but that is not the case with the latter. Thus, we first randomly sample without replacement N S &lt; H ? W points out of the output perspective field of size H ? W and then use Direct Linear Transform (DLT) <ref type="bibr" target="#b12">[13]</ref> to estimate the homography matrix in an end-to-end pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Perceptual Loss Functions</head><p>In the second part of the pipeline, homography H ST estimated by HEN is used to transform source image I S into I S = W arp(I S , H ST ). Rather than encouraging the pixels of the warped source image I S to match the pixels of the target image I T (similar to <ref type="bibr" target="#b26">[27]</ref>), we rely on perceptual similarity of high-level features extracted from pretrained Loss Network g. Let g j (I) be the feature map of size C j ? H j ? W j after jth layer of the Loss Network g for a given image I. We define the mask M of the same size as g j (I) consisting of all ones and use M = W arp(M, H ST ) to make sure the loss is applied only to the part visible on both images. The perceptual loss can be defined in multiple ways. Implicit Homography Estimation with MAE/MSE Loss. The simplest approach is to define loss as per pixel normalized distance in feature space produced by g between <ref type="figure">Figure 2</ref>. Proposed bidirectional implicit Homography Estimation (biHomE) loss, where f is any architecture producing homography HST between the input images IS and IT and g is a pretrained and frozen Loss Network. In our experiments, we use ResNet34 <ref type="bibr" target="#b13">[14]</ref> as feature extractor g.</p><p>warped source image I S and target image I T :</p><formula xml:id="formula_1">L d (I S , I T ) = Hj ,Wj ,Cj i M i ? d(g ji (I S ), g ji (I T )) Hj ,Wj ,Cj i M i (2)</formula><p>where d is either L 1 distance or L 2 distance squared, resulting in masked M AE or masked M SE perceptual loss functions. Implicit Homography Estimation Loss (iHomE). Rather than directly minimizing only the distance between I T and I S , we can encourage the network to simultaneously push I T away from the original I S in triplet fashion. The loss is highly inspired by Zhang et al. <ref type="bibr" target="#b44">[44]</ref>, but instead of learning the feature space to compare images, we use a fixed pre-trained Loss Network and non-learnable mask M S . The iHomE loss can be formulated as:</p><formula xml:id="formula_2">L iHomE (I S , I T ) = Hj ,Wj i M i ? max(ap ji ? an ji + m, 0) Hj ,Wj i M i (3)</formula><p>where m is triplet loss margin and ap ji and an ji are anchorpositive and anchor-negative channel-aggregated distances:</p><formula xml:id="formula_3">ap ji = 1 C Cj k ||g jik (I T ) ? g jik (I S )|| 1 an ji = 1 C Cj k ||g jik (I T ) ? g jik (I S )|| 1<label>(4)</label></formula><p>Bidirectional implicit Homography Estimation Loss (bi-HomE). We can additionally leverage the fact that homography is invertible by estimating the transformation from the target I T to the source I S images. Following Zhang et al. <ref type="bibr" target="#b44">[44]</ref>, we also add a constraint that enforces H ST and H T S to be inverse. We can formulate biHomE as:</p><formula xml:id="formula_4">L biHomE (I S , I T ) =L iHomE (I S , I T )+ L iHomE (I T , I S )+ ?||H ST H T S ? 1|| 2 2 (5)</formula><p>where ? is the balancing hyperparameter <ref type="bibr" target="#b44">[44]</ref> and 1 is 3 ? 3 identity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets</head><p>Since collecting the data for homography learning along with ground truth is hard, DeTone et al. <ref type="bibr" target="#b3">[4]</ref> proposed to generate the dataset applying random projective transformations to COCO <ref type="bibr" target="#b19">[20]</ref>. The Synthetic COCO (S-COCO) dataset was accepted by the research community, however, it does not model photometric distortion present in real-world images, such as contrast, brightness, and saturation changes. Here, we introduce a Photometrically Distorted Synthetic COCO (PDS-COCO) where we artificially model illumination changes by utilizing photometric distortion techniques used in <ref type="bibr" target="#b21">[22]</ref> as augmentation for SSD object detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Synthetic COCO (S-COCO)</head><p>The dataset was introduced by DeTone et al. <ref type="bibr" target="#b3">[4]</ref> in which source and target candidates are generated by duplicating the same COCO image <ref type="bibr" target="#b19">[20]</ref>. The source patch I S is generated by randomly cropping a source candidate at position p with a size of 128 ? 128 pixels. Then the patch's corners are randomly perturbed vertically and horizontally by values within the range [??, ?] and the four correspondences define a homography H ST . The inverse of this homography H T S = (H ST ) ?1 is applied to the target candidate and from the resulted warped image a target patch I T is cropped at the same location p. Both I S and I T are the input data with the homography H ST as ground truth as shown in <ref type="figure" target="#fig_0">Figure 3</ref>. Such a procedure not only allows us to use large scale datasets but also creates ground truth homography labels for each transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Photometrically Distorted Synthetic COCO (PDS-COCO)</head><p>To test the robustness of different homography estimation architectures to illumination changes we introduce Photometrically Distorted Synthetic COCO (PDS-COCO) dataset. The photometric distortion is inspired by Liu et al. <ref type="bibr" target="#b21">[22]</ref> and the first step involves adjusting the brightness of the image using randomly picked value ? b ? U(?32, 32). Next, contrast, saturation and hue noise is applied with the following values: ? c ? U(0.5, 1.5), ? s ? U(0.5, 1.5) and are randomly swapped with a probability of 0.5. A composition of these distortions produces vastly different images, which constitutes a challenge for current homography estimation architectures. Such a photometric distortion procedure is applied to the original image independently to create source and target candidates. The rest of the procedure is the same as for S-COCO dataset generation as shown in the <ref type="figure" target="#fig_0">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>We implemented all the methods in PyTorch <ref type="bibr" target="#b29">[30]</ref> and Kornia <ref type="bibr" target="#b32">[32]</ref>. We modified the HEN architectures (if necessary) to use a ResNet34-like <ref type="bibr" target="#b13">[14]</ref> feature extractor, to make sure the performance difference between methods comes from better design instead of a better CNN backbone. All methods are trained using the Adam optimizer <ref type="bibr" target="#b17">[18]</ref> with a batch size of 64 and for 90k iterations. The initial learning rate (Detone and Nguyen: 5e ?3 , Zeng: 1e ?3 , Zhang: 1e ?2 ) is decayed by a factor of 10 every 30k iterations. We use ResNet34 as a fixed Loss Network g with features taken <ref type="figure">Figure 4</ref>. The viewpoint and illumination robustness comparisons of three unsupervised methods for different levels of viewpoint ? and illumination ? changes. S-COCO is equivalent to the box with ? = 32 and ? = 0 and PDS-COCO is equivalent to the box with ? = 32 and ? = 32. NC stands for Not Converged (the methods which were not able to converge at least once per twenty independent runs). The method of Nguyen et al. <ref type="bibr" target="#b26">[27]</ref> is robust to big viewpoint changes (big values of ?) but is not able to converge for big illumination distortion (big values of ?). Zhang et al. <ref type="bibr" target="#b44">[44]</ref> method is highly robust to illumination change ? but only for small viewpoint change ?, whereas our biHomE loss is robust to both big illumination and viewpoint changes.</p><p>after the first residual block. We also used N S = 1024 for Zeng method and ? = 0.01, unless stated otherwise. The homography estimation quality is reported using Mean Absolute Corner Error (MACE) <ref type="bibr" target="#b3">[4]</ref>. We report the mean and standard deviation of three runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">S-COCO and PDS-COCO Results</head><p>Our implementation achieves much lower MACE on S-COCO than originally reported by DeTone et al. <ref type="bibr" target="#b3">[4]</ref> and Nguyen et al. <ref type="bibr" target="#b26">[27]</ref>: 1.96 and 2.07 against 9.20 and 12.91, respectively. This is because we employ a modern CNN architecture -ResNet34 <ref type="bibr" target="#b13">[14]</ref> instead of originally used VGGlike structure <ref type="bibr" target="#b36">[36]</ref>. Having the same backbone, Zeng's et al. <ref type="bibr" target="#b43">[43]</ref> model is only about 13% better than DeTone's et al. <ref type="bibr" target="#b3">[4]</ref> model -instead of 564% improvement reported in their paper.</p><p>As shown in the <ref type="figure">Figure 1</ref>, our architecture gives the best performance compared to all other unsupervised methods. Specifically, using our biHomE loss with the same HEN as Zhang et al. <ref type="bibr" target="#b44">[44]</ref> demonstrates the beneficial effect of disentangling the homography estimation learning from feature representation learning. Moreover, our biHomE loss achieves comparable (Zeng et al. <ref type="bibr" target="#b43">[43]</ref>) or better (DeTone et al. <ref type="bibr" target="#b3">[4]</ref>) homography estimation performance than supervised methods on S-COCO.</p><p>Homography estimation on PDS-COCO presents a bigger challenge, because of additional photometric distortions on the images. Supervised approaches perform much worse in terms of MACE and neither Nguyen's <ref type="bibr" target="#b26">[27]</ref> nor Zhang's <ref type="bibr" target="#b44">[44]</ref> methods can converge. Our biHomE loss is the only method trained in an unsupervised manner that can converge on PDS-COCO. Similar to S-COCO, our biHomE loss achieves comparable or better performance compared to supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Illumination and Viewpoint Robustness Study</head><p>The real-life images can exhibit big illumination and viewpoint changes. It is hard to collect such datasets with ground truth homography labels, however, we can use the COCO dataset and simulate different conditions by two parameters: viewpoint change ? and illumination change ?.</p><p>Viewpoint change ? defines the maximum range of corner perturbation, and it was already introduced in Section 4.1. Since photometric distortion has a few elements, we bring them all together by illumination change ? so that all other deltas are controlled by one parameter:</p><formula xml:id="formula_5">? x ? U(?X ? ? 32 , X ? ? 32 )<label>(6)</label></formula><p>where ? x is one of ? b , ? c , ? s or ? h (following the notation from Section 4.1), and X is the corresponding perturbation range. The bigger ? the more viewpoint change and respectively the bigger ? the more illumination change between input images. For such a notation S-COCO is produced by ? = 32 and ? = 0 and PDS-COCO is produced by ? = 32 and ? = 32.</p><p>As presented in <ref type="figure">Figure 4</ref>, the method of Nguyen et al. <ref type="bibr" target="#b26">[27]</ref> based on photometric loss is robust to big viewpoint changes (big values of ?), but is not able to produce any reasonable model for big illumination distortion ?. On the other hand, Zhang et al. <ref type="bibr" target="#b44">[44]</ref> method is highly robust to illumination change ? but only for small viewpoint change ?. Our biHomE loss, which for a fair comparison we add on top of the same HEN as Zhang et al. <ref type="bibr" target="#b44">[44]</ref> is robust both to big illumination ? and viewpoint ? changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">From Perceptual Loss to biHomE</head><p>Perceptual Loss was already introduced in the tasks of super-resolution <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">41]</ref>, style transfer <ref type="bibr" target="#b15">[16]</ref>, image denoising <ref type="bibr" target="#b8">[9]</ref>, or training autoencoders <ref type="bibr" target="#b31">[31]</ref>. The idea was to rely on perceptual similarity of high-level features extracted from pre-trained networks. The same property is desired in homography estimation. The architecture of Loss Network designed for super-resolution task, however, could be not optimal in the task of image comparison for homography estimation. To study the effects of different components of the perceptual loss we gradually modified the State-of-the-Art perceptual loss proposed by Wang et al. <ref type="bibr" target="#b41">[41]</ref> and compare their performance in the homography estimation task as presented in <ref type="table" target="#tab_1">Table 1</ref>. The red color indicates the main improvement compared with the previous model. We use DeTone et al. <ref type="bibr" target="#b3">[4]</ref> in all configurations and only change elements of perceptual loss. The average MACE of three runs is presented. A detailed discussion is provided as follows.</p><p>ResNet34. We first move from VGG19 <ref type="bibr" target="#b36">[36]</ref> network to ResNet34 <ref type="bibr" target="#b13">[14]</ref>. Using ResNet as a Loss Network significantly improves the homography estimation accuracy, which can be caused by a better-learned feature representation. We also noticed that learning is more likely to converge with ResNet, probably due to better gradient flow of residual connections. The detailed comparison is out of the scope of this article. For a comprehensive analysis of these networks please refer to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>. Block1. Ledig et al. <ref type="bibr" target="#b18">[19]</ref> argues to use higher-level features from deeper network layers with the potential to focus more on the content of the image. It is essential for the Super-Resolution task, but as depicted in <ref type="table" target="#tab_1">Table 1</ref>, in Homography Estimation low-level features matter more. Features after activation. In contrast to Wang et al. <ref type="bibr" target="#b41">[41]</ref>, we show that it is beneficial to use features after the ReLU activation function <ref type="bibr" target="#b9">[10]</ref>. A possible explanation is that lowlevel features are activated mostly on edges and corners, which are crucial for image alignment purposes (please refer to <ref type="figure">Figure 6a</ref> of Wang et al. <ref type="bibr" target="#b41">[41]</ref> for a visual comparison). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HEN</head><formula xml:id="formula_6">L 1 distance.</formula><p>Replacing M SE loss function with L 1 distance yields better homography estimates. Due to warping and illumination differences, feature maps could contain some artifacts, and L 1 loss is known to be more robust to outliers <ref type="bibr" target="#b10">[11]</ref>. iHomE. Triplet loss introduced by Zhang et al. <ref type="bibr" target="#b44">[44]</ref> can further improve the homography estimation performance. This is probably because the model has additional information of original I1 image features. biHomE. Using triplet loss in both directions similarly to Zhang et al. <ref type="bibr" target="#b44">[44]</ref> can additionally improve the results. The improvement is even more profound for other HENs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Margin Ablation</head><p>Zhang et al. <ref type="bibr" target="#b44">[44]</ref> used a margin value of m = 1 in their triplet loss formulation. However, we found out that on S-COCO bigger values of m yield better results ( <ref type="table" target="#tab_2">Table 2</ref>). The best performance is produced when the triplet loss is never saturated, which is equivalent to setting the margin to an infinite value:</p><formula xml:id="formula_7">L iHomE j (I S , I T ) = Hj Wj i M i ? (ap ji ? an ji ) Hj Wj i M i<label>(7)</label></formula><p>In all the experiments for both Zhang et al. <ref type="bibr" target="#b44">[44]</ref> and our iHomE and biHomE losses we use above Formulation 7 instead of Equation 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Channel Information Aggregation</head><p>In contrast to Zhang et al. <ref type="bibr" target="#b44">[44]</ref>, our feature map obtained by Loss Network g has more than one channel, thus we have to figure out how the channel information will be aggregated. One can either calculate distances for every channel and then apply triplet loss in a channel-agnostic way per every spatial location or apply triplet per every channel location (channel-wise). The former is presented in equations 3 and 4, and the latter can be formulated as: </p><p>where;</p><formula xml:id="formula_9">ap ji = ||g ji (I T ) ? g ji (I S )|| 1 an ji = ||g ji (I T ) ? g ji (I S )|| 1<label>(9)</label></formula><p>As presented in <ref type="table" target="#tab_2">Table 2</ref> the channel-agnostic way achieves better performance on the S-COCO dataset. We hypothesize that forcing every channel of I T feature description to be closer to I S than to I S is probably a very hard task and looking at all channels before comparison makes the optimization easier. Channel-agnostic and channel-wise versions of biHomE loss with infinite margin are mathematically equivalent, so we report only one number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">To Freeze or not to Freeze</head><p>We also want to find out how important is freezing the Loss Network for effective homography estimation learning. As shown in <ref type="table">Table 3</ref>, fixed Loss Network performs better. One of the possible reasons is that freezing weights of g allows using a bigger learning rate, which can result in better convergence <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38]</ref>. But even for the same learning rate value freezing the Loss Network is still a preferable policy.  <ref type="table">Table 3</ref>. S-COCO performance of Zhang et al. <ref type="bibr" target="#b44">[44]</ref> as HEN and our biHomE loss when using fixed and learnable Loss Network.</p><p>Fixing weights of g allows using bigger learning rate, which can result in better convergence <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">biHomE Performance on Out-of-Distribution Dataset</head><p>One of the shortcomings of using models pre-trained on ImageNet <ref type="bibr" target="#b35">[35]</ref> as Loss Network is possible sub-optimal results on out-of-distribution datasets. To test this vulnerability we compare Zhang et al. <ref type="bibr" target="#b44">[44]</ref> method with the original and our biHomE loss on FLIR Dataset <ref type="bibr" target="#b6">[7]</ref> preprocessed similar to S-COCO. As shown in <ref type="table" target="#tab_5">Table 4</ref>, although thermal images are vastly different than color images the Loss Network was trained on, our biHomE loss still achieves better performance.   <ref type="bibr" target="#b6">[7]</ref> containing thermal images for ADAS systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-FLIR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Image Alignment for Change Captioning</head><p>In this Section, we study the influence of image alignment using homography on change captioning task. The goal is to generate captions for an image pair before and after the change. There are six change types (color/material change, adding/dropping/moving an object, and no change) and each is combined with illumination/viewpoint change. Recently, Park et al. <ref type="bibr" target="#b27">[28]</ref> proposed a new architecture called Dual Dynamic Attention Model (DUDA) for generating change captions. It entails three main components: feature extractor (ResNet101 <ref type="bibr" target="#b13">[14]</ref>), Dual Attention and Dynamic Speaker modules. The Dual Attention module takes features produced by the feature extractor and generates a sparse feature vector with a description of the changes between input images. The Dynamic Speaker module then produces the caption word-by-word dynamically moving the attention between the first and second images depending on the stage of the sentence.</p><p>In their evaluation Section, Park et al. <ref type="bibr" target="#b27">[28]</ref> showed degradation of captioning performance as viewpoint shift increases -our reproduced results are shown in <ref type="figure">Figure 5</ref>. The Dual Attention module intrinsically assumes the im- <ref type="figure">Figure 5</ref>. Change captioning performance breakdown by viewpoint shift (measured by IoU). Aligning images with Zhang <ref type="bibr" target="#b44">[44]</ref> method before learning to generate captions improves the metrics, but performance degradation between smaller and bigger viewpoint shifts is still present. Learning Zhang <ref type="bibr" target="#b44">[44]</ref> with our biHomE loss seems to alleviate this problem.  <ref type="table">Table 5</ref>. Change Captioning evaluation on CLEVR Change dataset. Aligning images before learning to generate captions improves all captioning metrics: BLEU-4 (B), CIDEr (C), METEOR (M), and SPICE (S) in each setting (i.e. Total, Scene Change, Distractor). For more details, we refer to Park et al. <ref type="bibr" target="#b27">[28]</ref>. The model trained with our biHomE loss achieves the best result in most of the cases. ages before and after the change are roughly aligned, thus finding corresponding objects on images with big viewpoint change is challenging. We argue that aligning those images helps DUDA architecture learn better captions of the change on the scene.</p><p>We conducted experiments for Zhang et al. <ref type="bibr" target="#b44">[44]</ref> architecture with and without our biHomE loss. Unless stated otherwise, the following are the steps that have been followed in all our experiments: first, the homography estimation is learned. Second, we transform images after change onto images before change using the trained homography estimation model. The pre-processed images are trained using the DUDA model and the captioning quality metric is reported in <ref type="table">Table 5</ref>. Moreover, we also prepare similar validation of the robustness of the model to viewpoint shift, according to the methodology shown in Park et al. <ref type="bibr" target="#b27">[28]</ref> which is depicted in <ref type="figure">Figure 5</ref>.</p><p>Experimental evaluation shows that aligning images before learning to caption is beneficial not only for overall captioning quality but also for performance degradation between smaller and bigger viewpoint shifts. A sample image the small brown metal cylinder that is in front of the small purple rubber object has been newly placed the scene remains the same before change after change DUDA DUDA + iHomE <ref type="figure">Figure 6</ref>. Qualitative result comparing DUDA <ref type="bibr" target="#b27">[28]</ref> output with and without image alignment. The blue and red attention maps are applied before and after the change, respectively. After adding the image alignment step, DUDA <ref type="bibr" target="#b27">[28]</ref> architecture produces the correct caption.</p><p>pair with caption generated with and without image alignment step is presented in <ref type="figure">Figure 6</ref>. Note that using our bi-HomE loss the performance degradation is negligible for the CLEVR Change dataset, which can be explained by facilitated correspondence search in the Dual Attention module on aligned images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We presented an unsupervised approach to homography estimation that is robust to big illumination and viewpoint changes at the same time. We showed that disentangling the homography estimation from representation learning provides better estimates. We also proposed to use an additional photometric distortion step in the synthetic COCO dataset generation (PDS-COCO) and encourage future works to use it as a new evaluation benchmark of robust homography estimation. Then, we presented a study of modern homography estimation baselines along with our bidirectional implicit Homography Estimation (bi-HomE) loss on both S-COCO and PDS-COCO. biHomE loss achieves a new state-of-the-art performance for unsupervised homography estimation, which is also comparable or better compared to supervised approaches. Furthermore, we investigated the influence of image alignment using homography on change captioning quality. We showed that aligning the images using our biHomE loss is not only beneficial for the overall captioning quality of DUDA architecture on the CLEVR Change dataset but also for performance degradation from smaller to bigger viewpoint shifts. , where the target image was shifted by a given number of pixels in both X and Y axes. We used pretrained ResNet34 up to the first residual block as perceptual Loss Network and MSE as photometric distance. For S-COCO distance statistics is similar for both distances, but for PDS-COCO perceptual distance is smoother.  <ref type="table">Table 6</ref>. The performance of the original Zhang et al. <ref type="bibr" target="#b44">[44]</ref> method (reported in the paper and reproduced by us) and trained with our biHomE loss on their dataset <ref type="bibr" target="#b44">[44]</ref>. The performance of the original Zhang et al. <ref type="bibr" target="#b44">[44]</ref> method is only slightly better than using our biHomE loss. We hypothesize this is because this dataset consists mostly of image pairs with small viewpoint and illumination changes. and L 1 distance in feature space produced by the Loss Network g between both images. To bring both distances in the same range we normalize them by the maximum observed distance. The average of one hundred images for both S-COCO and PDS-COCO is presented in <ref type="figure" target="#fig_2">Figure 7</ref>. The distance curve of photometric loss and perceptual loss on S-COCO is similar, so we expect the comparable performance of both loss functions. However, when photometric distortion is introduced, the perceptual loss function is smoother and will likely produce better results. Indeed, both conclusions are supported by the illumination robustness experiments shown in Section 5.3 of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. biHomE Performance on Real-World Dataset</head><p>The dataset proposed by Zhang et al. <ref type="bibr" target="#b44">[44]</ref> is composed of 80k image pairs extracted from short video clips containing small camera movements and dynamic objects on the scene. The image pairs are divided into 5 categories: regular (RE), low-texture (LT), low-light (LL), small-foregrounds (SF), and large-foreground (LF) scenes. We reproduced the original Zhang et al. <ref type="bibr" target="#b44">[44]</ref> method and using the same learning setting we also learned our biHomE loss.</p><p>As one can observe in <ref type="table">Table 6</ref> the performance of the original Zhang et al. <ref type="bibr" target="#b44">[44]</ref> method is only slightly better than using our biHomE loss. A similar effect could be observed also in Section 5.3, where for small viewpoint change ? and small photometric distortion ? original Zhang method is also better than with our biHomE loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>? h ? U(?18, 18). Finally, the color channels of the image coco image source-candidate d is to rt / c o p y crop target-candidate d is to rt / c o p y perturb HT SISIT PDS-COCO dataset generation. Two separate photometric distortions are performed for a given COCO image resulting in the creation of source-candidate and target-candidate. Next, a random crop is selected for the source-candidate. The random crop is perturbed and given these correspondences, HST is computed. HT S = (HST ) ?1 is applied to the target-candidate. Finally, source image IS and target image IT are extracted. Substituting photometric distortion with copy operation gives the original S-COCO dataset generation procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, I T ) = Hj Wj Cj i M i ? max(ap ji ? an ji + m, 0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>Normalized distance as a function of image shift. The Figure is prepared for one hundred random images from S-COCO (dashed) PDS-COCO dataset (solid)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>?0.05 7.15 ?0.<ref type="bibr" target="#b15">16</ref> 4.18 ?0.<ref type="bibr" target="#b15">16</ref> 4.74 ?0.05 4.11 ?0.11 3.82 ?0.11 3.67 ?0.10 2.39 ?0.<ref type="bibr" target="#b19">20</ref> Comparison of performance on PDS-COCO of different settings of perceptual loss. Each column represents a model with DeTone's<ref type="bibr" target="#b3">[4]</ref> architecture as HEN and a particular perceptual loss configuration. The red color indicates the main improvement compared with the previous model starting fromWang et al. [41]  setting.</figDesc><table><row><cell></cell><cell>#1</cell><cell>#2</cell><cell>#3</cell><cell>#4</cell><cell>#5</cell><cell>#6</cell><cell>#7</cell><cell>#8</cell></row><row><cell>Loss network</cell><cell>VGG19</cell><cell cols="4">ResNet34 ResNet34 ResNet34 ResNet34</cell><cell>ResNet34</cell><cell>ResNet34</cell><cell>ResNet34</cell></row><row><cell>Layers</cell><cell>VGG22</cell><cell>Block2</cell><cell>Block1</cell><cell>Block0</cell><cell>Block1</cell><cell>Block1</cell><cell>Block1</cell><cell>Block1</cell></row><row><cell>Activation</cell><cell>Before</cell><cell>Before</cell><cell>Before</cell><cell>Before</cell><cell>After</cell><cell>After</cell><cell>After</cell><cell>After</cell></row><row><cell>Loss function</cell><cell>MSE</cell><cell>MSE</cell><cell>MSE</cell><cell>MSE</cell><cell>MSE</cell><cell>L 1</cell><cell>iHomE</cell><cell>biHomE</cell></row><row><cell>MACE</cell><cell>8.59</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>?0.27 1.86 ?0.16 m = ? 2.08 ?0.11 1.87 ?0.07 1.87 ?0.07 The performance on the S-COCO dataset of Zhang et al.</figDesc><table><row><cell></cell><cell>Zhang</cell><cell>Zhang</cell><cell>Zhang</cell></row><row><cell cols="2">Loss Function Triplet Loss</cell><cell>biHomE</cell><cell>biHomE</cell></row><row><cell>Channel-</cell><cell>-</cell><cell>c-wise</cell><cell>c-agnostic</cell></row><row><cell>m = 1</cell><cell>3.07 ?0.34</cell><cell cols="2">4.57 ?0.21 3.33 ?0.34</cell></row><row><cell>m = 100</cell><cell>2.50 ?0.25</cell><cell>2.84</cell><cell></cell></row></table><note>[44] and our biHomE loss with different margin m settings and different channel information aggregation. The infinite margin version is consistently better for both losses and it is used in all experiments in the paper.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>The performance of our biHomE loss vs. original Zhang et al. [44] method on S-FLIR dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>115.2 37.4 31.3 50.8 100.3 33.2 27.8 62.3 115.9 49.8 34.8 {Zhang [44] + TripletLoss} + DUDA [28] 54.5 120.7 40.2 33.0 52.1 111.4 36.0 31.4 61.2 115.3 51.6 34.6 {Zhang [44] + biHomE} + DUDA [28] 53.0 124.5 40.2 33.0 50.6 117.3 36.9 30.9 63.7 117.1 50.5 35.0</figDesc><table><row><cell></cell><cell></cell><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Scene Change</cell><cell></cell><cell></cell><cell cols="2">Distractor</cell><cell></cell></row><row><cell>Approach</cell><cell>B</cell><cell>C</cell><cell>M</cell><cell>S</cell><cell>B</cell><cell>C</cell><cell>M</cell><cell>S</cell><cell>B</cell><cell>C</cell><cell>M</cell><cell>S</cell></row><row><cell>{No alignment} + DUDA [28]</cell><cell>53.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Zhang (reproduced) 1.813 ?0.013 2.139 ?0.013 1.906 ?0.013 1.837 ?0.010 1.894 ?0.006 1.918 ?0.006 Zhang + biHomE 1.822 ?0.006 2.178 ?0.031 1.924 ?0.011 1.842 ?0.006 1.994 ?0.009 1.941 ?0.008</figDesc><table><row><cell></cell><cell></cell><cell>LT</cell><cell>LL</cell><cell>SF</cell><cell>LF</cell><cell>Avg</cell></row><row><cell>Zhang (reported)</cell><cell>1.81</cell><cell>1.90</cell><cell>1.94</cell><cell>1.75</cell><cell>1.72</cell><cell>1.82</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices Appendix A. Photometric or Perceptual</head><p>In this Section, we want to additionally explore the possible reason behind the effectiveness of perceptual loss functions. Photometric loss is known to be sensitive to illumination conditions, while high-level features extracted from pretrained networks care more about perceptual similarity <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">41]</ref>. To better understand the behavior of both losses we prepare a simple experiment, where the target image is shifted by a given number of pixels in the X and Y axis w.r.t the source image. Then we report L 1 distance in pixel space</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Benchmark Analysis of Representative Deep Neural Network Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Celona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Napoletano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="64270" to="64277" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Super-Resolution with Deep Convolutional Sufficient Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Homography-based change detection for space-based satellite inspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Buffington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcinroy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">8044</biblScope>
			<biblScope unit="page">804409</biblScope>
		</imprint>
	</monogr>
	<note>Sensors and Systems for Space Applications IV</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03798</idno>
		<title level="m">Deep Image Homography Estimation</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">LSD-SLAM: Large-scale direct monocular SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sch?ps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parametric Image Alignment Using EnhancedCorrelation Coefficient Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Georgios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Emmanouil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Psarakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1858" to="1865" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">FLIR Thermal Dataset for Algorithm Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Group</surname></persName>
		</author>
		<ptr target="https://www.flir.in/oem/adas/adas-dataset-form.accessed26.02.2021.7" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Learning for Low-Dose CT Denoising Using Perceptual Loss and Edge Detection Layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Gholizadeh-Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javad</forename><surname>Alirezaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Babyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Sparse Rectifier Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint Video Stitching and Stabilization From Moving Cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moncef</forename><surname>Gabbouj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5491" to="5503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Umme Zahoora, and Aqsa Saeed Qureshi. A survey of the Recent Architectures of Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asifullah</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anabia</forename><surname>Sohail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5455" to="5516" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meshflow: Minimum latency online video stabilization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="800" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SSD: Single Shot MultiBox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An Iterative Image Registration Technique with an Application to Stereo Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 7th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="674" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ahmed Bilal Ashraf, and Sridha Sridharan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajitha</forename><surname>Navarathna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1383" to="1396" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Fourier Lucas-Kanade Algorithm</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ORB-SLAM: a versatile and accurate monocular SLAM system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose Maria Martinez</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1147" to="1163" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised Deep Homography: A Fast and Robust Homography Estimation Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ty</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shreyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camillo Jose</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2346" to="2353" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust Change Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Dong Huk Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4624" to="4633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Illumination Change Robustness in Direct Visual SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonwook</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sch?ps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4523" to="4530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving Image Autoencoder Embeddings with Perceptual Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Grund Pihlgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Sandin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Kornia: an Open Source Differentiable Computer Vision Library for PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3674" to="3683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ORB: An efficient alternative to SIFT or SURF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2564" to="2571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1-learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Topin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11006</biblScope>
			<biblScope unit="page">1100612</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Image alignment and stitching: A tutorial. Foundations and Trends? in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical 2.5D Scene Alignment for Change Detection with Large Viewpoint Differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gijs</forename><surname>Dennis Wjm Van De Wouw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter Hn De</forename><surname>Dubbelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>With</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="361" to="368" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An improved RANSAC homography algorithm for feature based image mosaic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianyong</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th WSEAS International Conference on Signal Processing</title>
		<meeting>the 7th WSEAS International Conference on Signal Processing</meeting>
		<imprint>
			<publisher>WSEAS</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Computational Geometry &amp; Artificial Vision</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rethinking Planar Homography Estimation Using Perspective Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridha</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="571" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanpeng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianjin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05983</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Content-Aware Unsupervised Deep Homography Estimation. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
