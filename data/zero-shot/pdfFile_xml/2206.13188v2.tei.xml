<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Learning in Remote Sensing: A Review</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Yi</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Conrad</forename><forename type="middle">M</forename><surname>Albrecht</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassim</forename><surname>Ait</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Braham</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Mou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Zhu</surname></persName>
						</author>
						<title level="a" type="main">Self-supervised Learning in Remote Sensing: A Review</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep learning</term>
					<term>self-supervised learning</term>
					<term>com- puter vision</term>
					<term>remote sensing</term>
					<term>Earth observation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Category Sub-category Representative methods Generative Autoencoder Autoencoder (AE) [54]: encode and reconstruct the input. Sparse AE [55]: sparsity constraints on the hidden units. Denoising AE [56]: reconstruct clear image from noisy input. VAE [57]: encode the input to a normal distribution. MAE [58]: reconstruct randomly masked patches with vision transformer. GAN GAN [59]: adversarial training with a generator and a discriminator. AAE [60]: adversarial variational autoencoder. BiGAN [61]: an additional encoder to map samples to representations. Predictive Spatial Relative position [62]: predict relative position of random patch pairs. Jigsaw [63]: predict the correct order of a jigsaw puzzle. Rotation [49]: predict the rotation angle of rotated images. Inpainting [64]: recover a missing patch of the input image. Spectral Colorization [65]: predict colorful image from gray-scale input. Temporal Frame order [66]: predict the order of frame sequences. Others Counting [67]: count the visual primitives within the input image. Artifact [68]: spot and predict artifacts. Audio [69]: predict a statistical summary of the sound associated with a video. Contrastive Negative sampling Triplet loss [70]: learn a similarity metric discriminatively with triplet loss. CPC [71]: contrastive predictive coding with InfoNCE loss. DIM [72]: maximize mututal information between global-local feature pairs. InstDisc [73]: maximally scatter the features of different images over a unit sphere. PIRL [74]: pretext tasks as data augmentation. MoCo [75]: store negative samples in a queue and momentum-update the key encoder. SimCLR [76]: end-to-end contrastive structure with large batchsize and strong data augmentation. MoCo-v3 [77]: introduce vision transformer as the encoder backbone for MoCo-v2 [78]. Clustering DeepCluster [51]: iteratively leverage k-means clustering to yield pseudo labels for prediction. LocalAgg [79]: optimize a local soft-clustering metric. SeLa [80]: solve an optimal transport problem to obtain the pseudo-labels. SwAV [81]: contrastive learning with online clustering. PCL [82]: perform iterative clustering and representation learning in an EM-based framework. Knowledge distillation BYOL [83]: mean-teacher network with a predictor on top of the teacher encoder. SimSiam [84]: explore the simplest design of contrastive self-supervised learning. DINO [52]: explores the self-distillation scheme with vision transformer backbones. EsViT [85]: improve DINO with additional region-level contrastive task. iBOT [86]: cross-view and in-view self-distillation with masked image modeling. Redundancy reduction Barlow Twins [87]: redundancy reduction on the cross-correlation matrix between features. VICReg [88]: variance-invariance-covariance regularization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-In deep learning research, self-supervised learning (SSL) has received great attention triggering interest within both the computer vision and remote sensing communities. While there has been a big success in computer vision, most of the potential of SSL in the domain of earth observation remains locked. In this paper, we provide an introduction to, and a review of the concepts and latest developments in SSL for computer vision in the context of remote sensing. Further, we provide a preliminary benchmark of modern SSL algorithms on popular remote sensing datasets, verifying the potential of SSL in remote sensing and providing an extended study on data augmentations. Finally, we identify a list of promising directions of future research in SSL for earth observation (SSL4EO) to pave the way for fruitful interaction of both domains.</p><p>Index Terms-Deep learning, self-supervised learning, computer vision, remote sensing, Earth observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A DVANCES of deep neural networks to model the rich structure of large amounts of data has led to major breakthroughs in computer vision, natural language processing, automatic speech recognition, and time series analysis <ref type="bibr" target="#b16">[1]</ref>. However, the performance of deep neural networks is very sensitive to the size and quality of the training data. Thus, a plurality of annotated datasets (e.g. ImageNet <ref type="bibr" target="#b17">[2]</ref>) have been generated for supervised training in the last decade, driving progress in many fields. Unfortunately, annotating large-scale datasets is an extremely laborious, time-consuming, and expensive procedure. This limitation of the supervised learning paradigm strongly impedes the applicability of deep learning in real-world scenarios.</p><p>A lot of research efforts have been deployed to tackle the challenge of data annotation in the machine learning literature.</p><p>The work is jointly supported by the Helmholtz Association through the Framework of Helmholtz AI (grant number: ZT-I-PF-5-01) -Local Unit "Munich Unit @Aeronautics, Space and Transport (MASTr)" and Helmholtz Excellent Professorship "Data Science in Earth Observation -Big Data Fusion for Urban Research"(grant number: W2-W3-100), by the German Federal Ministry of Education and Research (BMBF) in the framework of the international future AI lab "AI4EO -Artificial Intelligence for Earth Observation: Reasoning, Uncertainties, Ethics and Beyond" (grant number: 01DD20001) and by German Federal Ministry for Economic Affairs and Climate Action in the framework of the "national center of excellence ML4Earth" (grant number: 50EE2201C). (Corresponding author: Xiao Xiang Zhu.) Y. Wang, N.A.A. Braham, L. Mou and X. Zhu are with the Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Germany and with the Chair of Data Science in Earth Observation, Technical University of Munich (TUM), Germany (e-mails: yi.wang@dlr.de; Nassim.aitalibraham@dlr.de; lichao.mou@dlr.de; xiaoxiang.zhu@dlr.de).</p><p>C. M Albrecht is with the Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Germany (e-mail: conrad.albrecht@dlr.de).</p><p>In fact, numerous alternatives to vanilla supervised learning have been studied, such as unsupervised learning <ref type="bibr" target="#b18">[3]</ref>, semisupervised learning <ref type="bibr" target="#b19">[4]</ref>, weakly supervised learning <ref type="bibr" target="#b20">[5]</ref> and meta-learning <ref type="bibr" target="#b21">[6]</ref>. Recently, self-supervised learning (SSL) did raise considerable attention in computer vision (see <ref type="figure" target="#fig_0">Fig.  1</ref>) and achieved significant milestones towards the reduction of human supervision. Indeed, by distilling representative features from unlabeled data, SSL algorithms are already outperforming supervised pre-training on many problems <ref type="bibr" target="#b22">[7]</ref>.</p><p>Meanwhile, the success of deep learning did also spark remarkable progress in remote sensing <ref type="bibr" target="#b23">[8]</ref>. Yet, data annotation remains a major challenge for earth observation as well. In that regard, self-supervised learning is a promising avenue of research for remote sensing. While there exist comprehensive surveys of SSL in computer vision <ref type="bibr" target="#b24">[9,</ref><ref type="bibr" target="#b25">10,</ref><ref type="bibr" target="#b26">11,</ref><ref type="bibr" target="#b27">12]</ref>, a didactic and up-to-date introduction to the remote sensing community is missing. This review aims at bridging this gap, documenting progress in self-supervised visual representation learning, and how it did get applied in remote sensing to date. Specifically:</p><p>? To the remote sensing community, we provide a comprehensive introduction and literature review on selfsupervised visual representation learning. <ref type="bibr">?</ref> We summarize a taxonomy of self-supervised methods covering existing works from both computer vision and remote sensing communities. <ref type="bibr">?</ref> We provide quantification of performance benchmarks on established SSL methodologies utilizing the ImageNet dataset <ref type="bibr" target="#b17">[2]</ref>, and extend the analysis to three multispectral satellite imagery datasets: BigEarthNet <ref type="bibr" target="#b28">[13]</ref>, SEN12MS <ref type="bibr" target="#b29">[14]</ref>, and So2Sat-LCZ42 <ref type="bibr" target="#b30">[15]</ref>. <ref type="bibr">?</ref> We discuss the link of natural imagery and remotely sensed data, providing insights for future work in selfsupervised learning for remote sensing and earth observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supervision: Data vs. Labels</head><p>A significant fraction of real-world applications of machine/ deep learning methodologies calls for massive amounts of training data. Fortunately, the increasing adoption of open access in earth observation provides remote sensing researchers with a plethora of such. However, annotation of such vast amounts of earth observation data implies significant human interaction with frequent updates needed every single day. For example, apart from the large amounts of sampled data to label, a single large-scale dataset may require separate annotation depending on downstream applications. Moreover, datasets sensing the physical world get outdated over time due to both natural and man-made changes. Both aspects significantly amplify the efforts in annotating remotely sensed data of the Earth. Another big challenge of machine learning in remote sensing is label noise. Due to the quality of remotely sensed images and the complexity of various specific applications, it is very difficult to generate perfect labels during large-scale data annotation. Therefore, there always exists a trade-off between the number of annotations and their quality: large but noisy labeled datasets can bias the model, whereas small amounts of good-quality labels usually lead to overfitting.</p><p>In addition, it has become popular to execute pre-training on established benchmark datasets before fine-tuning deeply learned models for downstream tasks that do not have enough labels. This procedure is commonly referred to as transfer learning <ref type="bibr" target="#b31">[16]</ref>. However, the performance of supervised pretraining depends on the domain difference between source and target data. A good pre-trained model renders well on a similar dataset but is not as useful on a very different one.</p><p>All the above challenges emphasize the growing gap between an increasing amount of remote sensing data and the shortage of good-quality labels, calling for techniques to exploit the corpus of unlabeled data to learn valuable information that easily transfers to multiple applications. Self-supervised learning offers a paradigm to approach this dilemma. data. Afterward, the model f can be further transferred to supervised downstream tasks for real-world applications. The most common strategies to design such self-supervision typically exploit three types of objectives: (1) reconstructing input data x, f (x) ? x, (2) predicting a self-produced label c which usually comes from contextual information and data augmentation (e.g., predicting the rotation angle of a rotated image), f (x) ? c, (3) contrasting semantically similar inputs x 1 and x 2 (e.g., the encoded features of two augmented views of the same image should be identical), |f (x 1 ) ? f (x 2 )| ? 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Self-supervision: Learning Task-agnostic Representations of Data</head><p>Given self-supervised training succeeded, the pre-trained model f may get transferred to downstream tasks. As Opposed to supervised pre-training, models pre-trained by selfsupervision bear the potential to leverage more general representations and offer a paradigm to mitigate the shortcomings of supervised learning: (1) no human annotation is needed for pre-training, (2) small amounts of labels are sufficient for good performance on downstream tasks, (3) little domain gap between pre-training and downstream dataset can be ensured by collecting unlabeled data from the target application.</p><p>Link to semi-supervised learning. Self-supervised learning algorithms may be considered as part of semi-supervised learning-a branch of machine learning concerned with labeled and unlabelled data. Van Engelen et al. <ref type="bibr" target="#b19">[4]</ref> proposed a comprehensive taxonomy of semi-supervised classification algorithms based on conceptual and methodological aspects for processing unlabelled data. Within that scheme, selfsupervised learning may get categorized as unsupervised preprocessing: in the first step, unlabeled data gets transformed to extract feature representations; labeled data is then utilized to adapt the model to specific tasks.</p><p>Within the remote sensing community, semi-supervised learning has been long studied and enjoys applications in, e.g., hyperspectral image recognition and processing <ref type="bibr" target="#b34">[19,</ref><ref type="bibr" target="#b35">20,</ref><ref type="bibr" target="#b36">21,</ref><ref type="bibr" target="#b37">22,</ref><ref type="bibr" target="#b38">23,</ref><ref type="bibr" target="#b39">24,</ref><ref type="bibr" target="#b40">25,</ref><ref type="bibr" target="#b41">26,</ref><ref type="bibr" target="#b42">27,</ref><ref type="bibr" target="#b43">28]</ref>, multi-spectral image segmentation <ref type="bibr" target="#b44">[29,</ref><ref type="bibr" target="#b45">30,</ref><ref type="bibr" target="#b46">31,</ref><ref type="bibr" target="#b47">32,</ref><ref type="bibr" target="#b48">33,</ref><ref type="bibr" target="#b49">34,</ref><ref type="bibr" target="#b50">35]</ref> and SAR-optical data fusion <ref type="bibr" target="#b51">[36]</ref>.</p><p>Link to unsupervised learning. Self-supervised learning is also a discipline in the realm of unsupervised learningthe general set of machine learning methods that are independent of human annotation. However, it is very ambiguous in various communities the difference between self-supervised and unsupervised learning. In this paper, we separate them in terms of methodology. Traditional unsupervised algorithms tend to utilize statistics of the input data and generate groups by dimensionality reduction <ref type="bibr" target="#b52">[37,</ref><ref type="bibr" target="#b53">38]</ref> or clustering <ref type="bibr" target="#b54">[39]</ref>. On the other hand, self-supervised learning is a recent terminology that refers to approaches in which a model (e.g., a neural network) is trained to learn good data representations using supervision signals that are automatically generated from the data itself.</p><p>In remote sensing, unsupervised learning has also been actively employed in various applications, e.g., in scene classification <ref type="bibr" target="#b55">[40,</ref><ref type="bibr" target="#b53">38,</ref><ref type="bibr" target="#b56">41]</ref>, semantic segmentation <ref type="bibr" target="#b57">[42,</ref><ref type="bibr" target="#b58">43,</ref><ref type="bibr" target="#b59">44]</ref>, change detection <ref type="bibr" target="#b60">[45,</ref><ref type="bibr" target="#b61">46]</ref> and multi-sensor data analysis <ref type="bibr" target="#b62">[47,</ref><ref type="bibr" target="#b63">48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Evaluation in Self-Supervised Learning</head><p>To evaluate the performance of self-supervised learning methods, downstream tasks are commonly defined in order to transfer the pre-trained model f to specific applications, such as scene classification, semantic segmentation, and object detection. The performance of transfer learning on these highlevel vision tasks estimates the generalizability of the model f . In practice, three common procedures are used for quantitative evaluation:</p><p>? Linear probing (or linear classification): refers to fixing the parameters of the learned model f (frozen encoder), and training a linear classifier g on top of the generated representations. This approach measures how linearly separable the embeddings produced by the pre-trained model f are. ? K nearest neighbors (KNN): refers to applying weighted voting of the K nearest neighbors of the input test image x in the feature space. This approach is nonparametric and can be used for classification tasks. ? Fine-tuning: refers to training a model on the downstream task by using the parameters of the pre-trained model f as an initialization. It is the most general procedure since it is not only limited to classification. In addition to quantitative evaluation, qualitative visualization methods can also be used to derive insights on the features learnt during the self-supervised training of the encoder f . Among these, two popular techniques are kernel visualization <ref type="bibr" target="#b64">[49,</ref><ref type="bibr" target="#b65">50,</ref><ref type="bibr" target="#b66">51]</ref> for CNN and feature map visualization <ref type="bibr" target="#b64">[49,</ref><ref type="bibr" target="#b67">52,</ref><ref type="bibr" target="#b68">53]</ref> for both CNN and ViT. Kernel visualization plots kernels of the first convolutional layer of f and compares it with kernels from corresponding fully supervised training. The similarity of kernels learned by supervised and self-supervised training sheds light on the efficacy of self-supervision. Along the lines, feature map visualization displays hidden layer feature maps of f in order to analyze the spatial attention of  f on the input x. Once again, the feature maps are compared with their counterparts of supervised training for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. GENERATIVE, PREDICTIVE, AND CONTRASTIVE: A TAXONOMY OF SELF-SUPERVISED LEARNING</head><p>In this section, we review works in self-supervised learning following an extended taxonomy: generative, contrastive, and predictive methods, as displayed in <ref type="figure" target="#fig_3">Fig. 3</ref>. Generative methods learn to reconstruct or generate input data, predictive methods learn to predict self-generated labels, and contrastive methods learn to maximize the similarity between semantically identical inputs. While some existing categorizations solely classify self-supervised methods into generative and contrastive <ref type="bibr" target="#b25">[10,</ref><ref type="bibr" target="#b26">11]</ref>, we add the predictive category conceptually based on the processing level of supervision (i.e., the self-supervision for generative methods is raw input, while for predictive methods it is carefully designed labels with high-level semantic information that come after processing of the input). Adopting such categorization also provides a historical perspective to the development of self-supervised learning. In the following sub-sections, we introduce in detail the three types of selfsupervised methodologies in computer vision and link them to works in remote sensing for a side-by-side comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generative Methods</head><p>Generative self-supervised methods learn representations by reconstructing or generating input data. A prominent set of methods in this category are autoencoders (AE), which train an encoder E to map input x to a latent vector z = E(x), and a decoder D to reconstruct x = D(z) from z. While E serves the purpose of f (the feature extractor), the joint function D?E contributes to a self-supervised loss: ||x ? D(E(x))||. Another class of methods, generative adversarial networks (GAN), approach the data generation problem from a game theory perspective. In a nutshell, a GAN consists of two models: a generator G and a discriminator D. The generator takes as input a random vector z and outputs a synthetic sample x = G(z). At the same time, the discriminator is trained to distinguish between real samples {x} from the training dataset and synthetic samples generated by G. By doing so, the distribution P z of synthetic data gradually converges to the distribution P z of the training dataset, thereby leading to realistic data generation. The vanilla GAN was not designed for feature extraction, but since formally G ?1 = f , there exist GAN-inspired methods to construct representations z.</p><p>1) Autoencoder (AE): The concept of autoencoders was introduced as early as in <ref type="bibr" target="#b69">[54]</ref> to serve as pre-training of artificial neural networks. Conceptually, an autoencoder D?E is a feedforward Encoder-Decoder network trained to reproduce its input at the output layer. However, one can easily imagine an autoencoder to fail: should E = D = 1, the autoencoder will learn a trivial identity mapping, D ? E = 1. Thus demanding the latent vector z to retain information on x, by itself, is not sufficient to yield expressive representations. Constraints are needed to prevent such scenarios. For example, in early practices, the encoder E typically resembles a bottlenecklike structure such that the dimension of the input data gets compressed down, i.e. dim x &gt; dim z. This links autoencoders to dimensionality reduction. In fact, a loose relation between linear autoencoders and Principal Component Analysis (PCA) <ref type="bibr" target="#b105">[89]</ref> has previously been studied in the literature <ref type="bibr" target="#b106">[90]</ref>.</p><p>Constraining a small dimension of z is not a must to prevent identity mapping. It is also an option to construct D ? E such that the dimension of z is greater than the input's x dimension with additional sparsity constraints, building so-called sparse autoencoder <ref type="bibr" target="#b70">[55]</ref>. Other popular approaches include: denoising autoencoder <ref type="bibr" target="#b71">[56]</ref> and variational autoencoder (VAE) <ref type="bibr" target="#b72">[57]</ref>.</p><p>Denoising autoencoder is trained to enforce robustness against noise in data. Its reconstruction loss gets modified to ||x ? D(E(x + ))||, with a noise modelling term.</p><p>VAE decouples the encoder and the decoder by encoding a latent distribution. For each input sample x i , instead of a single deterministic latent representation z i , the encoder E generates two latent vectors ? i and ? i representing the mean and variance of a latent Gaussian distribution. The decoder then randomly samples a representation vector z i sampled from the Gaussian distribution N (</p><formula xml:id="formula_0">? i , ? i ) ? exp ?(z i ? ? i ) 2 /? 2 i</formula><p>to reconstruct x i , (see <ref type="figure" target="#fig_4">Fig. 4</ref>).</p><p>The required mathematical machinery to properly define the VAE training loss builds on variational inference <ref type="bibr" target="#b107">[91]</ref>. However, in practice, it amounts to adding a loss term that forces the latent distribution to a unit Gaussian, i.e., N (? i , ? i ) ? N (0, 1) for all training samples x i . This can be realized by minimizing the KL divergence between the two distributions:</p><formula xml:id="formula_1">KL(N (?, ? 2 ) N (0, 1)) = 1 2 (? log ? 2 + ? 2 + ? 2 ? 1) (1)</formula><p>Together with the reconstruction term, the total loss of the VAE reads:  <ref type="bibr" target="#b72">[57]</ref>. Instead of encoding the input X to a fixed latent vector, VAE maps input (x 1 , x 2 , . . . ) to a multi-dimensional Gaussian distribution with non-zero mean ? = (? 1 , ? 2 , . . . ) and diagonal covariance matrix ? = diag(? 2 1 , ? 2 2 , . . . ). Reconstruction works through decoding sampled latent vectors (z 1 , z 2 , . . . ) from this distribution.</p><formula xml:id="formula_2">L VAE = |x ? D(E(x))| + ?(? log ? 2 + ? 2 + ? 2 ? 1) (2)</formula><p>where ? is a weighting parameter.</p><p>Most recently, masked autoencoder (MAE) <ref type="bibr" target="#b73">[58]</ref> raised great attention in the computer vision community with a breakthrough in autoencoding self-supervised pre-training of vision transformers. Inspired by denoising autoencoder, MAE masks out random patches of the input image, sends visible patches to the encoder, and reconstructs the missing patches from the latent representation and masked tokens. This work proves the potential of transformer-based autoencoders for self-supervised visual representation learning.</p><p>Remote sensing. Autoencoders have been widely used to learn representation from various remote sensing data like multispectral images <ref type="bibr" target="#b108">[92,</ref><ref type="bibr" target="#b109">93,</ref><ref type="bibr" target="#b110">94,</ref><ref type="bibr" target="#b111">95,</ref><ref type="bibr" target="#b112">96,</ref><ref type="bibr" target="#b113">97,</ref><ref type="bibr" target="#b114">98,</ref><ref type="bibr" target="#b115">99]</ref>, hyperspectral images <ref type="bibr" target="#b116">[100,</ref><ref type="bibr" target="#b117">101,</ref><ref type="bibr" target="#b118">102,</ref><ref type="bibr" target="#b119">103,</ref><ref type="bibr" target="#b120">104,</ref><ref type="bibr" target="#b121">105,</ref><ref type="bibr" target="#b122">106,</ref><ref type="bibr" target="#b123">107]</ref> and SAR images <ref type="bibr" target="#b124">[108,</ref><ref type="bibr" target="#b125">109,</ref><ref type="bibr" target="#b126">110,</ref><ref type="bibr" target="#b127">111]</ref>. Lu et al. <ref type="bibr" target="#b108">[92]</ref> proposed a combination of a shallowly weighted de-convolution network with a spatial pyramid model in order to learn multi-layer feature maps and filters for input images. In a subsequent step, these get classified by a support vector machine (SVM) for scene classification. Zhang et al. <ref type="bibr" target="#b109">[93]</ref> utilized a stacked denoising autoencoder to learn image features for change detection of imagery with various spatial resolutions. In hyperspectral image analysis, autoencoders are either exploited for pretraining <ref type="bibr" target="#b117">[101]</ref>, or become ingredients in common downstream tasks, such as hyperspectral image classification <ref type="bibr" target="#b116">[100]</ref>, hyperspectral image denoising <ref type="bibr" target="#b128">[112]</ref>, hyperspectral image restoration <ref type="bibr" target="#b118">[102,</ref><ref type="bibr" target="#b129">113]</ref>, and hyperspectral image unmixing <ref type="bibr" target="#b119">[103,</ref><ref type="bibr" target="#b115">99,</ref><ref type="bibr" target="#b123">107]</ref>.</p><p>2) Generative Adversarial Networks (GAN): Generative adversarial networks <ref type="bibr" target="#b74">[59,</ref><ref type="bibr" target="#b130">114]</ref> proposed the adversarial training framework-a strategy ruled by minimax optimization <ref type="bibr" target="#b131">[115]</ref>. GAN training may be viewed as a two players game: the generator G generates fake samples x = G(z) from random latent vector z, and the discriminator D aims to distinguish x from real data samples x. When the discriminator's output is interpreted as probability distribution, i.e. D(x) ? [0, 1], a widely adopted, unified form of the GAN-loss reads</p><formula xml:id="formula_3">max D min G L GAN = log D(x) + log[1 ? D(G(z))]<label>(3)</label></formula><p>where x represents a real sample and G(z) represents a fake sample. During training, G is optimized to fool the discriminator by maximizing D(G(z)) for any z, while D is tuned to minimize D(x) for fake data x and maximize D(x) for real data x. Accordingly, the parameters of G and D get simultaneously tuned to minimize and maximize L GAN , respectively.</p><p>Contrarily to autoencoders where z = E(x), a GAN's latent representation z is implicitly modeled through the inverse of the generator, G ?1 . To approximate G ?1 and obtain z for a given sample x, adversarial autoencoders (AAE) <ref type="bibr" target="#b75">[60]</ref> can be used. For real samples x, G gets fed by z = E(x) of a (trained) autoencoder D ? E. The discriminator contributes to the loss by distinguishing fakes x = G(z) from real samplesx = G(E(x)). Once G, D, and D?E are simultaneously optimized, E serves to encode data points x into learnt representations z.</p><p>To make GAN's training scheme more symmetric, Bi-GAN <ref type="bibr" target="#b76">[61,</ref><ref type="bibr" target="#b132">116]</ref> and ALI <ref type="bibr" target="#b133">[117]</ref> have been proposed. As depicted by <ref type="figure" target="#fig_5">Fig. 5</ref>, in addition to generating fake data x = G(z) through G, an encoder E generates latent space representations z = E(x). A discriminator D = D(x, z) is trained to distinguish tuples of fakes (x, z) from real ones, (x, z).</p><p>Remote sensing. While GAN-based methods in remote sensing are few for self-supervised pre-training, several works tend to integrate such methods to target applications. Zhu et al. <ref type="bibr" target="#b134">[118]</ref> presented a first work to explore the potential of GAN for hyperspectral image classification. Jin et al. <ref type="bibr" target="#b135">[119]</ref> proposed to use adversarial autoencoder for unsupervised hyperspectral unmixing. Hughes et al. <ref type="bibr" target="#b124">[108]</ref> proposed a GANbased framework to generate similar, but novel samples from a given image. Subsequently, these are defined as hard-negative samples to match imagery derived from radar and optical sensors. Also, replacing the generator branch of the GAN with a variational auto-encoder improves the quality of negative samples. In another study, Alvarez et al. <ref type="bibr" target="#b136">[120]</ref> employed the discriminator D of a GAN for binary change detection, while the generator G is optimized to induce the distribution of unaltered samples. Walter et al. <ref type="bibr" target="#b137">[121]</ref> experimented with and evaluate the performance of the BiGAN approach for remote sensing image retrieval based on the similarity of image features. Cheng et al. <ref type="bibr" target="#b138">[122]</ref> proposed perturbation-seeking GAN in a defense framework for remote sensing image scene classification. Ozkan et al. <ref type="bibr" target="#b139">[123]</ref> proposed to use Wasserstein GAN loss to optimize the multinomial mixture model for hyperspectral unmixing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Predictive Methods</head><p>While most generative methods perform pixel-level reconstruction or generation, predictive self-supervised methods are based on auto-generated labels. In fact, one may argue that being able to generate very high dimensional data points, such as images, is not necessary to learn useful representations for many downstream tasks. Instead, one can focus on predicting specific properties of the data, which is the general idea behind predictive methods. To this end, the so-called pretext tasks 2 get utilized. A predictive method firstly designs a suitable pretext task for the dataset, prepares self-generated labels, and trains a model to predict such labels and learn data representations.</p><p>Predictive self-supervised learning targets two possible drawbacks associated with generative methods that perform pixel-level reconstruction: (1) pixel-level loss functions may overly focus on low-level details whereas in practice such details are irrelevant for a human to recognize the contents of an image, (2) pixel-based reconstruction typically do not involve pixel-to-pixel (long-range) correlations that can be important for image understanding. Based on the assumption that providing the network with relevant high-level pretext tasks, the network may learn high-level semantic information.</p><p>In general, the design of different pretext tasks harnesses various context information of the input data. According to distinct context attributes, we categorize pretext tasks as follows: spatial context, spectral context, temporal context, and other semantic contexts.</p><p>1) Spatial Context: Images contain rich spatial information for designing self-supervised pretext tasks. Doersch et al. <ref type="bibr" target="#b77">[62]</ref> proposed the first example of such methods, predicting the relative position of pairs of randomly cropped image patches drawn from a given input sample. It is assumed that doing well on this task requires a global understanding of the scene and its objects contained. Accordingly, a valuable visual representation is expected to extract the composition of objects in order to reason about their relative spatial location.</p><p>Following this paradigm, the literature reveals a multitude of methods to learn image features solving an increasingly complex set of spatial puzzles <ref type="bibr" target="#b78">[63,</ref><ref type="bibr" target="#b140">124,</ref><ref type="bibr" target="#b65">50,</ref><ref type="bibr" target="#b141">125,</ref><ref type="bibr" target="#b142">126]</ref>. For example, Noroozi et al. <ref type="bibr" target="#b78">[63]</ref> built a jigsaw puzzle from randomly ordered tiles of an image. The network was then trained to predict the correct order of tiles. However, given 9 image patches, there are 9! = 362, 880 distinct permutations and a network is unlikely to recognize all of them due to visual ambiguity. To limit the number of permutations, the Hamming distance <ref type="bibr" target="#b143">[127]</ref> was employed to pick a subset of permutations that are significantly diverse. Borrowed from information theory, here the Hamming distance essentially measures the minimum number of tile permutations required in order to recover the image. Further works building on this approach tried to improve the jigsaw puzzles baseline <ref type="bibr" target="#b140">[124]</ref> by introducing tiles from other images <ref type="bibr" target="#b65">[50,</ref><ref type="bibr" target="#b142">126]</ref>, and by extending to larger-size puzzles <ref type="bibr" target="#b141">[125]</ref>. Along this line, one important thing to be taken into account is that if carelessly designed, the model can "cheat" from low-level details like edges.</p><p>Another set of spatial-context-based pretext tasks exploit geometric transformations of input imagery. Among others, predicting rotation angles <ref type="bibr" target="#b64">[49,</ref><ref type="bibr" target="#b144">128]</ref> and image inpainting <ref type="bibr" target="#b80">[64,</ref><ref type="bibr" target="#b145">129]</ref> are popular approaches. In <ref type="bibr" target="#b64">[49]</ref>, the input image was transformed by four separate rotation angles, which then served as the label for the network to predict. <ref type="bibr" target="#b80">[64]</ref> cut a patch from the input for the network to recover. We note that this set of image inpainting strategies have also close relation to generative self-supervised methods-as discussed in <ref type="figure">Fig. II</ref></p><formula xml:id="formula_4">-A.</formula><p>Remote sensing. Zhao et al. <ref type="bibr" target="#b146">[130]</ref> proposed a multi-task framework to simultaneously learn from rotation pretext and scene classification to distill task-specific features adopting a semi-supervised perspective. Zhang et al. <ref type="bibr" target="#b147">[131]</ref> proposed to predict a set of rotation angles from a sequence of rotated SAR-probed targets for object detection. Singh et al. <ref type="bibr" target="#b148">[132]</ref> utilized image inpainting as a pretext task for semantic segmentation arguing for spatial correlation of the pretext task to the downstream task. In <ref type="bibr" target="#b149">[133]</ref> the authors exercised both relative position and inpainting as pretext tasks for remote sensing scene classification. The study added a contrastive self-supervised component referred to as instance discrimination which we discuss in the following section. Ji et al. <ref type="bibr" target="#b150">[134]</ref> tackled the few-shot scene classification problem by incorporating two pretext tasks into training: rotation prediction and contrastive prediction. An additional adversarial model perturbation term is also used for regularization.</p><p>Notably, jigsaw puzzles are rarely leveraged in remote sensing. Potentially, spatial correlation in overhead imagery is less dominant. Indeed, translational invariance is prominent in blocks of urban areas, across water surfaces, and many other kinds of natural scenes (desert, forest, mountain ranges, etc.).</p><p>2) Spectral Context: Spectral information is another basis for the design of pretext tasks which is inspired by image colorization in computer vision. Zhang et al. <ref type="bibr" target="#b81">[65]</ref> proposed one of the first of such tasks: a self-supervised network learns channel-specific representations by predicting a spectral channel taking other channels as input in CIELAB color space <ref type="bibr" target="#b152">[136]</ref>. It turned out that the effectiveness of the color space roots in encoding according to human perception: the distance of two points reflects the amount of visually perceived change of the corresponding colors. The work was further extended to hue/chroma color space, and per-pixel color histograms were applied for better representation learning <ref type="bibr" target="#b153">[137]</ref>. While these two works learn the representation of a single channel (the gray-scale intensity), a cross-channel representation learning method was proposed in <ref type="bibr" target="#b154">[138]</ref>. Two sub-encoders got utilized to extract distinctive color channel representations, which are then concatenated to provide full-channel represen- tations. Larsson et al. <ref type="bibr" target="#b155">[139]</ref> proposed a systematic analysis on image colorization as a pretext task. Remote sensing. Given remote sensing imagery contains spectral bands beyond the standard RGB color space, there may not exist straightforward extensions to the multi/hyperspectral domain based on methods established in computer vision. Indeed, sensing data with increased spectral resolution imprints fine details of the physical properties of the earth's surface. Thus, designing pretext tasks related to spectral bands is a subtle exercise to be treated with scientific care. A recent work established by Vincenzi et al. <ref type="bibr" target="#b151">[135]</ref> provided an initial attempt to leverage spectral context for self-supervised learning ( <ref type="figure" target="#fig_7">Fig. 7</ref>). Close to an autoencoder, it predicts CIELABencoded RGB imagery of a scene from multi-spectral bands for extraction of representations z s . At the same time, a second encoder pre-trained on ImageNet generates representations z c from the RGB imagery. Downstream classification tasks fine-tune corresponding linear layers L s/c such that the final prediction reads [L c (z c )+L s (z s ))]/2. Wu et al. <ref type="bibr" target="#b156">[140]</ref> perform hyperspectral dimensionality reduction using self-supervised learning by training a model to predict low-dimensional representations generated by classic nonlinear manifold embedding methods (e.g., LLE).</p><p>3) Temporal Context: Among others, temporal context is very important, for example, in video understanding. Without the cut, footage clearly carries temporal correlation from frame to frame. With a similarity to both spatial and spectral predictive methods, temporal pretext tasks can be designed in two ways: (1) shuffling timestamps of frames to let a neural network predict the correct sequence, (2) masking one or several frames for the network to predict the missing frame.</p><p>Missing frame prediction in self-supervised learning typically estimates future snapshots from a short clip of video recordings. Provided its superior performance to model temporal dynamics, LSTM <ref type="bibr" target="#b143">[127]</ref> or LSTM variants dominate existing methods to encode temporal correlation in video data <ref type="bibr" target="#b157">[141,</ref><ref type="bibr" target="#b158">142,</ref><ref type="bibr" target="#b159">143]</ref>.</p><p>Frame order prediction in self-supervised learning can be further divided into two distinct approaches: (1) temporal order verification <ref type="bibr" target="#b160">[144,</ref><ref type="bibr" target="#b161">145,</ref><ref type="bibr" target="#b162">146,</ref><ref type="bibr" target="#b163">147]</ref>, and (2) temporal order recognition <ref type="bibr" target="#b164">[148,</ref><ref type="bibr" target="#b165">149,</ref><ref type="bibr" target="#b82">66]</ref>. While the former amounts to binary classification whether or not a sequence of frames is in temporal order, the latter goes beyond by explicitly assigning timestamps to a set of given frames.</p><p>One important thing to note for video understanding is that efficient self-supervised training of footage data may involve various steps of preprocessing. For example, the frames of a video may have different importance for the understanding of the event. Along this line, frame sampling strategies play a big role to boost the performance of temporal pretext tasks. A representative method was proposed by Misraet al. <ref type="bibr" target="#b160">[144]</ref> that sampled video recordings from frames with significant motion as indicated by the magnitude of the optical flow <ref type="bibr" target="#b166">[150]</ref>.</p><p>Remote sensing. Though having a promising future, satellite-based video recording is not yet common in remote sensing. However, temporal stamps of remote sensing data are very important for applications like change detection or crop type classification. Dong et al. <ref type="bibr" target="#b167">[151]</ref> quantified temporal context by coherence in time, proposing a self-supervised representation learning technique for remote sensing change detection. The model gets optimized to identify sample patches in two snapshots of the same geospatial area. The identification network (snapshot one vs. two) is designed to imitate the discriminator D of generative adversarial networks with G generating the self-supervised data representation, as displayed in Section II-A. The method yields improved, robust differentiation for change detection. Yuan et al. <ref type="bibr" target="#b168">[152]</ref> proposed a Transformer-based self-supervised methods for satellite time series classification. By predicting randomly contaminated observations given an entire time series of a pixel, the model is trained to leverage the inherent temporal structure of satellite time series to learn general-purpose spectral-temporal representations. The work was further improved in <ref type="bibr" target="#b170">[153]</ref>, where the network is asked to regress the central pixels of the masked patches for patch-based representation learning. 4) Other Semantic Contexts: Apart from the above three types of contexts, there are also other semantic contexts that can be seen using integrated information of the abovementioned contexts. Noroozi et al. <ref type="bibr" target="#b83">[67]</ref> proposed an artificial supervision signal based on counting visual primitives from an input image and the sum of primitives from cropped tiles. Jenni et al. <ref type="bibr" target="#b84">[68]</ref> proposed a self-supervised learning method based on spotting and predicting artifacts in an adversarial manner. To generate images with artifacts, the authors pre-train a high-capacity autoencoder and then implement a damage-andrepair strategy. A discriminator is finally trained to distinguish artifact images and predict what entries in the feature were dropped when damaging and repairing.</p><p>Multi-sensor or multi-modal self-supervised learning gathers another set of semantic contexts. Owens et al. <ref type="bibr" target="#b85">[69]</ref> proposed to predict a statistical summary of the sound associated with a video frame. By defining explicit sound categories, the authors formulate this visual recognition problem as a classification task. Ren et al. <ref type="bibr" target="#b171">[154]</ref> trained an encoder-decoder network to predict depth map, surface normal map, and instance contour map from a synthetic image, and a discriminator network is trained to distinguish between features from the synthetic image and the real image.</p><p>With various pretext tasks proven to be useful for selfsupervised representation learning, there's also a trend of gathering different pretext tasks together. A direct example can be seen in <ref type="bibr" target="#b172">[155]</ref>, where the authors investigated the combination of four self-supervised tasks: relative position <ref type="bibr" target="#b77">[62]</ref>, colorization <ref type="bibr" target="#b81">[65]</ref>, exemplar <ref type="bibr" target="#b173">[156]</ref> (creating a pseudo-class from the augmented view of a single image and predicting the class, this is rather a contrastive method which will be discussed in the next section) and motion segmentation <ref type="bibr" target="#b174">[157]</ref> (the network learns to classify which pixels of a single frame will move in subsequent frames). It was shown that combining self-supervised tasks will in general improve the performance and lead to faster training.</p><p>Remote sensing. Due to the difference between remote sensing data and common images studied in the computer vision community, there's also a large potential for designing remote sensing-specific pretext tasks. Hermann et al. <ref type="bibr" target="#b175">[158]</ref> proposed to predict different views (pose, projection, and depth) as well as reconstruction of the input for monocular depth estimation. Gao et al. <ref type="bibr" target="#b176">[159]</ref> proposed to train the model to realize envelope data extrapolation from a frequency band to its adjacent low frequency band. He et al. <ref type="bibr" target="#b177">[160]</ref> proposed to reconstruct the corrupted Seismic data with consecutively missing traces, in which the pseudolabels are automatically created from the uncorrupted parts of the observed data. Ayush et al. <ref type="bibr" target="#b178">[161]</ref> designed a pretext task based on predicting the geo-location of the input image. Li et al. <ref type="bibr" target="#b179">[162]</ref> made use of GlobeLand30 <ref type="bibr" target="#b180">[163]</ref>, a global land cover product that divides the earth into different areas and includes ten different land cover types. By aligning the geo-location of the input image with the land cover map from GlobeLand30, the land cover class of the input image is then used as a pretext task for self-supervised learning. However, it has also to be noted that pretexts like geo-location itself are usually too simple to learn a good representation, thus they are often used as auxiliary tasks for self-supervised learning (as in the two examples above). In general, how to design or integrate a suitable pretext task is still an important research question to be explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Contrastive Methods</head><p>The performance of predictive self-supervised learning depends largely on a good pretext task, which is often very difficult to design and may even lead to pretext-specific representation, decreasing the network's generalizability. To tackle this problem, contrastive methods come into play, giving the network more freedom to learn high-level representations that do not rely on a single pretext task. As the name implies, contrastive methods 3 train a model by contrasting semantically identical inputs (e.g., two augmented views of the same image) and pushing them to be close-by in the representation space. Therefore, by design, contrastive methods usually follow a common Siamese-like architecture design. A side-by-side comparison with generative and predictive methods is provided in <ref type="figure" target="#fig_8">Fig. 8</ref>.</p><p>However, only enforcing similarity between pairs of input can easily lead to a trivial solution. Indeed, a constant mapping would be a valid solution to the problem since every pair of input would have identical representations, and thus a maximum similarity. This phenomenon is often referred to as model collapse and many solutions have been proposed in the SSL literature to mitigate it. In fact, depending on how collapsing is handled, one can form a sub-taxonomy of contrastive selfsupervised learning: negative sampling, clustering, knowledge distillation, and redundancy reduction.</p><p>1) Negative Sampling: A basic strategy to avoid model collapse is to include and utilize dissimilar samples to have both positive and negative pairs. This strategy is the first one that has been used in contrastive representation learning. For any data point x (the anchor), the encoder f is trained such that: where x + is a data point similar to x (positive sample), x ? is a data point dissimilar to x (negative sample), and sim represents a metric that measures the similarity between two pairs of features encoded by f . Positive samples need to be generated in a way that preserves the semantics of the anchor x (e.g., using data augmentation). Negative samples on the other hand come from other data points in the dataset. The intuition is that, in order to output similar representations for visually different, yet semantically similar inputs, while repulsing negative samples in the embedding space, the network has to learn useful high-level representations of the input data. Once pre-trained, the encoder f can be further transferred to extract representative features of downstream datasets.</p><formula xml:id="formula_5">sim f (x), f x + sim f (x), f x ? (4) AR E AR E AR E AR E Anchor Positive ?3 ?2 ?1 +1 E Negative * InfoNCE ? exp exp + ? exp</formula><p>A lot of methods have been developed based on the general objective of Eq. 4, of which the earliest works <ref type="bibr" target="#b86">[70,</ref><ref type="bibr" target="#b181">164,</ref><ref type="bibr" target="#b182">165]</ref> proposed triplet losses using a max-margin approach to separate positive from negative examples:</p><formula xml:id="formula_6">L x, x + , x ? = max 0, x ? x + 2 ? x ? x ? 2 + 1 (5) where x, x + , x ? represent</formula><p>the anchor, the positive sample and the negative sample, respectively. Noroozi et al. <ref type="bibr" target="#b83">[67]</ref> extended this triplet scheme for better transfer learning performance by solving an additional pretext task: counting the visual primitives of tiled patches.</p><p>Inspired by noise-contrastive estimation (NCE) <ref type="bibr" target="#b183">[166]</ref> and word2vec <ref type="bibr" target="#b184">[167,</ref><ref type="bibr" target="#b185">168]</ref>, Oord et al. <ref type="bibr" target="#b87">[71]</ref> started another set of contrastive methods with negative sampling by introducing contrastive predictive coding (CPC) with InfoNCE loss:</p><formula xml:id="formula_7">L N = ?E X log exp f (x) T f (x + ) N j=1 exp (f (x) T f (x j ))<label>(6)</label></formula><p>where N is the number of samples, and the denominator consists of one positive and N ? 1 negative samples (see <ref type="figure" target="#fig_9">Fig.  9</ref>). This is indeed the cross-entropy loss for an N-way softmax classifier that classifies positive and negative samples, with the dot product as the score function in Eq. 4. Take a piece of time series as an example, a consecutive time stamp is a positive sample, while clips randomly sampled from other scenes are negative samples (see <ref type="figure" target="#fig_9">Fig. 9</ref>). This work was further improved specifically for image recognition in <ref type="bibr" target="#b186">[169]</ref>. InfoNCE loss is also connected to mutual information, as minimizing the InfoNCE loss can be seen as maximizing a lower bound of the mutual information between f (x) and f (x + ) <ref type="bibr" target="#b187">[170]</ref>. This bridges the connection to mutualinformation-based contrastive learning which comes out in parallel with CPC. Deep information maximization (DIM) <ref type="bibr" target="#b88">[72]</ref> for Some further empirical evidence <ref type="bibr" target="#b189">[172]</ref> claimed that the success of the models mentioned above (maximizing mutual information) is only loosely connected to mutual information. Instead, an upper bound on mutual information estimator leads to ill-conditioned and lower performance representations. Therefore, more attention should be attributed to the encoder architecture and a negative sampling strategy related to metric learning <ref type="bibr" target="#b190">[173]</ref>. Along this line, instance-instance contrastive learning discards mutual information and directly studies the relationships between different samples' instance-level representations. A prototype work is instance discrimination (InstDisc, see <ref type="figure" target="#fig_0">Fig. 10</ref>) <ref type="bibr" target="#b89">[73]</ref>, which extended the idea of exemplar <ref type="bibr" target="#b173">[156]</ref> with non-parametric NCE loss (both utilize instance-level discrimination). In fact, both mutual information and instance discrimination are useful for representation learning, and further works tend to combine both of them implicitly.</p><p>There are mainly two questions to consider for a contrastive method with negative sampling: <ref type="bibr" target="#b16">(1)</ref> what to compare; (2) how to choose positive/negative pairs. Though different methods vary slightly in the contrasting level (mainly context-level like CPC <ref type="bibr" target="#b87">[71]</ref> and DIM <ref type="bibr" target="#b88">[72]</ref> or instance level like InstDisc <ref type="bibr" target="#b89">[73]</ref>), what to compare is commonly the encoded features f (x) of positive/negative samples. The choice of positive/negative samples then leads to a range of different methods. While CPC <ref type="bibr" target="#b87">[71]</ref> and DIM <ref type="bibr" target="#b88">[72]</ref> defined features from the same input image as positive pairs and those from other images as negative pairs, AMDIM <ref type="bibr" target="#b188">[171]</ref> extended the choice of the positive sample by sampling from an augmented view of the input im-age. Contrastive multiview coding (CMC) <ref type="bibr" target="#b191">[174]</ref> extended the idea to several different views (depth, luminance, luminance, chrominance, surface normal, and semantic labels) of one image and samples another irrelevant image as the negative. This work was further improved by InfoMin <ref type="bibr" target="#b193">[175]</ref> studying how to choose best views. In addition, Misra et al. <ref type="bibr" target="#b90">[74]</ref> proposed pretext-invariant representation learning (PIRL), using pretext tasks in predictive self-supervised learning to transform the input image and let the model learn the invariances.</p><p>Contrastive methods tend to work better with more negative examples, since a presumably larger number of negative examples may cover the underlying distribution more effectively. However, the number of negative samples is usually restricted to the size of the mini-batch as the gradients flow back through the encoders of both the positive and negative samples. Consequently, larger mini-batches are preferable, but this in turn is going to be limited by hardware memory constraints. A possible solution is to maintain a separate dictionary, called memory bank, which stores and updates the embeddings of samples with the most recent ones at regular intervals. The memory bank contains a feature representation m x for each sample x in dataset X. The representation m x is an exponentially moving average of feature representations that were computed in prior epochs. It enables replacing negative samples m x ? by their memory bank representations without increasing the batch size. PIRL <ref type="bibr" target="#b90">[74]</ref> and InstDisc <ref type="bibr" target="#b89">[73]</ref> are two representative methods that use a memory bank to boost performance.</p><p>However, maintaining a memory bank during training can be a complicated task, as (1) it can be computationally expensive to update the representations in the memory bank as the representations get outdated quickly in a few passes; <ref type="bibr" target="#b17">(2)</ref> the representations in the memory bank are always one step behind the current encoding, which might bring unnecessary mismatches. To address these issues, the memory bank got replaced by a separate module called momentum encoder in MoCo <ref type="bibr" target="#b91">[75]</ref>. The momentum encoder generates a dictionary as a queue of encoded keys with the current mini-batch enqueued and the oldest mini-batch dequeued. The dictionary keys are defined on the fly by a set of data samples in the batch during training. MoCo also abandons the traditional end-toend training framework by updating the momentum encoder <ref type="figure" target="#fig_0">Fig. 11</ref>. Conceptual comparison of MoCo <ref type="bibr" target="#b91">[75]</ref> with two previous contrastive mechanisms. Rather than end-to-end back-propagation or sampling from a memory bank, MoCo encodes the new keys (key representation) on the fly by a momentum-updated encoder and maintains a queue of keys (a dictionary) for storing negative samples. The size of the dictionary can be much larger than a typical mini-batch size, and the samples in the dictionary are progressively replaced. ?[2020] IEEE. Data augmentation operators studied in SimCLR <ref type="bibr" target="#b92">[76]</ref>. The selected best group of data augmentations ("random cropping, color jittering, grayscaling, Gaussian blurring and horizontal flipping") are widely referenced in following self-supervised studies.</p><p>based on the query encoder ( <ref type="figure" target="#fig_0">Fig. 11)</ref>:</p><formula xml:id="formula_8">? k ? m? k + (1 ? m)? q<label>(7)</label></formula><p>where m is the momentum coefficient and k, q represent key encoder (momentum encoder, no back-propagation) and query encoder (back-propagation) respectively. As a result, it does not require training two separate models and there is no need to maintain a memory bank. Based on previous works, Chen et al. <ref type="bibr" target="#b92">[76]</ref> proposed a milestone of contrastive learning: A simple framework for contrastive learning of visual representations (SimCLR). SimCLR follows the end-to-end training framework and chooses a batch size as large as 8196 to handle the performance bottleneck of the number of negative samples. The main contribution of Sim-CLR is that it illustrates the importance of a hard positive sampling strategy by introducing data augmentation in 10 forms, and provides a standard scheme of data augmentation for most further works (see <ref type="figure" target="#fig_0">Fig. 12</ref>). SimCLR also provides some other practical techniques for contrastive learning, including an additional learnable nonlinear transformation between the representation and the contrastive loss (the projection head), doubling negative samples and more training steps. Following the strategies of SimCLR, the authors of MoCo proposed an improved version MoCo-v2 <ref type="bibr" target="#b94">[78]</ref> by improving the data augmentation, adding a projection head, and adapting cosine Anchor Positive Negative <ref type="figure" target="#fig_0">Fig. 13</ref>. Tile2Vec <ref type="bibr" target="#b195">[177]</ref>. A triplet loss is optimized to move neighbour patches close and distant patches far away in the feature space. decay on the learning rate. The authors also proposed a transformer version MoCo-v3 <ref type="bibr" target="#b93">[77]</ref> by replacing the ResNet <ref type="bibr" target="#b32">[17]</ref> backbone of the encoder to a vision transformer <ref type="bibr" target="#b33">[18]</ref>. Meanwhile, the authors of SimCLR improved their model to a second version SimCLR-v2 <ref type="bibr" target="#b194">[176]</ref> for better performance on semi-supervised learning using distillation, with larger encoder networks, larger batch size, and deeper projection heads.</p><p>Remote sensing. While contrastive self-supervised learning is relatively new, the wide use of contrastive loss in remote sensing <ref type="bibr" target="#b196">[178,</ref><ref type="bibr" target="#b197">179,</ref><ref type="bibr" target="#b198">180,</ref><ref type="bibr" target="#b199">181,</ref><ref type="bibr" target="#b200">182,</ref><ref type="bibr" target="#b201">183]</ref> can date back to <ref type="bibr" target="#b202">[184]</ref>, where the authors imposed a supervised contrastive regularization term on the CNN features for remote sensing scene classification. The first self-supervised work making use of contrastive learning for remote sensing image representation learning is Tile2Vec proposed by Jean et al. <ref type="bibr" target="#b195">[177]</ref>. Similar to CPC <ref type="bibr" target="#b87">[71]</ref>, this work was also inspired by word2vec <ref type="bibr" target="#b184">[167,</ref><ref type="bibr" target="#b185">168]</ref>. The authors trained a convolutional neural network on triplets of tiles, where each triplet consists of an anchor tile t a , a neighbor tile t n that is close geographically, and a distant tile t d that is farther away. A triplet loss is used to move closer neighbour tiles and move further distant tiles in feature space. Jung et al. <ref type="bibr" target="#b203">[185]</ref> later reformulated the triplet loss to binary classification loss and added no-updated fully connected layers to improve robustness. Leenstra et al. <ref type="bibr" target="#b204">[186]</ref> proposed a combination of triplet loss and binary cross-entropy loss (positive pair as 1 and negative pair as 0) for self-supervised change detection. Inspired by DIM, Li et al. <ref type="bibr" target="#b205">[187]</ref> proposed a deep mutual information subspace clustering (DMISC) network for hyperspectral subspace clustering. Hou et al. <ref type="bibr" target="#b206">[188]</ref> rely on contrastive learning, following a design similar to SimCLR, to pre-train an HSI classification model on a single scene and reduce the need for dense annotations. Contrastive learning is used on the unlabeled pixels with spectral-spatial feature extraction followed by a fine-tuning stage. Similarly, Zhao et al. <ref type="bibr" target="#b207">[189]</ref> also apply SimCLR for HSI classification showing promising results with very limited labels. In the same line of works, Zhu et al. <ref type="bibr" target="#b208">[190]</ref> leverage SimCLR for HSI classification using a multi-scale feature extraction approach. PuHong et al. <ref type="bibr" target="#b209">[191]</ref> utilized MoCo for oil spill detection in hyperspectral images. It is worth mentioning that all these hyperspectral data related works rely on simple spatial and spectral augmentations (e.g., random cropping, Gaussian noise, etc.) for view generation. We believe there is room for improvement in this direction.</p><p>Jung et al. <ref type="bibr" target="#b210">[192]</ref> combined the sampling idea of tile2vec and contrastive architecture of SimCLR, utilizing smoothed representation of three neighbor tiles as a positive sample. Montanaro et al. <ref type="bibr" target="#b211">[193]</ref> proposed to use SimCLR for representation learning of the encoder and perturbation invariant autoencoder for segmentation training of the decoder to perform land cover classification. Scheibenreif et al. <ref type="bibr" target="#b212">[194]</ref> tackled land cover classification and segmentation problems using SimCLR with Swin Transformers and by contrasting optical Sentinel 2 and SAR Sentinel 1 patches. Stevenson et al. <ref type="bibr" target="#b213">[195]</ref> proposed to use SimCLR for representation learning of LiDAR elevation data. Kang et al. <ref type="bibr" target="#b214">[196]</ref> define spatial augmentation criteria to uncover semantic relationships among land cover tiles and use MoCo for contrastive self-supervised learning. Li et al. <ref type="bibr" target="#b215">[197]</ref> added one local matching contrastive loss (between patches within an image) to commonly used global contrastive loss (between different images) to learn rich pixel-level information for semantic segmentation. All above-mentioned methods consider the spatial contexts of remote sensing images and based on that build positive/negative pairs. Apart from these methods, contrastive multi-view coding (CMC) is used in <ref type="bibr" target="#b216">[198,</ref><ref type="bibr" target="#b217">199]</ref> and <ref type="bibr" target="#b218">[200]</ref> for multispectral and hyperspectral representation learning, and seasonal contrast (SeCo) is proposed in <ref type="bibr" target="#b219">[201]</ref> to utilize temporal (season) information to create different views. Heidler et al. <ref type="bibr" target="#b220">[202]</ref> proposed a combination of triplet loss and all possible pairings (called batch triplet loss) for audio-visual multi-modal self-supervised learning, and Ayush et al. <ref type="bibr" target="#b178">[161]</ref> combined contrastive learning and predictive learning, proposing a combination of MoCo-v2 <ref type="bibr" target="#b94">[78]</ref> and geo-location as a pretext task for better representation learning of geo-related images. Guan et al. <ref type="bibr" target="#b221">[203]</ref> proposed to integrate contrastive NCE loss and masked image modeling for cross-domain hyperspectral image classification.</p><p>2) Clustering: Referring back to one of the earliest unsupervised learning algorithms, the series of clustering-based self-supervised methods learn data representation by using a clustering algorithm to group similar features together in the embedding space. In supervised learning, this pullingnear process is accomplished via label supervision; in selfsupervised learning, however, we do not have such labels. To solve the problem, DeepCluster <ref type="bibr" target="#b66">[51]</ref> proposed to iteratively leverage K-means clustering to yield pseudo labels and ask a discriminator to predict the labels. This is also the first work towards clustering-based self-supervised learning.</p><p>Self-labeling <ref type="bibr" target="#b96">[80]</ref> and local aggregation <ref type="bibr" target="#b95">[79]</ref> further pushed forward the boundary of clustering-based methods. Like Deep-Cluster <ref type="bibr" target="#b66">[51]</ref>, local aggregation and self-labeling also use an iterative training procedure, but the specific process within each iteration differs significantly. In self-labeling, instead of K-means clustering, an optimal transport problem is solved to obtain the pseudo-labels. In local aggregation, unlike the clustering step of DeepCluster where all examples are divided into mutually-exclusive clusters, the method identifies neighbors separately for each example, allowing for more flexible statistical structures. In addition, local aggregation employs Despite the early success of clustering-based selfsupervised learning, the two-stage training paradigm is timeconsuming and poor-performing compared to later instancediscrimination-based methods such as SimCLR <ref type="bibr" target="#b92">[76]</ref> and MoCo <ref type="bibr" target="#b91">[75]</ref>, which have gotten rid of the slow clustering stage and introduced efficient data augmentation strategies to boost the performance. In light of these problems, the authors of DeepCluster <ref type="bibr" target="#b66">[51]</ref> brought the idea of online clustering and multi-view data augmentation into the clustering-based contrastive self-supervised approach, proposing to learn features by swapping assignments between multiple views of the same image (SwAV) <ref type="bibr" target="#b97">[81]</ref>. The intuition is that, given some (clustered) prototypes, different views of the same images should be assigned to the same prototypes. Meanwhile, an online computing strategy was designed to accelerate code (cluster assignment) computing (see <ref type="figure" target="#fig_0">Fig. 14)</ref>. Based on SwAV, SEER <ref type="bibr" target="#b22">[7]</ref> trained a RegNetY <ref type="bibr" target="#b222">[204]</ref> with 1.3B parameters on 1B random images, which for the first time surpassed the best supervised pre-trained model. In addition, some recent works presented other efforts towards bridging contrastive learning and clustering, such as Prototypical Contrastive Learning (PCL) <ref type="bibr" target="#b98">[82]</ref>, Jigsaw clustering <ref type="bibr" target="#b142">[126]</ref> and Contrastive Clustering (CC) <ref type="bibr" target="#b223">[205]</ref>.</p><p>Remote sensing. Saha et al. <ref type="bibr" target="#b224">[206]</ref> proposed to use Deep-Cluster and triplet contrastive learning to encode multi-modal multi-temporal remote sensing images for change detection. The authors further integrate DeepCluster, BYOL and MoCo-v2 in the pixel-level to produce segmentation maps <ref type="bibr" target="#b225">[207]</ref>. Walter et al. <ref type="bibr" target="#b137">[121]</ref> presented a comparison between Deep-Cluster, VAE, clolorization as pretext task and BiGAN for remote sensing image retrieval. Similarly, Cao et al. <ref type="bibr" target="#b226">[208]</ref> presented a comparison between VAE, AAE and PCL for hyperspectral image classification. Hu et al. <ref type="bibr" target="#b227">[209]</ref> proposed spatial-spectral subspace clustering for hyperspectral images based on contrastive clustering <ref type="bibr" target="#b223">[205]</ref>. Liu et al. <ref type="bibr" target="#b228">[210]</ref> proposed dual dynamic graph convolutional network (DDGCN), contributing a novel clustering-based contrastive loss to capture the structures of views and scenes. In addition to naive contrastive learning on views, scene structures are added into the loss term by clustering scene indexes within the minibatch. Proj. Pred.</p><p>Proj.</p><p>projector predictor online target ema <ref type="figure" target="#fig_0">Fig. 15</ref>. Bootstrap Your Own Latent (BYOL) <ref type="bibr" target="#b99">[83]</ref>. A similarity loss is minimized between features from the online (student) and the target (teacher) branch. A stop-gradient (sg) operator is applied on the teacher whose parameters are updated with an exponential moving average (ema) of the student parameters.</p><p>3) Knowledge Distillation: Another set of methods for contrastive self-supervised learning is based on knowledge distillation <ref type="bibr" target="#b229">[211]</ref>. These methods commonly use a teacherstudent network (still Siamese-like structure) and optimize a similarity metric of two augmented views of the same input image. Either asymmetric learning rules or asymmetric architectures are utilized to transfer knowledge between the student network and the teacher network, and thus no negative samples are necessary.</p><p>Grill et al. <ref type="bibr" target="#b99">[83]</ref> proposed a first milestone called BYOL (bootstrap your own latent, see <ref type="figure" target="#fig_0">Fig. 15</ref>) for self-supervised learning based on knowledge distillation. The general architecture is similar to MoCo <ref type="bibr" target="#b91">[75]</ref> but without the usage of negative samples. It was shown that, if a fixed randomly initialized network (which would not collapse because it is not trained) is used to serve as the key encoder, the representation produced by the query encoder would still be improved during training. If then the target encoder is set to be the trained query encoder and iterates this procedure, it will progressively achieve better performance. Therefore, BYOL proposed an architecture with an exponential moving average strategy to update the target encoder just as MoCo does, and used mean square error as the similarity measurement. Additionally, BYOL is also similar to mean teacher <ref type="bibr" target="#b230">[212]</ref> (a semi-supervised method including also a classification loss), which would collapse if removing the classification loss. To prevent collapse, BYOL introduced an additional predictor on top of the online (teacher) network.</p><p>SimSiam <ref type="bibr" target="#b100">[84]</ref> presented a systematic study on the importance of different tricks in avoiding collapse and proposed a simplified version of the previous self-supervised contrastive methods, arguing that the additional predictor of BYOL is helpful but not necessary to prevent model collapse. Instead, the stop gradient operation of the teacher (target) network is the most critical component to make target representation stable. In addition, the authors showed the relationship between SimSiam and other popular contrastive methods: <ref type="bibr" target="#b16">(1)</ref> SimSiam can be thought of as "SimCLR without negatives";</p><p>(2) SimSiam is conceptually analogous to "SwAV without online clustering"; (3) SimSiam can be seen as "BYOL without the momentum encoder".</p><p>DINO <ref type="bibr" target="#b67">[52]</ref> further explored the self-distillation scheme with vision transformer backbones, proposing a new state-of-theart self-supervised learning algorithm. Following a common teacher-student network architecture, the output of the teacher </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature dimension</head><p>Target Cross-corr. <ref type="figure" target="#fig_0">Fig. 16</ref>. Barlow Twins <ref type="bibr" target="#b103">[87]</ref>. The objective function measures the crosscorrelation matrix between the embeddings of two identical networks fed with distorted versions of a batch of samples and tries to make this matrix close to the identity. This causes the embedding vectors of distorted versions of a sample to be similar while minimizing the redundancy between the components of these vectors.</p><p>network is centered with a mean computed over the batch. Each network outputs a K dimensional feature that is normalized with a temperature softmax over the feature dimension and their similarity is then measured with a cross-entropy loss. Stop gradient and momentum update are used to improve the performance. A following work EsViT <ref type="bibr" target="#b101">[85]</ref> improved DINO by introducing additional region-level contrastive task apart from commonly used global view-level task. Recently, Zhou et al. <ref type="bibr" target="#b102">[86]</ref> integrated masked image modeling into the contrastive self-distillation scheme with vision transforms, proposing image BERT pre-training with Online Tokenizer (iBOT).</p><p>Remote sensing. Guo et al. <ref type="bibr" target="#b231">[213]</ref> proposed to combine GAN and BYOL for better discriminative representation learning. Following a normal GAN structure, a generator is used to generate fake images and a discriminator is used to distinguish them from real images. An additional similarity loss is combined with the discriminator loss by seeing the discriminator as a self-supervised encoder, which encodes both fake and real images as two input views to a BYOL-like Siamese network. In addition, gated self-attention and pyramidal convolution are proposed to be used in both the generator and the discriminator. Chen et al. <ref type="bibr" target="#b232">[214,</ref><ref type="bibr" target="#b233">215]</ref> utilized Siamese ResUNet to distill related representations from different modalities (SAR and optical), and a shift transformation to learn pixel-wise feature representations. Hu et al. <ref type="bibr" target="#b234">[216]</ref> utilized transformer as encoder backbone and BYOL as baseline structure for hyperspectral image classification. Dong et al. <ref type="bibr" target="#b235">[217]</ref> employed SimSiam for ViT based PolSAR image classification. Wang et al. <ref type="bibr" target="#b236">[218]</ref> leverage BYOL for SAR ship detection. Jain et al. <ref type="bibr" target="#b237">[219]</ref> perform pre-training by contrasting SAR and optical images using BYOL. Wang et al. <ref type="bibr" target="#b238">[220]</ref> proposed DINO-MM, a joint SAR-optical representation learning approach with vision transformers. Following the main self-supervised mechanism as DINO, the authors introduced RandomSensorDrop to let the model see all possible combinations of both modalities during training. Zhang et al. <ref type="bibr" target="#b239">[221]</ref> introduced a self-supervised spectral-spatial attention-based vision transformer (SSVT), where global and local augmented views are contrasted based on self-distillation <ref type="bibr" target="#b240">[222]</ref>. Muhtar et al. <ref type="bibr" target="#b241">[223]</ref> proposed In-dexNet, a dense self-supervised method for remote sensing image segmentation. Their approach is built on BYOL and performs contrastive at image and pixel level to preserve spatial information. In a similar fashion, Chen et al. <ref type="bibr">[</ref>  <ref type="bibr" target="#b17">[2]</ref>. All are reported as unsupervised pre-training on the ImageNet-1M training set, followed by supervised linear classification trained on frozen features, evaluated on the validation set. The parameter counts are those of the feature extractors which are commonly ResNet <ref type="bibr" target="#b32">[17]</ref> or ViT <ref type="bibr" target="#b33">[18]</ref>.</p><p>propose a pixel-level self supervised learning approach for change detection. Their method is based on SimSiam with the objective of enforcing point-level consistency across views. They also propose a background-swap augmentation to focus on the foreground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Redundancy Reduction:</head><p>The idea of redundancy reduction for self-supervised learning comes from neuroscience. Indeed, H. Barlow <ref type="bibr" target="#b243">[225]</ref> states that the goal of sensory processing is to record highly redundant sensory inputs into a factorial code (a code with statistically independent com-ponents). Inspired by this principle, Zbontar et al. proposed Barlow Twins <ref type="bibr" target="#b103">[87]</ref>, a method that uses redundancy reduction as a way to avoid trivial solution in contrastive learning without explicit (e.g. MoCo <ref type="bibr" target="#b91">[75]</ref> and SimCLR <ref type="bibr" target="#b92">[76]</ref>) or implicit (e.g. DeepCluster <ref type="bibr" target="#b66">[51]</ref> and SwAV <ref type="bibr" target="#b97">[81]</ref>) negative samples. With the main network architecture similar to previous contrastive learning, Barlow Twins' objective function measures the crosscorrelation matrix between the embeddings of two identical networks fed with distorted versions of a batch of samples, and tries to make this matrix close to the identity <ref type="figure" target="#fig_0">(Fig. 16</ref>). This causes the embedding vectors of distorted versions of a sample to be similar while minimizing the redundancy between the components of these vectors. The loss function of Barlow Twins is:</p><formula xml:id="formula_9">L BT i (1 ? C ii ) 2 invariance term +? i j =i C 2 ij</formula><p>redundancy reduction term <ref type="bibr" target="#b23">(8)</ref> where ? is a trade-off parameter and C is the cross-correlation matrix computed between the outputs of the two identical networks along the batch dimension:</p><formula xml:id="formula_10">C ij b z A b,i z B b,j b z A b,i 2 b z B b,j 2<label>(9)</label></formula><p>where b indexes batch samples, i, j index the vector dimension of the networks' outputs, and A, B represent two different views.</p><p>Bardes et al. <ref type="bibr" target="#b104">[88]</ref> borrowed the decorrelation mechanism from Barlow Twins, proposing a new redundancy-reductionbased self-supervised method VICReg (variance-invariancecovariance regularization). VICReg follows common contrastive learning architecture, and avoids the trivial solution by introducing variance along the batch dimension (a hinge loss which constrains the standard deviation), invariance cross different views (mean-squared euclidean distance), and covariance along feature dimension (penalizing the off-diagonal coefficients of the covariance matrix of the embeddings).</p><p>Remote sensing. Since redundancy-reduction-based algorithms (Barlow Twins <ref type="bibr" target="#b103">[87]</ref> and VICReg <ref type="bibr" target="#b104">[88]</ref>) are relatively new, there are only works using these methods in the remote sensing community. Marsocci et al. <ref type="bibr" target="#b244">[226]</ref> proposed to use Barlow-Twins in a continual learning setting to pre-train a model on a set of heterogeneous datasets while avoiding catastrophic forgetting. Their approach is evaluated on segmentation problems. Ait Ali Braham et al. <ref type="bibr" target="#b245">[227]</ref> used Barlow-Twins for few-shot HSI classification in a two-stages approach. They propose two pair sampling strategies for pixel and patch level classification using spatial information from the scene to generate positive pairs, along with a set of data augmentations. Their results support the effectiveness of redundancy reduction methods on hyperspectral images. In the next section, we further discuss the conceptual idea of redundancy reduction and how it could be beneficial for remote sensing image understanding. We believe there is a large potential to be explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FROM IMAGENET TO REMOTE SENSING IMAGERY:</head><p>SELF-SUPERVISED LEARNING ON GEOSPATIAL EARTH OBSERVATION DATA Self-supervised learning is a data-driven methodology that learns the representation of a dataset without human annotation. However, most existing self-supervised methods in the computer vision community deal with natural RGB images (e.g., ImageNet <ref type="bibr" target="#b17">[2]</ref>) which are different from remote sensing imagery, and there are various types of data from various types <ref type="figure" target="#fig_0">Fig. 19</ref>. Geography-aware self-supervised learning <ref type="bibr" target="#b178">[161]</ref>. Spatially aligned images over time are used to construct temporal positive pairs in contrastive learning, and geo-location is used as an additional pre-text task to boost representation learning for remote sensing images. ?[2021] IEEE. of sensors that require specific considerations. In addition, though intended to learn common representations for various downstream tasks, there's an unignorable gap between pixellevel, patch-level and image-level tasks that may benefit from different designs of self-supervision. In this section, we dig into the implementation of self-supervised methods on different types of remote sensing data and image-/patch-/pixel-level applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Characterisics and Challenges of Remote Sensing Data</head><p>Remote-sensing data contains multiple modalities, e.g., from optical (multi-and hyperspectral), Lidar, and synthetic aperture radar (SAR) sensors, where the imaging geometries and contents are completely different <ref type="bibr" target="#b23">[8]</ref>. Yet before we discuss those different modalities, there exist some characteristics that all data types share as a common remote sensing specific property compared to natural images.</p><p>First, remote sensing data are geolocated, which means each image pixel corresponds to a geospatial coordinate. This information provides additional chances for the design of selfsupervised pretext tasks (e.g., predicting the geo-location of an input image <ref type="bibr" target="#b178">[161]</ref>, see <ref type="figure" target="#fig_0">Fig. 19</ref>), as well as indirect cooperation with geographical resources (e.g., land cover database <ref type="bibr" target="#b179">[162]</ref>).</p><p>Second, remote sensing data are geodetic measurements and usually correspond to specific physical meanings. If not well analyzed, this characteristic can raise issues with data augmentation. Specifically, data augmentation is very important especially for contrastive methods to help the network learn semantic invariance, yet a careless design of data augmentation might change certain physical properties which are indeed important to recognize the objects in a remote sensing image.</p><p>Third, remote sensing images usually contain many more objects than natural images (e.g., compared to an image from ImageNet which generally contains one main object, a remote sensing image usually includes multiple repeated objects). This difference influences both pretext tasks and data augmentation. For example, jigsaw puzzles can be expected to be difficult for representation learning on low-resolution satellite images, and rotation can affect the shadows of buildings which are important for the model to learn relative heights. On the other </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modality</head><p>Application Method</p><p>Multi-spectral hand, when image-level representation is important for natural images, patch-level and pixel-level representation can be equally important for remote sensing images (e.g., considering the spatial relationship of neighboring and distant patches to sample positive/negative pairs <ref type="bibr" target="#b195">[177,</ref><ref type="bibr" target="#b214">196]</ref> as shown in <ref type="figure" target="#fig_0">Fig. 13  and 20</ref>). Last but not least, the time variable is becoming increasingly important despite the modality. This raises the interest of getting inspiration from video self-supervised learning, yet time stamps in remote sensing images are quite different from frames in videos, as well as the special objectives (e.g. change detection in remote sensing). Therefore, how to make the best of temporal information is a common question to be answered in all modalities. An early progress is shown in <ref type="figure" target="#fig_0">Fig. 21</ref> where seasonal information is used for data augmentation <ref type="bibr" target="#b219">[201]</ref>.</p><p>Apart from these important common characteristics, below we discuss different modalities of remote sensing data. A list of recent self-supervised works classified w.r.t modality is shown in <ref type="table" target="#tab_0">Table II.</ref> 1) Multispectral Images: Multispectral images are captured within specific wavelengths ranging across the electromagnetic spectrum, which allows for the extraction of information covering or beyond the perception range of human eyes. Compared to hyperspectral imaging, multispectral imaging measures light in a small number (typically 3 to 15) of spectral bands with relatively low spectral resolution but <ref type="figure">Fig. 20</ref>. Spatially augmented momentum contrast <ref type="bibr" target="#b214">[196]</ref>. Following the underlying assumption of Tile2Vec <ref type="bibr" target="#b195">[177]</ref>, nearby cropped tiles are seen as spatial augmentation for the anchor tile, and a similarity loss is optimized based on momentum contrast <ref type="bibr" target="#b91">[75]</ref>. ?[2020] IEEE.  <ref type="bibr" target="#b219">[201]</ref>. Three embedding subspaces are trained to be invariant to temporal (seasonal), synthetic and both temporal and synthetic transforms T 1 , T 2 and T 0 , respectively. high spatial resolution. Multispectral images are the most commonly used remote sensing data, as they are relatively easy to acquire, and are close to natural images which enable easier human perception and convenient technology transfer from the computer vision community. Due to the popularity and the similarity of multispectral images to natural images, a lot of self-supervised methods have been directly transferred to the remote sensing field (see <ref type="figure" target="#fig_3">Fig. 23</ref>). Generally, all categories of self-supervised methods in CV can be extended to remote sensing multi-spectral image analysis, but there is some special attention to pay: (1) the common characteristics of remote sensing data for all modalities need to be considered as have been discussed above, (2) spectral contexts of predictive methods (e.g., colorization) now have possibly more channel information that can be used to design the pretext tasks, (3) color-related data augmentations (e.g., color distortions shown in <ref type="figure" target="#fig_0">Fig. 12</ref>) which are very important for contrastive selfsupervised learning need to be carefully modified to fit into multiple channels.</p><p>2) Hyperspectral Images: Hyperspectral imaging also captures images across the electromagnetic spectrum, but lights in a far bigger number (up to hundreds) of spectral bands than multispectral imaging with a trade-off on the spatial resolution.</p><p>This very high spectral resolution enables us to identify the materials contained in the pixel via spectroscopic analysis, yet, on the other hand, requires special care about band-wise analysis. In fact, with a basic objective of dimensionality reduction, PCA/ICA/clustering and manifold-learning-based methods have been widely studied in the field before the era of neural networks. Along the lines, deep-learning-based selfsupervised methods have been focusing on autoencoders and 3D convolution to compress spectral representations. Because of the large number of spectral channels, the network usually takes a single-pixel or a small patch as input. In both cases, spatial contexts for pretext tasks or data augmentations tend to consider intra-sample instead of inner-sample information (e.g., neighboring patches or pixels can be seen as positive samples in contrastive negative sampling). Meanwhile, spectral contexts have a large potential to be explored because of the high spectral dimensionality of hyperspectral data. For example, two spectrally divided parts of a hyperspectral image can be two augmented views.</p><p>3) SAR Images: Synthetic-aperture radar (SAR) is a form of radar that is used to create two-dimensional images or threedimensional reconstructions of objects. Due to the imaging principle, SAR images are substantially different from multispectral and hyperspectral images in characteristics like dynamic range, speckle statistics, imaging geometry and complex domain information. While self-supervised learning with SAR images is also a new research direction and has not been widely explored, there's one fact that makes it a valuable topic: the prevailing lack of ground truth for regression-type tasks. Though simulators can be used to provide training data for supervised learning, this bears the risk that the networks will learn models that are far too simplified. Therefore, it would be very beneficial if we can use self-supervised techniques to learn representations from real SAR data. In addition, the general self-supervised schemes can also be easily transferred to SAR images with specific considerations. For example, the data augmentation of color distortion is not suitable for SAR data, the speckle noise of SAR images has a similar effect as Gaussian blurring, and the ascending and descending order may serve as a natural augmentation strategy. <ref type="figure" target="#fig_1">Fig. 22</ref>. Shift invariance <ref type="bibr" target="#b232">[214]</ref>. The two inputs have an offset but keep an overlap. A shift transformation is included in the one branch for aligning representations between two branches which are optimized by a contrastive loss.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Multi-sensor Fusion:</head><p>As a result of the multi-modality of remote sensing data, multi-sensor fusion is always an important task for various objectives, which can benefit from multi-modal self-supervised learning. There are generally two ways of combining multi-modal inputs: (1) similar to various input channels, different modalities can be seen as augmented views of semantically the same input which can be used for designing pretext tasks or specific data augmentation in contrastive self-supervised learning; (2) two encoders are used separately to encode features from two different modalities and then concatenated together for a joint representation learning. Regarding remote sensing data, a common multi-modality is from optical and SAR images, leading to the important topic SAR-optical data fusion, which has been shown by some recent works that benefits from self-supervised representation learning <ref type="bibr" target="#b232">[214]</ref> (see <ref type="figure" target="#fig_1">Fig. 22</ref>) and <ref type="bibr" target="#b238">[220]</ref>. In addition, the fusion of other modalities like audio and social media data is also raising increasing interest <ref type="bibr" target="#b220">[202]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Applications</head><p>Though the general goal of self-supervised learning is to perform task-independent representation learning which can benefit various downstream tasks, it has to be noted that different self-supervision can have different influences on different tasks. Therefore, it is necessary to pre-consider the choice of self-supervised methods when we are targeting a specific application in the end. In fact, so far a large number of self-supervised works in the remote sensing field are based on a specific application. In general, the various applications can be separated into three categories: image-level tasks, pixellevel tasks and patch-level tasks. A list of representative recent self-supervised works classified w.r.t applications is shown in <ref type="table" target="#tab_0">Table II</ref>. We refer the interested reader to the numerous surveys on machine learning and deep learning in remote sensing <ref type="bibr" target="#b23">[8,</ref><ref type="bibr" target="#b257">239,</ref><ref type="bibr" target="#b258">240,</ref><ref type="bibr" target="#b259">241,</ref><ref type="bibr" target="#b260">242,</ref><ref type="bibr" target="#b261">243,</ref><ref type="bibr" target="#b262">244]</ref> for more details about common downstream tasks and available datasets.</p><p>1) Image-level Tasks: Image-level tasks correspond to the applications that expect the recognition of the whole image or image patches, focusing more on global knowledge. The most common image-level task is scene classification for multispectral <ref type="bibr" target="#b108">[92,</ref><ref type="bibr" target="#b146">130,</ref><ref type="bibr" target="#b149">133,</ref><ref type="bibr" target="#b231">213]</ref>, SAR <ref type="bibr" target="#b253">[235]</ref> and other possible modalities. Scene classification is directly related to natural image classification and is usually the default downstream task for the evaluation of a self-supervised method. Like most of the above-mentioned predictive and contrastive selfsupervised methods, an image-level task requires focusing on a global representation that can be done by predicting high-level pretexts or contrasting the encoded features of two augmented image views. Other image-level tasks include time series classification <ref type="bibr" target="#b168">[152]</ref> and image retrieval <ref type="bibr" target="#b137">[121]</ref>.</p><p>2) Pixel-level Tasks: Pixel-level tasks correspond to the applications that expect the recognition of each image pixel, focusing more on the local details. The most common pixellevel task is semantic segmentation <ref type="bibr" target="#b263">[245,</ref><ref type="bibr" target="#b247">229,</ref><ref type="bibr" target="#b148">132,</ref><ref type="bibr" target="#b215">197]</ref>, which requires finer representation in pixel level. Generative self-supervised methods like autoencoder and pretext tasks / data augmentations like image inpainting and pixel based contrastive learning can be helpful. Change detection <ref type="bibr" target="#b109">[93,</ref><ref type="bibr" target="#b136">120,</ref><ref type="bibr" target="#b167">151,</ref><ref type="bibr" target="#b114">98,</ref><ref type="bibr" target="#b248">230,</ref><ref type="bibr" target="#b217">199,</ref><ref type="bibr" target="#b233">215,</ref><ref type="bibr" target="#b204">186,</ref><ref type="bibr" target="#b204">186]</ref> is usually also a pixellevel task which utilizes multitemporal information to detect changing pixels. In hyperspectral image analysis, most of the tasks are based on pixel level, including hyperspectral image classification <ref type="bibr" target="#b19">4</ref>  <ref type="bibr" target="#b122">[106,</ref><ref type="bibr" target="#b116">100,</ref><ref type="bibr" target="#b117">101]</ref>, image denoising <ref type="bibr" target="#b251">[233]</ref>, spectral unmixing <ref type="bibr" target="#b115">[99]</ref>, target detection <ref type="bibr" target="#b250">[232]</ref>, image restoration <ref type="bibr" target="#b118">[102]</ref> and super-resolution <ref type="bibr" target="#b121">[105,</ref><ref type="bibr" target="#b120">104]</ref>. Other pixel-level tasks include depth estimation <ref type="bibr" target="#b175">[158,</ref><ref type="bibr" target="#b111">95]</ref> and SAR despeckling <ref type="bibr" target="#b126">[110,</ref><ref type="bibr" target="#b125">109]</ref>.</p><p>3) Patch-level Tasks: Patch-level tasks mainly correspond to object detection, lying in-between image-level and pixellevel applications. When requiring both the location and the label of the object in the image, patch-level tasks benefit from both global representation and local details. Hence for good performance in the application of object detection, an integrated representation learning pipeline deserves consideration <ref type="bibr" target="#b249">[231]</ref>. In addition, patch-based change detection considers comparisons between tiles of images (change or no change) rather than pixels due to reasons like possible misalignment between time stamps <ref type="bibr" target="#b264">[246]</ref>, or the requirement of fast preprocessing to retrieve region of interest and limited bandwidth for data communication <ref type="bibr" target="#b265">[247]</ref>. We note that patch-level change detection can also be classified into image-level tasks. However, we put them here as there is a stronger correlation between the small patches and the big scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. A PRELIMINARY BENCHMARK</head><p>As a milestone along the road of large-scale unsupervised pre-training in the image domain, the past two years have  <ref type="figure" target="#fig_4">Fig. 24. (a)</ref> A preliminary benchmark of four popular contrastive self-supervised methods (each representing one category in section II-C) on Sentinel-2 images of BigEarthNet <ref type="bibr" target="#b28">[13]</ref>, SEN12MS <ref type="bibr" target="#b29">[14]</ref> and So2Sat-LCZ42 <ref type="bibr" target="#b30">[15]</ref> (culture-10 version) datasets. We use ResNet-18 as encoder backbones, follow the official settings of the models for self-supervised pre-training, and evaluate the performance by training a linear classifier on frozen features. (b) The "collapsing" training curve of SimSiam. The loss goes down quickly in the beginning 10 epochs and becomes very unstable in the following epochs.</p><p>witnessed big advances with contrastive self-supervised learning. However, as most of the recent representative methods are developed and verified on natural images, there lacks a comprehensive study about their performance on remote sensing imagery. Therefore, with the hope to build a reference for the remote sensing community, we provide in this section a preliminary benchmark of recent contrastive self-supervised methods on several popular remote sensing datasets. Codes are available at https://github.com/zhu-xlab/SSL4EO-Review.</p><p>A. Methods, Datasets and Implementation Details 1) Methods and Datasets: We consider four representative contrastive self-supervised methods to provide the benchmark: MoCo-v2 <ref type="bibr" target="#b94">[78]</ref>, SwAV <ref type="bibr" target="#b97">[81]</ref>, SimSiam <ref type="bibr" target="#b100">[84]</ref> and Barlow Twins <ref type="bibr" target="#b103">[87]</ref>, each representing one sub-category of modern contrastive self-supervised learning: negative sampling, clustering, knowledge distillation and redundancy reduction. We use three popular remote sensing datasets for pre-training and evaluation: BigEarthNet <ref type="bibr" target="#b28">[13]</ref>, SEN12MS <ref type="bibr" target="#b29">[14]</ref> and So2Sat-LCZ42 <ref type="bibr" target="#b30">[15]</ref>. BigEarthNet is a multi-label scene classification dataset, containing 590,326 non-overlapping Sentinel-2 image patches of size 128*128 covering 10 European countries. SEN12MS is a global dataset with 180,662 256*256 triplets of Sentinel-1, Sentinel-2 and MODIS land cover maps. So2Sat-LCZ42 is a dataset built for local climate zone classification, containing 400,673 32*32 Sentinel-1/2 pairs covering 42 cities in the world. They are all large-scale datasets with different geographical coverage and built for different tasks, which we believe provide solid diversity for evaluation of the generalizability of self-supervised methods. In addition, we use EuroSAT <ref type="bibr" target="#b266">[248]</ref> for transfer learning evaluation, which is a small single-label land cover classification dataset containing 27,000 64*64 Sentinel-2 patches.</p><p>2) Implementation Details: We pre-train ResNet-18 encoders for 100 epochs using each of the four methods on each of the three pre-training datasets. Our experiments are based on Sentinel-2 only. In terms of pre-training, we use 311,667 patches for pre-training on BigEarthNet; for SEN12MS, all images are cropped to 128*128 to avoid the 50% overlap be-tween neighbouring patches of the original dataset; for So2Sat-LCZ42 (culture-10 version), we pre-train on the 352,366 training split. In terms of downstream evaluation, we do linear probing or fine-tuning on the training split and report testing split accuracy.</p><p>We mainly follow the default implementation in the official repositories of the four self-supervised methods. Importantly, the queue size is 65536 and the feature dimension is 128 for MoCo-v2; the number of prototypes is 60 for SwAV; the feature dimension is 2048 for the encoder and 512 for the projector in SimSiam; three projectors with dimension 2048 are used for Barlow Twins. The data augmentations include cropping, grayscaling, Gaussian blurring, horizontal flipping and channel dropping. For simplicity, we do not include color jitterring for the main benchmark (except for SimSiam due to collapse which will be discussed later) as it is designed for natural RGB images. 4 NVIDIA A100 GPUs are used for the experiments, on which the batch size is in total 256. Links to the four methods' official sites and details of all the hyperparameters we use can be found in our code repository. In downstream tasks, we transfer 100-epoch pretrained encoders for MoCo, SwAV and Barlow Twins, and 10epoch pre-trained encoders for SimSiam (the model collpases afterwards). In addition, we report the performance of fully supervised learning and training with randomly initialized encoders for more thorough comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmark Results 1) General Comparsion of SSL Methods:</head><p>The general benchmark results with linear probing evaluation are shown in <ref type="figure" target="#fig_4">Fig. 24 (a)</ref>. A first conclusion is that with pure simple transfer from natural images to satellite images, all the selfsupervised methods provide meaningful representations for all datasets (compared to random initialization). With frozen features that come from label-independent pre-training, we can train much fewer parameters (e.g. a full ResNet-18 has 10M parameters while the last linear layer contains only 500K) with comparable performance towards fully supervised training.</p><p>The second observation is that MoCo-v2 <ref type="bibr" target="#b94">[78]</ref>, which represents contrastive negative sampling, generally outperforms the other methods across datasets. This proves the robustness and transferability of using negative samples for self-supervised learning in remote sensing, which is also consistent with the fact that most of the recent self-supervised works in the remote sensing community prefer MoCo <ref type="bibr" target="#b91">[75]</ref> and MoCo-v2 <ref type="bibr" target="#b94">[78]</ref> as a baseline structure.</p><p>A third observation is that compared to the other methods, the training of SimSiam <ref type="bibr" target="#b100">[84]</ref> is very unstable. As can be seen from the training curves in <ref type="figure" target="#fig_4">Fig. 24 (b)</ref>, the training loss of SimSiam drops very quickly and significantly in the beginning epochs, and starts vibrating in the following epochs. This can be attributed to SimSiam's simplified design (as has been described in section II-C), which makes the similarity matching task easy to solve for the models. When utilizing checkpoints of the later epochs to serve as pre-trained weights, the model fails to beat even random initialization. That is to say, SimSiam gets collapsed after 10 epochs. Thus in real applications, it is better to add a momentum encoder or other tricks for better performance and more stable training.</p><p>2) Data Augmentation: In this subsection, we provide an extended study on the impact of data augmentations which have been proved very important for natural images <ref type="bibr" target="#b92">[76]</ref>. Focusing on BigEarthNet, we do self-supervised pre-training considering a rich set of augmentations: "RandomResized-Crop", "RandomColorJitter" (a simplified version which only changes contrast and brightness), "RandomGrayscale", "Ran-domGaussianBlur", "RandomHorizontalFlip" and "Random-ChannelDrop" (replacing one or several channels by zero). As can be seen in <ref type="figure" target="#fig_5">Fig. 25</ref>, the results show both common and different trends compared to the computer vision community. When dropping only the "RandomResizedCrop" transform, SwAV and Barlow Twins collapse immediately and MoCo-v2 and SimSiam see a significant performance drop. All methods confirm the importance of cropping which is in line with natural images. Surprisingly, almost no visible performance drop is observed when using only cropping as the data augmentation for all four methods. This is a significant difference compared to natural images, where other augmentations (especially color distortion) bear big importance as well. Therefore, apart from cropping which is a key augmentation, further study is re- <ref type="figure" target="#fig_6">Fig. 26</ref>. BigEarthNet linear classification and fine-tuning results using different amounts of labels pre-trained with MoCo-v2. Self-supervision outperforms supervision when reducing the number of labels. quired on the design of other augmentations to work well on remote sensing multispectral images. In addition, to further analyze the impact of cropping, we explore different settings for the minimum cropping size. The results show that, the bigger the minimum size (less aggressive cropping), the worse the performance is. This is inline with the computer vision literature which emphasizes the importance of having stronger augmentations to make the training task hard enough for the network.</p><p>3) Regime of Limited Labels: Though <ref type="figure" target="#fig_4">Fig. 24</ref> proves the capability of self-supervised pre-trained models, the linear probing results are still below the upper bound of fully supervised learning with full labels. Thus, we wonder how these pre-trained models behave when reducing the number of labels and turning on fine-tuning. We pre-train ResNet18 with MoCo-v2 for 100 epochs and evaluate on BigEarthNet. <ref type="figure" target="#fig_6">Fig. 26</ref> shows the linear classification and fine tuning performance when reducing the amount of labeled samples to 50%, 10%, 1% and 0.1%. The general trend of the figure proves the potential of self-supervised pre-training when we do not have enough labels: linear classification with self-supervision outperforms vanilla supervised learning when reducing the amount of available labels to 1%. The advantage is more significant for fine-tuning, which outperforms supervised learning on all scenarios and provide huge improvements on tiny amount of labels. In general, the fewer labels we use, the bigger the advantage of self-supervised pre-training is. 4) Transfer Learning: In addition, we report transfer learning performance by linear classification and fine-tuning on EuroSat with a ResNet18 pre-trained on BigEarthNet. As is shown in <ref type="figure" target="#fig_7">Fig. 27</ref>, the performance plots are similar to <ref type="figure" target="#fig_6">Fig. 26</ref>, proving the pre-trained models transfer well the data representations across datasets. Promisingly, fine tuning on 50% of labels outperforms supervised learning with full labels. In fact, fine tuning on 10% of labels gives already very close performance compared to full-label supervised learning.</p><p>V. CHALLENGES AND FUTURE DIRECTIONS Self-supervised learning has been achieving great success in various vision tasks, yet it is still a relatively new branch of technologies in both computer vision and remote sensing communities. In this section, we summarize the challenges of self-supervised learning in remote sensing and discuss possible future directions.</p><p>Model collapse. Model collapse is one of the main challenges along with the development of self-supervised learning, especially for modern contrastive learning. Though it has been shown that contrastive self-supervised learning achieved great success in the field, the fundamental theory behind those contrastive methods (especially those using no negative samples) is still not well understood. There are already some works like SimSiam <ref type="bibr" target="#b100">[84]</ref> and <ref type="bibr" target="#b267">[249]</ref> that try to investigate the underline theory, yet they are not enough. In fact, as has been mentioned in section IV, SimSiam easily gets collapsed in Earth observation data. Future works need to go deeper into self-supervised representation learning and explore the theoretical foundations behind model collapse in remote sensing data.</p><p>Pretext tasks and data augmentations. Pretext tasks and data augmentations play a similar and very important role in self-supervised learning, as they correspond directly to what invariance the networks need to learn and what information is important for image understanding. There have been some early studies on the evaluation of different pretext tasks and data augmentations and possible improvements <ref type="bibr" target="#b268">[250,</ref><ref type="bibr" target="#b269">251]</ref>, yet they mainly focus on natural images and evaluate on imagelevel classification tasks. As has been proved in section IV, the findings from common computer vision benchmarks may not completely hold for multispectral imagery, not to mention other modalities. In section III, we discussed the characteristics of different remote sensing data, yet still experiments are needed. Therefore, in general more studies on pretext tasks and data augmentations are required to better understand which ones are useful for different types of remote sensing data.</p><p>Pre-training datasets. Most of existing remote sensing datasets are intended for supervised learning. Though they can also be used for self-supervised pre-training by discarding the labels, the amount of data is limited and the data is usually biased towards the tasks of the datasets. Therefore, it needs to be studied the necessity of self-supervised pretraining on large-scale uncurated datasets. There have been some early contributions following this thread: Leenstra et al. <ref type="bibr" target="#b204">[186]</ref> proposed a Sentinel-2 multitemporal cities pairs dataset for change detection; Manas et al. <ref type="bibr" target="#b219">[201]</ref> proposed a large-scale self-supervised Sentinel-2 dataset; Heidler et al. <ref type="bibr" target="#b220">[202]</ref> proposed an audio-visual dataset for multi-modal self-supervised learning. However, there is still much space to explore and contribute (e.g., dataset size, image sampling strategies, and the availability of more modalities).</p><p>Multi-modal/temporal self-supervised learning. As one of the most important characteristics of remote sensing data, multi-modality is a significant aspect to be explored in selfsupervised representation learning <ref type="bibr" target="#b232">[214,</ref><ref type="bibr" target="#b238">220,</ref><ref type="bibr" target="#b167">151]</ref>. On the other hand, multi-temporal image analysis is raising more interest because of the increasing frequency of data acquisition and transferring. Without the need for any human annotation, self-supervised representation learning has a big advantage to performing big data analysis which is usually a challenge for multi-modal and multi-temporal data. However, adding modalities and time stamps will also bring complexity to the model to be trained. Thus how to balance different modalities or time stamps such that the model can learn good representations from both modalities is a challenging task to be explored.</p><p>Computing efficient self-supervised learning. Selfsupervised pre-training usually requires a large amount of computing resources because of big pre-training data, various data augmentations, the requirement of large batch sizes and more training epochs than supervised learning, etc. Few efforts have been made towards reducing computational cost, which is however an important factor for practical usage. Therefore, efficient data compression <ref type="bibr" target="#b270">[252]</ref>, data loading, model design <ref type="bibr" target="#b73">[58]</ref> and hardware acceleration are necessary to be explored.</p><p>Network backbones. Most of existing self-supervised methods utilize ResNet <ref type="bibr" target="#b32">[17]</ref> as their model backbones, while there has been a hot trend towards vision transformers (ViT) <ref type="bibr" target="#b33">[18]</ref>. Since ViT has shown promising results in selfsupervised learning <ref type="bibr" target="#b93">[77,</ref><ref type="bibr" target="#b67">52,</ref><ref type="bibr" target="#b101">85]</ref>, and recent advances are exploring the link between large-scale pretraining for both language and vision <ref type="bibr" target="#b73">[58,</ref><ref type="bibr" target="#b102">86]</ref>, it deserves more exploration in remote sensing images as well. In addition, the specific encoder architecture itself is also important to study, as most existing methods tend to keep it unchanged with a focus on the conceptual design of self-supervision.</p><p>Task-oriented weakly-supervised learning. Last but not least, in parallel to the progress of task-agnostic selfsupervised learning, there's also the need to integrate "few labels" or "weak labels" for better task-dependant weaklysupervised learning in practice. In other words, not only can self-supervised learning provide a pre-trained model for downstream tasks, but there's also the possibility of bringing representation learning online. For example, both data representations and the supervised task can be learned together, where the self-supervised branch helps the model capture more useful information. This scenario is especially important when moving to extreme applications that do not have a common large dataset for pre-training. In addition, this can also be a way to mitigate the generally high computational cost of training SSL algorihtms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This article provides a systematic introduction and literature review of self-supervised learning for the remote sensing community. We summarize three main categories of selfsupervised methods, introduce representative works and link them from natural images to remote sensing data. Moreover, we provide a preliminary benchmark and extened analysis of four modern contrastive self-supervised learning on three popular remote sensing datasets. Finally, fundamental problems and future directions are listed and discussed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The number of recent publications related to self-supervised learning (SSL). While a clear trend of increased efforts to advance SSL is observed, activity in remote sensing lags behind.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 Fig. 2 .</head><label>22</label><figDesc>schematically depicts the general underlying principle of self-supervised learning. Based on a certain self-produced objective (the so-called self-supervision), a large amount of unlabeled data is exploited to train a model f 1 by optimizing this objective without requiring any manual annotation. With a carefully designed self-supervised problem, the model f gets the ability to capture high-level representations of the input 1 The general pipeline of self-supervised learning. The visual representation is learned through self-supervision that comes from the unlabeled data. The learned parameters serve as a pre-trained model and are transferred to supervised downstream tasks for fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>A taxonomy of self-supervised learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Variational AutoEncoder (VAE)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Bidirectional Generative Adversarial Networks (BiGAN)<ref type="bibr" target="#b76">[61]</ref>. BiGAN includes an encoder E which maps data x to latent representations z. The BiGAN discriminator D jointly acts in data and latent space: x versus G(z)), and E(x) versus z, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Examples of different predictive pretext tasks. (a),(b),(c), and (d) are based on spatial contexts; (e) is based on spectral context; (f) illustrates temporal context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>The color out of space<ref type="bibr" target="#b151">[135]</ref>. CIELAB-encoded RGB imagery of a scene is predicted from other spectral bands for the extraction of representations. ?[2021] IEEE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>A comparison of the general structures between generative, predictive, and contrastive self-supervised learning. E and D refer to the encoder and decoder network. While generative and predictive methods calculate the loss in the output space, contrastive methods calculate the loss in the representation space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Contrastive Predictive Coding (CPC)<ref type="bibr" target="#b87">[71]</ref>. Take time series as an example, the consecutive time stamp is a positive sample, while clips sampled from other scenes are negative samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Instance Discrimination (InstDisc)<ref type="bibr" target="#b89">[73]</ref>. A backbone CNN is used to encode each image as a feature vector, which is projected to a 128-dimensional space and L2 normalized. The optimal feature embedding is learned via instance-level discrimination, which tries to maximally scatter the features of training samples over the 128-dimensional unit sphere. ?[2018] IEEE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>example, learns image representations by classifying whether a pair of global features and local features are from the same image. The global features are the final output of a convolutional encoder and local features are the output of an intermediate layer in the encoder. This work was further improved by enhancing the association of positive pairs in augmented multi-scale DIM (AMDIM) [171], where a positive sample is sampled from an augmented view of the input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) Original (b) Crop &amp; resize (c) Crop, resize &amp; flip (d) Color drop (e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Swapping Assignments between Views (SwAV)<ref type="bibr" target="#b97">[81]</ref>. "Codes" are obtained by assigning features to learnable prototype vectors, a "swapped" prediction problem is then solved wherein the codes obtained from one data augmented view are predicted using the other view.an objective function that directly optimizes a local softclustering metric, requiring no extra readout layer and only a small amount of additional computation on top of the feature representation training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 21 .</head><label>21</label><figDesc>Seasonal Contrast (SeCo)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 23 .</head><label>23</label><figDesc>Statistics of recent publications related to self-supervised learning in remote sensing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 25 .</head><label>25</label><figDesc>A study of data augmentation on BigEarthNet. The baseline includes all augmentations, "no crop" means all augmentations except cropping, and "crop only" means only using cropping as the augmentation with minimum cropping size 0.2, 0.5 and 0.8 of the original size, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 27 .</head><label>27</label><figDesc>EuroSAT linear classification and fine-tuning results pre-trained with MoCo-v2 on BigEarthNet. Self-supervision with 50% labels outperforms supervised learning with full labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>This work was jointly supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. [ERC-2016-StG-714087], Acronym: So2Sat and grant agreement No. [957407], Acronym: DAPHNE), by the Helmholtz Association through the Framework of Helmholtz AI (grant number: ZT-I-PF-5-01) -Local Unit "Munich Unit @Aeronautics, Space and Transport (MASTr)", the Helmholtz Klimainitiative (HICAM) and Helmholtz Excellent Professorship "Data Science in Earth Observation -Big Data Fusion for Urban Research" (grant number: W2-W3-100)) and by the German Federal Ministry of Education and Research (BMBF) in the framework of the international future AI lab "AI4EO -Artificial Intelligence for Earth Observation: Reasoning, Uncertainties, Ethics and Beyond" (grant number: 01DD20001). The computing resources were supported by the Helmholtz Association's Initiative and Networking Fund on the HAICORE@FZJ partition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I A</head><label>I</label><figDesc>GALLERY OF REPRESENTATIVE SELF-SUPERVISED METHODS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Fig. 18. Comparison of SSL methods under the linear classification protocol on ImageNet</figDesc><table><row><cell>grad</cell><cell>contrastive</cell><cell>grad</cell><cell>grad</cell><cell>similarity</cell><cell>grad</cell><cell>similarity</cell><cell>grad</cell><cell cols="2">redundancy</cell><cell>grad</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>sinkhorn</cell><cell>predictor</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Encoder</cell><cell>Encoder</cell><cell>Encoder</cell><cell>Encoder</cell><cell>student</cell><cell>teacher</cell><cell cols="2">Encoder</cell><cell cols="2">Encoder</cell></row><row><cell></cell><cell>image</cell><cell></cell><cell></cell><cell>image</cell><cell></cell><cell>image</cell><cell></cell><cell>image</cell><cell></cell></row><row><cell cols="3">(a) Contrasting negative samples</cell><cell cols="2">(b) Clustering</cell><cell cols="2">(c) Distillation</cell><cell cols="4">(d) Redundancy reduction</cell></row><row><cell cols="5">Fig. 17. A comparison of contrastive self-supervised methods.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Method</cell><cell cols="4">Architecture Params (M) Top-1 acc</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">BigBiGAN [116]</cell><cell>R50</cell><cell>24</cell><cell></cell><cell>56.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MAE [58]</cell><cell>ViT-L/16</cell><cell>307</cell><cell></cell><cell>73.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RelativePosition [62]</cell><cell>R50w2X</cell><cell>94</cell><cell></cell><cell>51.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Jigsaw [63]</cell><cell>R50w2X</cell><cell>94</cell><cell></cell><cell>44.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Rotation [49]</cell><cell>Rv50w4X</cell><cell>86</cell><cell></cell><cell>55.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Colorization [65]</cell><cell>R101</cell><cell>28</cell><cell></cell><cell>39.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CPC-v1 [71]</cell><cell>R101</cell><cell>28</cell><cell></cell><cell>48.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CPC-v2 [169]</cell><cell>R161</cell><cell>305</cell><cell></cell><cell>71.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AMDIM [171]</cell><cell>R-custom</cell><cell>194</cell><cell></cell><cell>63.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CMC [174]</cell><cell>R50</cell><cell>47</cell><cell></cell><cell>66.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">InstDisc [73]</cell><cell>R50</cell><cell>24</cell><cell></cell><cell>54.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PIRL [74]</cell><cell>R50</cell><cell>24</cell><cell></cell><cell>63.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MoCo [75]</cell><cell>R50</cell><cell>24</cell><cell></cell><cell>60.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MoCo-2X</cell><cell>R50w2X</cell><cell>94</cell><cell></cell><cell>65.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SimCLR [76]</cell><cell>R50</cell><cell>24</cell><cell></cell><cell>69.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SimCLR-2X</cell><cell>R50w2X</cell><cell>94</cell><cell></cell><cell>74.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MoCo-v2 [78]</cell><cell>R50</cell><cell>24</cell><cell></cell><cell>71.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SimCLR-v2 [176]</cell><cell>R50</cell><cell>24</cell><cell></cell><cell>71.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">MoCo-v3 (ViT-B) [77] ViT-B/16</cell><cell>86</cell><cell></cell><cell>76.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MoCo-v3 (ViT-L)</cell><cell>ViT-L/16</cell><cell>304</cell><cell></cell><cell>81.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DeepCluster [51]</cell><cell>VGG</cell><cell>15</cell><cell></cell><cell>48.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">LocalAgg [79]</cell><cell>R50</cell><cell>24</cell><cell></cell><cell>60.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SwAV [81]</cell><cell>R50</cell><cell>24</cell><cell></cell><cell>75.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SwAV-2X</cell><cell>R50w2X</cell><cell>94</cell><cell></cell><cell>77.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">BarlowTwins [87]</cell><cell>R50</cell><cell>24</cell><cell></cell><cell>73.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">VICReg [88]</cell><cell>R50</cell><cell>24</cell><cell></cell><cell>73.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">BYOL [83]</cell><cell>R50</cell><cell>24</cell><cell></cell><cell>74.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SimSiam [84]</cell><cell>R50</cell><cell>24</cell><cell></cell><cell>71.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DINO (RN) [52]</cell><cell>R50</cell><cell>24</cell><cell></cell><cell>75.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DINO (ViT)</cell><cell>ViT-B/16</cell><cell>85</cell><cell></cell><cell>78.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">EsViT [85]</cell><cell>Swin-B</cell><cell>87</cell><cell></cell><cell>81.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">iBOT [86]</cell><cell>ViT-L/16</cell><cell>307</cell><cell></cell><cell>81.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Supervised (ResNet)</cell><cell>R50</cell><cell>24</cell><cell></cell><cell>76.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Supervised (ViT)</cell><cell>ViT-B/16</cell><cell>85</cell><cell></cell><cell>79.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>224]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II A</head><label>II</label><figDesc>GALLERY OF RECENT SELF-SUPERVISED WORKS IN REMOTE SENSING.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Typically a convolutional neural network (CNN), e.g. ResNet<ref type="bibr" target="#b32">[17]</ref>. Recent work also show promising results with vision transformers (ViT)<ref type="bibr" target="#b33">[18]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We note that the term pretext task can also represent any generic objective associated with self-supervised methods. In this paper, we restrict such notion to predictive approaches (where the term originally came from) to make the taxonomy more clear.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In some literature, the term contrastive is only used to denote methods that include negative samples. Yet, we follow the more common terminology in which contrastive methods also include the recent works without negative samples.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that with the goal to classify the center pixel, hyperspectral image classification tends to work on a small patch to benefit from spatial relationships, linking towards image and patch level tasks.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincenzi</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>135]: colorization as pretext task. tile2vec [177]: contrastive learning with triplet loss</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">185]: replace triple loss of tile2vec to binary cross entropy loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Stojnic et al. [198]: contrastive multiview coding. CSF [228]: contrastive multiview coding. Jung et al. [192]: SimCLR with smoothed view</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">MoCo + geo-location as pretext task</title>
		<imprint/>
	</monogr>
	<note>SauMoCo [196]: MoCo with spatial augmentation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">MoCo + seasonal contrast</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">supervision (landcover map) + teacher-student network</title>
		<idno>GeoKR [162]: geo-</idno>
		<imprint/>
	</monogr>
	<note>Scene classification Lu et al. [92]: autoencoder</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">130]: rotation as pretext task + classification loss. Tao et al. [133]: inpainting, relative position and instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>SGSAGANs [213</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semantic segmentation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<title level="m">inpainting/rotation as pretext tasks + contrastive learning</title>
		<imprint/>
	</monogr>
	<note>Singh et al. [132]: GAN + inpainting as pretext task. Li et al. [197]: global and local contrastive learning. Change detection Zhang et al. [93]: denoising Autoencoder</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GAN discriminator for temporal prediction. Cai et al. [230]: clustering for hard sample mining. Leenstra et al. [186]: triplet loss + binary cross entropy loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<idno>S2-cGAN [120]: conditional GAN discriminator + reconstruction loss. Dong et al. [151</idno>
	</analytic>
	<monogr>
		<title level="m">contrastive multiview coding + BYOL. Time series classification Yuan</title>
		<imprint>
		</imprint>
	</monogr>
	<note>et al. [152]: transformer + temporal pretext task. Object detection DUPR [231]: patch re-identification + multi-level contrastive loss Image retrieval Walter et al. [121]: DeepCluster/BiGAN/VAE/Colorization. Depth estimation Madhuanand et al. [95]: autoencoder + multi-task prediction + triplet loss</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mou</surname></persName>
		</author>
		<title level="m">autoencoder + subspace clustering</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
		</imprint>
	</monogr>
	<note>contrastive multi-view coding. Hu et al. [209]: spatial-spectral contrastive clustering. Cao et al. [208]: VAE/AAE/PCL. Hu et al. [216]: BYOL + transformer</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Target detection Yao et al. [232]: clustering + pseudo-label + contrastive N-pair loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egu-Net</forename><surname>Unmixing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">autoencoder + Siamese weight-sharing network</title>
		<imprint/>
	</monogr>
	<note>Denoising SHDN [233]: noise prediction as pretext task. Restoration Imamura et al. [102]: denoising autoencoder. Super-resolution CUCaNet [104]: autoencoder + Siamese cross-attention</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">predict another speckled view. Speckle2Void [109]: blind-spot CNN + physical regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno>SAR Despeckling BDSS [110]: autoencoder,</idno>
		<ptr target="adversarialcontrastivelearning.multi-modalSAR-opticalfusion" />
	</analytic>
	<monogr>
		<title level="m">Siamese contrastive loss + pseudo label</title>
		<imprint>
			<biblScope unit="volume">234</biblScope>
		</imprint>
	</monogr>
	<note>Scene classification MI-SSL [235]: PolSAR modal similarity + instance contrastive. Image retrieval Park et al. [236]: MoCo. Target recognition RR-SSL [131]: rotation as pretext task. UACL [237</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cha</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>238]: contrastive multiview coding. UniFeat&amp;CoRe [193]: SimCLR + inpainting as pretext</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<title level="m">instance discrimination + multiview &amp; shift invariance</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sar-Optical Change Detection Chen</surname></persName>
		</author>
		<title level="m">contrastive multiview coding + BYOL</title>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DeepCluster + bitemporal multi-sensor contrastive. SAR-optical matching Hughes et al. [108]: VAE+GAN, hard negative sample mining. Audio-image fusion Heidler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">instance discrimination + multiview &amp; shift invariance</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
		</imprint>
	</monogr>
	<note>et al. [202]: audio-visual contrastive triplet loss</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>nature 521.7553</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
	<note>cit. on p. 1)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>cit. on pp. 1, 14, 15</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning based on artificial neural network: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Happiness Ugochi Dike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Cyborg and Bionic Systems (CBS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="322" to="327" />
		</imprint>
	</monogr>
	<note>cit. on p. 1)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey on semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jesper E Van Engelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="373" to="440" />
		</imprint>
	</monogr>
	<note>cit. on pp. 1, 2)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A brief introduction to weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National science review</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="44" to="53" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cit. on p. 1)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated Machine Learning</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="35" to="61" />
		</imprint>
	</monogr>
	<note>cit. on p. 1)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Self-supervised pretraining of visual features in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01988</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 1, 12)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing: A comprehensive review and list of resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao Xiang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Magazine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="8" to="36" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cit. on pp. 1, 15, 18)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cit. on p. 1)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A survey on contrastive selfsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Jaiswal</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In: Technologies 9.1 (2021), p. 2 (cit. on pp. 1, 3)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit. on pp. 1, 3)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Review on selfsupervised image recognition using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kriti</forename><surname>Ohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukesh</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">224</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bigearthnet: A large-scale benchmark archive for remote sensing image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gencer</forename><surname>Sumbul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium. IEEE. 2019</title>
		<imprint>
			<biblScope unit="page" from="5901" to="5904" />
		</imprint>
	</monogr>
	<note>cit. on pp. 1, 19)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">SEN12MS-A Curated Dataset of Georeferenced Multi-Spectral Sentinel-1/2 Imagery for Deep Learning and Data Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schmitt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07789</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 1, 19)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">So2Sat LCZ42: A benchmark dataset for global local climate zones classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao Xiang</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12171</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 1, 19)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A comprehensive survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 109.1 (2020)</title>
		<meeting>the IEEE 109.1 (2020)</meeting>
		<imprint>
			<biblScope unit="page" from="43" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>cit. on pp. 2, 11, 14, 21</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 2, 11, 14, 21</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semisupervised selflearning for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inmaculada</forename><surname>D?pido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on geoscience and remote sensing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="4032" to="4044" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semisupervised neural networks for efficient hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2271" to="2282" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-supervised low-rank representation (SSLRR) for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuebin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="5658" to="5672" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Wu-net: A weakly-supervised unmixing network for remotely sensed hyperspectral imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="373" to="376" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to propagate labels on graphs: An iterative multitask regression framework for semi-supervised hyperspectral dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS journal of photogrammetry and remote sensing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="35" to="49" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint and progressive subspace analysis (JPSA) with spatial-spectral manifold alignment for semisupervised hyperspectral dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SDFL-FC: Semisupervised Deep Feature Learning With Feature Consistency for Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3D convolutional siamese network for few-shot hyperspectral classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">48504</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep Self-Supervised Learning for Few-Shot Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2020-2020 IEEE International Geoscience and Remote Sensing Symposium. IEEE. 2020</title>
		<imprint>
			<biblScope unit="page" from="501" to="504" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning With Adaptive Distillation for Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">CoSpace: Common subspace learning from hyperspectral-multispectral correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="4349" to="4359" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learnable manifold alignment (LeMA): A semi-supervised cross-modality learning framework for land cover and land use classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS journal of photogrammetry and remote sensing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="193" to="205" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">X-ModalNet: A semi-supervised deep cross-modal network for classification of remote sensing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page" from="12" to="23" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptation Using a Teacher-Student Network for Cross-City Classification of SENTINEL-2 Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingliang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1569" to="1574" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic segmentation of remote sensing images with sparse annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuansheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Aerial scene understanding in the wild: Multi-scene recognition via prototype-based memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuansheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="89" to="102" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semisupervised change detection using graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipan</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="607" to="611" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">MIMA: MAPPER-induced manifold alignment for semi-supervised fusion of optical image and polarimetric SAR data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingliang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="9025" to="9040" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynne</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wiley interdisciplinary reviews: computational statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="433" to="459" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for aerial scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheriyadat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="439" to="451" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Data clustering: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anil K Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick J</forename><surname>Narasimha Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM computing surveys (CSUR)</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="264" to="323" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised multilayer feature learning for satellite image scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="157" to="161" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised deep feature extraction for remote sensing image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustau</forename><surname>Camps-Valls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1349" to="1362" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multiobjective genetic clustering for pixel classification in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghamitra</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ujjwal</forename><surname>Maulik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1506" to="1511" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Single point iterative weighted fuzzy C-means clustering algorithm for remote sensing image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2527" to="2540" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unsupervised deep joint segmentation of multitemporal high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipan</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="8780" to="8792" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fuzzy clustering algorithms for unsupervised change detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niladri</forename><surname>Shekhar Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susmita</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Sciences</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page" from="699" to="715" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Use of principal component analysis (PCA) of remote sensing images in wetland change detection on the Kafue Flats, Zambia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Munyati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geocarto International</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="11" to="22" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Novel folded-PCA for improved feature extraction and data reduction with hyperspectral imaging and SAR in remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Zabalza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="112" to="122" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A remote sensing image fusion method based on PCA transform and wavelet packet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bicheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Networks and Signal Processing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="976" to="981" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2003. cit. on p. 3</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cit. on pp. 3, 4, 6, 14</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9359" to="9367" />
		</imprint>
	</monogr>
	<note>cit. on pp. 3, 6)</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
	<note>cit. on pp. 3, 4, 12, 14</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Emerging properties in selfsupervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 3, 4, 13, 14, 21</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">XCiT: Cross-Covariance Image Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09681</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Modular learning in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI</title>
		<imprint>
			<biblScope unit="volume">647</biblScope>
			<biblScope unit="page" from="279" to="284" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
	<note>cit. on p. 4)</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Sparse autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<idno>CS294A Lecture notes 72.2011</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
	<note>cit. on p. 4)</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>cit. on p. 4)</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 4, 5</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Masked Autoencoders Are Scalable Vision Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 4, 5, 14, 21</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
	<note>cit. on pp. 4, 5</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 4, 5</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 4, 5</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE interna</title>
		<meeting>the IEEE interna</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
	<note>cit. on pp. 4, 6, 8, 14</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
	<note>cit. on pp. 4, 6, 14</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
	<note>cit. on pp. 4, 6</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
	<note>cit. on pp. 4, 6, 8, 14</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning with spacetime cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8545" to="8552" />
		</imprint>
	</monogr>
	<note>cit. on pp. 4, 7</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5898" to="5906" />
		</imprint>
	</monogr>
	<note>cit. on pp. 4, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Self-supervised feature learning by learning to spot artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2733" to="2742" />
		</imprint>
	</monogr>
	<note>cit. on pp. 4, 8</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="801" to="816" />
		</imprint>
	</monogr>
	<note>cit. on pp. 4, 8</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
	<note>cit. on pp. 4, 9</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 4, 9-11</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 4, 10</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
	<note>cit. on pp. 4, 10, 14</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020</meeting>
		<imprint>
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
	<note>cit. on pp. 4, 10, 14</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020</meeting>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="10" to="14" />
		</imprint>
	</monogr>
	<note>cit. on pp.</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<idno>PMLR. 2020</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
	<note>cit. on pp. 4, 11, 12, 14, 20</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 4, 11, 14, 21</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 4, 11, 12, 14, 19, 20</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
	<note>cit. on pp. 4, 12, 14</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">Markus</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05371</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 4, 12</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 4, 12, 14, 19</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 4, 12</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 4, 13, 14</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021</meeting>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="19" to="21" />
		</imprint>
	</monogr>
	<note>cit. on pp. 4, 13</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Efficient Self-supervised Vision Transformers for Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09785</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 4, 13, 14, 21</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghao</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07832</idno>
		<title level="m">Image BERT Pre-Training with Online Tokenizer. 2021</title>
		<imprint/>
	</monogr>
	<note>cs.CV] (cit. on pp. 4, 13, 14, 21</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03230</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="13" to="15" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp.</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Vicreg: Variance-invariance-covariance regularization for self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bardes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04906</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 4, 14, 15</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svante</forename><surname>Wold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Esbensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Geladi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chemometrics and intelligent laboratory systems</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="52" />
		</imprint>
	</monogr>
	<note>cit. on p. 4)</note>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">From principal subspaces to principal components with linear autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Plaut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10253</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on p. 4)</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cit. on p. 4)</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Remote sensing scene classification by unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="5148" to="5157" />
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 16</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Change detection based on deep feature representation and mapping transformation for multi-spatial-resolution remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puzhao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="24" to="41" />
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 16</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning of Satellite-Derived Vegetation Indices for Clustering and Visualization of Vegetation Types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keitarou</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth estimation from oblique UAV videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logambal</forename><surname>Madhuanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Nex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 16</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Urban Flood Mapping With Bitemporal Multispectral Imagery Via a Self-Supervised Learning Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2001" to="2016" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cit. on p. 5</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Self-Supervised Multi-Image Super-Resolution for Push-Frame Satellite Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Long</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1121" to="1131" />
		</imprint>
	</monogr>
	<note>cit. on p. 5</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Heterogeneous change detection with self-supervised deep canonically correlated autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomenotti</forename><surname>F Figari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2020-2020 IEEE International Geoscience and Remote Sensing Symposium. IEEE. 2020</title>
		<imprint>
			<biblScope unit="page" from="680" to="683" />
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 18</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Blind hyperspectral unmixing using autoencoders: A critical comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burkni</forename><surname>Palsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><forename type="middle">O</forename><surname>Johannes R Sveinsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ulfarsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1340" to="1372" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 18</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Self-taught feature learning for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="2693" to="2705" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 18</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Unsupervised spectral-spatial feature learning via deep residual Conv-Deconv network for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedram</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="391" to="406" />
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 16</note>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Self-supervised Hyperspectral Image Restoration using Separable Image Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuji</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuki</forename><surname>Itasaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Okuda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00651</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 5, 16</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Endmember-Guided Unmixing Network (EGU-Net): A General Deep Learning Framework for Self-Supervised Hyperspectral Unmixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 16</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Cross-attention in coupled unmixing nets for unsupervised hyperspectral superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="208" to="224" />
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 16</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Hyperspectral image super-resolution with selfsupervised spectral-spatial residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">1260</biblScope>
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 18</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Self-Supervised Deep Subspace Clustering for Hyperspectral Images With Adaptive Self-Expressive Coefficient Matrix Initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3215" to="3227" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 16</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Endnet: Sparse autoencoder network for endmember extraction and hyperspectral unmixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Savas</forename><surname>Ozkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berk</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gozde</forename><surname>Bozdagi Akar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="482" to="496" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cit. on p. 5</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Mining hard negative samples for SARoptical image matching using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lloyd Haydn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Remote Sensing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1552</biblScope>
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 16</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Speckle2Void: Deep self-supervised SAR despeckling with blind-spot convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bordone Molini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 16</note>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Blind SAR image despeckling using self-supervised dense dilated convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01608</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 5, 16</note>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">SAE-net: A deep neural network for SAR autofocus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>cit. on p. 5</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">A Self-Supervised Denoising Network for Satellite-Airborne-Ground Hyperspectral Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2021.3064429</idno>
		<idno>DOI: 10.1109/ TGRS.2021.3064429</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Hyperspectral Image Restoration With Self-Supervised Learning: A Two-Stage Training Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit. on p. 5</note>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on p. 5</note>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Robust and adaptive network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebrahim</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1218" to="1242" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>cit. on p. 5</note>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02544</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 5, 14</note>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on p. 5</note>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="5046" to="5063" />
		</imprint>
	</monogr>
	<note>cit. on p. 5</note>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Adversarial autoencoder network for hyperspectral unmixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiwen</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit. on p. 5</note>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">S2-cGAN: Self-supervised adversarial representation learning for binary change detection in multispectral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Luis Holgado Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdyar</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beg?m</forename><surname>Demir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2020-2020 IEEE International Geoscience and Remote Sensing Symposium. IEEE. 2020</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2515" to="2518" />
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 16</note>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Self-Supervised Remote Sensing Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kane</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arcot</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sowmya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2020-2020 IEEE International Geoscience and Remote Sensing Symposium. IEEE. 2020</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1683" to="1686" />
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 12, 16</note>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Perturbation-seeking generative adversarial networks: A defense framework for remote sensing image scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>cit. on p. 6</note>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Spectral unmixing with multinomial mixture Kernel and Wasserstein generative adversarial loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Savas</forename><surname>Ozkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gozde</forename><surname>Bozdagi Akar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06859</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on p. 6</note>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Iterative reorganization with weak spatial constraints: Solving arbitrary jigsaw puzzles for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1910" to="1919" />
		</imprint>
	</monogr>
	<note>cit. on p. 6</note>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Visual permutation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">Santa</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cruz</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="3100" to="3114" />
		</imprint>
	</monogr>
	<note>cit. on p. 6</note>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Jigsaw Clustering for Unsupervised Visual Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021</meeting>
		<imprint>
			<biblScope unit="page" from="11526" to="11535" />
		</imprint>
	</monogr>
	<note>cit. on pp. 6, 12</note>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Error detecting and error correcting codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Hamming</surname></persName>
		</author>
		<idno type="DOI">10.1002/j.1538-7305.1950.tb00463.x</idno>
	</analytic>
	<monogr>
		<title level="j">The Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="147" to="160" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
	<note>tb00463.x (cit. on pp. 6, 7</note>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Selfsupervised representation learning by rotation feature decoupling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10364" to="10374" />
		</imprint>
	</monogr>
	<note>cit. on p. 6</note>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cit. on p. 6</note>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">When self-supervised learning meets scene classification: Remote sensing scene classification based on a multitask learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Remote Sensing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">3276</biblScope>
		</imprint>
	</monogr>
	<note>cit. on pp. 6, 16</note>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Rotation Awareness Based Self-Supervised Learning for SAR Target Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium. IEEE. 2019</title>
		<imprint>
			<biblScope unit="page" from="1378" to="1381" />
		</imprint>
	</monogr>
	<note>cit. on pp. 6, 16</note>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Self-Supervised Feature Learning for Semantic Segmentation of Overhead Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suriya</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification with self-supervised paradigm under limited labeled samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cit. on pp. 6, 16</note>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Few-shot Scene Classification of Optical Remote Sensing Images Leveraging Calibrated Pretext Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>cit. on p. 6</note>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">The color out of space: learning self-supervised representations for Earth Observation imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Vincenzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR). IEEE. 2021</title>
		<imprint>
			<biblScope unit="page" from="3034" to="3041" />
		</imprint>
	</monogr>
	<note>cit. on pp. 7, 8, 16</note>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Comparative analysis of the quantization of color spaces on the basis of the CIELAB colordifference formula</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><forename type="middle">Wilhelm</forename><surname>Vorhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="109" to="154" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>TOG). cit. on p. 6</note>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="577" to="593" />
		</imprint>
	</monogr>
	<note>cit. on p. 6</note>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017 (cit</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017 (cit</meeting>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Hyper-Embedder: Learning a Deep Embedder for Self-Supervised Hyperspectral Dimensionality Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<idno>PMLR. 2015</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b158">
	<monogr>
		<title level="m" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08033</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">Unsupervised learning using sequential verification for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.085612.7</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Optical flow modeling and computation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Fortun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Kervrann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Self-supervised representation learning for remote sensing image change detection based on temporal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihui</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Remote Sensing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1868</biblScope>
		</imprint>
	</monogr>
	<note>cit. on pp. 7, 16, 18, 21</note>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Self-Supervised Pretraining of Transformers for Satellite Image Time Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="474" to="487" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cit. on pp. 8, 16</note>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">SITS-Former: A pre-trained spatiospectral-temporal representation model for Sentinel-2 time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Earth Observation and Geoinformation</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">102651</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Cross-domain self-supervised multi-task feature learning using synthetic imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="762" to="771" />
		</imprint>
	</monogr>
	<note>cit. on p. 8)</note>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Multi-task selfsupervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2051" to="2060" />
		</imprint>
	</monogr>
	<note>cit. on p. 8)</note>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
	<note>cit. on pp. 8, 10</note>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2701" to="2710" />
		</imprint>
	</monogr>
	<note>cit. on p. 8)</note>
</biblStruct>

<biblStruct xml:id="b175">
	<monogr>
		<title level="m" type="main">Self-Supervised Learning for Monocular Depth Estimation from Aerial Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Hermann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07246</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 8, 18</note>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Self-Supervised Deep Learning For Nonlinear Seismic Full Waveform Inversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoqi</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>cit. on p. 8)</note>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Self-Supervised Deep Learning to Reconstruct Seismic Data With Consecutively Missing Traces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>cit. on p. 8)</note>
</biblStruct>

<biblStruct xml:id="b178">
	<monogr>
		<title level="m" type="main">Geography-aware self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Ayush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09980</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 8, 12, 15, 16)</note>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<title level="m" type="main">Geographical Knowledge-driven Representation Learning for Remote Sensing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.05276</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 8, 15, 16)</note>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Open access to Earth land-cover map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifang</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songnian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">514</biblScope>
			<biblScope unit="page" from="434" to="434" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>cit. on p. 8)</note>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>cit. on p. 9</note>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
	<note>cit. on p. 9</note>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
	<note>cit. on p. 9</note>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>cit. on pp. 9</note>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Henaff</surname></persName>
		</author>
		<idno>PMLR. 2020</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
	<note>cit. on pp. 9, 14</note>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno>PMLR. 2019</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b188">
	<monogr>
		<title level="m" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00910</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 10, 14</note>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<title level="m" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13625</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Metric learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XI 16</note>
</biblStruct>

<biblStruct xml:id="b192">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 11, 14</note>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Tile2vec: Unsupervised representation learning for spatially distributed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Jean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3967" to="3974" />
		</imprint>
	</monogr>
	<note>cit. on pp. 11, 16, 17</note>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">A Semisupervised Convolution Neural Network for Partial Unlabeled Remote-Sensing Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">A laboratory open-set Martian rock classification method based on spectral signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Transferable network with Siamese architecture for anomaly detection in hyperspectral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiang</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Earth Observation and Geoinformation</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Radar Target Detection With Multi-Task Learning in Heterogeneous Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Unsupervised deep representation learning and few-shot classification of PolSAR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lamei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Multi-View Urban Scene Classification with a Complementary-Information Learning Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxuan</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuanggen</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Photogrammetric Engineering &amp; Remote Sensing</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">When deep learning meets metric learning: Remote sensing image scene classification via learning discriminative CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on geoscience and remote sensing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Self-supervised learning with randomised layers for remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heechul</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taegyun</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<monogr>
		<title level="m" type="main">Self-supervised pre-training enhances change detection in Sentinel-2 imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marrit</forename><surname>Leenstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 11, 16, 18, 21</note>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Deep Mutual Information Subspace Clustering Network for Hyperspectral Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Hyperspectral Imagery Classification Based on Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikang</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Hyperspectral Image Classification With Contrastive Self-Supervised Learning Under Limited Labeled Samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">SC-EADNet: A Self-Supervised Contrastive Efficient Asymmetric Dilated Network for Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhen</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Self-supervised learning-based oil spill detection of hyperspectral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puhong</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Technological Sciences</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Contrastive Self-Supervised Learning With Smoothed Representation for Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heechul</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<monogr>
		<title level="m" type="main">Self-supervised learning for joint SAR and multispectral land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Montanaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.09075</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Self-Supervised Vision Transformers for Land-Cover Segmentation and Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linus</forename><surname>Scheibenreif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022</meeting>
		<imprint>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Deep residential representations: Using unsupervised learning to unlock elevation data for geodemographic prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Mues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristi?n</forename><surname>Bravo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Deep unsupervised embedding for remotely sensed images based on spatially augmented momentum contrast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Geoscience and</title>
		<imprint>
			<biblScope unit="page" from="2598" to="2610" />
		</imprint>
	</monogr>
	<note>Remote Sensing 59.3 (2020). cit. on pp. 12, 16, 17</note>
</biblStruct>

<biblStruct xml:id="b215">
	<monogr>
		<title level="m" type="main">Remote Sensing Images Semantic Segmentation with General Remote Sensing Vision Model via a Self-Supervised Contrastive Learning Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10605</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 12, 16</note>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning of Remote Sensing Scene Representations Using Contrastive Multiview Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladan</forename><surname>Stojnic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Risojevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021</meeting>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b217">
	<monogr>
		<title level="m" type="main">Self-supervised Change Detection in Multi-view Remote Sensing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bruzzone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.05969</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 12, 16</note>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Deep multiview learning for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<monogr>
		<title level="m" type="main">Seasonal Contrast: Unsupervised Pre-Training from Uncurated Remote Sensing Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Ma?as</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16607</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 12, 16, 17, 21</note>
</biblStruct>

<biblStruct xml:id="b220">
	<monogr>
		<title level="m" type="main">Self-supervised Audiovisual Representation Learning for Remote Sensing Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Heidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.00688</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 12, 16, 18, 21</note>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Cross-domain Contrastive Learning for Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Edmund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020</meeting>
		<imprint>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Contrastive clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 AAAI Conference on Artificial Intelligence (AAAI). 2021 (cit</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<monogr>
		<title level="m" type="main">Self-supervised Multisensor Change Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipan</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.05102</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Unsupervised Single-Scene Semantic Segmentation for Earth Observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipan</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">ContrastNet: Unsupervised feature learning by autoencoder and prototypical contrastive learning for hyperspectral imagery classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">460</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Deep Spatial-Spectral Subspace Clustering for Hyperspectral Images Based on Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Contrastive Learning-Based Dual Dynamic GCN for SAR Image Scene Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b230">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01780</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">Self-Supervised GANs With Similarity Loss for Remote Sensing Image Scene Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2508" to="2521" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit. on pp. 13, 16</note>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Self-supervised SAR-optical Data Fusion of Sentinel-1/-2 Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit. on pp. 13, 16, 18, 21</note>
</biblStruct>

<biblStruct xml:id="b233">
	<monogr>
		<title level="m" type="main">Self-supervised Remote Sensing Images Change Detection at Pixellevel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bruzzone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08501</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 13, 16</note>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Contrastive Learning Based on Transformer for Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied Sciences</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">Exploring vision transformers for polarimetric SAR image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lamei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">FIAD net: a Fast SAR ship detection network based on feature integration attention and self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b237">
	<monogr>
		<title level="m" type="main">Self-Supervised Learning for Invariant Representations from Multi-Spectral and SAR Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Schoen-Phelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ross</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.02049</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b238">
	<monogr>
		<title level="m" type="main">Self-supervised Vision Transformers for Joint SARoptical Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Conrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05381</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 13, 18, 21</note>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">The Self-Supervised Spectral-Spatial Vision Transformer Network for Accurate Prediction of Wheat Nitrogen Status from UAV Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Remote Sensing</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Index Your Position: A Novel Self-Supervised Learning Method for Remote Sensing Images Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilxat</forename><surname>Muhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfeng</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<monogr>
		<title level="m" type="main">Semantic-aware Dense Representation Learning for Remote Sensing Image Change Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.13769</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b243">
	<monogr>
		<title level="m" type="main">Possible principles underlying the transformation of sensory messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Horace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barlow</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Sensory communication 1.01 (1961) (cit. on p. 14</note>
</biblStruct>

<biblStruct xml:id="b244">
	<monogr>
		<title level="m" type="main">Continual Barlow Twins: continual self-supervised learning for remote sensing semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Marsocci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Scardapane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11319</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Self Supervised Learning For Few Shot Hyperspectral Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassim Ait Ali</forename><surname>Braham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Geoscience and Remote Sensing Symposium (IGARSS) 2022. IEEE. 2022 (cit</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<monogr>
		<title level="m" type="main">Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle T</forename><surname>Rudelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Story</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.05094</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Semantic Segmentation of Remote Sensing Images With Self-Supervised Multitask Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">Taskrelated self-supervised learning for remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhinan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2021</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b249">
	<monogr>
		<title level="m" type="main">Unsupervised Pretraining for Object Detection by Patch Reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04814</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b250">
	<monogr>
		<title level="m" type="main">Selfsupervised spectral matching network for hyperspectral target detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04078</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">A Self-Supervised Denoising Network for Satellite-Airborne-Ground Hyperspectral Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">SAR Image Classification Using Contrastive Learning and Pseudo-Labels With Limited Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main">A Mutual Information-Based Self-Supervised Learning Model for PolSAR Land Cover Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<monogr>
		<title level="m" type="main">Homography augumented momentum constrastive learning for SAR image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonho</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10329</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">Adversarial Self-Supervised Learning for Robust SAR Target Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">Contrastive Multiview Coding With Electro-Optics for SAR Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keumgang</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeji</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing applications: A meta-analysis and review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS journal of photogrammetry and remote sensing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="166" to="177" />
		</imprint>
	</monogr>
	<note>cit. on p. 18</note>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">Deep learning for remote sensing image classification: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main">A review of deep learning methods for semantic segmentation of remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichuan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">Deep learning for change detection in remote sensing images: Comprehensive review and meta-analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lazhar</forename><surname>Khelifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Mignotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="126385" to="126400" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cit. on p. 18</note>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">Deep learning meets hyperspectral image analysis: A multidisciplinary review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Signoroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">Deep learning meets SAR: Concepts, models, pitfalls, and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao Xiang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Geoscience and Remote Sensing Magazine</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="143" to="172" />
		</imprint>
	</monogr>
	<note>cit. on p. 18</note>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">H2O-Net: Self-Supervised Flood Segmentation via Adversarial Domain Adaptation and Label Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peri</forename><surname>Akiva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). 2021</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). 2021</meeting>
		<imprint>
			<biblScope unit="page" from="111" to="122" />
		</imprint>
	</monogr>
	<note>cit. on p. 18</note>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">Patch-level unsupervised planetary change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipan</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cit. on p. 18</note>
</biblStruct>

<biblStruct xml:id="b265">
	<monogr>
		<title level="m" type="main">Unsupervised Change Detection of Extreme Events Using ML On-Board</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V?t</forename><surname>R??i?ka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.02995</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main">Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Helber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b267">
	<monogr>
		<title level="m" type="main">Understanding self-supervised learning dynamics without contrastive pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06810</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main">Improvements to context based self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>T Nathan Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry Y</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b269">
	<monogr>
		<title level="m" type="main">Leveraging background augmentations to encourage semantic focus in self-supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morcos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12719</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on p. 21</note>
</biblStruct>

<biblStruct xml:id="b270">
	<monogr>
		<title level="m" type="main">Rethinking selfsupervised learning: Small is beautiful</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13559</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main">He is pursuing his Ph.D. degree at the German Aerospace Center</title>
	</analytic>
	<monogr>
		<title level="m">2018 and his M.Sc. degree in geomatics engineering from University of Stuttgart</title>
		<meeting><address><addrLine>Wuhan, China; Stuttgart, Germany; Wessling, 82234, Germany,; Munich, Germany; Stuttgart, Germany</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
		<respStmt>
			<orgName>Yi Wang received his B.E. degree in remote sensing science and technology from Wuhan University ; at Technical University of Munich</orgName>
		</respStmt>
	</monogr>
	<note>2020, he spent three months at perception system group, Sony Corporation. His research interests include remote sensing, deep learning, computer vision and self-supervised learning</note>
</biblStruct>

<biblStruct xml:id="b272">
	<analytic>
		<title level="a" type="main">Conrad&apos;s research agenda interconnect physical models and numerical analysis, employing Big Data technologies and machine learning. As part of the &quot;Data Intensive Physical Analytics&quot; team in IBM Research, he significantly contributed to industry-level solutions processing geospatial information with focus on machine-learning driven remote sensing applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Conrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Albrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">M&apos;17) received an undergraduate degree in physics from Technical University Dresden, Germany, in 2007 and a Ph.D. degree in physics with an extra certification in computer science from Heidelberg University</title>
		<meeting><address><addrLine>Germany; Germany; Yorktown Heights, NY, USA. Currently, since; Oberpfaffenhofen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-04" />
		</imprint>
	</monogr>
	<note>Spanning the fields of physics, mathematics and computer science, among others, he was a visiting scientist with CERN, Switzerland, in 2010, and with the Dresden Max Planck Institute for the Physics of Complex Systems. He co-organized workshops at the IEEE BigData conference and the AAAS annual meeting. Home to the US and the EU, Conrad&apos;s scientific agenda aims to strengthen transatlantic collaboration of corporate research and academia</note>
</biblStruct>

<biblStruct xml:id="b273">
	<analytic>
		<title level="a" type="main">2019 and M.Sc. degree in Artificial Intelligence and Data Science from Universit? Paris Dauphine-PSL</title>
	</analytic>
	<monogr>
		<title level="m">2020, he spent six months at the LAMSADE-CNRS laboratory</title>
		<meeting><address><addrLine>Algiers, Algeria; Paris, France; Munich, Germany; Lyon, France; Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
		<respStmt>
			<orgName>Nassim Ait Ali Braham received his M.Sc. degree in computer science from Ecole nationale Sup?rieure d&apos;Informatique (ESI ; Wessling, Germany, and at Technical University of Munich ; PSL Research University</orgName>
		</respStmt>
	</monogr>
	<note>he spent six months at the LIRIS-CNRS laboratory. His research interests include deep learning, computer vision, selfsupervised learning and remote sensing</note>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main">He is currently a Guest Professor at the Munich AI Future Lab AI4EO, TUM and the Head of Visual Learning and Reasoning team at the Department &quot;EO Data Science</title>
	</analytic>
	<monogr>
		<title level="m">2012, the Master&apos;s degree in signal and information processing from the University of Chinese Academy of Sciences (UCAS), China, in 2015, and the Dr.-Ing. degree from the Technical University of Munich (TUM)</title>
		<meeting><address><addrLine>Xi&apos;an, China; Munich, Germany; Wessling, Germany; University of Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>German Aerospace Center</publisher>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
		<respStmt>
			<orgName>Lichao Mou received the Bachelor&apos;s degree in automation from the Xi&apos;an University of Posts and Telecommunications</orgName>
		</respStmt>
	</monogr>
	<note>. He was the recipient of the first place in the 2016 IEEE GRSS Data Fusion Contest and finalists for the Best. Student Paper Award at the 2017 Joint Urban Remote Sensing Event and 2019 Joint Urban Remote Sensing Event</note>
</biblStruct>

<biblStruct xml:id="b275">
	<analytic>
		<title level="a" type="main">She is currently a visiting AI professor at ESA&apos;s Phi-lab. Her main research interests are remote sensing and Earth observation, signal processing, machine learning and data science, with their applications in tackling societal grand challenges, e.g. Global Urbanization, UN&apos;s SDGs and Climate Change. Dr. Zhu is a member of young academy (Junge Akademie/Junges Kolleg) at the Berlin-Brandenburg Academy of Sciences and Humanities and the German National Academy of Sciences Leopoldina and the Bavarian Academy of Sciences and Humanities. She serves in the scientific advisory board in several research organizations, among others the German Research Center for Geosciences (GFZ) and Potsdam Institute for Climate Impact Research (PIK)</title>
	</analytic>
	<monogr>
		<title level="m">Xiao Xiang Zhu (S&apos;10-M&apos;12-SM&apos;14-F&apos;21) received the Master (M.Sc.) degree, her doctor of engineering (Dr.-Ing.) degree and her &quot;Habilitation&quot; in the field of signal processing from Technical University of Munich (TUM)</title>
		<meeting><address><addrLine>Munich, Germany; Munich, Germany; Naples, Italy, Fudan University, Shanghai, China, the University of Tokyo, Tokyo, Japan and University of California, Los Angeles, United States</addrLine></address></meeting>
		<imprint>
			<publisher>German Aerospace Center (DLR</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Since October 2020, she also serves as a co-director of the Munich Data Science Institute (MDSI), TUM. Prof. Zhu was a guest scientist or visiting professor at the Italian National Research Council (CNR-IREA). She is an associate Editor of IEEE Transactions on Geoscience and Remote Sensing and serves as the area editor responsible for special issues of IEEE Signal Processing Magazine. She is a Fellow of IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
