<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anomaly Detection via Reverse Distillation from One-Class Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqiu</forename><surname>Deng</surname></persName>
							<email>hanqiu1@ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Alberta</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Li</surname></persName>
							<email>xingyu@ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Alberta</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Anomaly Detection via Reverse Distillation from One-Class Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge distillation (KD) achieves promising results on the challenging problem of unsupervised anomaly detection (AD). The representation discrepancy of anomalies in the teacher-student (T-S) model provides essential evidence for AD. However, using similar or identical architectures to build the teacher and student models in previous studies hinders the diversity of anomalous representations. To tackle this problem, we propose a novel T-S model consisting of a teacher encoder and a student decoder and introduce a simple yet effective "reverse distillation" paradigm accordingly. Instead of receiving raw images directly, the student network takes teacher model's one-class embedding as input and targets to restore the teacher's multiscale representations. Inherently, knowledge distillation in this study starts from abstract, high-level presentations to low-level features. In addition, we introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S model. The obtained compact embedding effectively preserves essential information on normal patterns, but abandons anomaly perturbations. Extensive experimentation on AD and one-class novelty detection benchmarks shows that our method surpasses SOTA performance, demonstrating our proposed approach's effectiveness and generalizability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Anomaly detection (AD) refers to identifying and localizing anomalies with limited, even no, prior knowledge of abnormality. The wide applications of AD, such as industrial defect detection <ref type="bibr" target="#b2">[3]</ref>, medical out-of-distribution detection <ref type="bibr" target="#b50">[50]</ref>, and video surveillance <ref type="bibr" target="#b23">[24]</ref>, makes it a critical task as well as a spotlight. In the context of unsupervised AD, no prior information on anomalies is available. Instead, a set of normal samples is provided for reference. To tackle this problem, previous efforts attempt to construct various self-supervision tasks on those anomaly-free samples. These tasks include, but not limited to, sample reconstruction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b48">48]</ref>, pseudo-outlier augmen- <ref type="bibr">Figure 1.</ref> Anomaly detection examples on MVTec <ref type="bibr" target="#b2">[3]</ref>. Multiresolution Knowledge Distillation (MKD) <ref type="bibr" target="#b32">[33]</ref> adopts the conventional KD architecture in <ref type="figure">Fig. Fig. 2(a)</ref>. Our reverse distillation method is capable of precisely localising a variate of anomalies. tation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b46">46]</ref>, knowledge distillation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">39]</ref>, etc.</p><p>In this study, we tackle the problem of unsupervised anomaly detection from the knowledge distillation-based point of view. In knowledge distillation (KD) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>, knowledge is transferred within a teacher-student (T-S) pair. In the context of unsupervised AD, since the student experiences only normal samples during training, it is likely to generate discrepant representations from the teacher when a query is anomalous. This hypothesis forms the basis of KD-based methods for anomaly detection. However, this hypothesis is not always true in practice due to <ref type="bibr" target="#b0">(1)</ref> the identical or similar architectures of the teacher and student networks (i.e., non-distinguishing filters <ref type="bibr" target="#b32">[33]</ref>) and (2) the same data flow in the T-S model during knowledge transfer/distillation. Though the use of a smaller student network partially addresses this issue <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">39]</ref>, the weaker representation capability of shallow architectures hinders the model from precisely detecting and localizing anomalies.</p><p>To holistically address the issue mentioned above, we propose a new paradigm of knowledge distillation, namely Reverse Distillation, for anomaly detection. We use simple diagrams in <ref type="figure">Fig. 2</ref> to highlight the systematic difference between conventional knowledge distillation and the proposed reverse distillation. First, unlike the conventional knowledge distillation framework where both teacher and student adopt the encoder structure, the T-S model in our <ref type="bibr">Figure 2</ref>. T-S models and data flow in (a) conventional KD framework <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref> and (b) our Reverse Distillation paradigm. reverse distillation consists of heterogeneous architectures: a teacher encoder and a student decoder. Second, instead of directly feeding the raw data to the T-S model simultaneously, the student decoder takes the low-dimensional embedding as input, targeting to mimic the teacher's behavior by restoring the teacher model's representations in different scales. From the regression perspective, our reverse distillation uses the student network to predict the representation of the teacher model. Therefore, "reverse" here indicates both the reverse shapes of teacher encoder and student decoder and the distinct knowledge distillation order where high-level representation is first distilled, followed by low-level features. It is noteworthy that our reverse distillation presents two significant advantages: i) Non-similarity structure. In the proposed T-S model, one can consider the teacher encoder as a down-sampling filter and the student decoder as an up-sampling filter. The "reverse structures" avoid the confusion caused by non-distinguishing filters <ref type="bibr" target="#b32">[33]</ref> as we discussed above. ii) Compactness embedding. The low-dimensional embedding fed to the student decoder acts as an information bottleneck for normal pattern restoration. Let's formulate anomaly features as perturbations on normal patterns. Then the compact embedding helps to prohibit the propagation of such unusual perturbations to the student model and thus boosts the T-S model's representation discrepancy on anomalies. Notably, traditional AE-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26]</ref> detect anomalies utilising pixel differences, whereas we perform discrimination with dense descriptive features. Deep features as region-aware descriptors provide more effective discriminative information than per-pixel in images.</p><p>In addition, since the compactness of the bottleneck embedding is vital for anomaly detection (as discussed above), we introduce a one-class bottleneck embedding (OCBE) module to condense the feature codes further. Our OCBE module consists of a multi-scale feature fusion (MFF) block and one-class embedding (OCE) block, both jointly optimized with the student decoder. Notably, the former aggregates low-and high-level features to construct a rich embedding for normal pattern reconstruction. The latter targets to retain essential information favorable for the student to decode out the teacher's response.</p><p>We perform extensive experiments on public benchmarks. The experimental results indicate that our reverse distillation paradigm achieves comparable performance with prior arts. The proposed OCBE module further improves the performance to a new state-of-the-art (SOTA) record. Our main contributions are summarized as follows:</p><p>? We introduce a simple, yet effective Reverse Distillation paradigm for anomaly detection. The encoderdecoder structure and reverse knowledge distillation strategy holistically address the non-distinguishing filter problem in conventional KD models, boosting the T-S model's discrimination capability on anomalies.</p><p>? We propose a one-class bottleneck embedding module to project the teacher's high-dimensional features to a compact one-class embedding space. This innovation facilitates retaining rich yet compact codes for anomaly-free representation restoration at the student.</p><p>? We perform extensive experiments and show that our approach achieves new SOTA performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section briefly reviews previous efforts on unsupervised anomaly detection. We will highlight the similarity and difference between the proposed method and prior arts.</p><p>Classical anomaly detection methods focus on defining a compact closed one-class distribution using normal support vectors. The pioneer studies include one-class support vector machine (OC-SVM) <ref type="bibr" target="#b35">[35]</ref> and support vector data description (SVDD) <ref type="bibr" target="#b36">[36]</ref>. To cope with high-dimensional data, DeepSVDD <ref type="bibr" target="#b30">[31]</ref> and PatchSVDD <ref type="bibr" target="#b43">[43]</ref> estimate data representations through deep networks.</p><p>Another unsupervised AD prototype is the use of generative models, such as AutoEncoder (AE) <ref type="bibr" target="#b18">[19]</ref> and Generative Adversarial Nets (GAN) <ref type="bibr" target="#b11">[12]</ref>, for sample reconstruction. These methods rely on the hypothesis that generative models trained on normal samples only can successfully reconstruct anomaly-free regions, but fail for anomalous regions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">34]</ref>. However, recent studies show that deep models generalize so well that even anomalous regions can be well-restored <ref type="bibr" target="#b46">[46]</ref>. To address this issue, memory mechanism <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26]</ref> , image masking strategy <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b46">46]</ref> and pseudo-anomaly <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b45">45]</ref> are incorporated in reconstruction-based methods. However, these methods still lack a strong discriminating ability for real-world anomaly detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. Recently, Metaformer (MF) <ref type="bibr" target="#b40">[40]</ref> proposes the use of meta-learning <ref type="bibr" target="#b8">[9]</ref> to bridge model adaptation and reconstruction gap for reconstruction-based approaches. Notably, the proposed reverse knowledge distillation also adopts the encoder-decoder architecture, but it differs from construction-based methods in two-folds. First, the encoder in a generative model is jointly trained with the decoder, while our reverse distillation freezes a pre-trained model as the teacher. Second, instead of pixel-level reconstruction error, it performs anomaly detection on the semantic feature space.</p><p>Data augmentation strategy is also widely used. By adding pseudo anomalies in the provided anomaly-free samples, the unsupervised task is converted to a supervised learning task <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b46">46]</ref>. However, these approaches are prone to bias towards pseudo outliers and fail to detect a large variety of anomaly types. For example, CutPaste <ref type="bibr" target="#b22">[23]</ref> generates pseudo outliers by adding small patches onto normal images and trains a model to detect these anomalous regions. Since the model focuses on detecting local features such as edge discontinuity and texture perturbations, it fails to detect and localize large defects and global structural anomalies as shown in <ref type="figure">Fig. 6</ref>.</p><p>Recently, networks pre-trained on the large dataset are proven to be capable of extracting discriminative features for anomaly detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. With a pre-trained model, memorizing its anomaly-free features helps to identify anomalous samples <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref>. The studies in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref> show that using the Mahalanobis distance to measure the similarity between anomalies and anomaly-free features leads to accurate anomaly detection. Since these methods require memorizing all features from training samples, they are computationally expensive.</p><p>Knowledge distillation from pre-trained models is another potential solution to anomaly detection. In the context of unsupervised AD, since the student model is exposed to anomaly-free samples in knowledge distillation, the T-S model is expected to generate discrepant features on anomalies in inference <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">39]</ref>. To further increase the discrimnating capability of the T-S model on various types of abnormalities, different strategies are introduced. For instance, in order to capture multi-scale anomaly, US <ref type="bibr" target="#b3">[4]</ref> ensembles several models trained on normal data at different scales, and MKD <ref type="bibr" target="#b32">[33]</ref> propose to use multi-level features alignment. It should be noted that though the proposed method is also based on knowledge distillation, our reverse distillation is the first to adopt an encoder and a decoder to construct the T-S model. The heterogeneity of the teacher and student networks and reverse data flow in knowledge distillation distinguishes our method from prior arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>Problem formulation: Let I t = {I t 1 , ..., I t n } be a set of available anomaly-free images and I q = {I q 1 , ..., I q m } be a query set containing both normal and abnormal samples. The goal is to train a model to recognize and localize anomalies in the query set. In the anomaly detection setting, normal samples in both I t and I q follow the same distribu-tion. Out-of-distribution samples are considered anomalies.</p><p>System overview: <ref type="figure" target="#fig_0">Fig. 3</ref> depicts the proposed reserve distillation framework for anomaly detection. Our reverse distillation framework consists of three modules: a fixed pre-trained teacher encoder E, a trainable one-class bottleneck embedding module, and a student decoder D. Given an input sample I ? I t , the teacher E extracts multiscale representations. We propose to train a student D to restore the features from the bottleneck embedding. During testing/inference, the representation extracted by the teacher E can capture abnormal, out-of-distribution features in anomalous samples. However, the student decoder D fails to reconstruct these anomalous features from the corresponding embedding. The low similarity of anomalous representations in the proposed T-S model indicates a high abnormality score. We argue that the heterogeneous encoder and decoder structures and reverse knowledge distillation order contribute a lot to the discrepant representations of anomalies. In addition, the trainable OCBE module further condenses the multi-scale patterns into an extreme low-dimensional space for downstream normal representation reconstruction. This further improves feature discrepancy on anomalies in our T-S model, as abnormal representations generated by the teacher model are likely to be abandoned by OCBE. In the rest of this section, we first specify the reverse distillation paradigm. Then, we elaborate on the OCBE module. Finally, we describe anomaly detection and localization using reserve distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Reverse Distillation</head><p>In conventional KD, the student network adopts a similar or identical neural network to the teacher model, accepts raw data/images as input, and targets to match its feature activations to the teacher's <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref>. In the context of one-class distillation for unsupervised AD, the student model is expected to generate highly different representations from the teacher when the queries are anomalous samples <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref>. However, the activation discrepancy on anomalies vanishes sometimes, leading to anomaly detection failure. We argue that this issue is attributed to the similar architectures of the teacher and student nets and the same data flow during T-S knowledge transfer. To improve the T-S model's representation diversity on unknown, out-of-distribution samples, we propose a novel reserves distillation paradigm, where the T-S model adopts the encoder-decoder architecture and knowledge is distilled from teacher's deep layers to its early layers, i.e., high-level, semantic knowledge being transferred to the student first. To further facilitate the oneclass distillation, we designed a trainable OCEB module to connect the teacher and student models (Sec. 3.2).</p><p>In the reverse distillation paradigm, the teacher encoder E aims to extract comprehensive representations. We follow previous work and use a pre-trained encoder on Ima- geNet <ref type="bibr" target="#b20">[21]</ref> as our backbone E. To avoid the T-S model converging to trivial solutions, all parameters of teacher E are frozen during knowledge distillation. We show in the ablation study that both ResNet <ref type="bibr" target="#b13">[14]</ref> and WideResNet <ref type="bibr" target="#b44">[44]</ref> are good candidates, as they are capable of extracting rich features from images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>To match the intermediate representations of E, the architecture of student decoder D is symmetrical but reversed compared to E. The reverse design facilitates eliminating the response of the student network to abnormalities, while the symmetry allows it to have the same representation dimension as the teacher network. For instance, when we take ResNet as the teacher model, the student D consists of several residual-like decoding blocks for mirror symmetry. Specifically, the downsampling in ResNet is realized by a convolutional layer with a kernel size of 1 and a stride of 2 <ref type="bibr" target="#b13">[14]</ref>. The corresponding decoding block in the student D adopts deconvolutional layers <ref type="bibr" target="#b47">[47]</ref> with a kernel size of 2 and a stride of 2. More details on the student decoder design are given in Supplementary Material.</p><p>In our reverse distillation, the student decoder D targets to mimic the behavior of the teacher encoder E during training. In this work, we explore multi-scale feature-based distillation for anomaly detection. The motivation behind this is that shallow layers of a neural network extract local descriptors for low-level information (e.g., color, edge, texture, etc.), while deep layers have wider receptive fields, capable of characterizing regional/global semantic and structural information. That is, low similarity of low-and highlevel features in the T-S model suggests local abnormalities and regional/global structural outliers, respectively.</p><p>Mathematically, let ? indicates the projection from raw data I to the one-class bottleneck embedding space, the paired activation correspondence in our T-S model is</p><formula xml:id="formula_0">{f k E = E k (I), f k D = D k (?)},</formula><p>where E k and D k represent the k th encoding and decoding block in the teacher and student model,</p><formula xml:id="formula_1">respectively. f k E , f k D ? R C k ?H k ?W k ,</formula><p>where C k , H k and W k denote the number of channels, height and width of the k th layer activation tensor. For knowledge transfer in the T-S model, the cosine similarity is taken as the KD loss, as it is more precisely to capture the relation in both highand low-dimensional information <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b49">49]</ref>. Specifically, for feature tensors f k E and f k D , we calculate their vector-wise cosine similarity loss along the channel axis and obtain a 2-D anomaly map M k ? R H k ?W k :</p><formula xml:id="formula_2">M k (h, w) = 1 ? (f k E (h, w)) T ? f k D (h, w) f k E (h, w) f k D (h, w) .<label>(1)</label></formula><p>A large value in M k indicates high anomaly in that location. Considering the multi-scale knowledge distillation, the scalar loss function for student's optimization is obtained by accumulating multi-scale anomaly maps:</p><formula xml:id="formula_3">L KD = K k=1 1 H k W k H k h=1 W k w=1 M k (h, w) ,<label>(2)</label></formula><p>where K indicates the number of feature layers used in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">One-Class Bottleneck Embedding</head><p>Since the student model D attempts to restore representations of a teacher model E in our reverse knowledge distillation paradigm, one can directly feed the activation output of the last encoding block in backbone to D. However, this naive connection has two shortfalls. First, the teacher model in KD usually has a high capacity. Though the high-capacity model helps extract rich features, the obtained high-dimensional descriptors likely have a considerable redundancy. The high freedom and redundancy of representations are harmful to the student model to decode the essential anomaly-free features. Second, the activation of the last encoder block in backbone usually characterizes semantic and structural information of the input data. Due to the reverse order of knowledge distillation, directly feeding this high-level representation to the student decoder set a challenge for low-level features reconstruction. Previous efforts on data reconstruction usually introduce skip paths to connect the encoder and decoder. However, this approach doesn't work in knowledge distillation, as the skip paths leak anomaly information to the student during inference.</p><p>To tackle the first shortfall above in one-class distillation, we introduce a trainable one-class embedding block to project the teacher model's high-dimensional representations into a low-dimensional space. Let's formulate anomaly features as perturbations on normal patterns. Then the compact embedding acts as an information bottleneck and helps to prohibit the propagation of unusual perturbations to the student model, therefore boosting the T-S model's representation discrepancy on anomalies. In this study, we adopt the 4th residule block of ResNet <ref type="bibr" target="#b13">[14]</ref> as the one-class embedding block.</p><p>To address the problem on low-level feature restoration at decoder D, the MFF block concatenates multi-scale representations before one-class embedding. To achieve representation alignment in feature concatenation, we downsample the shallow features through one or more 3 ? 3 convolutional layers with stride of 2, followed by batch normalization and ReLU activation function. Then a 1?1 convolutional layer with stride of 1 and a batch normalization with relu activation are exploited for a rich yet compact feature.</p><p>We depict the OCBE module in <ref type="figure" target="#fig_1">Fig. 4</ref>, where MFF aggregates low-and high-level features to construct a rich em- bedding for normal pattern reconstruction and OCE targets to retain essential information favorable for the student to decode out the teacher's response. The convolutional layers in grey and ResBlock in green in <ref type="figure" target="#fig_1">Fig. 4</ref> are trainable and optimized jointly with the student model D during knowledge distillation on normal samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Anomaly Scoring</head><p>At the inference stage, We first consider the measurement of pixel-level anomaly score for anomaly localization (AL). When a query sample is anomalous, the teacher model is capable of reflecting abnormality in its features. However, the student model is likely to fail in abnormal feature restoration, since the student decoder only learns to restore anomaly-free representations from the compact oneclass embedding in knowledge distillation. In other words, the student D generates discrepant representations from the teacher when the query is anomalous. Following Eq. (1), we obtain a set of anomaly maps from T-S representation pairs, where the value in a map M k reflects the point-wise anomaly of the k th feature tensors. To localize anomalies in a query image, we up-samples M k to image size. Let ? denotes the bilinear up-sampling operation used in this study. Then a precise score map S I q is formulated as the pixel-wise accumulation of all anomaly maps:</p><formula xml:id="formula_4">S AL = L i=1 ?(M i ).<label>(3)</label></formula><p>In order to remove the noises in the score map, we smooth S AL by a Gaussian filter. For anomaly detection, averaging all values in a score map S AL is unfair for samples with small anomalous regions. The most responsive point exists for any size of Image Size 128 256 Category/Method MKD <ref type="bibr" target="#b32">[33]</ref> Ours GT <ref type="bibr" target="#b9">[10]</ref> GN <ref type="bibr" target="#b1">[2]</ref> US <ref type="bibr" target="#b3">[4]</ref> PSVDD <ref type="bibr" target="#b43">[43]</ref> DAAD <ref type="bibr" target="#b15">[16]</ref> MF <ref type="bibr" target="#b40">[40]</ref> PaDiM <ref type="bibr" target="#b7">[8]</ref> CutPaste <ref type="bibr" target="#b22">[23]</ref> Ours anomalous region. Hence, we define the maximum value in S AL as sample-level anomaly score S AD . The intuition is that no significant response exists in their anomaly score map for normal samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Discussions</head><p>Empirical evaluations are carried on both the MVTec anomaly detection and localization benchmark and unsupervised one-class novelty detection datasets. In addition, we perform ablation study on the MVTec benchmark, investigating the effects of different modules/blocks on the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Anomaly Detection and Localization</head><p>Dataset. MVTec <ref type="bibr" target="#b2">[3]</ref> contains 15 real-world datasets for anomaly detection, with 5 classes of textures and 10 classes of objects. The training set comprises a total of 3,629 anomaly-free images. The test set has both anomalous and anomaly-free images, totaling 1,725. Each class has multiple defects for testing. In addition, pixel-level annotations are available in the test dataset for anomaly localization evaluation.</p><p>Experimental settings. All images in MVTec are resized to a specific resolution (e.g. 128 ? 128, 256 ? 256 etc.). Following convention in prior works, anomaly detection and localization are performed on one category at a time. In this experiment, we adopt WideResNet50 as Backbone E in our T-S model. We also report the AD results with ResNet18 and ResNet50 in ablation study. To train our reserve distillation model, we utilize Adam optimizer <ref type="bibr" target="#b17">[18]</ref> with ? = (0.5, 0.999). The learning rate is set to 0.005. We train 200 epochs with a batch size of 16. A Gaussian filter with ? = 4 is used to smooth the anomaly score map (as described in Sec. 3.3).</p><p>For Anomaly dectction, we take area under the receiver operating characteristic (AUROC) as the evaluation metric. We include prior arts in this experiments, including MKD <ref type="bibr" target="#b32">[33]</ref>, GT <ref type="bibr" target="#b9">[10]</ref>, GANomaly (GN) <ref type="bibr" target="#b1">[2]</ref>, Uninformed Student (US) <ref type="bibr" target="#b3">[4]</ref>, PSVDD <ref type="bibr" target="#b43">[43]</ref>, DAAD <ref type="bibr" target="#b15">[16]</ref>, MetaFormer (MF) <ref type="bibr" target="#b40">[40]</ref>, PaDiM (WResNet50) <ref type="bibr" target="#b7">[8]</ref> and CutPaste <ref type="bibr" target="#b22">[23]</ref>.</p><p>For Anomaly Localization, we report both AUROC and per-region-overlap (PRO) <ref type="bibr" target="#b3">[4]</ref>. Different from AUROC, which is used for per-pixel measurement, the PRO score treats anomaly regions with any size equally. The comparison baselines includes MKD <ref type="bibr" target="#b32">[33]</ref>, US <ref type="bibr" target="#b3">[4]</ref>, MF <ref type="bibr" target="#b40">[40]</ref>, SPADE (WResNet50) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref>, PaDiM (WResNet50) <ref type="bibr" target="#b7">[8]</ref>, RIAD <ref type="bibr" target="#b46">[46]</ref> and CutPaste <ref type="bibr" target="#b22">[23]</ref>.</p><p>Experimental results and discussions. Anomaly detection results on MVTec are shown in Tab. 1. The average outcome shows that our method exceeds SOTA by 2.5%. For textures and objects, our model reaches new SOTA of 99.5% and 98.0% of AUROC, respectively. The statistics of the anomaly scores are shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. The non-overlap distribution of normal (blue) and anomalies (red) indicates the strong AD capability in our T-S model.</p><p>Quantitative results on anomaly localization are summarized in Tab. 2. For both AUROC and PRO average scores over all categories, our approach surpasses state-of-the-art with 97.8% and 93.9%. To investigate the robustness of our method to various anomalies, we classify the defect types into two categories: large defects or structural anomalies and tiny or inconspicuous defects, and qualitative evaluate the performance by visualization in <ref type="figure" target="#fig_3">Fig. 6 and Fig. 7</ref>. Compared to the runner-up (i.e. CutPaste <ref type="bibr" target="#b22">[23]</ref>) in Tab. 1, our method produces a significant response to the whole  anomaly region. Complexity analysis. Recent pre-trained model based approaches achieve promising performance by extracting features from anomaly-free samples as a measurement <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. However, storing feature models leads to large memory consumption. In comparison, our approach achieves better performance depending only on an extra CNN model.  <ref type="table">Table 3</ref>. Comparison of pre-trained based approaches in terms of inference time (second on Intel i7), memory usage (MB), and performance (AD-AUROC/AL-AUROC/AL-PRO ) on MVTec <ref type="bibr" target="#b2">[3]</ref>.</p><p>Limitations. We observe that the localization performance on the transistor dataset is relatively weak, despite the good AD performance. This performance drop is caused by misinterpretation between prediction and annotation. As shown in <ref type="figure">Fig. 6</ref>, our method localize the misplaced regions, while the ground truth covers both misplaced and original areas. Alleviating this problem requires associating more features with contextual relationships. We empirically find that a higher-level feature layer with a wider perceptive field can improve the performance. For instance, anomaly detection with the second and third layer features achieves 94.5% AUROC, while using only the third layer improve the performance to 97.3%. In addition, reducing image resolution to 128?128 also achieves 97.6% AUROC. We present more cases of anomaly detection and localization, both positive and negative, in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">One-Class Novelty Detection</head><p>To evaluate the generality of proposed approach, we conduct one-class novelty detection experiment on 3 sematic datasets <ref type="bibr" target="#b31">[32]</ref>, MNIST <ref type="bibr" target="#b21">[22]</ref>, FashionMNIST <ref type="bibr" target="#b41">[41]</ref> and CI-FAR10 <ref type="bibr" target="#b19">[20]</ref>. MNIST is a hand-written digits dataset from numbers 0-9. FashionMNIST consists of images from 10 fashion product classes. Both datasets includes 60K samples for training and 10K samples for testing, all in resolution of 28 ? 28. CIFAR10 is a challenging dataset for novelty detection because of its inclusion of diverse natural objects. It includes 50K training images and 10K test images with scale of 32 ? 32 in 10 categories.</p><p>Following the protocol mentioned in <ref type="bibr" target="#b26">[27]</ref>, we train the model with samples from a single class and detect novel samples. Note that the novelty score is defined as the sum <ref type="figure">Figure 6</ref>. Anomalies from top to bottom: "flip" on "metal nut", "misplaced" on "transistor" and "crack" on "hazelnut". Normal samples are provided as reference.   of scores in the similarity map. The baselines in this experiment include LSA <ref type="bibr" target="#b0">[1]</ref>, OCGAN <ref type="bibr" target="#b26">[27]</ref>, HRN <ref type="bibr" target="#b16">[17]</ref>, DAAD <ref type="bibr" target="#b15">[16]</ref> and MKD <ref type="bibr" target="#b32">[33]</ref>. We also include the comparision with OiG <ref type="bibr" target="#b45">[45]</ref> and G2D <ref type="bibr" target="#b27">[28]</ref> on Caltech-256 <ref type="bibr" target="#b12">[13]</ref>.</p><p>Tab. 4 summarizes the quantitative results on the three datasets. Remarkably, our approach produces excellent results. Details of the experiments and the results of per-class comparisons are provided in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation analysis</head><p>We investigate effective of OCE and MFF blocks on AD and reports the numerical results in Tab. 5. We take the pre-trained residual block <ref type="bibr" target="#b13">[14]</ref> as baseline. Embedding from pre-trained residual block may contain anomaly features, which decreases the T-S model's representation discrepancy. Our trainable OCE block condenses feature codes and the MFM block fuses rich features into embedding, allows for more accurate anomaly detection and localization.  <ref type="table">Table 5</ref>. Ablation study on pre-trained bottleneck, OCE, and MFF.</p><p>Tab. 6 displays qualitative comparisons of different backbone networks as the teacher model. Intuitively, a deeper and wider network usually have a stronger representative capacity, which facilitates detecting anomalies precisely. Noteworthy that even with a smaller neural network such as ResNet18, our reverse distillation method still achieves excellent performance.  Besides, we also explored the impact of different network layers on anomaly detection and shown the results in Tab. 7. For single-layer features, M 2 yields the best result as it trades off both local texture and global structure information. Multi-scale feature fusion helps to cover more types of anomalies.  <ref type="table">Table 7</ref>. Ablation study on multi-scale feature distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a novel knowledge distillation paradigm, reverse distillation, for anomaly detection. It holistically addressed the problem in previous KD-based AD methods and boosted the T-S model's response on anomalies. In addition, we introduced trainable one-class embedding and multiscale feature fusion blocks in reverse distillation to improve one-class knowledge transfer. Experiments showed that our method significantly outperformed previous arts in anomaly detection, anomaly localization, and novelty detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Overview of our reverse distillation framework for anomaly detection and localization. (a) Our model consists of a pre-trained teacher encoder E, a trainable one-class bottleneck embedding module (OCBE), and a student decoder D. We use a multi-scale feature fusion (MFF) block to ensemble low-and high-level features from E and map them onto a compact code by one-class embedding (OCE) block. During training, the student D learns to mimic the behavior of E by minimizing the similarity loss L. (b) During inference, E extracts the features truthfully, while D outputs anomaly-free ones. A low similarity between the feature vectors at the corresponding position of E and D implies an abnormality. (c) The final prediction is calculated by the accumulation of multi-scale similarity maps M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Our one-class bottleneck embedding module consists of trainable MFF and OCE blocks. MFF aligns multi-scale features from teacher E and OCE condenses the obtained rich feature to a compact bottleneck code ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Histogram of anomaly scores for all categories of MVTec<ref type="bibr" target="#b2">[3]</ref> (x-axis: anomaly score from 0 to 1, and y-axis: count).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Visualization on tiny or inconspicuous anomalies. From left to right: carpet, tile, wood, cable, pill, toothbrush, and screw.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Anomaly Detection results on MVTec<ref type="bibr" target="#b2">[3]</ref>. For each category with images of 256 ? 256 resolution, methods achieved for the top two AUROC (%) are highlighted in bold. Our method ranks first according to the average scores of textures, objects and overall.</figDesc><table><row><cell></cell><cell>Carpet</cell><cell>79.3</cell><cell>99.2</cell><cell>43.7</cell><cell>69.9</cell><cell>91.6</cell><cell>92.9</cell><cell>86.6</cell><cell>94.0</cell><cell>99.8</cell><cell>93.9</cell><cell>98.9</cell></row><row><cell>Textures</cell><cell>Grid Leather Tile Wood</cell><cell>78.0 95.1 91.6 94.3</cell><cell>95.7 100 99.4 98.8</cell><cell>61.9 84.1 41.7 61.1</cell><cell>70.8 84.2 79.4 83.4</cell><cell>81.0 88.2 99.1 97.7</cell><cell>94.6 90.9 97.8 96.5</cell><cell>95.7 86.2 88.2 98.2</cell><cell>85.9 99.2 99.0 99.2</cell><cell>96.7 100 98.1 99.2</cell><cell>100 100 94.6 99.1</cell><cell>100 100 99.3 99.2</cell></row><row><cell></cell><cell>Average</cell><cell>87.7</cell><cell>98.6</cell><cell>58.5</cell><cell>77.5</cell><cell>91.5</cell><cell>94.5</cell><cell>91.0</cell><cell>95.5</cell><cell>98.8</cell><cell>97.5</cell><cell>99.5</cell></row><row><cell></cell><cell>Bottle</cell><cell>99.4</cell><cell>100</cell><cell>74.4</cell><cell>89.2</cell><cell>99.0</cell><cell>98.6</cell><cell>97.6</cell><cell>99.1</cell><cell>99.9</cell><cell>98.2</cell><cell>100</cell></row><row><cell></cell><cell>Cable</cell><cell>89.2</cell><cell>97.1</cell><cell>78.3</cell><cell>75.7</cell><cell>86.2</cell><cell>90.3</cell><cell>84.4</cell><cell>97.1</cell><cell>92.7</cell><cell>81.2</cell><cell>95.0</cell></row><row><cell></cell><cell>Capsule</cell><cell>80.5</cell><cell>89.5</cell><cell>67.0</cell><cell>73.2</cell><cell>86.1</cell><cell>76.7</cell><cell>76.7</cell><cell>87.5</cell><cell>91.3</cell><cell>98.2</cell><cell>96.3</cell></row><row><cell></cell><cell>Hazelnut</cell><cell>98.4</cell><cell>99.8</cell><cell>35.9</cell><cell>78.5</cell><cell>93.1</cell><cell>92.0</cell><cell>92.1</cell><cell>99.4</cell><cell>92.0</cell><cell>98.3</cell><cell>99.9</cell></row><row><cell>Objects</cell><cell>Metal Nut Pill Screw</cell><cell>73.6 82.7 83.3</cell><cell>99.2 93.3 91.1</cell><cell>81.3 63.0 50.0</cell><cell>70.0 74.3 74.6</cell><cell>82.0 87.9 54.9</cell><cell>94.0 86.1 81.3</cell><cell>75.8 90.0 98.7</cell><cell>96.2 90.1 97.5</cell><cell>98.7 93.3 85.8</cell><cell>99.9 94.9 88.7</cell><cell>100 96.6 97.0</cell></row><row><cell></cell><cell>Toothbrush</cell><cell>92.2</cell><cell>90.3</cell><cell>97.2</cell><cell>65.3</cell><cell>95.3</cell><cell>100</cell><cell>99.2</cell><cell>100</cell><cell>96.1</cell><cell>99.4</cell><cell>99.5</cell></row><row><cell></cell><cell>Transistor</cell><cell>85.6</cell><cell>99.5</cell><cell>86.9</cell><cell>79.2</cell><cell>81.8</cell><cell>91.5</cell><cell>87.6</cell><cell>94.4</cell><cell>97.4</cell><cell>96.1</cell><cell>96.7</cell></row><row><cell></cell><cell>Zipper</cell><cell>93.2</cell><cell>94.3</cell><cell>82.0</cell><cell>74.5</cell><cell>91.9</cell><cell>97.9</cell><cell>85.9</cell><cell>98.6</cell><cell>90.3</cell><cell>99.9</cell><cell>98.5</cell></row><row><cell></cell><cell>Average</cell><cell>87.8</cell><cell>95.4</cell><cell>71.6</cell><cell>75.5</cell><cell>85.8</cell><cell>90.8</cell><cell>88.8</cell><cell>96.0</cell><cell>93.8</cell><cell>95.5</cell><cell>98.0</cell></row><row><cell cols="2">Total Average</cell><cell>87.8</cell><cell>96.5</cell><cell>67.2</cell><cell>76.2</cell><cell>87.7</cell><cell>92.1</cell><cell>89.5</cell><cell>95.8</cell><cell>95.5</cell><cell>96.1</cell><cell>98.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Anomaly Localization results with AUROC and PRO on MVTec<ref type="bibr" target="#b2">[3]</ref>. AUROC represents a pixel-wise comparison, while PRO focuses on region-based behavior. We show the best results for AUROC and PRO in bold. Remarkable, our approach is robust and represents state-of-the-art performance under both metrics.</figDesc><table><row><cell></cell><cell>Image Size</cell><cell>128</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>256</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Category/Method</cell><cell>MKD [33]</cell><cell>Ours</cell><cell cols="6">US [4] MF [40] SPADE [7] PaDiM [8] RIAD [46] CutPaste [23]</cell><cell>Ours</cell></row><row><cell></cell><cell>Carpet</cell><cell>95.6/-</cell><cell>98.1/95.3</cell><cell>-/87.9</cell><cell>-/87.8</cell><cell>97.5/94.7</cell><cell>99.1/96.2</cell><cell>96.3/-</cell><cell>98.3/-</cell><cell>98.9/97.0</cell></row><row><cell>Textures</cell><cell>Grid Leather Tile Wood</cell><cell>91.8/-98.1/-82.8/-84.8/-</cell><cell>97.3/92.6 99.0/98.6 92.6/84.8 92.1/82.3</cell><cell>-/95.2 -/94.5 -/94.6 -/91.1</cell><cell>-/86.5 -/95.9 -/88.1 -/84.8</cell><cell>93.7/86.7 97.6/97.2 87.4/75.9 88.5/87.4</cell><cell>97.3/94.6 99.2/97.8 94.1/86.0 94.9/91.1</cell><cell>98.8/-99.4/-89.1/-85.8/-</cell><cell>97.5/-99.5/-90.5/-95.5/-</cell><cell>99.3/97.6 99.4/99.1 95.6/90.6 95.3/90.9</cell></row><row><cell></cell><cell>Average</cell><cell>90.6/-</cell><cell>95.8/90.7</cell><cell>-/92.7</cell><cell>-/88.6</cell><cell>92.9/88.4</cell><cell>96.9/93.2</cell><cell>93.9/-</cell><cell>96.3/-</cell><cell>97.7/95.0</cell></row><row><cell>Objects</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>As shown in Tab. 3. Our model obtain performance gain with low time and memory complexity.</figDesc><table><row><cell>Methods</cell><cell>Infer. time</cell><cell>Memory</cell><cell>Performance</cell></row><row><cell>SPADE (WResNet50)</cell><cell>1.40</cell><cell>1400</cell><cell>85.5/96.5/91.7</cell></row><row><cell>PaDiM (WResNet50)</cell><cell>0.95</cell><cell>3800</cell><cell>95.5/97.5/92.1</cell></row><row><cell>Ours (WResNet50)</cell><cell>0.31</cell><cell>352</cell><cell>98.5/97.8/93.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>AUROC(%) results for One-Class Novelty Detection. The best results are marked in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Quantitative comparison with different backbones.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ganomaly: Semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2018</title>
		<editor>C. V. Jawahar, Hongdong Li, Greg Mori, and Konrad Schindler</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mvtec ad -a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sindy</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distilling knowledge via knowledge review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5008" to="5017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sub-image anomaly detection with deep pyramid correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Padim: A patch distribution modeling framework for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Defard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Setkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelique</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romaric</forename><surname>Audigier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS&apos;18</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems, NIPS&apos;18<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9781" to="9791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Divide-and-assemble: Learning block-wise memory for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8791" to="8800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hrn: A holistic approach to one class learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="19111" to="19124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc. 4</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cutpaste: Self-supervised learning for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9664" to="9674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection -a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anomaly detection in nanofibrous materials by cnn-based self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavio</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raimondo</forename><surname>Schettini</surname></persName>
		</author>
		<idno>2018. 3</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning memory-guided normality for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoun</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ocgan: One-class novelty detection using gans with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2898" to="2906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">G2d: generate to detect anomaly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masoud</forename><surname>Pourreza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahram</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Khaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samir</forename><surname>Bouindour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hichem</forename><surname>Snoussi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Panda: Adapting pretrained features for anomaly detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liron</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2806" to="2814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling the distribution of normal data in pre-trained deep features for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorit</forename><surname>Merhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6726" to="6733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Hossein Rohban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14051</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niousha</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroosh</forename><surname>Baselizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Rohban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiresolution knowledge distillation for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rabiee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="14902" to="14912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">f-anogan: Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Support vector data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention guided anomaly localization in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashanka</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rajat Vikram Singh, and Abhijit Mahalanobis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="485" to="503" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Student-teacher feature pyramid matching for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning unsupervised metaformer for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jhih-Ciang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding-Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chiou-Shann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Fuh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Fashionmnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning semantic context from normal samples for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaidong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Patch svdd: Patch-level svdd for anomaly detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2020-11" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<editor>Edwin R. Hancock Richard C. Wilson and William A. P. Smith</editor>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="87" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Old is gold: Redefining the adversarially learned one-class classifier training paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ha</forename><surname>Muhammad Zaigham Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Ik</forename><surname>Astrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="14183" to="14193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reconstruction by inpainting for visual anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitjan</forename><surname>Zavrtanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijel</forename><surname>Sko?aj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Encoding structure-texture relation with p-net for anomaly detection in retinal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK, Au</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="360" to="377" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XX 16</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Complementary relation contrastive distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinguo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yakun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aijun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="9260" to="9269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Medical out-ofdistribution analysis challenge 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zimmerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>J?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Full</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ro?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annika</forename><surname>Reinke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lena</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Maier-Hein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
