<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Estimating Image Depth in the Comics Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deblina</forename><surname>Bhattacharjee</surname></persName>
							<email>deblina.bhattacharjee@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Communication Sciences</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Everaert</surname></persName>
							<email>martin.everaert@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Communication Sciences</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
							<email>mathieu.salzmann@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Communication Sciences</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>S?sstrunk</surname></persName>
							<email>sabine.susstrunk@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Communication Sciences</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Estimating Image Depth in the Comics Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: I) Estimating depth in the comics domain is subject to many challenges, including a) occlusions between characters; b) unusual object sizes (the bird here); c) unusual perspective; d) and e) different illustrative styles. II) Overview of our model, which uses an unsupervised I2I translation method to translate the comics image to the natural image domain and then, employs a contextual depth estimator with Laplacian edges and a feature-based GAN to ultimately predict depth. III) Comparative depth estimation results using MIDAS [34], Contextual Depth Estimation (CDE) [20] and Ours. IV) Application of our method to image retargeting, where our depth estimation model guides the retargetting model [3].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Estimating the depth of comics images is challenging as such images a) are monocular; b) lack ground-truth depth annotations; c) differ across different artistic styles; d) are sparse and noisy. We thus, use an off-the-shelf unsupervised image to image translation method to translate the comics images to natural ones and then use an attention-guided monocular depth estimator to predict their depth. This lets us leverage the depth annotations of existing natural images to train the depth estimator. Furthermore, our model learns to distinguish between text and images in the comics panels to reduce text-based artefacts in the depth estimates. Our method consistently outperforms the existing state-ofthe-art approaches across all metrics on both the DCM and eBDtheque images. Finally, we introduce a dataset to evaluate depth prediction on comics. Our project website can be accessed at https://github.com/IVRL/ ComicsDepth.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Depth estimation for comics images can provide important information for applications such as comics image retargeting <ref type="bibr">[8,</ref><ref type="bibr" target="#b35">36]</ref>, scene reconstruction <ref type="bibr">[10]</ref> and reconfiguration of comics <ref type="bibr" target="#b34">[35]</ref>, i.e., transferring the stories from paper to an interactive graphical media, for instance, video games based on comics or comics animations. The problem of depth estimation can be framed as that of predicting a metric depth for each pixel in a given input image. In comics, the depth estimation problem is monocular, which makes it inherently ill-posed <ref type="bibr" target="#b33">[34]</ref>. This is further complicated by the fact that most scenes in the comics domain have large content variations, object occlusions, geometric detailing (different perspectives and size scales), sparse or noisy scenes and non-homogeneous illustrations as shown in <ref type="figure" target="#fig_2">Figure 1</ref>. As a consequence, while estimating the depth of a comics scene is easy for humans, it remains highly challenging for computational models.</p><p>To address this, we explore the extensive research done in the field of monocular depth estimation over the past years, which reports computational models that leverage monocular cues, such as perspective information, object sizes, object localization, and occlusions, to estimate the depth of scenes <ref type="bibr" target="#b29">[30]</ref>. Note that, while much work has also been done for depth estimation from stereo images <ref type="bibr">[4,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> or video sequences <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref>, such approaches do not match the monocular setting we face in the comics domain.</p><p>Because the state-of-the-art monocular depth estimation models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref> have been trained on natural images, they fail to predict the depth of comics images accurately, resulting in vague, overlapping or missing objects ( <ref type="figure" target="#fig_2">Figure 1</ref>). An immediate solution would be to retrain the depth models on comics images, either in a supervised manner, which would in turn require ground-truth depth annotations of comics images, or in an unsupervised manner, which would require employing domain adaptation techniques <ref type="bibr" target="#b48">[49]</ref>. As there exist no dataset with ground-truth depth annotations for comics images and manually annotating the depth of a large number of comics images would be expensive and timeconsuming, we employ an unsupervised image-to-image (I2I) translation method <ref type="bibr">[5]</ref> to translate the images from the comics domain to the real one. Once translated to the real domain, we leverage the ground-truth depth of real images to train our depth model and thereby predict the depth of the translated comics??real image. The result of this process, compared to the direct application of a trained depth estimation network, is shown in <ref type="figure" target="#fig_3">Figure 2</ref>.</p><p>To improve the performance of the depth estimation, we exploit contextual attention, both spatially and channelwise, as focusing on the scene context parallels how humans estimate the depth of a scene. To this end, we introduce a local context model that leverages a Laplacian edge detector to guide depth estimation. This builds on the intuition that depth features significantly depend on edge cues and yields a sharper foreground vs. background separation. Furthermore, we incorporate a feature-based GAN that encourages the inner feature representations of the depth model to follow similar distributions for the real and translated images. Additionally, we include a text detector in our model to remove the artefacts in the depth predictions arising from the text or speech balloons in comics images.</p><p>Our main contributions therefore are as follows:</p><p>? We introduce a cross-domain depth estimation method by leveraging an off-the-shelf unsupervised I2I trans-Comics Input CDE <ref type="bibr" target="#b19">[20]</ref>-Comics Image Translated Image CDE <ref type="bibr" target="#b19">[20]</ref>-Translated Image <ref type="figure" target="#fig_3">Figure 2</ref>: Leveraging monocular depth estimation models. When employed directly on a comics image, the stateof-the-art monocular depth estimation model <ref type="bibr" target="#b19">[20]</ref> fails to predict accurate depth. We therefore, translate the comics image to the natural image domain and then apply the CDE depth estimator as mentioned in <ref type="bibr" target="#b19">[20]</ref>.</p><p>lation method. ? We exploit the contextual information for depth prediction of a given scene. We use an inner featurebased GAN to enforce similarity between the domains, as well as a Laplacian edge detector to obtain distinct foreground vs. background separations. ? By introducing a text detector in our cross-domain depth model, we reduce the artefacts from text and speech balloons in the depth predictions, which are specific to comics. ? Finally, we introduce a benchmark dataset for comics images with 450 manually annotated image-depth pairs comprising 300 images from the standard DCM <ref type="bibr" target="#b32">[33]</ref> validation dataset and 150 images from the standard eBDtheque <ref type="bibr" target="#b11">[12]</ref> validation dataset. This can be used as a benchmark for future papers for depth evaluations, as there is no existing benchmark with depth annotations for comics.</p><p>Our experiments on our manually annotated benchmark show that our approach outperforms the state-of-the-art unsupervised monocular depth estimation methods across all the different comics styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Monocular Depth Estimation</head><p>Over the past decade, there has been a significant development in monocular depth estimation. Laina et al. <ref type="bibr" target="#b24">[25]</ref> proposed fully convolutional networks with the fast upprojection method using residual learning to model the mapping between RGB images and depth maps. Kuznietsov et al. <ref type="bibr" target="#b23">[24]</ref> introduced a semi-supervised approach to overcome the deficiency and limitation of sparse ground-truth lidar maps. Godard et al. <ref type="bibr" target="#b10">[11]</ref> suggested unsupervised training objective to replace the use of labeled depth maps. The network generates the left and right disparity maps and calculates the reconstruction, smoothness, and left-right consistency losses. Guo et al. <ref type="bibr" target="#b12">[13]</ref> incorporated a synthetic depth dataset to acquire a considerable amount of groundtruth images. Subsequently, they trained a network with synthetic data and fine-tuned with a real dataset. Finally, they mitigated the domain gap between the ground-truth and synthetic dataset by distilling stereo networks. Amirkolaee and Arefi <ref type="bibr">[1]</ref> constructed a depth prediction network with the encoder-decoder and skip connection structure to integrate the global and local contexts. In <ref type="bibr" target="#b19">[20]</ref>, a context based monocular depth estimation method exploits the contextual information between objects via inter object attention to extract visual cues for estimating depth. While these approaches produce improved and consistent depth results, training them is challenging because of 1) inherently different representations of depth: direct vs. inverse depth representations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>, 2) scale ambiguity: for some data sources, depth is only given up to an unknown scale <ref type="bibr">[6,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, 3) shift ambiguity: some datasets provide disparity only up to an unknown global disparity shift <ref type="bibr" target="#b43">[44]</ref>. Further, in the presence of occluded regions (i.e. groups of pixels that are seen in one image but not the other), these methods produce meaningless values due to failed disparity calculations. To mitigate this, in <ref type="bibr" target="#b33">[34]</ref>, the authors propose a new loss function that is invariant to both scale and global shift so that the monocular depth estimation model can learn from diverse ground-truth depth maps obtained from disparate domains. Nevertheless, it does not generalise well to either paintings or comics domain.</p><p>With the development of image style transfer and its connection with domain adaptation, researchers adopted the style transfer and adversarial training to estimate depth maps in real scenes <ref type="bibr">[2,</ref><ref type="bibr" target="#b22">23]</ref>, which relied on the models trained with a large amounts of synthetic data. DispNet <ref type="bibr" target="#b28">[29]</ref> was the first network that introduced image style transfer for depth estimation. Thereafter, Zheng et al. <ref type="bibr" target="#b48">[49]</ref> proposed a two-module domain adaptive network, T2Net, where one module was trained with synthetic and real images and reconstructed each other with the reconstruction loss and generative adversarial loss <ref type="bibr">[7,</ref><ref type="bibr" target="#b21">22]</ref>, and these outputs were input into the other module to predict the real depth maps. As this method is close to our approach, we consider the T2Net as a baseline for comparison. Besides, there are more models with cycle consistency <ref type="bibr" target="#b47">[48]</ref>, cross-domain <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">41]</ref>, and others for domain adaptation to predict monocular depth maps. In this vein, we apply an unsupervised I2I translation method to minimize the domain disparity between comics and real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Domain Adaptation via I2I Translation</head><p>The advent of I2I translation methods began with the invention of conditional GAN <ref type="bibr" target="#b30">[31]</ref>, which have been applied to a multitude of tasks, such as scene translation <ref type="bibr" target="#b17">[18]</ref> and sketch-to-photo translation <ref type="bibr" target="#b42">[43]</ref>. While conditional GANs yield impressive results, they require paired images during training. Unfortunately, in comics??real I2I translation scenario, such paired training data is lacking and expensive to collect. To overcome this, cycleGAN <ref type="bibr" target="#b49">[50]</ref>, with its cycle consistency loss between the source and target domains, is a possible solution for translating the comics images to real images, thereby producing consistent images. Nevertheless, neither conditional GANs, nor cycleGAN account for the multi-modality of comics??real I2I translation; in general, a single comics image can be translated to real domain in many different, yet equally realistic ways. This is also due to the different artistic styles present in a single comics domain, which in turn, gives rise to intra-comics domain style variability. Addressing this issue of multi-modality, more recently, MUNIT <ref type="bibr" target="#b16">[17]</ref> and DRIT <ref type="bibr" target="#b25">[26]</ref> introduced solutions by learning a disentangled representation with a domaininvariant content space and a domain-specific attribute/style space. While effective, all the above-mentioned methods perform image-level translation, without considering the object instances. As such, they tend to yield less realistic results when translating complex scenes with many objects. This is also the task addressed by INIT <ref type="bibr" target="#b37">[38]</ref> and DUNIT <ref type="bibr">[5]</ref>. While INIT <ref type="bibr" target="#b37">[38]</ref> proposed to define a style bank to translate the instances and the global image separately, DUNIT <ref type="bibr">[5]</ref> proposed to unify the translation of the image and its instances, thus preserving the detailed content of object instances. We, therefore, use DUNIT <ref type="bibr">[5]</ref> as our I2I translation model to translate the comics images to real domain. Once translated, we leverage a depth estimator trained with depth annotations from real images, to ultimately, predict the depth of comics images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation and Overview</head><p>We aim to learn a cross-domain depth mapping between two visual domains C ? R H?W ?3 and R ? R H?W ?3 , where C is the comics domain and R is the real image domain. To this end, first we employ the DUNIT model <ref type="bibr">[5]</ref> to translate the given comics image to the real domain. Second, we use a contextual monocular depth estimator on the translated image. Thus, the problem can be formulated as D c = f (R(C)), where D c is the depth prediction for the given comics image C, R(C) is the comics??real translated image and f (R(c)) is the depth estimator trained on real images and applied to R(c). The detailed architecture  <ref type="bibr" target="#b19">[20]</ref>. Bottom Right: Local Context Module <ref type="bibr" target="#b19">[20]</ref> with the added Laplacian in the spatial attention branch. of our method is provided in <ref type="figure" target="#fig_0">Figure 3</ref>. We now explain the components of our network in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>To handle unpaired training images between the comics and real domains, we follow the cycle-consistency approach. In essence, this process mirrors that described in DUNIT <ref type="bibr">[5]</ref>. Additionally, to study the effect of the I2I translation model on the performance of the depth estimator, we replace the DUNIT method with CycleGAN <ref type="bibr" target="#b49">[50]</ref> and DRIT <ref type="bibr" target="#b25">[26]</ref>. These methods do not reason about the instance-level translations and thus, perform poorly in contrast to DUNIT. We report these results in the next section. Below, we detail the loss function and training procedure for the resulting I2I translation based depth model.</p><p>Image-to-image translation module. Our method is built on the DUNIT <ref type="bibr">[5]</ref> backbone which embeds the input images onto a shared style space and a domain specific content space. As such, we use the same weight-sharing strategy as DUNIT for the two style encoders (E s x , E s y ) and exploit the same loss terms. They include:</p><formula xml:id="formula_0">? A content adversarial loss L content adv (E c x , E c y , D c ) rely-</formula><p>ing on a content discriminator D c and the two content encoders (E c x , E c y ), whose goal is to distinguish the content features of both domains;</p><formula xml:id="formula_1">? Domain adversarial losses L x adv (E c y , E s x , G x , D x ) and L y adv (E c x , E ci x , E s y , G y , D y )</formula><p>, one for each domain, with corresponding domain classifiers D x and D y , corresponding domain generators G x and G y and instance content encoder E ci x ;</p><formula xml:id="formula_2">? A cross-cycle consistency loss L cc 1 (G x , G y , E c x , E ci x , E c y , E s x , E s y )</formula><p>that exploits the disentangled content and style representations for cyclic reconstruction <ref type="bibr" target="#b44">[45]</ref>;</p><formula xml:id="formula_3">? Self-reconstruction losses L x rec (E c x , E ci x , E s x , G x ) and L y rec (E c y , E s y , G y )</formula><p>, one for each domain, ensuring that the generators can reconstruct samples from their own domain;</p><p>? KL losses for each domain L x KL (E s x ) and L y KL (E s y ) encouraging the distribution of the style representations to be close to a standard normal distribution;</p><formula xml:id="formula_4">? Latent regression losses L x lat (E c x , E ci x , E s x , G x ) and L y lat (E c y , E s y , G y )</formula><p>, one for each domain, encouraging the mappings between the latent style representation and the image to be invertible;</p><p>? An instance consistency loss L ic 1 (P xi tl , P yi tl , P xi br , P y br ) encouraging the same object instances to be detected in the source domain image and in the corresponding image after translation, where P During training, the I2I module is trained along with the depth estimation module in an end-to-end manner. It has been observed in <ref type="bibr">[2,</ref><ref type="bibr" target="#b48">49]</ref> that an end-to-end approach yields consistent results on unknown domains, though it comes with a computational overhead. In our method, this computational cost depends mainly on the employed I2I translation module. For instance, DUNIT <ref type="bibr">[5]</ref> has a greater computational overhead than DRIT <ref type="bibr" target="#b25">[26]</ref> or CycleGAN <ref type="bibr" target="#b49">[50]</ref>. For further details we point the reader to the supplementary material.</p><p>Depth estimation module. As shown in <ref type="figure" target="#fig_0">Figure 3</ref>, our depth estimation module is an encoder-decoder model with skip connections in between the encoder and decoder layers. These skip connections model the local context in between the visual features by taking into account the spatial and channel attention. The architecture of our depth estimator is inspired from <ref type="bibr" target="#b19">[20]</ref>. It relies on a Global Context Module (GCM), mirroring that of <ref type="bibr" target="#b19">[20]</ref>, which explores the context of the entire scene, whereby it computes the spatial and channel attention between the objects present in the global image. To this end, the GCM is placed at the end of the encoder to obtain the global context information and pass meaningful features to the decoder. We further complement the GCM with a local context module processing the features extracted at different layers in the encoder of the depth estimator as shown in <ref type="bibr" target="#b19">[20]</ref>. Moreover, to clearly contrast the edge boundaries of the objects, we incorporate a Laplacian edge detector <ref type="bibr" target="#b15">[16]</ref> to the spatial branch of the local context module. Since depth leverages low-level visual cues, such as edge information, we have observed this Laplacian to facilitate depth estimation. In particular, the local context module feature (shown in blue in <ref type="figure" target="#fig_0">Figure 3</ref>), extracted by the encoder of the depth estimator, is processed spatially and channel-wise before being fed into the decoder layer. While the channel-wise processing mirrors that of CDE <ref type="bibr" target="#b19">[20]</ref>, the spatial processing (or the spatial attention branch as shown in <ref type="figure" target="#fig_0">Figure 3</ref>) employs multiple ASPP <ref type="bibr" target="#b14">[15]</ref> and convolutions to obtain a spatially-pooled feature, which is then multiplied with the original local context module feature and the Laplacian <ref type="bibr" target="#b15">[16]</ref>. Finally, the features from both the spatial and channel branch are added to the origi-nal feature, to produce the processed local context feature. This feature is fed into the decoder layer.</p><p>Our method uses two depth estimators, one taking the real images as input and the other the translated images. We use the zero-shot cross domain MIDAS model <ref type="bibr" target="#b33">[34]</ref> to generate pseudo ground-truth depth for the real domain. Note, however, that we could use any existing real-image dataset with ground-truth depth annotations, such as KITTI <ref type="bibr">[9]</ref> or NYU <ref type="bibr" target="#b31">[32]</ref>. However, these datasets are restrictive on the diversity of their scenes, i.e., they are not representative of the extreme scene diversity in comics that contain both indoor and outdoor scenes. Therefore, we use the MIDAS model, which was trained on a collection of five diverse real-world datasets comprising both indoor and outdoor scenes. We generate the pseudo ground-truth only once, before training our depth estimators.</p><p>Nevertheless, MIDAS fails when directly applied on comics images (shown in supplementary <ref type="figure" target="#fig_3">Figure 2</ref>), hence the need for our cross-domain context aware depth estimators. To train them, we initialize both with the MIDAS weights, setting a low learning rate of 10 ?6 to update the weights for 100 epochs with the Adam optimizer and the default hyper-parameters of <ref type="bibr" target="#b19">[20]</ref>. During the training phase, we use a shift and scale-invariant log loss function <ref type="bibr" target="#b33">[34]</ref> as objective function L depth for the depth estimator in the real domain. It can be expressed as</p><formula xml:id="formula_5">L depth (y, y * ) = 1 n i d 2 i ? 1 2n 2 i d i 2 ,<label>(1)</label></formula><p>where d i = log(y i ) ? log(y * i ), y is the predicted depth, y * is the pseudo ground-truth depth in the real domain and n is the number of pixels indexed by i.</p><p>As it learns the depth mappings, the depth estimator in the real domain shares its weight with the estimator in the comics??real translated domain. We then add an adversarial loss L adv to train the feature-based GAN between the two depth estimators <ref type="bibr" target="#b48">[49]</ref>, which encourages the inner feature representations of the two depth estimators to share similar distributions, since both stylistically represent real images. This loss is written as</p><formula xml:id="formula_6">L adv (f, D depth ) = E f c ? ?f C [log(1 ? D depth (f c ? ))] +E f r ? ?f R [log(D depth (f r ? ))] ,<label>(2)</label></formula><p>where f C and f R represent the encoded features extracted by the encoder of the depth estimators in the translated domain and the real domain respectively, and D depth is the discriminator of the feature GAN. Altogether, we write the overall objective function to train our depth estimators as</p><formula xml:id="formula_7">L obj (f, D depth ) = ? adv L adv (f, D depth )+ ? depth L depth (f ) .<label>(3)</label></formula><p>Text detection module. When the comics images are translated to the real domain, the translated images comprise text areas or speech balloons, which are in turn unknown to the depth estimator trained on the real domain. This leads to text-based artefacts in the depth results as the depth estimator considers such text areas as objects. Therefore, to control the position of the text areas in the translated images, we train a U-net <ref type="bibr" target="#b36">[37]</ref> in a supervised manner using the eBDtheque dataset <ref type="bibr" target="#b11">[12]</ref>, which contains text/speech balloon annotations. We mask the depth maps by multiplying them pixel-wise with the compliment of the textarea mask, before using the L1-loss between the (masked) pseudo ground-truth depth and the depth predictions. The detailed architecture for our method with the text detection module is given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>To validate our method, we conduct experiments on the following datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The main datasets used for this work are DCM <ref type="bibr" target="#b32">[33]</ref> and eBDtheque <ref type="bibr" target="#b11">[12]</ref> for the comics domain and the COCO dataset <ref type="bibr" target="#b26">[27]</ref> for the real-world domain. The DCM dataset comprises 772 full-page images with multiple comics panel images within. We extract 4470 single panel images from these full-page images using the panel annotations. Note that the panel annotations do not contain depth information. We thus, use these DCM panel images to train the I2I model. The eBDtheque dataset contains 100 full-page images with multiple comics panel images within. Again, we extract 850 single panel images as before. The eBDtheque dataset contains annotations for speech balloons and text lines, which we use to train a U-net <ref type="bibr" target="#b36">[37]</ref> to predict the text areas in a comics image. The detected text areas are then used by our depth model to remove text-based artefacts from the depth predictions. We employ the MS-COCO dataset <ref type="bibr" target="#b26">[27]</ref>, comprising 5000 real-world images, as realworld domain to train the I2I model.</p><p>Benchmark for evaluation. To evaluate and compare the different depth models, we introduce a benchmark including 300 DCM <ref type="bibr" target="#b32">[33]</ref> images and 150 eBDetheque <ref type="bibr" target="#b11">[12]</ref> images, from their validation set, along with the corresponding manually annotated ground-truth depth orderings, as illustrated in <ref type="figure">Figure 4</ref>. To manually annotate their depth, we carefully select 450 images from DCM and eBDtheque validation sets, such that, they contain diverse scenes across ten different artistic styles. These images were further tested for inter-observer variability, for instance, their diversity and artistic styles were analysed by three comics domain experts. Further, all three observers tested the manually annotated depth of comics images. We use depth orderings to an- <ref type="figure">Figure 4</ref>: Benchmark for evaluation. Left: Illustration of the idea of inter-object and intra-object depth ordering, used to annotate the comics images. The closer object is assigned a lower first number l 1 ; and the closer point within the same object is assigned a lower second number l 2 . Right: Annotated example from the benchmark. notate the images. In particular, the image pixel coordinates (x, y) are assigned two numbers (l 1 , l 2 ). The first number, l 1 , represents the inter-object depth ordering, such that two different l 1 values imply two different objects. Closer objects are assigned a lower number. The second number, l 2 , represents the intra-object depth ordering, such that annotations with the same first number but different second numbers indicate that the two points belong to the same object. A lower l 2 value indicates a closer point on the same object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>To evaluate our method, we evaluate the following four standard performance metrics, as used in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Absolute relative difference (AbsRel). The absolute relative difference is given by 1 |N | y?N |y ? y * |/y * where N is the number of available pixels in the manually annotated ground-truth.</p><p>Squared relative difference (SqRel). The squared relative difference is defined as 1 |N | y?N ?y ? y * ? 2 /y * . Root mean squared error (RMSE). The root mean squared error is defined as</p><formula xml:id="formula_8">1 |N | y?N ?y ? y * ? 2 . RMSE (log). The RMSE (log) is defined as 1 |N | y?N ? log y ? log y * ? 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Results</head><p>To evaluate our method, we compare it with the following four state-of-the-art depth estimation approaches.</p><p>? T2Net <ref type="bibr" target="#b48">[49]</ref>, which comprises a depth prediction model trained on synthetic image-depth pairs.</p><p>? Song et al. <ref type="bibr" target="#b38">[39]</ref>, which incorporates a Laplacian pyramid into the decoder architecture. In particular, the encoded features are fed into different streams for decoding depth residuals, defined by the Laplacian pyramid, and the corresponding outputs are progressively combined to reconstruct the final depth map from coarse to fine scales.</p><p>? MIDAS <ref type="bibr" target="#b33">[34]</ref>, which introduces a scale and shiftinvariant loss to estimate depth from a large collection of mixed real-world datasets, thereby presenting a depth model that generalises across multiple realworld datasets.</p><p>? CDE <ref type="bibr" target="#b19">[20]</ref>, which proposes an architecture that leverages contextual information in a given scene for monocular depth estimation. Thus, using the contextual attention it obtains meaningful semantic features to enhance the performance of the depth model.</p><p>We report the standard evaluation metrics for our method in comparison with the four state-of-the-art methods in <ref type="table">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> on the DCM and eBDtheque images, respectively. Note that to report the performance metrics, we compare the predicted depth by each method with our manually annotated ground-truth depth. For the results in <ref type="table">Table 1</ref>, we use the 300 manually annotated DCM imagedepth pairs from our benchmark. Further, for the results in <ref type="table" target="#tab_1">Table 2</ref>, we use the 150 manually annotated eBDtheque image-depth pairs from our benchmark. Our method outperforms the baselines on all the performance metrics for both DCM and eBDtheque images. Note that to evaluate the performance of the four state-of-the-art methods, the comics image is translated to the real domain using a pretrained DUNIT model and then, the respective methods are applied to predict its depth. This is imperative as the above state-of-the-art methods are trained on real domain, and thus to evaluate them fairly on comics, we translate the comics image to the real domain. To maintain consistency, we also evaluate our approach on the translated comics??real image. Nevertheless, our approach can also be directly applied on a comics image to predict its depth. We show this qualitatively in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head><p>In <ref type="figure">Figure 5</ref>, we compare our method with the depth predictions obtained by MIDAS <ref type="bibr" target="#b33">[34]</ref> and CDE <ref type="bibr" target="#b19">[20]</ref>. The examples demonstrate that our network can benefit from I2I translation in addition to the feature-based GAN and Laplacian. Moreover, we also qualitatively show the effect of our text-detection module. For instance, in the middle row of <ref type="figure">Figure 5</ref>, while MIDAS and CDE have text-based artefacts in the predictions, including vague depth values in the background from the speech balloons and incorrect depth from the text box in the foreground, our method correctly removes the speech balloon artefacts. Further, our model predicts the human object in the same depth plane as that of the text box in the foreground. Note that these predictions  were verified by comics domain experts. Our method threfore, yields sharper depth maps with clearer foreground vs. background separation and with well-defined object edges. Furthermore, in contrast to the baselines, the depth predictions by our method show greater consistency in their intraobject and inter-object depth values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>We now evaluate different aspects of our method. First, we study the influence of the I2I translation module on our depth model (including the feature GAN, Laplacian and the text module). To this end, we compare the results obtained using our depth model with the different state-of-theart I2I method, namely, cycleGAN <ref type="bibr" target="#b49">[50]</ref>, DRIT <ref type="bibr" target="#b25">[26]</ref> and DUNIT <ref type="bibr">[5]</ref>. We report the AbsRel, SqRel, RMSE and RMSE (log) on the DCM validation images <ref type="bibr" target="#b32">[33]</ref> from our benchmark in <ref type="table">Table 3</ref>. We observe that DUNIT consistently improves the results across all metrics, thereby demonstrat-Comics input MIDAS <ref type="bibr" target="#b33">[34]</ref> CDE <ref type="bibr" target="#b19">[20]</ref> Our <ref type="figure">Figure 5</ref>: Qualitative comparison of depth estimation on the translated DCM validation images <ref type="bibr" target="#b32">[33]</ref>   <ref type="bibr" target="#b25">[26]</ref> 0.269 0.333 0.983 0.317 DUNIT <ref type="bibr">[5]</ref> 0.251 0.318 0.971 0.305 <ref type="table">Table 3</ref>: Ablation Study on the effect of I2I model. We compare the effect of the different I2I translation model on our method. We report the four standard performance metrics (lower the better). Our method with the DUNIT model gives the best result. The best results are in bold and the second-best are underlined. Note that the DCM validation images <ref type="bibr" target="#b32">[33]</ref> from our annotated benchmark were used for this ablation study.</p><p>We then turn to exploring the effect of the feature GAN, Laplacian and text detection module on our method. To this end, we add each of these components one-by-one to the baseline approach comprising the DUNIT model and the CDE model, shown as I2I+depth in <ref type="table">Table 4</ref>. Note that this baseline approach is trained in an end-to-end manner. We report the standard four performance metrics on the DCM <ref type="bibr" target="#b32">[33]</ref> images from our benchmark in <ref type="table">Table 4</ref>. We show that the end-to-end baseline approach outperforms the CDE <ref type="bibr" target="#b19">[20]</ref> method when applied directly to the translated comics images, as shown in <ref type="table">Table 1</ref>. This solidifies the benefits of an end-to-end training approach. Moreover, the addition of each component of our method consistently improves the performance across all metrics. All the images were kept constant for the study of all the network components. We show qualitative results from this ablation study in our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>AbsRel?  <ref type="table">Table 4</ref>: Ablation Study on the effect of the different network components. We compare the effect of the different network components, namely, the feature GAN, Laplacian and text module on our method. We report the four standard performance metrics (lower the better). The above network components are added one-by-one and we observe that our model with feature GAN, Laplacian and text module outperforms on all performance metrics. The best results are in bold and the second-best are underlined. Note that the DCM images from our benchmark were used for this ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have introduced an approach to estimate image depth in the comics domain using unsupervised I2I translation to adapt the comics images to the real domain. To this end, we have leveraged a modified context-based depth model trained on real-world images with Laplacian. We also, have added a feature GAN approach to the depth estimators to enforce the semantic similarity between the translated and real images. We have further added a text-detection module to remove text-based artefacts in the depth predictions. To validate our experiments, we introduce a benchmark with manually annotated depth for images from the validation set of DCM and eBDtheque datasets, as there is no existing benchmark with depth annotations. In our experiments, our I2I translation-based modified depth estimators with Laplacian, feature GAN and text-detections, outperform the state-of-the-art methods. This is the first automated method to predict depth for comics images. Therefore, this work can be used for applications like comics image retargeting, scene reconstruction, comics animations or repurposing comics to augmented reality. In this supplementary material, we provide details about the text-detection module, additional qualitative comparison for the state-of-the-art methods with our approach, qualitative results for the ablation study of our network and an analysis on the computational cost of our network components. The document is structured as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimating Image Depth in the Comics Domain (Supplementary)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Text-detection Module</head><p>The generated real-images from the DUNIT [2] model have speech-balloons or text present in them, which are not recognised by the depth estimators trained on realdomain images. Therefore, the predicted depths contain text-based artefacts. In order to remove these artefacts, we use the text-detection module shown in <ref type="figure" target="#fig_2">Figure 1</ref>. Our textdetection module is a U-Net <ref type="bibr">[8]</ref>, trained in a supervised manner, on the text/ speech-balloon annotations from the eBDtheque <ref type="bibr">[3]</ref> dataset. The trained U-Net <ref type="bibr">[8]</ref> is then, used on the DCM <ref type="bibr">[6]</ref> training images to detect the text/ speechballoon areas in them, in the form of text masks. These text masks are then used to generate the text adder 'groundtruth' given by (1 ? M )A + M B, where M is the text mask, (1 ? M ) is its complement, A is the comics??real translated image (but without the text area) and B is the original comics image (containing the text area). Once the text adder 'ground-truth' is created, we train a text-adder generator with A, B and M as input. This generator takes the position of the mask, M , in the original comics image, B, and applies this positional information onto the comics??real translated image, A, to create a well-defined text area on the translated image. This generated output is trained using an L1 loss with the text adder 'ground-truth'. The reason to create a translated image with a well-defined text area is shown in <ref type="figure" target="#fig_2">Figure 1</ref>, top row, where we can see that a translated image when generated without the text area information contains text-based artefacts, which in turn, gives incorrect depth values after being fed into the depth estimator. However, the text-adder generator output produces no such text-based artefacts and gives a better depth prediction.</p><p>After the translated image with a well-defined text area is created, its fed into our depth estimator to predict the depth of the translated image with the text. Concurrently, the real image is passed to the other depth estimator to predict the depth of the real image. Both these estimators are trained in an end-to-end manner. Furthermore, to predict the depth of the translated image without the depth values from the text masks, we multiply the complement of the text mask with the prediction. This results in a clean depth prediction without any text-based artefacts. During inference, our approach can be directly applied on the original comics image with text. However, for fair comparison with the baseline approaches, we translate the comics image with text to a real image using a pretrained DUNIT (without the textdetection module) and then apply the different methods to predict their depth. As our approach has been trained with text information, it learns to separate the text-based artefacts and thus, produces a superior depth map. In <ref type="figure" target="#fig_0">Figure 3</ref>, last column, we observe the effect of our text module on the depth predictions for an input comics image from the DCM validation set of our benchmark (Please zoom in to observe the differences in the depth predictions).</p><p>Note that for our final approach (consisting I2I, depth, feature GAN, Laplacian and the text module), we use comics images without text areas to train our I2I module. This is done to facilitate the generation of real images without text artefacts (referred to as 'A' in <ref type="figure" target="#fig_2">Figure 1</ref>). To this end, we discuss the method to generate the original comics images without the text areas, in what follows.</p><p>Generating the comics-without-text dataset. To remove the text areas from the original comics, we randomly crop the original images along with their respective text mask prediction obtained by the trained U-Net <ref type="bibr">[8]</ref>, to a 384 x 384 size. We then, decrease the crop size by 1 unit per dimension, i.e., the image is cropped to 383 x 383, followed by 382 x 382, and so on. We repeat this process until the maximum area of the text in the image is 3% of the total im-age. After cropping, these images were checked manually for any remaining text areas and we found that none of the images contained significant text in them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Qualitative Comparison-Depth Results</head><p>We show the depth predictions on the translated comics images as reported in <ref type="figure">Figure 5</ref> of the main paper. Further, we show that the state-of-the-art methods like MIDAS <ref type="bibr">[7]</ref> and CDE <ref type="bibr">[4]</ref>, which are trained on real-world images, fail to predict depth accurately when applied to comics images directly. Specifically, as seen in <ref type="figure" target="#fig_3">Figure 2</ref>, MIDAS is unable to predict the depth of the sample DCM <ref type="bibr">[6]</ref> validation image from our benchmark, though it is trained on a large collection of real-world images from five different real-world datasets. This raises the need for applying these methods on a comics??real translated image. As seen in <ref type="figure" target="#fig_3">Figure 2</ref>, the baseline methods of MIDAS and CDE (trained on real images), benefit from the I2I translations. During inference, we first, translate the original comics image to the real domain by using a pretrained DUNIT <ref type="bibr">[2]</ref> model and then, we apply the baseline depth estimators on these translated images. Nevertheless, our approach can predict the depth on both the translated image and the original comics image, while outperforming all the baselines in both the scenarios. This is because our approach is trained in an end-to-end manner along with the I2I module. Note that, we could also train all the baseline models (including T2Net <ref type="bibr">[10]</ref>, Song et.al <ref type="bibr">[9]</ref> and MIDAS <ref type="bibr">[7]</ref>), from scratch, in an end-to-end manner with our I2I model, but this was not addressed. This is because we observed that training the baseline method of CDE <ref type="bibr">[4]</ref> from scratch, in an end-to-end manner, results in poorer results than our approach, as shown in <ref type="table">Table 4</ref> (first row) in our main paper. Note that though CDE was the best performing baseline method, it fails in comparison to our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Qualitative Results-Ablation Study</head><p>We validate the results observed in <ref type="table">Table 4</ref> of the main paper with additional qualitative results. In <ref type="figure" target="#fig_0">Figure 3</ref>, we observe the effect of each of our network components on the depth predictions when applied to a translated comics image from the DCM validation set of our benchmark. Note that each network component was added one-by-one. We see, qualitatively, that the DUNIT (I2I)+ CDE (D) method, when trained in an end-to-end manner, outperforms the baseline CDE <ref type="bibr">[4]</ref> method (cross-referring to <ref type="figure" target="#fig_3">Figure 2</ref>-first row and third column). We also see that the addition of the feature-based GAN (FG) greatly benefits the depth predictions as it encourages the similarity in distribution between the comics and the real domain. Moreover, the Laplacian (L) when added to our depth estimator, refines the edge contrasts and gives a better depth prediction. However, some  <ref type="table">Table 1</ref>: Computational cost of training the different network components. We compare the cost of the different network components, namely, the I2I, depth, feature GAN, Laplacian and text module in our method. We report the percentage of the total computational time taken by each of these components. We report that the I2I module dominates the training time. Note that the above methods were trained using 4 GPUs following consistent resolution for all the input images and constant batch size.</p><p>text-based artefacts still remain in the depth prediction, resulting in vague depth values. To remedy this, we add the text module (TM) to finally, have superior depth predictions as seen in <ref type="figure" target="#fig_0">Figure 3</ref> here and <ref type="table">Table 4</ref> in our main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Computational Cost Analysis</head><p>We have seen, thus far, that training our approach in an end-to-end manner improves the predicted depth maps and thereby benefits our method. However, this leads to a computational overhead. We report the computational cost incurred by the different components of our network, when trained in an end-to-end manner in <ref type="table">Table 1</ref>. We see that the training of the I2I module dominates the computational time, regardless of the I2I method employed. This was followed by the training of the depth estimators. Note that the training in both the scenarios (i.e. using DUNIT and DRIT as the I2I module) was done using 4 V100, 7 Tflops GPUs with 32 GB memory. The total time taken by our approach with DUNIT [2] is 36 hours, while that with DRIT <ref type="bibr">[5]</ref> is 27 hours. The extra computational time for DUNIT comes from the instance-level translations. Nevertheless, the inference time for both the methods are comparable and is equal to 217 milliseconds for the method employing DUNIT and 203 milliseconds for the one with DRIT, processed on a single V100 GPU. Acknowledgement. This work was supported in part by the Swiss National Science Foundation via the Sinergia grant CRSII5?180359.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation for our text module</head><p>Overview of our approach with the text module <ref type="figure" target="#fig_2">Figure 1</ref>: Our depth estimation approach with the text-detection module. We show, (Top): the motivation for our text module and (Bottom): the overall architecture of our approach incorporating the text module. In our text module, the masks are generated using a U-Net <ref type="bibr">[8]</ref> trained on the text annotations form eBDtheque <ref type="bibr">[3]</ref> dataset. The generated masks by the trained U-Net is used to train the text adder generator and the text adder 'ground-truth' as discussed above. The generated real image with text is then fed into the depth estimator. This predicts the depth with text. To remove the text based artifacts in the depth prediction, the complement of the text mask is multiplied with the predicted depth with text to, finally, predict the depth without the text.   We show the depth predictions on the translated DCM validation images <ref type="bibr">[6]</ref> from our benchmark. We report, from left to right, the depth predictions obtained by the model comprising I2I (DUNIT <ref type="bibr">[2]</ref>) and Depth (CDE <ref type="bibr">[4]</ref>) trained in an end-to-end manner; the result using the model comprising I2I, CDE and feature GAN; the result using the model comprising I2I, CDE, feature GAN and Laplacian; and Our model (comprising I2I, CDE, feature GAN, Laplacian and the text module), respectively. Cooler colors are farther and warmer colors are nearer (Best viewed in color).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Detailed overview of our architecture. Top: Overall architecture as discussed in Section 3. Bottom Left: Global Context Module as detailed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>are the bounding box top-left and bottom-right corner pixels for detected instances in the two domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>? Section 1 :</head><label>1</label><figDesc>Text-detection Module ? Section 2: Qualitative Comparison-Depth Results ? Section 3: Qualitative Results-Ablation Study ? Section 4: Computational Cost Analysis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Qualitative comparison of depth estimation on the DCM validation images [6] from our benchmark. (Top Row): Depth predictions on the translated comics images as seen in the main paper. (Bottom Row): Depth predictions on the actual comics images (not translated</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative comparison for the ablation study showing the effect of the different network components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison (eBDtheque images).</figDesc><table><row><cell>We compare our approach with the state-of-the-art methods on the translated eBDtheque validation images [12] from our benchmark. We report the Absolute Relative Differ-ence (AbsRel), Squared Relative Difference (SqRel), Root Mean Squared Error (RMSE), and RMSE log (lower the better). Our contextual depth estimator with the feature-based GAN, Laplacian and text detection module gives the best result. The best results are in bold and the second-best are underlined.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>from our benchmark, using the text detection module (top row and middle row) and without using the text detection module (bottom row). We show, from left to right, the input image in the comics domain, the result using the MIDAS [34] model directly on the translated comics image, the result using the CDE<ref type="bibr" target="#b19">[20]</ref> model directly on the translated comics image, and Our model applied to the translated comics image, respectively.</figDesc><table><row><cell cols="4">ing the benefits of instance-level translations on our method, in contrast to the image-level translations in cycleGAN and DRIT.</cell></row><row><cell>Method</cell><cell cols="3">AbsRel? SqRel? RMSE? RMSE log?</cell></row><row><cell cols="2">CycleGAN [50] 0.282 DRIT</cell><cell>0.346 0.995</cell><cell>0.329</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Deblina Bhattacharjee, Martin Everaert, Mathieu Salzmann, Sabine S?sstrunk School of Computer and Communication Sciences, EPFL, Switzerland {deblina.bhattacharjee, martin.everaert, mathieu.salzmann, sabine.susstrunk}@epfl.ch</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>). We show, from left to right, the input image in the comics domain, the result using the MIDAS [7] model, the result using the CDE[4]  model, and Our model (comprising I2I, depth, feature GAN, Laplacian and the text module), respectively. We show that all the methods benefit from the I2I module. Further, we show that our approach can predict depth when applied both to the translated image, as well as the original comics image; while outperforming the baselines in both the scenarios. Cooler colors are farther and warmer colors are nearer (Best viewed in color).</figDesc><table><row><cell>I2I+D</cell><cell>I2I+D+FG</cell><cell>I2I+D+FG+L</cell><cell>I2I+D+FG+L+TM (Ours)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work was supported in part by the Swiss National Science Foundation via the Sinergia grant CRSII5?180359.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with geometrical guidance using a multilevel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Amini Amirkolaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Arefi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page">105714</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2800" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Seam carving for contentaware image resizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2007-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bi3d: Stereo depth estimation via binary classifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Badki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Troccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dunit: Detection-based unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deblina</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Vizier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial-learning-based image-to-image transformation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">411</biblScope>
			<biblScope unit="page" from="468" to="486" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stereoscopic image retargeting based on deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoting</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
	<note>Yuming Fang, Nam Ling, and Qingming Huang</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d scene reconstruction by stereo methods for analysis and visualization of sports scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijela</forename><surname>Markovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Science in Sport -Mission and Methods, number 08372 in Dagstuhl Seminar Proceedings</title>
		<editor>Arnold Baca, Martin Lames, Keith Lyons, Bernhard Nebel, and Josef Wiemeyer</editor>
		<meeting><address><addrLine>Dagstuhl, Germany; Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Schloss Dagstuhl -Leibniz-Zentrum fuer Informatik</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Brostow. Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ebdtheque: a representative database of comics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename><surname>Cl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Gu&amp;apos;erin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farid</forename><surname>Mercier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karell</forename><surname>Ammar-Boudjelal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Bertet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Christophe</forename><surname>Bouju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Burie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Ogier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Revel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<meeting>the 12th International Conference on Document Analysis and Recognition (ICDAR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1145" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="484" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for depth map estimation from rgb video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Kin Gwn Lore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><forename type="middle">A</forename><surname>Giering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="346" to="361" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Laplacian score for feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Y. Weiss, B. Sch?lkopf, and J. Platt</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno>abs/1804.04732</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-supervised monocular trained depth estimation using self-attention and discrete disparity volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Leveraging contextual information for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihaeng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE Access</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="147808" to="147817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust consistent video depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejian</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Monocular depth prediction using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Aran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchendra</forename><forename type="middle">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukta</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="413" to="4138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adadepth: Unsupervised content congruent adaptation for depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phani</forename><forename type="middle">Krishna</forename><surname>Uppala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semisupervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Vasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diverse imageto-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Consistent video depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proceedings of ACM SIG-GRAPH)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Single image depth estimation: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alican</forename><surname>Mertan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><forename type="middle">Jade</forename><surname>Duff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gozde</forename><surname>Unal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets. CoRR, abs/1411.1784</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet Kohli Nathan</forename><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Digital comics image indexing based on deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Nhu-Van Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Christophe</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High-quality depth from uncalibrated small motion clip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rauscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-No?l</forename><surname>Thon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Park</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comics and Videogames: From Hybrid Medialities to Transmedia Expansions</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Segmentation and indexation of complex objects in comic book images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Rigaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ELCVIA Electronic Letters on Computer Vision and Image Analysis</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Towards instance-level imageto-image translation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using laplacian pyramid-based depth residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokjae</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Hitnet: Hierarchical iterative tile refinement network for real-time stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Tankovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofien</forename><surname>Bouaziz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for depth prediction from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2396" to="2409" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Real-time self-adaptive deep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spatial pattern templates for recognition of objects with regular structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radim</forename><surname>Tyle?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radim??ra</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="364" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Web stereo video supervision for depth prediction from dynamic scenes. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Unsupervised learning for cross-domain medical image synthesis using deformation invariant cycle consistency networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gillian</forename><surname>Macnaught</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Papanastasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Macgillivray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Newby</surname></persName>
		</author>
		<idno>abs/1808.03944</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised framework for depth estimation and camera motion prediction from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbing</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiafu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huosheng</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">385</biblScope>
			<biblScope unit="page" from="169" to="185" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unsupervised learning of geometry with edge-aware depth-normal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Geometry-aware symmetric domain adaptation for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9788" to="9798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">T2net: Synthetic-to-realistic translation for solving single-image depth estimation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanxia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="767" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Seam carving for contentaware image resizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2007-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dunit: Detection-based unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deblina</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Vizier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">ebdtheque: a representative database of comics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename><surname>Cl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Gu&amp;apos;erin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farid</forename><surname>Mercier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karell</forename><surname>Ammar-Boudjelal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Bertet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Christophe</forename><surname>Bouju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Burie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Ogier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Revel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<meeting>the 12th International Conference on Document Analysis and Recognition (ICDAR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1145" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Leveraging contextual information for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihaeng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE Access</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="147808" to="147817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Diverse imageto-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Digital comics image indexing based on deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Nhu-Van Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Christophe</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using laplacian pyramid-based depth residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokjae</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">T2net: Synthetic-to-realistic translation for solving single-image depth estimation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanxia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="767" to="783" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
