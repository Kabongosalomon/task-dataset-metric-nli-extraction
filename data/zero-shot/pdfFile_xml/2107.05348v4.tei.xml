<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-shot Visual Question Answering using Knowledge Graph</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
							<email>zhuo.chen@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science &amp; Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
							<email>jiaoyan.chen@cs.ox.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxia</forename><surname>Geng</surname></persName>
							<email>gengyx@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science &amp; Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">The University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonggang</forename><surname>Yuan</surname></persName>
							<email>yuanzonggang@huawei.com</email>
							<affiliation key="aff4">
								<orgName type="laboratory">NAIE CTO Office, Huawei Technologies Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
							<email>huajunsir@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science &amp; Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-shot Visual Question Answering using Knowledge Graph</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Visual Question Answering ? Zero-shot Learning ? Knowl- edge Graph</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Incorporating external knowledge to Visual Question Answering (VQA) has become a vital practical need. Existing methods mostly adopt pipeline approaches with different components for knowledge matching and extraction, feature learning, etc. However, such pipeline approaches suffer when some component does not perform well, which leads to error cascading and poor overall performance. Furthermore, the majority of existing approaches ignore the answer bias issue -many answers may have never appeared during training (i.e., unseen answers) in real-word application. To bridge these gaps, in this paper, we propose a Zero-shot VQA algorithm using knowledge graph and a mask-based learning mechanism for better incorporating external knowledge, and present new answer-based Zero-shot VQA splits for the F-VQA dataset. Experiments show that our method can achieve state-of-the-art performance in Zero-shot VQA with unseen answers, meanwhile dramatically augment existing end-to-end models on the normal F-VQA task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual Question Answering (VQA) is to answer natural language questions according to given images. It plays an important role in many applications such as advertising and personal assistants to the visually impaired. It has been widely investigated with promising results achieved due to the development of image and natural language processing techniques. However, most of the current solutions still cannot address the open-world scene understanding where the answer is not directly contained in the image but comes from or relies on external knowledge. Considering the question "Q1: Normally you play this game with your?" in <ref type="figure" target="#fig_1">Figure 1</ref>, some additional knowledge is indispensable since that the answer "dog" cannot be found out with the content in the image alone. Some VQA methods have been developed to utilize external knowledge for open-world scene understanding. For example, Marino et al. <ref type="bibr" target="#b15">[16]</ref> extensively utilize unstructured text information from the Web as external information but fail to address the noise (irrelevant information) in the text. Wang et al. <ref type="bibr" target="#b26">[27]</ref> first extract visual concepts from images and then link them to an external knowledge graph (KG). The corresponding questions can then be transformed into a series of queries to the KG (e.g., SPARQL queries) to retrieve answers. Zhu et al. <ref type="bibr" target="#b30">[31]</ref> instead construct a multi-modal heterogeneous graph by incorporating the spatial relationships and descriptive semantic relationships between visual concepts, as well as supporting facts retrieved from KGs, and then apply a modality-aware graph convolutional network to infer the answer. However, the performance of all these methods would be dramatically impacted if one module of the pipeline does not perform that well (a.k.a. error cascading <ref type="bibr" target="#b6">[7]</ref>). Although some end-toend models such as <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2]</ref> have been proposed to avoid error cascading, they are still quite preliminary, especially on utilizing external knowledge, with worse performance than the pipeline methods on many VQA tasks.</p><p>Another important issue raised in VQA is the dependence on labeled training data, i.e., the model is trained by a dataset of (question, image, answer) tuples, and generalizes to answer questions about objects and situations that have already been presented in the training set. However, for new types of questions or answers, and objects newly emerge in images, there is a need for collecting labeled tuples and training the model from the scratch. Targeting such a limitation, Zero-shot VQA (ZS-VQA), which aims to predict with objects, questions or answers that have never appeared in training samples, has been proposed. Teney et al. <ref type="bibr" target="#b24">[25]</ref> address questions that include new words; while <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref> address images that contain new objects. However, all of these VQA methods still fo-cus on the closed-world scene understanding without considering unseen answers and rarely make full use of KG. In this paper, we utilize KG to study VQA with open-world scene understanding, which requires external knowledge to answer the question, and ZS-VQA, especially the sub-task that addresses new answers.</p><p>In this paper, we present a ZS-VQA algorithm using KG and a mask-based learning mechanism, and at the same time propose a new Zero-shot Fact VQA (ZS-F-VQA) dataset which is to evaluate ZS-VQA for unseen answers. Firstly, we learn three different feature mapping spaces separately, which are semantic space about relations, object space about support entities, and knowledge space about answers. Each of them is used to align the joint embedding of imagequestion pair (I-Q pair) with corresponding target. Via the combination between all those chosen supporting entities and relations, masks are decided according to a mapping table which contains all triplets in a fact KG, which guides the alignment process of unseen answer prediction. Specially, the marks can be used as hard masks or soft masks, depending on the VQA tasks. Hard marks are used in ZS-VQA tasks; e.g., with the ZS-F-VQA dataset, our method achieves state-of-the-art performance and far superior (30 ? 40%) to other methods. On the other hand, soft marks are used in standard VQA tasks; e.g., with the F-VQA dataset, our method achieves a stable improvement (6 ? 9%) on baseline end-to-end method and well alleviates the error cascading problem of pipeline models. To sum up, the main contributions are summarized below:</p><p>-We propose a robust ZS-VQA algorithm using KGs <ref type="bibr" target="#b5">6</ref> , which adjusts answer prediction score via masking based on the alignments between supporting entities/relations and fusion I-Q pairs in two feature spaces. -We define a new ZS-VQA problem which requires external knowledge and considers unseen answers. Accordingly, we develop a ZS-F-VQA dataset for evaluation. -Our KG-based ZS-VQA algorithm is quite flexible. It can successfully address both normal VQA tasks that rely on external knowledge and ZS-VQA tasks, and can be directly used to augment existing end-to-end models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Question Answering</head><p>Traditional VQA Methods. Since proposed in 2015 by <ref type="bibr" target="#b2">[3]</ref>, a few VQA methods, which apply multi-modal feature fusion between question and image for final answer decision, have been proposed. Various attention mechanisms <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2]</ref> are adopted to refine specific regions of the image for corresponding question meanwhile to make the prediction process interpretable. Graph-based approaches such as <ref type="bibr" target="#b5">[6]</ref> combine multi-modal information and enhance the interaction among significant entities in texts and images. Knowledge-based VQA. Utilizing symbolic knowledge is a straight forward solution to augment VQA. To study incorporating external knowledge with VQA, datasets such as F-VQA <ref type="bibr" target="#b26">[27]</ref>, OK-VQA <ref type="bibr" target="#b15">[16]</ref> and KVQA <ref type="bibr" target="#b22">[23]</ref> have been developed. Each question in F-VQA refers to a specific fact triple in relevant KG like ConceptNet. While OK-VQA is manually marked without a guided KG as reference which leads to its difficulty. KVQA targets at world knowledge where questions are about the relationship between characteristics.</p><p>To incorporate such external knowledge, <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> generate SPARQL queries for querying the constructed sub-KG according to I-Q pairs. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref> extract entities from image and question to get related concepts from KG for answer prediction. Marino et al. <ref type="bibr" target="#b15">[16]</ref> take unstructured knowledge on the Web to enhance the semantic representation of I-Q joint feature. All of the above methods utilize pipeline approaches to narrow the answer scope, but they are often ad-hoc, which limits their deployment and generalization to new datasets. Most importantly, the errors will be magnified during running since each module usually has no ability to correct previous modules' errors. End-to-end model like <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2]</ref> are more general and can avoid error cascading, but they are still preliminary, especially in addressing VQA problems which require external knowledge.</p><p>Different from these approaches, our proposed framework leverages the advantages of both end-to-end and pipeline approaches. We improve the model transferability meanwhile effectively avoid the error cascading (see our case study as illustrated in <ref type="figure" target="#fig_5">Figure 5</ref>), making it quite general to different tasks and very robust with promising performance achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Zero-shot VQA</head><p>Machine learning often follows a closed world assumption where classes to predict all have training samples. However, the real-world is not completely closed and it is impractical to always annotate sufficient samples to re-train the model for new classes. Targeting such a limitation, zero-shot learning (ZSL) is proposed to handle these novel classes without seeing their training samples (i.e., unseen classes) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b4">5]</ref>. Teney et al. <ref type="bibr" target="#b24">[25]</ref> first propose Zero-shot VQA (ZS-VQA) and introduce novel concepts on language semantic side, where a test sample is regarded as unseen if there is at least one novel word in its question or answer. Ramakrishnan et al. <ref type="bibr" target="#b21">[22]</ref> incorporate prior knowledge into model through pre-training with unstructured external data (from both visual and semantic level). Farazi et al. <ref type="bibr" target="#b8">[9]</ref> reformulate ZS-VQA as a transfer learning task that applies closely seen instances (I-Q pairs) for reasoning about unseen concepts. A major limitation of these approaches is that they seldom consider the imbalance and low resources problem regarding the answer itself. Open-answer VQA requires models to select answer with the highest activation from fixed possible K answer categories, but the model cannot tackle unseen answers because answers are isolated with no specific meaning. Besides, VQA is defined as a classification problem without utilizing enough semantic information of the answer. Agrawal et al. <ref type="bibr" target="#b0">[1]</ref> propose a new setting for VQA where the test question-answer pairs are compositionally novel compared to training question-answer pairs. Some methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref> try to align answer with I-Q joint embedding through feature representation for realizing unseen answer prediction or simply for concatenating their representation as the input of a fully connected layer for score prediction <ref type="bibr" target="#b24">[25]</ref>. However, all of them are powerless to answer those I-Q pairs that require external knowledge, and the relevance among answers are still not strong enough with insufficient external information. The ZS-VQA method proposed in this paper incorporates richer and more relevant knowledge by using KGs, through which the existing common sense is well utilized and more accurate answers are often given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Visual Question Answering (VQA) and Zero-shot VQA. A VQA task is to provide a correct answer a given an image i paired with a question q. Following the open-answer VQA setting defined in <ref type="bibr" target="#b11">[12]</ref>, let a be a member of the answer pool A = {a 1 , ..., a n }, the candidates of which are the top K (e.g. 500) most frequent answers of the whole dataset. A dataset is represented by a set of distinctive triplets D = {(i, q, a)|i ? I, q ? Q, a ? A} where I and Q are respectively image and question sets. A testing dataset is denoted as D te with each triplet (i, q, a) not belonging to training dataset D tr . We denote D zsl</p><formula xml:id="formula_0">tr = {(i, q, a)|i ? I, q ? Q, a ? A s } and D zsl te = {(i, q, a)|i ? I, q ? Q, a ? A u },</formula><p>where A s and A u respectively denote the seen answer set and the unseen answer set with A u ? A s = ?. ZS-VQA is much harder than normal VQA, since information in the image and question is insufficient for answers that have never appeared in the training samples. Specifically, we study two settings at testing stage of ZS-VQA : one is the standard ZSL, where the candidates answers of a testing sample (i, q, a) are A u , while the other is the generalized ZSL (GZSL) with A u ? A s as candidates answers during testing. It should be noted that regular VQA only predicts with seen answers, while VQA under the GZSL setting predicts with both seen and unseen answers. Knowledge Graph (KG). KGs have been widely used in knowledge representation and knowledge management <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19]</ref> . The KG we used is a subset of three KGs (DBpedia, ConceptNet, WebChild) selected by Wang et al. <ref type="bibr" target="#b26">[27]</ref> (in the form of RDF 7 triple). It is used to establish the prior knowledge connection, which includes a set of answer nodes and concept (tool) nodes to enrich the relationships among answers. Besides, different relations (edges) are applied to represent the fact graph (triples).</p><p>Taking <ref type="figure" target="#fig_1">Figure 1</ref> as an example, all (i, q) pairs could be divided into two categories according to their answer sources : 1) Those answers which are outside the images and questions. Such as the answer "dog" to question "Q1: Normally you play this game with your?", the data source of the answer here is the external KG which contains the triple &lt;frisbee, BelongTo, toy&gt; for QA support. 2) Those answers that can be found in images or questions. In this situation, there are often more than one object in image/question for screening through some implicit common sense relations (e.g., "Q2: Which object in this image is like a plate?" targets at finding the correct object related to plate ). Then, one fact triple (e.g. &lt;plate, RelatedTo, frisbee &gt;) could play the role of answer guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Idea</head><p>Our main idea is motivated by two deficiencies in current knowledge-based VQA approaches. Firstly, in those methods it is common to build intermediate models and involve KG queries in a pipeline way, which leads to error cascading and poor generalization. Secondly, most of them define VQA as a classification problem which does not utilize enough knowledge of the answers, and fails to predict unseen answers or to transfer across datasets whose candidate answers have little or no overlap. For example, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>, if concept "frisbee" has not appeared in training set, traditional VQA will fail to recognize it in testing phase for answer out-of-vocabulary (OOV) problem. While other method <ref type="bibr" target="#b11">[12]</ref> which takes answer semantics into account has lost the relation information: "Desires" came from entity "dog", or "RelatedTo" came from entity "plate". By utilizing semantics embedding feature as answer representation, we convert VQA from a classification task into a mapping task. After parameter learning, the distribution of the joint embedding between question and image can partly get close to answer's one with shadow knowledge included in. We call it the knowledge space about answers. Besides, we independently define two other feature spaces: semantic space about relations and object space about support entities. Semantic space aims to project (i, q) joint feature into a relation according to the semantic information in triplets, while object space targets at establishing relevant connection between (i, q) and a support entity (a.k.a. entity on KG ). They play the role for answer guidance when combined together (see Section 4.2 for detail). In order to overcome those limitations proposed in Section 2.1, we provide a soft/hard mask method in this situation to effectively enhance alignment process meanwhile alleviating error cascading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Establishment of Multiple Feature Spaces</head><p>Following <ref type="bibr" target="#b11">[12]</ref>, we establish connection between an answer and its corresponding (i, q) pair via projecting them into a common feature space and get close to each other. Firstly, a fusion feature extractor F ? (i, q) between q and i is leveraged to combine multimodal information. Meanwhile, we define G ? (a) as the representation of answer a. We follow the probabilistic model of compatibility (PMC) drawn from <ref type="bibr" target="#b11">[12]</ref> and add loss temperature ? for better optimization:</p><formula xml:id="formula_1">P (a | i n , q n ) = exp F ? (i n , q n ) G ? (a)/? a ?A exp F ? (i n , q n ) G ? (a ) /? (1)</formula><p>where A denotes A u when the setting is standard ZSL else remain the same, and a is the correct answer of (i n , q n ). For learning the parameters to maximize the likelihood in overall PMC model, we employ following loss function:</p><formula xml:id="formula_2">a = ? N n b?A ?(a, b) log P (b | i n , q n )<label>(2)</label></formula><p>where weighting function ?(a, b) measures how much the predicted answer b can contribute to the objective function.</p><formula xml:id="formula_3">A nature design is ?(a, b) = I[a = b], where I[.]</formula><p>denotes binary indicator function, taking value of 1 if the condition is true else 0 for false. During testing, given the learned F ? (i, q) and G ? (a), we can apply following decision rule to predict the answer? to (i, q) pair:</p><formula xml:id="formula_4">a = arg max a?A F ? (i, q) G ? (a)<label>(3)</label></formula><p>Like the results shown in Section 5.3, the above feature projection process could learn shallow knowledge in VQA which requires external knowledge. However, it performs not well since network is not sufficient to model abundant prior knowledge with small amount of training data (see data statistics in <ref type="table" target="#tab_0">Table 1</ref>).</p><p>Matching the elements in images or questions to KG entities in an explicit <ref type="bibr" target="#b26">[27]</ref> or implicit <ref type="bibr" target="#b8">[9]</ref> way can augment the model with knowledge to well address the open-world scene understanding problem (see links in <ref type="figure" target="#fig_1">Figure 1</ref> toy example). In our method, the alignment between image/question and KG is implicitly done by multiple feature spaces rather than simply object detection. We leverage another two feature spaces for answer revising:</p><p>1) Semantic space focuses on the language information within (i, q), which works as a guidance toward the projection of triplet relations r in KG. In particular, the signal of q is more crucial than i in this part.</p><p>2) Compared with traditional image classification which identifies the correct category of a given image, the object space is more likely a feature space about support entity classifier which simultaneously observes images and texts for salient feature. Specifically, the alignment between support entity e embedding and (i, q) joint embedding avoids the direct learning of complex knowledge, meanwhile acts on the subsequent answer mask process together with the prediction relations r obtained in semantic space.</p><p>Similarly, we define their embedding function as G ? (r), G ? (e) and the corresponding (i, q) joint embedding function as F ? (i, q), F ? (i, q) for distinction. Other formulas and probability calculation methods remain the same as answer such as loss function r and e , which are model's overall optimization goal together with a . The parameters in these three pairs of models are independent except for the frozen input embedding layers.</p><p>Pre-trained word vector contains the latent semantics in real-world natural language. In order to get the initialized representation of the answer, relation and support entity, we employ GloVe embedding <ref type="bibr" target="#b20">[21]</ref> meanwhile compare other answer representation like KG embedding <ref type="bibr" target="#b3">[4]</ref> or ConceptNet embedding <ref type="bibr" target="#b14">[15]</ref> (see Section 5.4 for detail).</p><p>Besides, different surface forms (e.g., mice &amp; mouse) should be considered for the same meaning. <ref type="bibr" target="#b11">[12]</ref> takes advantage of the weighting function ?(a, b) with WUPS score, which is reliant on semantic similiarity scores between a and b. We find that it works well with singular and plural disambiguation (e.g. WUPS( dog, dogs ) ? 0.929), but fails in many cases of tense disambiguation (e.g., WUPS( cook, cooking ) ? 0.125, WUPS( play, played ) ? 0.182). So we apply NLTK tools (e.g., WordNetLemmatizer) to achieve more accurate word split and Minimum Edit Distance (MED) for concept disambiguation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Answer Mask via Knowledge</head><p>Masking is widely used in language model pre-training for improving machine's understanding of the text. Two examples are masking part of the words in the training corpus (e.g. BERT <ref type="bibr" target="#b7">[8]</ref>) and masking common sense concepts (e.g. AMS <ref type="bibr" target="#b29">[30]</ref>). But they rarely consider the direct constraint of knowledge in prediction results, ignoring that human beings know how to make reasonable decision under the guidance of existing prior knowledge. Different from all these methods, we propose an answer masking strategy for VQA.</p><p>With the learned F ? and F ? , we get the disjoint fusion embedding in two independent feature spaces, which are respectively taken as the basis for subsequent entity and relation matching: For a given (i, q) pair, vector similarity Sim is calculated via F ? (i, q) G ? (r n ) for relation, and F ? (i, q) G ? (e n ) for support entity. Those e and r, which correspond to the top-k Sim value, separately constitute the candidate set C ent and C rel where k is distinguished with k r and k e . Then target set C tar is collected as follows:</p><formula xml:id="formula_5">C tar = {t | (?(t, r, e) ? ?(e, r, t)) ? r ? C rel ? e ? C ent }<label>(4)</label></formula><p>C tar contributes to the masking strategy on all answers a n ? A via: sim((i, q), a n ) = (F ? (i, q) G ? (a n ))/? + s if a n ? C tar (F ? (i, q) G ? (a n ))/? otherwise <ref type="bibr" target="#b4">(5)</ref> where s represents the score for masking which is the mainly distinction between hard mask and soft mask (see Section 5.4 for detail). Soft score greatly reduces the error cascading caused by the pipeline method through the whole model, which will be discussed in Section 5.5. Meanwhile, the significance of hard mask comes from its superior performance in ZSL setting as shown in Section 5.3. Finally, the predicted answer? to the (i, q) pair is identified as:</p><formula xml:id="formula_6">a = arg max a?A sim((i, q), a)<label>(6)</label></formula><p>It should be noted that candidate targets cannot just be regarded as the candidate answers due to the existence of soft mask, which revises the answer probability rather than simply limits answer's range. Moreover, as mentioned in Section 5.4 and 5.4, k and s mentioned above are hyper parameters which can cause various influence toward the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We validate our approach for both normal VQA and ZS-VQA with ZSL/GZSL settings. In addition to the overall results, we conduct ablation studies for analyzing the impact of: 1) different factors in answer embedding; 2) the mask score; and 3) different hyper parameters (e.g. k e , k r ). Finally, we evaluate its advantage on data transferability and mitigating error cascading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Metrics</head><p>F-VQA. As a standard publicly available VQA benchmark which requires external knowledge, F-VQA <ref type="bibr" target="#b26">[27]</ref> consists of 2, 190 images, 5, 286 questions and a KG of 193, 449 facts. Each (i, q, a) in this dataset is supported by a corresponding common sense fact triple extracted from public structured databases (e.g., ConceptNet, DBPedia, and WebChild). The KG has 101K entities and 1833 relations in total, 833 entities are used as answer nodes. In order to achieve parallel comparison, we maintain the coincide experiment setting with <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b17">18]</ref> to use standard dataset setting which contains 5 splits (by images), and prescribe candidate answers to the top-500 (%94.30 to entire as our check) for experiments. The over all data statistics after disambiguation are shown in <ref type="table" target="#tab_0">Table 1</ref>. ZS-F-VQA. The ZS-F-VQA dataset is a new split of the F-VQA dataset for zero-shot problem. Firstly we obtain the original train/test split of F-VQA dataset and combine them together to filter out the triples whose answers appear in top-500 according to its occurrence frequency. Next, we randomly divide this set of answers into new training split (a.k.a. seen) A s and testing split (a.k.a. unseen) A u at the ratio of 1:1. With reference to F-VQA standard dataset, the division process is repeated 5 times. For each (i, q, a) triplet in original F-VQA dataset, it is divided into training set if a ? A s . Else it is divided into testing set. The data statistics are shown in <ref type="table" target="#tab_0">Table 1</ref>, where #class represents the number of data after deduplicated and #instance represents the number of samples. We denote "Overlap" as the intersection size of element sets within training and testing triples. Note that the overlap of answer instance between training and testing set in F-VQA are 2565 compared to 0 in ZS-F-VQA. Evaluation Metrics. We measure the performance by accuracy and choose Hit@1, Hit@3, Hit@10 here together with MRR/MR to judge the comprehensive performance of model. Hit@X indicates that the correct answer ranks in the top-k predicted answer sorted by probability. Mean Reciprocal Rank (MRR) measure the average reciprocal values of correct predicted answers compared to Mean Rank (MR). All the results we report are averaged across all splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>Fusion Model. We employ several models to parameterize the fusion function F ? . We follow <ref type="bibr" target="#b11">[12]</ref> to employ the Multi-layer Perceptron (MLP) and Stacked Attention Network (SAN) <ref type="bibr" target="#b28">[29]</ref> as the representation of grid based visual fusion model. Meanwhile, we choose Up-Down (Bottom-Up and Top-Down Attention) <ref type="bibr" target="#b1">[2]</ref> and Bilinear Attention Networks (BAN) <ref type="bibr" target="#b12">[13]</ref> to measure the impact of bottom-up issue on external knowledge VQA problem. Moreover, we directly compare with <ref type="bibr" target="#b26">[27]</ref> in some baselines like Qqmaping <ref type="bibr" target="#b26">[27]</ref> Hie <ref type="bibr" target="#b13">[14]</ref> under identical setting. Among all these methods, SAN is chosen as the base feature extractor F ? of our framework for its better performance(see <ref type="figure" target="#fig_2">Figure 2</ref>). Visual Features.  <ref type="bibr" target="#b20">[21]</ref> vector. The sequence of embedded words in question (average length is 9.5) is then fed into Bi-GRU for each time step. We have also tried to embed answer with GRU but find that it mostly leads to overfitting since the training set is not huge enough and average answer length is merely 1.2. So we simply represent the answer by averaging its word embedding. During training, we utilize Adam optimizer with the mini-batch size as 128. Dropout and batch normalization are adopted to stabilize the training. We use a gradual learning rate warm up (2.5 ? (epoch + 1) ? 5 ? 10 ?4 ) for the first 7 epochs, decay it at the rate of 0.7 for every 3 epochs for epochs 14 to 47, and remain the same in rest epochs. Meanwhile, the loss temperature ? is set to 0.01 and early stopping is used where patience is equal to 30. The model is trained offline, and thus the training time usually does not impact the method's application. In prediction, we currently consider 500 candidate answers for each testing sample. This makes the computation for evaluation affordable. <ref type="table">Table 2</ref>. The overall results (% for Hit@K) on standard F-VQA datasets (TOP-500). ? denotes that the model is modified under a mapping-based setting (i.e., remove the last classifier layer of the (i, q) fusion network), which contrasts with traditional classifier-based approach. Results on F-VQA. To demonstrate the effectiveness of our model under generalized VQA condition, we conduct experiments under standard F-VQA dataset. Results in <ref type="table">Table 2</ref> gives an overview of the comprehensive evaluation for some representative approaches over this datasets. It is interesting that the Up-Down and BAN behave worse than SAN, which may be caused by overfitting of the model due to more parameters and limited training data (less than 3000). But among all those settings, the results demonstrate that our models all outperform corresponding classifier-based or mapping-based models to varying degrees. The stable improvement (compare with SAN ? ) achieved by our model indicates that adding our method to other end-to-end framework under generalized knowledge VQA setting could also lead to stable performance improvement. Most importantly, our proposed KG-based framework is independent of fusion model, which makes it possible for multi-scene migration and multi-model replacement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Overall Results</head><p>Results on ZS-F-VQA. We report the prediction results under the standard ZSL setting and GZSL setting in <ref type="table" target="#tab_3">Table 3</ref>. Considering that the traditional classifier-based VQA model fail to work on ZS-VQA since there is no overlap of answer label between the testing set and training set (see <ref type="table" target="#tab_0">Table 1</ref> for detail), we simply skip these methods here. We set larger parameters k under ZSL/GZSL setting to mitigate the strong constraint on answer candidate caused by hard mask. From the overall view, the performance of our model has been significantly improved on the basis of SAN ? model. Most importantly, the models obtain the state-of-the-art performance under respective indicators with various parameter settings. Take the result in GZSL setting as an example, our method achieve 29.39% improvement for hit@1 (from 0.22% to 29.39%), 44.17% for hit@3 and 75.34% for hit@3. We have similar observations when the setting transforms into standard ZSL. To sum up. these observations demonstrate the effectiveness of the model in the ZSL/GZSL scenario, but it also reflects model's dependence on trade off between k r and k e (this will be discussed in Section 5.4).  and v(a) respectively denotes KG embedding (KGE), ConceptNet embedding <ref type="bibr" target="#b14">[15]</ref>, and original GloVe embedding. This KGE technique can be used to complete the KG with missing entities or links, meanwhile produce the embedding of nodes and links as their representations. Specially, we adopt TransE <ref type="bibr" target="#b3">[4]</ref> as x(.) and train it on our KG. As for h(a), we utilize the BERT-based node representations generated by a pre-trained common sense embedding model <ref type="bibr" target="#b14">[15]</ref>, which exploits the structural and semantic context of nodes within a large scale common sense KG. As the result shown in <ref type="table" target="#tab_4">Table 4</ref>, when work independently, word2vec representation (78.44%) of answers exceed graph based methods ( 71.94% for KGE and 77.34% for ConceptNet Embedding in Hit@10 ) in performance even though they contain more information. We guess that when the size of the dataset is small, the complexity of neural network limits model's sensitivity to the input representation. So finally we simply choose GloVe as the initial representation of all inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Studies</head><p>Impact of Mask Score. In this part we mainly discuss the effect of mask score on ZS-F-VQA and F-VQA which is reflected by hit@1 (Left), hit@3 (Middle) and hit@10 (Right) accuracy as shown in <ref type="figure">Figure 3</ref>. Caused by the sparsity of high-dimensional space vector, the value of F ? (i, q) G ? (f n )) is quite small as we observing on experiment. This is also another reasons why we define ? for the scale-up of vector similarity (in addition to accelerating model convergence). Considering that sim((i, q), a n ) distributes from 145 to 232, we simply take 100 as the dividing line of score between hard mask and soft mask which is big enough for correcting an incorrect answer into a correct one in testing stage. As shown in <ref type="figure">Figure 3</ref>, the result gaps between soft mask (i.e., low score) and hard mask (i.e., high score) are completely different in standard and GZSL VQA scenarios. We consider following reasons: 1) Firstly, do not try to rely on network to model complex common sense knowledge when data is scarce: When applied to ZS-F-VQA, we notice that model merely learns prior shallow knowledge representation and poor transfer capabilities for unseen answers (see Section 5.5).</p><p>In this case, the strong guiding capability of additional knowledge makes a great contribution to answer prediction. 2) Secondly, if the training samples are sufficient, the error cascading caused by pipeline mode may become the restriction of model performance: When applied to standard F-VQA, the model itself already has high confidence in correct answer and external knowledge should appropriately relax the constraint. We observe that overly strong guidance (i.e., hard mask) becomes a burden at this moment, so soft mask is in demand as a soft constraint. This reflects the necessity of defining different mask.</p><p>Impact of Support Entity and Relation. As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, we notice that hit@1 and hit@10 cannot simultaneously achieve the best, despite that the model can always exceed the baseline a lot with different k. This phenomenon is plausible since that the more restrictive target candidate set is, the more likely it succeed predicting answer in a smaller range, with the cost of missing some other true answers due to the error prediction of support entity/relation. The contrast between MRR and MR well reflects this view (see <ref type="table" target="#tab_3">Table 3</ref>).   To further validate the effectiveness of our knowledge-based ZS-VQA model, we visualize the output and intermediate process of our method compared to best baseline model SAN ? <ref type="bibr" target="#b11">[12]</ref>. <ref type="figure" target="#fig_5">Figure 5</ref> (Up) shows the detected support entities, relations, and answers for four examples in ZS-F-VQA dataset together with answer predicted by SAN ? and the groudtruth one. It indicates that normal models tend to align answer directly with meaning content in question/image (e.g. bicycle in Case 3) or match the seen answers (e.g. airplane in case 4 ), which is a lazy way of learning accompanied by overfitting. To some extend, the difficult common sense knowledge stored in structured data is utilized to playing a guiding role here. Our method can also be generalized to predict multiple answers since the probabilistic model can calculate scores for all candidates to select the top-K answers (see answer "tv" in Case 2 of <ref type="figure" target="#fig_5">Figure 5</ref>). Our method also works well under generalized VQA setting as illustrated in <ref type="figure" target="#fig_5">Figure 5</ref> (Down). For those simpler answers, it can increase the probability (e.g. Case 6) for correct prediction. More importantly, distinguish from the hard mask (dark shadows) in ZSL setting, the soft mask strategy here effectively alleviates error cascading which reduces the influence from previous model's error (e.g. failed prediction on support entity lead to the error mask on Case 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Interpretability</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a Zero-shot VQA model via knowledge graph for addressing the problem of exploiting external knowledge for Zero-shot VQA. The crucial factor to the success of our method is to consider both the knowledge contained in the answer itself and the external common sense knowledge from knowledge graphs. Meanwhile we convert VQA from a traditional classification task to a mapping-based alignment task for addressing unseen answer prediction. Experiments on multiple models support our claim that this method can not only achieve outstanding performance in zero-shot scenarios but also make steady progress at different end-to-end models on the general VQA task. Next we will further investigate KG construction and KG embedding methods for more robust but compact semantics for addressing ZS-VQA. Moreover, we will release and improve the ZS-VQA codes and data, in conjunction with K-ZSL <ref type="bibr" target="#b10">[11]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Corresponding author. arXiv:2107.05348v4 [cs.AI] 18 Oct 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Q3:Fig. 1 .</head><label>1</label><figDesc>Which object in this image is like a plate? Q2: What object in this image is a toy? Q1: Normally you play this game with your? R ec ei ve sA ct io n VQA Examples. Q1: the answer is outside the image and question; Q2 and Q3: the answers are within the images or questions but require additional knowledge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>An overview of our framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>71.9 74.3 75.5 74.2 71.7 69.2 68.9 72.5 74.7 75.9 73.7 71.1 68.4 69.5 73.1 75.3 76.0 73.6 70.6 68.0 69.8 73.1 75.4 76.0 73.3 70.1 67.2 69.9 73.3 75.5 76.0 72.8 69.8 Impact of #support entity (ke) and #relation (kr) on GZSL setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Cases under GZSL VQA (Up) and Generalized VQA (Down) setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The detailed data statistics. Average number of (i, q, a) in each train/test split in F-VQA is 2757/2735 compared to 2732/2760 of ZS-F-VQA.</figDesc><table><row><cell></cell><cell>Images</cell><cell>Question</cell><cell>Answer</cell><cell>Support Entity</cell></row><row><cell>#class</cell><cell cols="4">Train/Test/Overlap Train/Test/Overlap Train/Test/Overlap Train/Test/Overlap</cell></row><row><cell>F-VQA</cell><cell>1059 / 1064 / 0</cell><cell>2431 / 2409 / 573</cell><cell>387 / 401 / 288</cell><cell>1695 / 1668.8 / 312</cell></row><row><cell cols="2">ZS-F-VQA 1297 / 1312 / 486</cell><cell>2384 / 2380 / 264</cell><cell>250 / 250 / 0</cell><cell>1578 / 1477 / 86</cell></row><row><cell>#instance</cell><cell>Overlap</cell><cell>Overlap</cell><cell>Overlap</cell><cell>Overlap</cell></row><row><cell>F-VQA</cell><cell>0</cell><cell>1372</cell><cell>2565</cell><cell>312</cell></row><row><cell>ZS-F-VQA</cell><cell>990</cell><cell>814</cell><cell>0</cell><cell>218</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The overall results (% for Hit@K) on ZS-F-VQA datasets under the setting of ZSL/GZSL.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>GZSL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ZSL</cell><cell></cell></row><row><cell>Methods</cell><cell cols="10">Hit@1 Hit@3 Hit@10 M RR M R Hit@1 Hit@3 Hit@10 M RR M R</cell></row><row><cell>Up-Down  ?</cell><cell>0.00</cell><cell>2.67</cell><cell>16.48</cell><cell>?</cell><cell>?</cell><cell cols="2">13.88 25.87</cell><cell>45.15</cell><cell>?</cell><cell>?</cell></row><row><cell>BAN  ?</cell><cell>0.22</cell><cell>4.18</cell><cell>18.55</cell><cell>?</cell><cell>?</cell><cell cols="2">13.14 26.92</cell><cell>46.90</cell><cell>?</cell><cell>?</cell></row><row><cell>MLP  ?</cell><cell>0.07</cell><cell>4.07</cell><cell>27.40</cell><cell>?</cell><cell>?</cell><cell cols="2">18.84 37.85</cell><cell>59.88</cell><cell>?</cell><cell>?</cell></row><row><cell>SAN  ?</cell><cell>0.11</cell><cell>6.27</cell><cell>31.66</cell><cell cols="4">0.093 48.18 20.42 37.20</cell><cell>62.24</cell><cell cols="2">0.337 19.14</cell></row><row><cell></cell><cell></cell><cell cols="6">Our Method (hard mask score = 100)</cell><cell></cell><cell></cell></row><row><cell cols="3">kr = 25, ke = 1 29.39 43.71</cell><cell cols="4">62.17 0.401 29.52 46.87</cell><cell>62</cell><cell>78.14</cell><cell cols="2">0.572 12.22</cell></row><row><cell cols="3">kr = 15, ke = 3 12.22 50.44</cell><cell>73.10</cell><cell cols="4">0.339 22.2 50.51 70.44</cell><cell cols="3">84.24 0.625 9.27</cell></row><row><cell>kr = 15, ke = 5</cell><cell>6.69</cell><cell>42.91</cell><cell cols="5">75.34 0.293 20.61 49.11 71.17</cell><cell>86.06</cell><cell cols="2">0.622 8.6</cell></row><row><cell cols="2">kr = 25, ke = 15 1.96</cell><cell>24.8</cell><cell>72.85</cell><cell cols="4">0.208 18.72 40.21 67.04</cell><cell cols="3">88.51 0.563 7.68</cell></row><row><cell cols="2">kr = 25, ke = 25 1.19</cell><cell>18.81</cell><cell>66.97</cell><cell cols="4">0.180 18.14 35.87 61.86</cell><cell>88.09</cell><cell cols="2">0.528 7.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The impact of different answer embedding toward model performance (%) on standard F-VQA datasets (TOP-500). x(a), h(a) and v(a) respectively denote KGE, ConceptNet embedding, and original GloVe embedding. CLS is classifier-based method.Choice of Answer Embedding. To compare the influence of answer embedding in feature projection performance, we define g ? (a) = g ? (C[x(a); h(a); v(a)]) in this part where C denotes simple concatenate function. Specially, x(a), h(a)</figDesc><table><row><cell>Methods</cell><cell>Hit@1</cell><cell>Hit@3</cell><cell>Hit@10</cell></row><row><cell>CLS</cell><cell>38.64</cell><cell>54.87</cell><cell>69.38</cell></row><row><cell>v(a)</cell><cell>46.32</cell><cell>63.96</cell><cell>78.44</cell></row><row><cell>x(a)</cell><cell>44.13</cell><cell>59.94</cell><cell>71.94</cell></row><row><cell>h(a)</cell><cell>45.62</cell><cell>62.99</cell><cell>77.34</cell></row><row><cell>v(a) + h(a)</cell><cell>45.86</cell><cell>63.67</cell><cell>78.43</cell></row><row><cell>v(a) + h(a) + x(a)</cell><cell>45.18</cell><cell>62.95</cell><cell>77.14</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Our code and data are available at https://github.com/China-UK-ZSL/ZS-F-VQA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://www.w3.org/TR/2014/REC-rdf11-mt-20140225/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work is funded by 2018YFB1402800/NSFCU19B2027 /NSFC91846204. Jiaoyan Chen is founded by the SIRIUS Centre for Scalable Data Access (Research Council of Norway) and Samsung Research UK.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">C-VQA: A compositional split of the visual question answering (VQA) v1.0 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno>abs/1704.08243</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">VQA: visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge-aware zero-shot learning: Survey and perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI Survey Track</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph optimal transport for cross-domain alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICML</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1542" to="1553" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hybridqa: A dataset of multi-hop question answering over tabular and textual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP. pp</title>
		<imprint>
			<biblScope unit="page" from="1026" to="1036" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From known to the unknown: Transferring knowledge to answer questions about novel visual and semantic concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Farazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page">103985</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Ontozsl: Ontology-enhanced zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>WWW</publisher>
			<biblScope unit="page" from="3325" to="3336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">K-ZSL: resources for knowledge-driven zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/2106.15047</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning answer embeddings for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5428" to="5436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="1571" to="1581" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="289" to="297" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Commonsense knowledge base completion with structural and semantic context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="2925" to="2933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">OK-VQA: A visual question answering benchmark requiring external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3195" to="3204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Out of the box: Reasoning with graph convolution nets for factual visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="2659" to="2670" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Straight to the facts: Learning knowledge base retrieval for factual visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (8)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11212</biblScope>
			<biblScope unit="page" from="460" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Reasoning Web: Logical Foundation of Knowledge Graph Construction and Querying Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Calvanese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<editor>Zhao, Y.</editor>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exploiting Linked Data and Knowledge Graphs for Large Organisations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vetere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomez-Perez</surname></persName>
		</author>
		<editor>Wu, H.</editor>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP. pp</title>
		<imprint>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An empirical evaluation of visual question answering for novel objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7312" to="7321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">KVQA: knowledge-aware visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="8876" to="8884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Visual question answering with prior class semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shevchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<idno>abs/2005.01239</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Zero-shot visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<idno>abs/1611.05546</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Explicit knowledgebased reasoning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI. pp</title>
		<imprint>
			<biblScope unit="page" from="1290" to="1296" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">FVQA: fact-based visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2413" to="2427" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ask me anything: Free-form visual question answering based on knowledge from external sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4622" to="4630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Align, mask and select: A simple method for incorporating commonsense knowledge into language representation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<idno>abs/1908.06725</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mucko: Multi-layer crossmodal knowledge reasoning for fact-based visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI. pp</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1103" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
