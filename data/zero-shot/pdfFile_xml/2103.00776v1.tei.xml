<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single-Shot Motion Completion with Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglin</forename><surname>Duan</surname></persName>
							<email>duanyinglin@corp.netease.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Shi</surname></persName>
							<email>shitianyang@corp.netease.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxia</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yenan</forename><surname>Lin</surname></persName>
							<email>lin@foxmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhehui</forename><surname>Qian</surname></persName>
							<email>qianzhehui@corp.netease.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhang</surname></persName>
							<email>hzzhangbohan@corp.netease.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Netease</forename><surname>Yenan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yuan</surname></persName>
							<email>yuanyi@corp.netease.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">NetEase Fuxi AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">NetEase Fuxi AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Single-Shot Motion Completion with Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motion completion is a challenging and long-discussed problem, which is of great significance in film and game applications. For different motion completion scenarios (inbetweening, in-filling, and blending), most previous methods deal with the completion problems with case-by-case designs. In this work, we propose a simple but effective method to solve multiple motion completion problems under a unified framework and achieves a new state of the art accuracy under multiple evaluation settings. Inspired by the recent great success of attention-based models, we consider the completion as a sequence to sequence prediction problem. Our method consists of two modules -a standard transformer encoder with self-attention that learns long-range dependencies of input motions, and a trainable mixture embedding module that models temporal information and discriminates key-frames. Our method can run in a nonautoregressive manner and predict multiple missing frames within a single forward propagation in real time. We finally show the effectiveness of our method in music-dance applications. Our animated results can be found on our project page https://github.com/FuxiCV/SSMCT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Motion completion is an important and challenging problem that has been studied for a long time. Motion completion provides fundamental technical support for animation authoring of 3D characters, and has been recently successfully applied in film production and video games <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>In recent years, deep learning methods have greatly pro- <ref type="figure">Figure 1</ref>. Motion completion by our method based on input keyframes (blue ones). Our method is a unified framework that can solve multiple motion completion problems. We achieve a new state of the art on a high-quality motion completion dataset -LaFAN1 <ref type="bibr" target="#b15">[16]</ref>.</p><p>moted the research progress of motion completion. With recent advances in this field, manpower can now be greatly saved, where high-quality motion can be smoothly generated from a set of historical or sparsely key-frames by learning over a large scale of motion capture data <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b15">16]</ref>. However, most previous methods deal with the completion problem for different completion scenarios (in-betweening, infilling, and blending) with case-by-case designs. In this paper, we propose a novel framework that can unify the above processing scenarios. In our method, we leverage the recent popular deep learning architecture named Transformer <ref type="bibr" target="#b45">[46]</ref>, which is built on the self-attention mechanism and now has been widely used in neural language processing, computer vision, and reinforcement learning <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34]</ref>. We adopt BERT <ref type="bibr" target="#b9">[10]</ref>, a recent well-known transformer encoder as our backbone, where known frames (key-frames) and unknown frames (need to be complemented) are fed together orderly as input and thus all the frames can be predicted in a single propagation at inference. Our method works in a nonautoregressive manner and can be easily accelerated with GPU parallelization. As a comparison, most previous methods adopt recursive or auto-regressive prediction where the motion frames need to be generated iteratively that lacks parallelism. We consider motion completion in the following three scenarios:</p><p>* In-betweening. Animators are required to complement the motion frame-by-frame between the past frames and the provided further keyframe <ref type="bibr" target="#b15">[16]</ref>.</p><p>* In-filling. As an extension of in-betweening, in-filling poses the characters on specific positions of the timeline <ref type="bibr" target="#b7">[8]</ref>, and complements the rest of the frames <ref type="bibr" target="#b23">[24]</ref>. A sub-task of In-filling is temporal super-resolution, where the input motion frames are considered as keyframes with equal interval <ref type="bibr" target="#b14">[15]</ref>.</p><p>* Blending. Blending focuses on the automatic generation of the transition between a pair of pre-defined motions and has been widely used in video games 1 . For example, many games provide dance fragments for players to choreograph, and blending helps players to concatenate and smooth those chosen fragments.</p><p>We also introduce a mixture embedding module that further integrates temporal knowledge to the transformer encoder. Our embedding module contains two types of learnable embeddings: position embedding and keyframe embedding. Position embedding is a widely studied technology in recent transformer literature, where a set of predefined sinusoidal signals are usually used to introduce temporal knowledge to the transformer model <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. In our method, we further make this embedding trainable to deal with different motion completion scenarios. In addition to the encoding of input frame orders, we also introduce Keyframe embedding to annotate the input frames and tells the model which parts of the input frames are keyframes (already known) and which frames need to be predicted (unknown). Since the keyframe embedding may have different forms, our method can be easily applied to different completion scenarios regardless of how the input keyframes are arranged. Our design can be also considered as a motionversion of Mask Language Model (MLM) <ref type="bibr" target="#b43">[44]</ref>, where the unknown frames are represented by a deep bi-directional model trained with self-supervised losses <ref type="bibr" target="#b9">[10]</ref>.</p><p>The contributions of our paper are summarized as follows:</p><p>1. We investigate the capability of the transformer-based model in the motion completion task. We propose a simple but efficient method to solve the motion completion problems of different application scenarios under a unified framework.</p><p>2. Our method works in a parallel prediction manner with high computational efficiency. On a single CPU desktop (I7-8700K @ 3.70GHz), our method can run in real time (40 motion sequences per second, each with 30 frames long).</p><p>3. We achieve new in-betweening benchmarking accuracy under multiple evaluation settings. Results on other completion tasks (in-filling and blending) are also reported as baselines for fostering the research community 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works 2.1. Motion completion</head><p>Motion completion is an emerging research hot-spot in computer graphics and multimedia. Motion completion can be viewed as a conditional motion sequence generation problem. Different from those unconditional motion synthesis tasks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b42">43]</ref> that focus on the generation of unconstrained motion sequences by directly sampling from their posterior distribution, motion completion aims at filling the missing frames in a temporal sequence based on a given set of keyframes.</p><p>Motion completion has a long research history, which can be traced back to the late 1980s. Early works of motion completion typically adopt inverse kinematics to generate realistic transitions between keyframes. For example, space-time constraints and searching based methods were proposed in the 1980s-1990s <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b32">33]</ref> to compute optimal physically-realistic trajectory. By using such techniques, transitions between different motions can be smoothly generated <ref type="bibr" target="#b37">[38]</ref>. Also, probabilistic models like the maximum a posterior methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32]</ref>, the Gaussian process <ref type="bibr" target="#b46">[47]</ref>, Markov models <ref type="bibr" target="#b28">[29]</ref> were introduced to motion completion tasks and were commonly used after 2000s.</p><p>Recently, deep learning methods have greatly promoted the research and the performance of motion completion, where the recurrent neural network is the most commonly used framework for motion completion of the deep learning era <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b14">15]</ref>. For example, Harvey et al. introduce a novel framework named Recurrent Transition Networks (RTN) to learn a more complex representation of human motions with the help of LSTM <ref type="bibr" target="#b14">[15]</ref>. Besides, generative adversarial learning has been also introduced to the motion completion to make the output motion more realistic and naturalistic <ref type="bibr" target="#b18">[19]</ref>. Some non-recurrent motion completion models are also proposed very recently. Kaufmann et al. propose an end-to-end trainable convolutional autoencoder to fill in missing frames <ref type="bibr" target="#b23">[24]</ref>. Another recent progress by Harvey et al. introduces time-to-arrival embeddings and scheduled target-noise to further enhance the performance of RTN and achieve impressive completion results <ref type="bibr" target="#b15">[16]</ref>. Different from the previous RNN-based or convolution-based method, we propose a transformer-based model, which is a more unified solution to this task and can deal with arbitrary missing frames in a single-shot prediction manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Motion control</head><p>Motion control is a typical conditional motion generation task, which is also highly related to motion completion. In motion control, the control signal comes from a pre-defined temporal sequence, e.g. root trajectory, rather than a set of keyframes in motion completion.</p><p>Graph-based motion control is the most common type of method in this field before the deep learning era <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b4">5]</ref>. For example, Arikan et al. formulate the motion generation as a randomized search of the motion graph, which allows to edit complex motions interactively <ref type="bibr" target="#b0">[1]</ref>. Beaudoin et al. propose a string-based motif-finding algorithm named Motion-Motif Graphs, which further considers the motif length and the number of motions in a motif <ref type="bibr" target="#b4">[5]</ref>. Besides, there are many statistical methods proposed to avoid searching from predefined motion templates <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31]</ref>. Chai et al. propose a statistical dynamic model to generate motions with motion prior (e.g. user-defined trajectory) and formulate the constrained motion synthesis as a maximum a posterior problem <ref type="bibr" target="#b6">[7]</ref>. Ye et al. introduce a nonlinear probabilistic dynamic model that can handle perturbations <ref type="bibr" target="#b48">[49]</ref>. Levine et al. propose a probabilistic motion model that learns a low-dimensional space from example motions and generates character animation based on userspecified tasks <ref type="bibr" target="#b29">[30]</ref>.</p><p>Recently, deep learning methods have become the mainstream motion control method. Some popular architectures like convolutional autoencoder and recurrent neural network are widely used in this problem <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref>. Adversarial training has also played an important role in motion control and can help generate more realistic motion sequences <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>. Besides, some recent approaches also leverage reinforcement learning to further incorporate physical rules to improve the quality of the generation results [9, 50, 3, 37, 36, 6].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this work, we formulate the motion completion as a sequence-to-sequence prediction problem. The unknown motion frames can be generated in a single inference forward propagation conditioned by those input keyframes. We choose BERT, an off-the-shelf transformer architecture <ref type="bibr" target="#b9">[10]</ref> as our network backbone with minimum modifications, and thus the subsequent varieties can be introduced without impediment. <ref type="figure">Fig.2</ref> shows an overview of our method. Our network consists of 1) a mixture embedding module that converts the motion to a set of sequential tokens, and 2) a standard transformer encoder used to process sequential features. Our method supports multiple input coded format, e.g. [local positions &amp; rotations] or [global positions &amp; rotations] or [positions only]. Without loss of generality, we assume that the input has both positions (x, y, z) and rotations (q 0 , q 1 , q 2 , q 3 ) variables (no matter local or global), and therefore, a single pose contains a position coordinate matrix P ? R J?3 and a quaternion matrix Q ? R J?4 , where J represents the joint number of the input pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motion completion transformer</head><p>For each input pose, we firstly flatten P and Q into 1-D vectors p and q, and then concatenate the two vectors together into a long vector:</p><formula xml:id="formula_0">x = [p, q] ? R J?(3+4) .<label>(1)</label></formula><p>For those unknown input frames, we use the linear interpolation to fill in their missing values along the temporal dimension before feeding their pose vector to the model. We then use a 1-D temporal convolution layer to transform those pose vectors to a set of "language tokens":</p><formula xml:id="formula_1">Z = Conv1d([x 1 ; x 2 ; ...; x T ])<label>(2)</label></formula><p>where T is the length of the input sequence, Z ? R T ?F is the formated temporal feature of the tokens, and F is the output dimension of the Conv1d layer. The convolution is performed in the joint dimension. Note that different from the previous transformer-based models in computer vision <ref type="bibr" target="#b10">[11]</ref> that use a linear projection layer to generate the embeddings, here we use a convolution layer for better capturing the temporal information, e.g. velocity and acceleration.</p><p>Considering that the transformer cannot know the order and the exact location of the keyframes in the input sequence, we introduce a mixture embedding E to annotate these frames before feeding their features into transformer. For each group of input configuration of keyframes, an embedding E is learned as a global variable on the training data and will not change along with the pose feature Z. We represent the final annotated features Z as follows:</p><formula xml:id="formula_2">Z = [z 1 + e 1 ; z 2 + e 2 ; ...; z T + e T ]<label>(3)</label></formula><p>where z t and e t are the sub-vectors of input feature Z and mixture embedding E at the time t.</p><p>The BERT transformer we used consists of multiple encoder layers <ref type="bibr" target="#b9">[10]</ref>. Each encoder layer further consists of a multi-head self-attention layer (MHSA) and a feed-forward network (FFN). A residual connection <ref type="bibr" target="#b16">[17]</ref> is applied across the two layers. The forward mapping of the transformer can be written as follows: <ref type="figure">Figure 2</ref>. An overview of our method. Our method consists of a standard transformer encoder, a mixture embedding layer and input/output convolutional heads. In motion completion, the unknown input frames are first generated by using linear interpolation (shown in red) before being fed to the model. Our method takes in a whole masked sequence and complete the prediction within only a single forward propagation.</p><formula xml:id="formula_3">H l = Norm(H l?1 + MHSA(H l?1 )) H l = Norm(? l + FFN(? l ))<label>(4)</label></formula><p>where H is the output of hidden layers. l = 1, ..., L are the indices of encoder layers. "Norm" represents the layer normalization <ref type="bibr" target="#b1">[2]</ref> placed at the output end of the residual connections. We use H 0 = Norm(?) as the hidden representation of the input layer. In the multi-head self-attention layer (MHSA), a dense computation between each pair of input frames are conducted, and thus a very long-range temporal relations between frames can be captured. The processing of a single head can be represented as follows:</p><formula xml:id="formula_4">f att = Softmax( QK T ? )V<label>(5)</label></formula><p>where Q = W q H represents a query matrix, K = W k H represents a key matrix, and V = W v H represents a value matrix. W q , W k and W v are all learnable matrices. We follow the multi-head attention configuration in BERT and set the dimension of Q, K and V to 1 M of the input H, M represents the number of heads in the attention layer. Finally, the outputs from different heads are collected, concatenated and projected by a matrix W mhsa as the output of the MHSA layer:</p><formula xml:id="formula_5">MHSA(H) = W mhsa [f (1) att ; f (2) att ; ...; f (M ) att ].<label>(6)</label></formula><p>For the feed-forward network (FFN), it consists of two linear layers and a GeLU layer <ref type="bibr" target="#b17">[18]</ref>. FFN processes each of the frame individually:</p><formula xml:id="formula_6">FFN(H) = W (1) f f n (GeLU(W (2) f f n (H))),<label>(7)</label></formula><p>where W</p><p>(1) f f n and W</p><p>(2) f f n are the learnable linear projection matrices. Finally, we apply another 1d-convolution layer at the output end of the transformer, and final completion output Y can be written as follows:</p><formula xml:id="formula_7">Y = Conv1d(H N ).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Mixture embeddings</head><p>The mixture embedding E we used consists a positional embedding E pos ? R T ?F and a keyframe embedding E kf ? R T ?F , where T is the length of temporal sequence and F is the input feature dimension of transformer. <ref type="figure">Fig.3</ref> gives an illustration of the mixture embedding module.</p><p>The position embedding E pos is a matrix that contains T sub-vectors, each for a single time step:</p><formula xml:id="formula_8">E pos = [e 1</formula><p>pos , e 2 pos , ..., e T pos ].</p><p>The keyframe embeddings E kf are selected from a learnable dictionary D that contains three types of embedding vectors D = {? 0 ,? 1 ,? 2 }, which annotate the keyframes, unknown frames, and ignored frames, respectively (Keyframe = 0, Unknown = 1, ignored = 2). These <ref type="figure">Figure 3</ref>. An illustration of mixture embedding module. We design a position embedding and a keyframe embedding, where the former one integrates position information and the latter one annotates whether the frame is the keyframe or not. (Blue dash-lines represent ignored frames that exceeds the prediction range) types of frames can be configured in any forms according to different completion tasks and scenarios (e.g., inbetweening, in-filling, and blending). The keyframe embeddings are written as follows:</p><formula xml:id="formula_10">E kf = [e 1 kf , e 2 kf , ..., e T kf ],<label>(10)</label></formula><p>where e m kf ? {? 0 ,? 1 ,? 2 }. Finally, the two types of embeddings are mixed by adding together position-by-position:</p><formula xml:id="formula_11">E = E pos + E kf .<label>(11)</label></formula><p>It is worth noting that our keyframe embedding is not limited to the above three configurations shown in <ref type="figure">Fig.3</ref>. It can be in any format with practical meaning. As a special case, if the keyframes are randomly specified, then our keyframe embedding turns out to be the random token mask in the original BERT paper <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss function</head><p>We train our transformer and the embeddings with multitasks regression losses. Given a set of predicted motions and their ground truth, we design our pose reconstruction loss L rec as follows:</p><formula xml:id="formula_12">L rec = 1 N T N n=1 T t=1 ( p t n ?p t n 1 + q t n ?q t n 1 ) (12)</formula><p>where p t n and q t n are the position coordinates and rotation quaternions of the predicted motion sequence n at the time step t.p t n andq t n T are their ground truth. N represents the length of motion sequence and the total number of training sequences in the data set. Note that the above losses can be used in both global and local coordinate systems. When applied in the local one, the p t n can be replaced by r t n , i.e. root coordinates, since the T-pose (or offsets) will always be a constant vector for the same character.</p><p>In addition to the above pose reconstruction loss, we also introduce two kinematics losses to improve the results in different coordinate systems. 1) Forward Kinematics (FK) loss L F K . We follow Harvey et al. <ref type="bibr" target="#b15">[16]</ref> and apply FK loss to our multi-task training loss. The main idea behind the FK loss is to calculate the global position coordinates by using local ones with forward kinematics and weight the local reconstruction loss on different joints:</p><formula xml:id="formula_13">L F K = FK(r, q local ) ? p global 1 ,<label>(13)</label></formula><p>2). Inverse Kinematics (IK) loss L IK . We also apply IK loss to constrain the T-pose when in global coordinate system. We first compute the local position from the global one with inverse kinematics, and then we compare the offsets between the inverse output and the original input:</p><formula xml:id="formula_14">L IK = IK(p global , q global ) ? b 1<label>(14)</label></formula><p>where b represents the offset vector. We remove the root coordinate and keep predicted offsets only when computing the IK loss.</p><p>Our final training loss is written as follow:</p><formula xml:id="formula_15">L = ? rec L rec + ? K L K ,<label>(15)</label></formula><p>where ? rec and ? K are the coefficients to balance different loss terms. L K represents the kinematics loss in global system (L IK ) or in local system (L F K ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation details</head><p>In our method, we adopt BERT <ref type="bibr" target="#b9">[10]</ref> as the backbone of our transformer with 8 encoder layers. In each encoder layer, we set the number of attention heads to M = 8. For our input and output Conv1d layers, the kernel size is set to 3 and the padding is set to 1. We set the dimension of the feature embedding in the MHSA layers to 256, and set those in the FFN layers to 512. In our training loss, we set ? rec = 1.0 and ? K = 0.01. Consider that the quaternions q ? [0, 1] while the position coordinates p are in a much larger range, we scale the localization loss and the rotation loss to the same order of magnitude.</p><p>We train our network by using Adam optimizer <ref type="bibr" target="#b24">[25]</ref>. We set the maximum learning rate to 10 ?3 . The whole framework is implemented by using PyTorch <ref type="bibr" target="#b34">[35]</ref>. For a more detailed training configuration, please refer to our experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and motion completion tasks</head><p>In our experiment, we evaluate our method across three different motion completion tasks:</p><p>1. In-betweening on LaFAN1 <ref type="bibr" target="#b15">[16]</ref>: LaFAN1 is a public high-quality general motion dataset introduced by Harvey In the in-betweening completion task, given the past 10 keyframes and another future keyframe, we aim to predict the motion of the rest frames.</p><p>2. In-filling on Anidance <ref type="bibr" target="#b40">[41]</ref>: Anidance is a public music-dance dataset proposed by Tang et al. <ref type="bibr" target="#b40">[41]</ref>. We test our method on this dataset for the in-filling task, where equally spaced keyframes are given.</p><p>3. Blending on our dance dataset: We collect a new dance movement dataset, which contains high-quality dance movements performed by senior dancers and is more challenging than the previous ones. We evaluate this dataset for exploring the potential of our method in dance applications in game environments.</p><p>To build our dataset, we follow the classic choreography theory of Doris Humphrey <ref type="bibr" target="#b22">[23]</ref> and define dance phrases as our basic movement unit. We invited four senior dancers to perform five types of dance movements (including Jazz dance, Street dance, J-pop dance, Indian dance, Uygur dance). We use motion capture devices (Vicon V16 cameras) to record the dance movements in 30Hz. Finally, more than 130,000 frames are recorded in our dataset. For convenience, we re-target the dance movements on a standard character released by Harvey et al. <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics</head><p>We follow Harvey et al. <ref type="bibr" target="#b15">[16]</ref> and use L2Q, L2P, and NPSS as our evaluation metrics. The L2Q defines the average L2 distances of the global quaternions between the predicted motions and their ground truth. Similarly, the L2P defines the average L2 distances of the global positions. <ref type="bibr" target="#b2">3</ref> The NPSS, proposed by Gopalakrishnan <ref type="bibr" target="#b11">[12]</ref>, is a variant of L2Q, which computes the Normalized Power Spectrum Similarity and is based on angular frequency distance between the prediction and the groundtruth.</p><p>Note that when we generate our results based on the global coordinate system, we replace the T-pose of these results with the standard one under the local coordinate system (by the simplest IK, i.e. global to local coordinate transformation) for a fair comparison. We found this operation may slightly reduce our accuracy. We will give more discussion on this interesting observation in our Discussion section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Motion In-betweening</head><p>We evaluate our method on the LaFAN1 dataset <ref type="bibr" target="#b15">[16]</ref> for the motion in-betweening task. This dataset contains 496,672 frames performed by 5 motion subjects and are recorded by using Mocap in 30Hz. The training set and the test set are clearly separated, where the test set contains motions from subject No.5 only. Since the originally captured motions are in very long sequences, motion windows are introduced on this dataset, where the width of the window is set to 50 (65) frames, and the offset is set to 20 <ref type="bibr" target="#b24">(25)</ref> frames for training (test). Finally, there are 20,212 and 2,232 windows for training and test, respectively. We train our model on LaFAN1 for 1,000 epochs with random initialization. We set the maximum learning rate to 10 ?3 , and set the weight decay to 0.75 every 200 epochs. We further adopt a warm-up strategy for the first 50 training epochs, where the learning rate goes up from 0 to the maximum learning rate gradually. Since our method can take in arbitrary inputs, we set the transition length of in-betweening to 5?39 (since the maximum length is 39). For the unknown frames, before feeding them to our model, we interpolate  When the training stops, we evaluate our method on the test set of LaFAN1. Tab 1 shows the evaluation result, where interpolation and zero-velocity are used as our naive baselines <ref type="bibr" target="#b15">[16]</ref>. During the evaluation, only the first 10 keyframes and another keyframe at the frame 10 + L + 1 are given, where L is a predefined transition length. We keep the same configuration with Harvery et al., where transition lengths are set to 5, 15, and 30. We evaluate our method under both local and global coordinate systems. We show in Tab 1 that our method achieves a high accuracy on the LaFAN1 dataset even when the transition length = 5. We can also see that there is a noticeable accuracy im-provement when switching the generation mode from local to global. This may be because of the accumulative errors of rotations in local coordinates, and the previous method proposes to use the FK loss to reduce this error <ref type="bibr" target="#b15">[16]</ref>. Our results suggest that the motion completion can be better solved in the global coordinate system, although the T-pose predicted in the global may not be accurate enough (can be further improved by IK loss). We further evaluate the Mixture Embedding (ME) and IK loss used in our method. We can see both the two strategies significantly improves the accuracy in all evaluation settings (L=5, 15, and 30).</p><p>Besides, Tab 2 indicates that our method can achieve a very high inference speed on CPU device and can even run in real-time (&lt; 0.033s), which benefits from the nonautoregressive design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Dance in-filling</head><p>Next, we evaluate our method on the Anidance dataset <ref type="bibr" target="#b40">[41]</ref>. In this dataset, four types of dance movements (Cha-cha, Tango, Rumba, and Waltz) are captured in the global coordinate system. This dataset was originally designed for music-to-dance generation and contains 61 independent dance fragments with 101,390 frames.</p><p>We apply similar evaluation settings on this dataset, where the time window is set to 128 frames and the offset is set to 64. 20% dance fragments are randomly selected as the test set, and thus there are 1,117 sequences in the training set and 323 sequences in the test set. We train our model on this dataset with 3000 epochs, and the rest of the configurations are kept the same with Sec. 4.3. We set the interval of keyframes to 5?30 for in-filling, which means that there are only 5 keyframes are given to the model when their interval is 30, and the initial transition between keyframes are also interpolated by LERP. Tab 3 shows the evaluation results of our method on the test set of Anidance. The transition lengths are set to 5, 15, and 30 respectively. We evaluate our method under the global coordinate system since the anidance dataset contains global positions only. Similar to the results in the in-betweening task, our method can achieve high completion accuracy on long-term completion, and can also outperform LERP in short-term completion. Our method can also handle in-the-wild input (random keyframes) very well as shown in <ref type="figure" target="#fig_0">Fig.4</ref> and 5, where our method complete much more meaningful dance movements than the LERP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Dance blending</head><p>The above experiments show that our method can apply very well to the current public datasets. For further evaluating our method, we test our method on a very challenging dataset we built. Our dataset contains more complex and diverse dance movements. In our dataset, the time window is set to 64 and the offset is set to 32, and there are finally 3,220 sequences in the training set and 400 sequences in the test set. We keep the same configuration with Sec. 4.3, but use pre-trained weights from Sec. 4.3 for initializing.</p><p>We set the blending window to 5?32 frames. For example, when the window is 32, the first and last 16 frames are given as keyframes and rests are masked. Tab 4 shows our evaluation results with the window width = 8, 16, 32. <ref type="figure">Fig.  6</ref> shows our results with the window width = 16, which is a commonly-used setting (? 0.5s) in auto-blending. We can see our method can still achieve very high quality results on this challenging dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Discussion</head><p>Our method can work on both global and local coordinate systems. When directly predicting poses in the global coordinate system, the bone length may not be wellcontrolled, especially in those games with relative coordinate systems (e.g., local root and rotation), although it may bring a higher accuracy. For example, in the last two rows of Tab 4, we evaluate our result by using standard T-pose criteria and the penultimate one (global-native-Tpose) respectively. The global-native-Tpose is slightly more close to the ground truth but is hard to directly apply to local coordinate applications. We also notice that the global rotation may have discontinuity when the dance is very fast (caused by SLERP). However, this is not a serious problem on the LaFAN1 dataset since the motions in this dataset are slow. We will also further investigate this interesting problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a simple but effective method to solve motion completion problems under a unified framework. In our method, a standard transformer encoder is introduced to handle the arbitrary sequence input, and a mixture embedding is introduced to better encode temporal information of multiple input types. Our method can predict multiple missing frames in a single forward propagation at inference time rather than running in an auto-regressive manner. Experimental results show that our method can be well applied to different motion completion modes, including in-betweening, in-filling and blending, and achieves a new state of the art accuracy on the LaFAN1 dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Our transformer-based infilling results and linear interpolation based results on the anidance test set. The fist row is the ground truth. In the rest rows, red skeletons are predicted and black ones are input keyframes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Our transformer-based infilling results and linear interpolation based results on the anidance test set (In this experiment, keyframes are randomly chosen from the test set with a random order for simulating in-the-wild scenario). them based on the nearest two keyframes by linear interpolation (LERP) and spherical linear interpolation (SLERP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Experimental results on LaFAN1 dataset. A lower score indicates better performance. (*Note that for a fair comparison, the T-pose of our global results have been replaced by a standard one in local coordinate system.)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>L2Q</cell><cell></cell><cell></cell><cell>L2P</cell><cell></cell><cell></cell><cell>NPSS</cell></row><row><cell>Length</cell><cell></cell><cell>5</cell><cell>15</cell><cell>30</cell><cell>5</cell><cell>15</cell><cell>30</cell><cell>5</cell><cell>15</cell><cell>30</cell></row><row><cell>Zero-Vel</cell><cell></cell><cell>0.56</cell><cell>1.10</cell><cell>1.51</cell><cell>1.52</cell><cell>3.69</cell><cell>6.60</cell><cell>0.0053</cell><cell>0.0522</cell><cell>0.2318</cell></row><row><cell>Interp</cell><cell></cell><cell>0.22</cell><cell>0.62</cell><cell>0.98</cell><cell>0.37</cell><cell>1.25</cell><cell>2.32</cell><cell>0.0023</cell><cell>0.0391</cell><cell>0.2013</cell></row><row><cell cols="2">ERD-QV ([16])</cell><cell>0.17</cell><cell>0.42</cell><cell>0.69</cell><cell>0.23</cell><cell>0.65</cell><cell>1.28</cell><cell>0.0020</cell><cell>0.0258</cell><cell>0.1328</cell></row><row><cell cols="2">Ours (local w/o FK)</cell><cell>0.18</cell><cell>0.47</cell><cell>0.74</cell><cell>0.27</cell><cell>0.82</cell><cell>1.46</cell><cell>0.0020</cell><cell>0.0307</cell><cell>0.1487</cell></row><row><cell>Ours (local)</cell><cell></cell><cell>0.17</cell><cell>0.44</cell><cell>0.71</cell><cell>0.23</cell><cell>0.74</cell><cell>1.37</cell><cell>0.0019</cell><cell>0.0291</cell><cell>0.1430</cell></row><row><cell cols="3">Ours (global w/o ME &amp; IK) 0.16</cell><cell>0.37</cell><cell>0.63</cell><cell>0.24</cell><cell>0.61</cell><cell>1.16</cell><cell>0.0018</cell><cell>0.0243</cell><cell>0.1284</cell></row><row><cell cols="2">Ours (global w/o IK)</cell><cell>0.14</cell><cell>0.36</cell><cell>0.61</cell><cell>0.21</cell><cell>0.57</cell><cell>1.11</cell><cell>0.0016</cell><cell>0.0238</cell><cell>0.1241</cell></row><row><cell cols="2">Ours* (global-full)</cell><cell>0.14</cell><cell>0.36</cell><cell>0.61</cell><cell>0.22</cell><cell>0.56</cell><cell>1.10</cell><cell>0.0016</cell><cell>0.0234</cell><cell>0.1222</cell></row><row><cell cols="5">Table 2. Speed performance comparison. CPU inference time are</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">recorded in different batch sizes (1 &amp; 10) where Inbetweening</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">length is set to 30 frames (i.e. 1 second).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">1 x 30 10 x 30</cell><cell cols="2">CPU info</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ERD-QV [16] 0.31s</cell><cell>0.40s</cell><cell cols="2">E5-1650 @ 3.20GHz</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell cols="4">0.025s 0.083s I7-8700K @ 3.70GHz</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>et al. from Ubisoft.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Infilling results on anidance dataset<ref type="bibr" target="#b40">[41]</ref> (A lower score indicates a better performance).</figDesc><table><row><cell></cell><cell></cell><cell>L2P</cell><cell></cell></row><row><cell>Length</cell><cell>5</cell><cell>15</cell><cell>30</cell></row><row><cell>Zero-Vel</cell><cell>2.34</cell><cell>5.12</cell><cell>6.73</cell></row><row><cell>Interp</cell><cell>0.94</cell><cell>3.24</cell><cell>4.68</cell></row><row><cell>Ours (full)</cell><cell>0.84</cell><cell>1.46</cell><cell>1.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Blending results of our new dance dataset (A lower score indicates a better performance).</figDesc><table><row><cell></cell><cell></cell><cell>L2Q</cell><cell></cell><cell></cell><cell>L2P</cell><cell></cell><cell></cell><cell>NPSS</cell><cell></cell></row><row><cell>Length</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell>Zero-Vel</cell><cell>2.17</cell><cell>2.67</cell><cell>3.22</cell><cell>3.68</cell><cell>5.15</cell><cell>7.52</cell><cell>0.2061</cell><cell>0.6004</cell><cell>1.7998</cell></row><row><cell>Interp</cell><cell>2.00</cell><cell>2.55</cell><cell>3.14</cell><cell>1.84</cell><cell>2.87</cell><cell>4.19</cell><cell>0.1948</cell><cell>0.5781</cell><cell>1.7218</cell></row><row><cell>Ours (global-native-Tpose)</cell><cell>1.62</cell><cell>2.03</cell><cell>2.48</cell><cell>1.55</cell><cell>2.32</cell><cell>3.29</cell><cell>0.1906</cell><cell>0.5438</cell><cell>1.4758</cell></row><row><cell cols="2">Ours (global-standard-Tpose) 1.62</cell><cell>2.03</cell><cell>2.48</cell><cell>1.71</cell><cell>2.46</cell><cell>3.45</cell><cell>0.1906</cell><cell>0.5438</cell><cell>1.4758</cell></row><row><cell cols="4">Figure 6. Results of different blending methods on our dataset with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">window width = 16. Our results are much closer to the ground</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>truth.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://docs.unity3d.com/Packages/com.unity. timeline@1.6/manual/clp_blend.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We will make our model and pre-trained weights publically available.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In Harvey et al.'s implementation, the predicted positions and their ground truth are normalized by mean and std of the training set. We also follow this setting for a fair comparison.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interactive motion generation from examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Arikan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="483" to="490" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Modelbased adversarial imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Baram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oron</forename><surname>Anschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02179</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hp-gan: Probabilistic 3d human motion prediction via gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emad</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1418" to="1427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Motion-motif graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Beaudoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stelian</forename><surname>Coros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Michiel Van De Panne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</title>
		<meeting>the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Drecon: data-driven responsive control of physics-based characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Bergamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Clavet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Richard</forename><surname>Forbes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Constraint-based motion optimization using a statistical dynamic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2007 papers</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tangent-space optimization for interactive animation control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz?ztireli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert W</forename><surname>Sumner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust task-based control policies for physics-based characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stelian</forename><surname>Coros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Beaudoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH Asia 2009 papers</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A neural temporal model for human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Mali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Ororbia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12116" to="12125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Style-based inverse kinematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Grochow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoran</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Popovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-GRAPH 2004 Papers</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="522" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adversarial geometry-aware human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Liang-Yan Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="786" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent transition networks for character locomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>F?lix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2018 Technical Briefs</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust motion in-betweening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>F?lix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Yurick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="60" to="61" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human motion prediction via spatio-temporal inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Phasefunctioned neural networks for character control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A deep learning framework for character motion synthesis and editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning motion manifolds with convolutional autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joyce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2015 Technical Briefs</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The art of making dances. Dance Horizons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doris</forename><surname>Humphrey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional autoencoders for human motion infilling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Pece</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remo</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th international conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">263</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Motion graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Kovar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2008 classes</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Interactive control of avatars animated with human motion data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jehee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Reitsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><forename type="middle">S</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 29th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="491" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interactive character animation by learning multi-objective control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jehee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient nonlinear markov models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andreas M Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1314" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Continuous character control with low-dimensional embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoran</forename><surname>Haraux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Popovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Motion graphs++ a compact generative model for semantic motion analysis and synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Interactive generation of human animation with deformable motion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Lin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spacetime constraints revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 20th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="343" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stabilizing transformers for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhant</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Lopez Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seb</forename><surname>Noury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7487" to="7498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deepmimic: Example-guided deep reinforcement learning of physics-based character skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangkang</forename><surname>Berseth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient generation of motion transitions using spacetime constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Guenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bobby</forename><surname>Bodenheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 23rd annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Construction and optimal search of interpolated motion graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Safonova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-GRAPH 2007 papers</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">106</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Implicit probabilistic models of human motion for synthesis and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Sidenbladh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Anidance: Realtime dance motion synthesize to the song</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taoran</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1237" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">cloze procedure&quot;: A new tool for measuring readability</title>
		<imprint>
			<date type="published" when="1953" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="415" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Integrated graphic and computer modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spacetime constraints. ACM Siggraph Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="159" to="168" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Synthesis of responsive motion using a dynamic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="555" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Simbicon: Simple biped locomotion control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangkang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Loken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">105</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Data-driven autocompletion for keyframe animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual International Conference on Motion, Interaction, and Games</title>
		<meeting>the 11th Annual International Conference on Motion, Interaction, and Games</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
