<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Continual Training of Language Models for Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowei</forename><surname>Lin</surname></persName>
							<email>linhaowei@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Shao</surname></persName>
							<email>shaoyj@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
							<email>liub@uic.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Continual Training of Language Models for Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work on applying large language models (LMs) achieves impressive performance in many NLP applications. Adapting or posttraining an LM using an unlabeled domain corpus can produce even better performance for end-tasks in the domain. This paper proposes the problem of continually extending an LM by incrementally post-train the LM with a sequence of unlabeled domain corpora to expand its knowledge without forgetting its previous skills. The goal is to improve the few-shot end-task learning in these domains. The resulting system is called CPT (Continual Post-Training), which to our knowledge, is the first continual post-training system. Experimental results verify its effectiveness. 1   We are glad that reviewers appreciated our pro-001 posed method being straightforward, novel and con-002 vincing. Even though we got fairly high scores, the 003 previous submission was rejected due to some writ-004 ing issues, which resulted in some misunderstand-005 ings from the reviewers about the motivations and 006 technical details. We have addressed all reviewers' 007 comments below and incorporate their feedback in 008 our revised paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>009</head><p>A Clearer Motivation. We addressed this issue 010 in the Abstract and Introduction (lines 017-029).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>011</head><p>Falsely Claim Post-training as Contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>012</head><p>This is clearly an misunderstanding. As stated in 013 lines 020-029, we said that post-training is helpful 014 but our contribution is to propose the new continual 015 post-training paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>016</head><p>BERT as LM. In the previous submission, we 017 used BERT as our backbone language model (LM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>018</head><p>Even though our CPT has significantly improve-019 ments, reviewers raised the concern that we didn't 020 use a more recent and powerful LM. In this revised 021 version, we use a more recent and stronger LM,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>022</head><p>ROBERTa, as our backbone model. The experi-023 ments show that CPT can still achieve very good 024 results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>025</head><p>Individual Task Performance. We addressed 026 this issues in <ref type="table">Table 1</ref> in the paper. In the previous 027 submission, we only reported the average results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>028</head><p>Reviewers asked for individual performance so that 029 they can see more details. We added the results 030 for each domain after post-trained a sequence of 031 tasks. CPT outperforms the baselines (including 032 non-continual learning baselines where we train a 033 separate model for each domain) on average over 034 all domains, indicating the effectiveness of CPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>035</head><p>Whether CPT Can Prevent Forgetting. We 036 addressed this issue in <ref type="table">Table 1</ref> in the paper. We 037 added a new metric, forgetting rate, to show CPT's 038 ability to prevent forgetting. In the experiment 039 section, we can clearly see CPT has 0 forgetting 040 rate, indicating there is no forgetting at all and thus 041 very effective in preventing forgetting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>042</head><p>Effect of domain Order. We addressed this is-043 sue in <ref type="table">Table 6</ref> in the appendix. Reviewers were cu-044 rious about whether different order of domains may 045 affect the performance of CPT. We added experi-046 ments for different domain orders in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>047</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent work has shown that large LMs have the ability to perform few-shot (or even zero-shot) learning well <ref type="bibr">(Brown et al., 2020b;</ref><ref type="bibr">Rae et al., 2021;</ref><ref type="bibr">Smith et al., 2022)</ref>. Post-training (a.k.a., domain-adaptive pre-training or pre-finetuning) an LM with a large unlabeled domain corpus before end-task fine-tuning in the domain achieves better results <ref type="bibr">Gururangan et al., 2020a)</ref> than directly fine-tuning the LM. This paper goes a step further to study the problem of improving an LM's ability to handle new and ever emerging domains. For this, one needs to continually posttrain the LM with a sequence of domains. A key issue associated with this problem is catastrophic forgetting (CF). 2 This paper thus investigates how to continually extend the LM's knowledge without suffering from CF. From a broader perspective, since training a large LM from scratch is extremely expensive and computation intensive, incrementally updating the LM with the latest language data reflecting the ever changing development of the language itself, social events and the knowledge from different fields is becoming more and more critical. As humans are very effective at incremental learning, if we can imitate this human capability with little or no forgetting, we will be pushing the AI research forward significantly.</p><p>The proposed system, called CPT, is a continual learning (CL) system for post-training. Starting from a pre-trained LM (e.g., <ref type="bibr">RoBERTa (Liu et al., 2019b)</ref>), it incrementally post-trains the LM with a sequence of domains using their unlabeled corpora. Once a task (a domain in our case) 3 is trained, its data is no longer accessible. At any time, the resulting continually post-trained LM can be used by end-tasks in the trained domains. This is in the task-incremental learning (TIL) setting of CL, where the task id (domain id in our case) is provided when the learned model of a task needs to be used later (the use of domain id is discussed in Sec. 2.1). 4 This paper proposes an effective approach called CPT and focuses on the challenging and practical scenario of few-shot end-task learning after post-training a sequence of domains.</p><p>Continual post-training is different from conventional CL <ref type="bibr" target="#b5">(Chen and Liu, 2018)</ref>. The key difference is that in conventional CL, each task is an end-task, but in our case the end-task involves fine-tuning the continual post-trained LM (called p-LM). This causes major forgetting, which we call the catastrophic butterfly effect (CBE) and does not happen in conventional CL. Our proposed system, CPT, can solve both CF and CBE, based on a novel hard masking mechanism (Sec. 2.2) and can achieve no forgetting. As shown in Sec. 3.3, naively ap-plied existing CL systems cannot effectively prevent CF (even though some existing techniques have shown almost perfect CF prevention ability in conventional CL).</p><p>Experiments in 4 domains and their corresponding end-tasks demonstrate the effectiveness of the proposed CPT system. Related Work. Overcoming CF is a major goal of CL <ref type="bibr" target="#b5">(Chen and Liu, 2018)</ref>. There are many existing approaches, e.g., regularization-based approaches <ref type="bibr">(Kirkpatrick et al., 2016;</ref><ref type="bibr" target="#b11">Seff et al., 2017)</ref>, replay-based approaches <ref type="bibr" target="#b9">(Rebuffi et al., 2017;</ref><ref type="bibr">Lopez-Paz and Ranzato, 2017)</ref> and parameter isolation based approaches <ref type="bibr" target="#b12">(Serr? et al., 2018;</ref><ref type="bibr" target="#b8">Fernando et al., 2017)</ref>. Our CPT is based on parameter isolation and uses masks in continual post-training. Recently, CL has drawn attention in NLP. It has been used for slot filling <ref type="bibr" target="#b13">(Shen et al., 2019)</ref>, language learning <ref type="bibr">(Li et al., 2019)</ref>, sentence embedding <ref type="bibr">(Liu et al., 2019a</ref><ref type="bibr">), translation (Khayrallah et al., 2018</ref><ref type="bibr">), cross-lingual modeling (Liu et al., 2020b</ref>, question answering <ref type="bibr">(Greco et al., 2019)</ref> and text classification <ref type="bibr">(Ke et al., 2021a,b;</ref><ref type="bibr" target="#b17">Sun et al., 2020;</ref><ref type="bibr">Huang et al., 2021;</ref><ref type="bibr" target="#b6">Chuang et al., 2020;</ref><ref type="bibr">Mehta et al., 2021;</ref><ref type="bibr">Madotto et al., 2020)</ref>. However, none of them tries to improve an LM.</p><p>CPT is closely related to <ref type="bibr">ELLE (Qin et al., 2022)</ref>, which does continual pre-training. The key difference is that ELLE starts from random initialization, while our CPT starts from a pre-trained LM. We tried to adapt ELLE for continual post-training by learning from a pre-trained RoBERTa but it fails to converge. This also indicates it is non-trivial to do well in our setting. Readers can refer to Appendix A for a full coverage of the related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed CPT System</head><p>CPT continually post-trains RoBERTa <ref type="bibr">(Liu et al., 2019b)</ref>. This is achieved by two continual learning plug-in (called CL-plugin) modules inserted into each transformer layer of RoBERTa. CLplugin is inspired by adapters in <ref type="bibr">(Houlsby et al., 2019)</ref>. While adapters can isolate different tasks, one needs to allocate a new adapter for each task and no knowledge can be shared among different tasks' adapters. The CL-plugin, however, is a CL system that learns a sequence of tasks with adapters shared by all domains. <ref type="figure" target="#fig_0">Figure 1</ref> gives the CPT architecture with two CL-plugins added to RoBERTa.</p><p>Sequential vs. Parallel CL-plugin. Instead of following the original sequential adapter (Houlsby et al., 2019), CL-plugin adopts the parallel adapter idea in <ref type="bibr">(He et al., 2021)</ref>. The difference is that the former inserts an adapter after the FFN/attention layer while the latter inserts it before FFN/attention layer (see <ref type="figure" target="#fig_0">Fig. 1</ref>). We choose the parallel version as it performs better (see <ref type="bibr">Sec. 3.3)</ref>.</p><p>In post-training, only the two CL-plugins are trained. The components of the original pre-trained RoBERTa are fixed. In end-task fine-tuning, all components are trainable. A CL-plugin is a twolayer fully connected network with a task mask mechanism. It takes two inputs: (1) hidden states h (t) from the feed-forward layer in a transformer layer and (2) task ID t needed by task incremental learning (TIL). Inside a CL-plugin, task masks (TMs), which indicate task-specific neurons, are used to deal with CF. Since TMs is differentiable, the whole CPT can be trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Masks (TMs)</head><p>In each layer of a CL-plugin, task masks are used to protect those neurons that are important for pre-vious tasks to overcome CF. The masks basically forbid gradient updates to those neurons during backpropagation in learning a new task. Note that a task is also a domain in our case.</p><p>Learning a new task/domain consists of two main steps: (1) apply the mask in each layer for each old task to block off the gradient flow to protect the model for the old task, and (2) learn domain t and its masks for future use. We present (2) first.</p><p>Learning Task Masks for Overcoming CF. In learning each task t, a mask (a "soft" binary mask)</p><formula xml:id="formula_0">m (t)</formula><p>l is trained for the task at each layer l in CLplugin, indicating the neurons that are important for the task. We borrow the hard attention idea in <ref type="bibr" target="#b12">(Serr? et al., 2018)</ref> and leverage the task ID embedding to train the mask. For a task ID t, its embedding e (t) l consists of differentiable deterministic parameters that can be learned together with other parts of the network. To generate the task mask m</p><formula xml:id="formula_1">(t) l from e (t) l , Sigmoid is used as a pseudo-gate (mask) function. m (t) l is computed with m (t) l = ?(e (t) l /? ),<label>(1)</label></formula><p>where ? is a temperature variable, linearly annealed from 1 to ? min (a small positive value).</p><p>In the forward pass, given the output of each layer l, k (t) l , we element-wise multiply mask m</p><formula xml:id="formula_2">(t) l , o (t) l = k (t) l ? m (t) l .</formula><p>(2)</p><p>The masked output o (t) l of the last layer in CLplugin is fed to the next layer of the RoBERTa with a skip-connection. After learning task t, the final m (t) l is saved and added to the set {m (t) l }. Applying Task Masks. Before learning a new task t, we first accumulate and set the masks m (iprev) l on the neurons in each layer l for all old tasks i prev so that in backpropagation, the gradient g (t) l for task t will not flow to these neurons. Since m (iprev) l is pseudo binary, we use max-pooling to achieve the accumulation and condition the gradient:</p><formula xml:id="formula_3">g (t) l = g (t) l ? (1 ? (MaxPool({m (iprev) l }))). (3)</formula><p>Those gradients corresponding to the 1 entries in MaxPool({m (iprev) l }) are set to 0 (to block off gradient flow) while the others remain unchanged. In this way, neurons in old tasks are protected. Note that we expand (copy) the vector m (ta) l to match the dimensions of g (t) l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Catastrophic Butterfly Effect in Fine-tuning</head><p>To perform an end-task in a post-trained domain, we fine-tune the mask-protected model of the domain, which is indicated by the task/domain id. The fine-tuning uses the corresponding domain neurons for the specific end-task by setting ? = ? min and condition the output via Eq. 2. With the masks, there should be no forgetting for continual posttraining and the end-task fine-tuning performance should be similar to post-train each domain separately. However, we found that this is not the case. <ref type="bibr">5</ref> Our investigation found that the problem is due to the pseudo-gate function in Eq. 1. No matter how small ? is, Eq. 1 can only gives us a mask almost 0 (or 1). This causes the following: (1) During posttraining, the gradients for used neurons in Eq. 3 are not exactly 0 but a very small number.</p><p>(2) During fine-tuning, we cannot make use of the corresponding neurons for the specific end-task by simply setting ? = ? min . The small change in the neurons for old domains during post-training caused by (1) is neglect-able in conventional CL because in conventional CL we evaluate the model using test sets and no weights update involved. However, in CPT, the end-task needs to fine-tune the continually post-trained LM model (p-LM), which involves weight updating. A small change to the p-LM during continual post-training can result in a different initialization for the end-task fine-tuning and give totally different fine-tuning results. We call this butterfly effect inspired by the term indicating a small state change in nature (e.g., the flap of a butterfly's wings in Brazil) can result in large differences in a later state (e.g., a tornado in Texas). We propose a simple method to solve it, i.e., adding a threshold ? to the m (t) l to make it a hard binary mask,</p><formula xml:id="formula_4">m (t) l = 1, m (t) l &gt; ?, 0, m (t) l &lt; ?.<label>(4)</label></formula><p>We then apply it to Eq. 3 in gradient manipulation and Eq. 2 in end-task fine-tuning. ? can be easily set (we use 0.5) since Eq. 1 already gives a pseudobinary mask. Note that this has almost no effect on post-training as it is used to block the backward pass gradient flow during post-training and select the corresponding neurons during fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>The proposed paradigm uses a different evaluation from that of conventional continual learning (CL). After unsupervised continual post-training of an LM (RoBERTa in our case) with a sequence of domains, the resulting p-LM is used to fine-tune an end few-shot classification task from any posttrained domain. There is no CL during end-task fine-tuning. Each fine-tuning task is done separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Baselines</head><p>Datasets: We use 4 unlabeled domain datasets: Yelp Restaurant , AI Papers (Lo et al., 2020), ACL Papers (Lo et al., 2020) and AG-News <ref type="bibr" target="#b24">(Zhang et al., 2015)</ref> and their 4 corresponding end-task classification datasets. 6</p><p>Baselines. Since no existing method can perform our task, we use 6 non-CL and 7 adapted CL methods as our baselines. The non-CL baselines include (1) RoBERTa and (2) Adapter where we directly fine-tune the pre-trained model or adpater (without any post-training); (3) RoBERTa-ONE, (4) Adapter-ONE and (5) Prompt-ONE, where we build a model for each task using a separate network. It has no knowledge transfer or CF. (6) DEMIX (Gururangan et al., 2021) trains a separate adapter for each task and initializes the adapter from its most similar previous task adapter. The 7 adapted CL baselines include (7) RoBERTa-NCL and (8) Adapter-NCL, where we post-train the domains one by one with no mechanism to deal with CF/transfer. Other are state-of-the-art CL baselines and we adapt them for continual post-training. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>Architecture. We adopt RoBERTa BASE as our backbone LM. A masked language model head is applied for post-training. The fine-tuning follows the standard practice <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>, where we pass the final layer &lt;/s&gt; token representation 6 These are popularly used in related works. Details of the datasets are given in Appendix C. We conduct experiments using few-shot learning end-tasks. Following (Gu et al., 2021), we use 32 training samples for Restaurant and AGNews, 48 training samples for ACL and 56 training samples for AI due to different numbers of classes in each dataset. <ref type="bibr">7</ref> Readers can refer to Appendix D for the detailed of each of these baselines.</p><p>to a task-specific feed-forward layer for prediction. The feed-forward layer with softmax output is used as the classification heads, together with the categorical cross-entropy loss. Note that for the aspect sentiment classification task (see <ref type="table" target="#tab_5">Table 3</ref>), we adopt the ASC formulation in , where the aspect (e.g., "sound") and review sentence (e.g., "The sound is great") are concatenated via &lt;/s&gt;.</p><p>Hyperparameters. Unless otherwise stated, the same hyper-parameters are used in all experiments. We use 0.0025 for ? min in Eq. 1 and ? is set to 0.5 in Eq. 4 in the main paper. As shown in <ref type="figure" target="#fig_0">Figure  1</ref>, there are two CL-plugins for each Transformer layer (one at the bottom in parallel with attention and one at the top in parallel with FFN). We search the CL-plugin size within {128, 256, 512, 768, 1024} and adopt 512 for the bottom one and 768 for the top one based on validation experiments. The task id embeddings have the same size as the hidden layer dimension of the CL-plugin. The maximum input length is set to 164 which is long enough for all datasets. We use Adam optimizer and set the learning rate to 1e-4 for post-training and 5e-5 for fine-tuning. The batch size is set to 48 for post-training and 20 for fine-tuning. Since each of our domain-specific dataset has a different size, we train CPT on each task/domain for 1 epoch for post-training, which is approximately 13K steps, following <ref type="bibr">(Gururangan et al., 2020b;</ref>. We train on end-task fine-tuning datasets for 20 epochs and take the results for the last epoch, assuming no validation sets. We empirically found 20 epochs can give us a relatively stable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Results and Analysis</head><p>We report the average results of the 4 different finetuning tasks (or datasets) in accuracy and Macro-F1 after post-training on all unlabeled domain datasets in baselines without post-training (RoBERTa and Adapter), indicating CPT can learn new domains well. These two baselines are also significantly worse than other baselines, indicating that finetuning the pre-trained RoBERTa alone is weak.</p><p>Comparing with CL baselines, CPT achieves no forgetting (we can see the forgetting rate is 0), indicating the high effectiveness of the proposed approach. We also note that CPT is even slightly better than those ONE baselines, indicating some positive knowledge transfer in CPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablations</head><p>In <ref type="table" target="#tab_2">Table 2</ref>, we give the ablation results. We are interested in the following:</p><p>(1) Catastrophic butterfly effect (CBE). The third row with "w/o butterfly" shows results without the hard binary mask mechanism in Eq. 4. Clearly, the results are worse and the model suffers from forgetting. This indicates CBE and our approach is effective.</p><p>(2) Different Architecture. CPT is based on CL-plugin, which is inspired by adapters. Another popular way to use adapters is to make it sequential <ref type="bibr">(Houlsby et al., 2019)</ref>. Sequential adapter (first row) is clearly poorer than our current parallel one. This conforms to the observation in <ref type="bibr">(He et al., 2021)</ref>.</p><p>(3) Different Orders. <ref type="table" target="#tab_0">Table 1</ref> only reports the results of one fixed domain order (Restaurant?AI? ACL ?AGNews). We are interested in how the order impacts CPT results. We give the detailed results for all the other baselines and detailed domain orders in Appendix E. We can see the results of CPT does not change much and it  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper proposed to continually post-train an LM with a sequence of domains using their unlabeled domain corpora. An effective method (CPT) is also proposed. An end-task from any post-trained domain can fine-tune the resulting LM. Experimental results demonstrate the effectiveness of CPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations</head><p>We list two limitations of CPT. First, CPT adds CLplugins for continual post-training with no change to the underlying LM in training. Although a CLplugin is small compared to an LM, it is still interesting and may be more effective to explore the idea of updating the LM directly without any additional modules. Second, domain ids are needed in both training and testing for CPT. In some applications, it may be hard to provide a domain id for each fine-tuning end-task. We will explore these in our future work as specializing and/or incrementally improving an LM is an important problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work</head><p>Our work is related to continual learning, posttraining and few-shot learning.</p><p>Continual learning (CL). In general, overcoming CF is a major goal in CL <ref type="bibr" target="#b5">(Chen and Liu, 2018)</ref>. (1) Regularization methods <ref type="bibr">(Kirkpatrick et al., 2016;</ref><ref type="bibr" target="#b11">Seff et al., 2017)</ref> add a regularization to ensure minimal changes to weights for previous tasks.</p><p>(2) Replay methods retain <ref type="bibr" target="#b9">(Rebuffi et al., 2017;</ref><ref type="bibr">Lopez-Paz and Ranzato, 2017;</ref><ref type="bibr" target="#b20">Wang et al., 2020;</ref><ref type="bibr">Guo et al., 2022)</ref> or generate some data of old tasks <ref type="bibr" target="#b14">(Shin et al., 2017;</ref><ref type="bibr">He and Jaeger, 2018)</ref> and use them in learning a new task.</p><p>(3) Parameter isolation methods <ref type="bibr" target="#b12">(Serr? et al., 2018;</ref><ref type="bibr" target="#b8">Fernando et al., 2017)</ref> allocate parameters for different tasks and mask them out in learning a new task. Our CPT is based on <ref type="formula">(3)</ref>   <ref type="bibr" target="#b17">Sun et al., 2020;</ref><ref type="bibr">Huang et al., 2021;</ref><ref type="bibr" target="#b6">Chuang et al., 2020;</ref><ref type="bibr">Mehta et al., 2021;</ref><ref type="bibr">Madotto et al., 2020)</ref>. However, none of them tries to improve an LM.</p><p>Post-training is an effective approach to mitigate the discrepancies between pre-trained domains and the target domain. Researchers have applied post-training to many domains, e.g., reviews <ref type="bibr" target="#b16">Sun et al., 2019)</ref>, news and academic papers <ref type="bibr">(Gururangan et al., 2020b)</ref>, and shown improved end-task results. However, none of them consider the continual learning paradigm.</p><p>Few-shot learning (FL) aims to learn tasks with a few labeled examples. The main issue of FL is over-fitting, due to the scarcity of labeled training data. Existing methods to overcome over-fitting fall in three main families: (i) model-based methods try to reduce the hypothesis space of the few-shot task <ref type="bibr" target="#b18">(Triantafillou et al., 2017;</ref><ref type="bibr">Hu et al., 2018)</ref>, (ii) data-based methods try to augment additional data to the few-shot set <ref type="bibr" target="#b1">(Benaim and Wolf, 2018;</ref><ref type="bibr">Gao et al., 2020)</ref>, and (iii) algorithm-based solutions try to improve strategies for searching for the best hypothesis. Recently, a new paradigm using prompts achieves promising results for fewshot language learning as shown in GPT-3 <ref type="bibr" target="#b2">(Brown et al., 2020a)</ref>, PET <ref type="bibr" target="#b10">(Schick and Sch?tze, 2021)</ref> and <ref type="bibr">LM-BFF (Gao et al., 2021)</ref>. However, none of them does few-shot fine-tuning in continual posttraining.</p><p>Continual few-shot learning. Several researchers have studied this problem recently <ref type="bibr" target="#b0">(Antoniou et al., 2020;</ref><ref type="bibr">Qin and Joty, 2021;</ref><ref type="bibr">Jin et al., 2021;</ref><ref type="bibr" target="#b21">Xia et al., 2021;</ref><ref type="bibr" target="#b23">Yin et al., 2022)</ref>. It continually learns a sequence of few-shot tasks. However, this is very different from our continual posttraining because our continual learning happens in the post-training stage instead of the end-task finetuning stage. We only evaluate the proposed CPT system after continual post-training by conducting few-shot learning tasks individually by fine-tuning the post-trained language model (p-LM) in each of the post-trained domains. No continual learning is involved in few-shot learning. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the CPT architecture and the task mask learning. Note that fine-tuning is for evaluating the domain post-training and should not affect any parameters of post-training. During continual post-training <ref type="figure" target="#fig_2">(Figure 2 (A)</ref>), after training domain/task 1, we obtain its useful neurons indicated by the 1 entries. Before training domain/task 2, those useful neurons for domain 1 are first masked (those previous 1's entries are turned to 0's). After training domain 2, two neurons with 1 are used by the domain. When domain t arrives, all used neurons by domains 1 and 2 are masked before training, i.e., their entries are set to 0. After training domain t, we see that domains t and 1 have a shared neuron (the cell with two colors, red and green), which is used by both of domains. After continual post-training, we evaluate CPT by individual fine-tuning. During fine-tuning <ref type="figure" target="#fig_2">(Figure 2  (B)</ref>), we only make use of those neurons that are useful for domain/task id t (red cells) and freeze all other neurons (grey cells). <ref type="table" target="#tab_5">Table 3</ref> shows the statistics of the unlabeled domain datasets and end-task classification datasets. Note that the full AGNews is very large. We use only its author provided training split as our domainspecific datasets as our unlabeled AGNews dataset for continual post-training. The remaining testing set is used as the labeled end-task (AGNews-FT). The other three corresponding end task datasets are SemEval-res , ACL-ARC <ref type="bibr">(Jurgens et al., 2018), and</ref><ref type="bibr">SCIERC(Luan et al., 2018)</ref>. . Each CL-plugin module (to the right of the transformer) has two fully connected layers and a skip connection. On top of each fully connected layer, there is a mask computed from task ID t with the same size as the fully connected layer.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Illustration of Task Masks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Dataset Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Details of the CL baselines</head><p>Non-Continual Learning Baselines: Each of these baselines builds a separate model for each task independently. It thus has no CF.</p><p>(1,2) RoBERTa, Adapter <ref type="bibr">(Liu et al., 2019b;</ref><ref type="bibr">Houlsby et al., 2019)</ref> use the original RoBERTa/Adapter for the end-task fine-tuning without any post-training. These are the only two without any post-training. All the following baselines use the masked language model loss (MLM) for post-training.</p><p>(3) RoBERTa-ONE is the existing post-training method in <ref type="bibr">(Gururangan et al., 2020b)</ref>. To our knowledge, the existing post-training systems are all based on the MLM loss.</p><p>(4) Adapter-ONE <ref type="bibr">(Madotto et al., 2020;</ref><ref type="bibr">Houlsby et al., 2019)</ref> adds small adapter layers between layers of Transformer for post-training. We follow the adapter design in <ref type="bibr">(Madotto et al., 2020;</ref><ref type="bibr">Houlsby et al., 2019</ref>). An adapter is simply two fully connected layers. During post-training, the Transformer is fixed, only the added adapters are trainable. The bottleneck size (adapter size) is set to 128. During end-task fine-tuning, both RoBERTa and the adapters are trainable to ensure fair comparison.</p><p>(5) Prompt-ONE (Lester et al., 2021) adds a sequence of real vector tokens (called virtual tokens or prompt tokens) to the end of the original input sequence. In post-training, RoBERTa (the LM) is fixed and only the prompt tokens are trained. In end-task fine-tuning, both LM and the trained prompt are trainable. We initialize 100 tokens and set the learning rate of the prompt token to 0.3 in   Continual Learning (CL) Baselines.</p><p>(7) RoBERTa-NCL (Naive continual learning) is a naive extension of (Gururangan et al., 2020b), which continually/incrementally posttrains the LM to learn all domains using the MLM loss with no mechanism to deal with forgetting or CF.</p><p>(8) Adapter-NCL (Houlsby et al., 2019) is similar to the Adapter based system. The only difference is that the same set of adapters is shared across all domains, rather than using a new adapter for each new domain.</p><p>(9) Hard attention to overcome forgetting (HAT) is derived from HAT <ref type="bibr" target="#b12">(Serr? et al., 2018)</ref>, the state-of-the-art parameter-isolation based method with almost no forgetting. However, HAT suffers from forgetting in continual post-training due to the catastrophic butterfly effect.</p><p>(10) BCL (Ke et al., 2021c) is a continual learning model that can avoid forgetting and encourage knowledge transfer. It is similar to Adapter-NCL. The difference is that its adapters consist of two modules, one is a capsule network (a new capsule is added once a new domain arrives) to encourage transfer, and the other is similar to HAT to avoid forgetting. Similar to HAT, task/domain information is needed in end-task fine-tuning. We replace the backbone network from BERT with RoBERTa for fair comparison.</p><p>(11) Knowledge distillation (KD) (Hinton et al., 2015) minimizes the representational deviation between the learned representation and the new representation in post-training. We compute the KL divergence between the representations (the output before the masked language model prediction head) of each token of the previous post-trained LM and current LM as the distillation loss.</p><p>(12) EWC <ref type="bibr" target="#b4">(Buzzega et al., 2020</ref>) is a popular regularization-based continual learning method that adopts elastic weights consolidation to add L 2 regularization to penalize parameter changes.</p><p>(13) DER++ <ref type="bibr" target="#b4">(Buzzega et al., 2020</ref>) is a recent replay method using distillation to regularize the new task training. We store 16.4K tokens for each learned domain as the memory, which is the largest memory we can use for the system to run. <ref type="table" target="#tab_0">Table 1</ref> in the main paper reported the results for the order Restaurant ? AI ? ACL ? AGnews. We now look at how the order affects the results. <ref type="table">Table 6</ref> shows baselines and CPT's results of 4 different orders. Note that the results for the Non-CL baselines are the same across different orders <ref type="table">Table 6</ref>: CPT performance averaged over all domains after the final post-trained with different orders (averaged over 5 random seeds) and the average of these orders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Results for Different Domain Orders</head><p>(and the same as those in <ref type="table" target="#tab_0">Table 1</ref>) because they are not effected by orders. We can see CPT is always better than other baselines, and achieve 0 forgetting rate, demonstrating the effectiveness of CPT. We also note that some baselines in some sequence has negative forgetting rate, indicating they have some backward transfer (new domain learning helps learned domains). However, their final results are much worse than CPT's. <ref type="table" target="#tab_7">Table 4</ref> reports the standard deviations of the corresponding results in Table 1 (in the main paper) of CPT and the considered baselines over 5 runs with random seeds. We can see the results of CPT are stable. Some baselines (e.g., RoBERTa, RoBERTa-ONE) can have quite large standard deviations. <ref type="table" target="#tab_8">Table 5</ref> reports the standard deviations of the corresponding results in Table 2 (in the main paper) of CPT and the considered baselines over 5 runs with random seeds. We can see the results of sequential adapters has a high variance while CPT and other variants are stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Standard Deviations</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of CPT, which has two CLplugins inserted in the transformer layers of RoBERTa in a parallel manner (FFN: feed-forward network). (A) CPT for continual post-training. It uses a masked language model (MLM) head for unsupervised posttraining of the CL-plugins only. (B) CPT for individual fine-tuning. CPT is evaluated by the corresponding individual end-task performance of all post-trained tasks. Each CL-plugin has numbers and colors indicating its masks and is illustrated in Appendix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and uses masks in continual post-training. Recently, CL has drawn attention in NLP. It has been used for slot filling (Shen et al., 2019), language learning (Li et al., 2019), sentence embedding (Liu et al., 2019a), translation (Khayrallah et al., 2018), cross-lingual modeling (Liu et al., 2020b), question answering (Greco et al., 2019) and text classification (Ke et al., 2021a,b;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of CPT, which has two CL-plugins inserted in the transformer layers of RoBERTa in a parallel manner. (A) CPT for continual post-training. It uses a masked language model (MLM) head for unsupervised post-training of the plugins only. (B) CPT for individual fine-tuning. The performance of CPT is evaluated by the corresponding individual end-task performance of all post-trained tasks using the final post-trained model (with different mask)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The forgetting rate (forget R.) (Liu et al., 2020a) is also reported. The higher the forgetting rate is, the more forgetting it has. Negative rates indicate positive knowledge transfer. 8 Superiority of CPT. Clearly, CPT outperforms all baselines and achieves no forgetting. More specifically, CPT markedly outperforms the two 74.77 27.88 28.44 32.19 34.59 64.19 65.95 43.72 50.94 -Adapter 45.40 67.28 23.69 24.56 24.99 27.55 64.53 66.50 39.65 46.48 -RoBERTa-ONE 53.63 76.73 29.86 30.11 33.05 35.72 62.57 65.13 44.78 51.92 -Adapter-ONE 52.19 74.20 30.80 31.59 36.59 36.99 61.66 63.94 45.31 51.68 -Prompt-ONE 28.93 59.79 21.06 22.10 28.02 29.22 60.70 62.58 34.68 43.42 -DEMIX 53.14 75.28 27.68 27.29 37.63 38.57 63.18 65.13 45.41 51.57 -CL RoBERTa-NCL 42.59 67.56 31.57 31.62 33.07 34.54 60.18 63.50 41.85 49.30 3.27 2.82 Adapter-NCL 47.42 70.23 29.56 29.90 35.92 37.58 61.73 64.45 43.65 50.54 2.21 1.69 HAT 50.45 71.78 28.33 29.41 34.93 37.15 62.97 65.05 44.17 50.85 2.43 2.04 BCL 51.70 74.34 29.66 30.96 32.85 34.82 63.60 65.47 44.45 51.40 1.47 0.82 KD 39.75 67.11 29.63 29.33 38.30 42.09 62.85 65.39 42.63 50.98 4.92 3.07 EWC 48.32 71.59 30.96 31.01 35.96 38.05 62.29 64.95 44.38 51.40 1.40 0.80 DER++ 48.09 71.79 30.71 30.54 34.25 35.77 64.24 66.11 44.32 51.05 1.79 1.62 CPT 53.90 75.13 30.42 30.89 37.56 38.53 63.77 65.79 46.41 52.59 0.00 0.00Table 1: End-task macro-F1 (MF1), accuracy and forgetting rate results for all domains after continual posttraining of all domains. The results are averages of 5 random seeds (the domain training order is as they appear in the first row). Due to space limits, the results for different domain orders and the standard deviations are reported in Appendix E and Appendix F, respectively). Non-CL baselines has no forgetting.</figDesc><table><row><cell>Domain</cell><cell cols="2">Restaurant</cell><cell>AI</cell><cell></cell><cell cols="2">ACL</cell><cell cols="2">AGNews</cell><cell cols="2">Average</cell><cell>Forget R.</cell></row><row><cell>Model</cell><cell>MF1</cell><cell>Acc</cell><cell>MF1</cell><cell>Acc</cell><cell>MF1</cell><cell>Acc</cell><cell>MF1</cell><cell>Acc</cell><cell>MF1</cell><cell cols="2">Acc MF1 Acc</cell></row><row><cell>RoBERTa</cell><cell>50.61</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Non-CL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation experiment results.</figDesc><table /><note>still outperforms other baselines. This indicates the CPT's robustness to domain orders in post-training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, ( Event, August 1-6, 2021. Tianlin Liu, Lyle Ungar, and Joao Sedoc. 2019a. Continual learning for sentence representations using conceptors. arXiv preprint arXiv:1904.09187. Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, and Qianru Sun. 2020a. Mnemonics training: Multiclass incremental learning without forgetting. In CVPR.</figDesc><table><row><cell>2019. Parameter-efficient transfer learning for NLP. In ICML.</cell><cell></cell></row><row><cell>Zikun Hu, Xiang Li, Cunchao Tu, Zhiyuan Liu, and Maosong Sun. 2018. Few-shot charge prediction with discriminative legal attributes. In Proceedings of the 27th International Conference on Computa-tional Linguistics, COLING 2018, Santa Fe, New Mexico, USA, August 20-26, 2018. Yufan Huang, Yanzhe Zhang, Jiaao Chen, Xuezhi Wang, and Diyi Yang. 2021. Continual learn-ing for text classification with information disen-tanglement based regularization. arXiv preprint arXiv:2104.05489. Xisen Jin, Bill Yuchen Lin, Mohammad Rostami, and Xiang Ren. 2021. Learn continually, generalize rapidly: Lifelong knowledge accumulation for few-shot learning. In Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021.</cell><cell>Tianyu Gao, Xu Han, Ruobing Xie, Zhiyuan Liu, Fen Lin, Leyu Lin, and Maosong Sun. 2020. Neural snowball for few-shot relation learning. In The Thirty-Fourth AAAI Conference on Artificial Intelli-gence, AAAI 2020, The Thirty-Second Innovative Ap-plications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. Claudio Greco, Barbara Plank, Raquel Fern?ndez, and Raffaella Bernardi. 2019. Psycholinguistics meets continual learning: Measuring catastrophic forget-ting in visual question answering. arXiv preprint arXiv:1906.04229. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. Roberta: A robustly optimized BERT pretraining ap-proach. CoRR. Zihan Liu, Genta Indra Winata, Andrea Madotto, and Pascale Fung. 2020b. Exploring fine-tuning tech-niques for pre-trained cross-lingual models via con-tinual learning. arXiv preprint arXiv:2004.14218. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-ney, and Daniel S. Weld. 2020. S2ORC: the seman-tic scholar open research corpus. In ACL.</cell></row><row><cell>David Jurgens, Srijan Kumar, Raine Hoover, Daniel A. McFarland, and Dan Jurafsky. 2018. Measuring the evolution of a scientific field through citation frames. TACL.</cell><cell>David Lopez-Paz and Marc'Aurelio Ranzato. 2017. Gradient episodic memory for continual learning. In NIPS. Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of enti-ties, relations, and coreference for scientific knowl-edge graph construction. In ACL.</cell></row><row><cell>Zixuan Ke, Bing Liu, Hu Xu, and Lei Shu. 2021b. Clas-sic: Continual and contrastive learning of aspect sen-timent classification tasks. In EMNLP. Zixuan Ke, Hu Xu, and Bing Liu. 2021c. Adapting bert for continual learning of a sequence of aspect sentiment classification tasks. In NAACL. Huda Khayrallah, Brian Thompson, Kevin Duh, and Philipp Koehn. 2018. Regularized training objective for continued training for domain adaptation in neu-ral machine translation. In Proceedings of the 2nd Workshop on Neural Machine Translation and Gen-eration.</cell><cell>Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer. 2021. Demix layers: Disentangling domains for modular lan-guage modeling. Suchin Gururangan, Ana Marasovi?, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith. 2020a. Don't stop pretraining: adapt language models to domains and tasks. arXiv preprint arXiv:2004.10964. Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020b. Don't stop pretraining: Adapt language models to domains and tasks. In ACL. Andrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, Se-ungwhan Moon, Paul Crook, Bing Liu, Zhou Yu, Eu-njoon Cho, and Zhiguang Wang. 2020. Continual learning in task-oriented dialogue systems. arXiv preprint arXiv:2012.15504. Michael McCloskey and Neal J Cohen. 1989. Catas-trophic interference in connectionist networks: The sequential learning problem. In Psychology of learn-ing and motivation. Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. 2021. An empirical investiga-tion of the role of pre-training in lifelong learning. In ICML CL Workshop.</cell></row><row><cell>Learning Representations. continual learning. In International Conference on hamed Elhoseiny. 2019. Compositional language Yuanpeng Li, Liang Zhao, Kenneth Church, and Mo-James Kirkpatrick, Razvan Pascanu, Neil C. Rabi-nowitz, Joel Veness, Guillaume Desjardins, An-drei A. Rusu, Kieran Milan, John Quan, Tiago Ra-malho, Agnieszka Grabska-Barwinska, Demis Hass-abis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2016. Overcoming catastrophic forgetting in neural networks. CoRR. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In EMNLP.</cell><cell>Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 2021. Towards a unified view of parameter-efficient transfer learning. CoRR. Xu He and Herbert Jaeger. 2018. Overcoming catas-trophic interference using conceptor-aided back-propagation. In ICLR. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. preprint arXiv:1503.02531, (7). Distilling the knowledge in a neural network. arXiv Chengwei Qin and Shafiq Joty. 2021. LFPT5: A uni-fied framework for lifelong few-shot language learn-ing based on prompt tuning of T5. CoRR. Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2022. ELLE: ef-ficient lifelong pre-training for emerging data. In Findings of the Association for Computational Lin-guistics: ACL 2022, Dublin, Ireland, May 22-27, 2022.</cell></row></table><note>Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. 2021. Ppt: Pre-trained prompt tuning for few-shot learning. arXiv preprint arXiv:2109.04332. Yiduo Guo, Bing Liu, and Dongyan Zhao. 2022. On- line continual learning through mutual information maximization. In International Conference on Ma- chine Learning. PMLR.Zixuan Ke, Bing Liu, Nianzu Ma, Hu Xu, and Lei Shu. 2021a. Achieving forgetting prevention and knowl- edge transfer in continual learning. NeurIPS.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Statistics for unlabeled domain datasets and end task supervised classification datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>?0.0274 ?0.0208 ?0.0233 ?0.0391 ?0.0338 ?0.0121 ?0.014 ?0.0066 ?0.0062 Adapter ?0.0214 ?0.0223 ?0.0111 ?0.0102 ?0.0375 ?0.0386 ?0.0221 ?0.0224 ?0.0155 ?0.0142 RoBERTa-ONE ?0.0095 ?0.0087 ?0.0364 ?0.0358 ?0.0382 ?0.0432 ?0.0169 ?0.0162 ?0.0197 ?0.0187 Adapter-ONE ?0.0292 ?0.0223 ?0.0207 ?0.0222 ?0.0076 ?0.0063 ?0.0141 ?0.0157 ?0.0074 ?0.0054 Prompt-ONE ?0.0427 ?0.0991 ?0.0297 ?0.0254 ?0.0386 ?0.0325 ?0.0115 ?0.0100 ?0.0151 ?0.0292 DEMIX ?0.0329 ?0.0293 ?0.0259 ?0.0283 ?0.0297 ?0.0367 ?0.0336 ?0.0309 ?0.0152 ?0.0165 CL RoBERTa-NCL ?0.0374 ?0.0238 ?0.0156 ?0.0158 ?0.0293 ?0.0349 ?0.0218 ?0.0154 ?0.0130 ?0.0160 Adapter-NCL ?0.0250 ?0.0194 ?0.0232 ?0.0184 ?0.0183 ?0.0264 ?0.0136 ?0.0151 ?0.0095 ?0.0137 HAT ?0.0264 ?0.012 ?0.0236 ?0.0251 ?0.0294 ?0.0287 ?0.0106 ?0.009 ?0.0078 ?0.0112 BCL ?0.0255 ?0.0124 ?0.0121 ?0.0105 ?0.0182 ?0.0126 ?0.0100 ?0.0069 ?0.0094 ?0.0032 KD ?0.0642 ?0.0435 ?0.0295 ?0.0233 ?0.0271 ?0.0267 ?0.0160 ?0.0133 ?0.0117 ?0.0109 EWC ?0.0324 ?0.0259 ?0.0281 ?0.0189 ?0.0177 ?0.0196 ?0.0041 ?0.0096 ?0.0079 ?0.0062 DER++ ?0.0250 ?0.0183 ?0.0231 ?0.0319 ?0.0116 ?0.0163 ?0.0196 ?0.0178 ?0.0126 ?0.0128 CPT ?0.0264 ?0.0120 ?0.0236 ?0.0251 ?0.0294 ?0.0287 ?0.0106 ?0.0090 ?0.0078 ?0.0112</figDesc><table><row><cell>Category</cell><cell>Domain Model</cell><cell>Restaurant MF1 Acc</cell><cell>MF1</cell><cell>AI</cell><cell>Acc</cell><cell>MF1</cell><cell>ACL</cell><cell>Acc</cell><cell>AGNews MF1 Acc</cell><cell>Average MF1 Acc</cell></row><row><cell></cell><cell>RoBERTa</cell><cell>?0.0456</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Non-CL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Standard deviations of the corresponding metrics of the proposed CPT system and the baselines.</figDesc><table><row><cell>Model</cell><cell>Final Performance MF1 Acc</cell></row><row><cell cols="2">CPT (Sequential Adapter) ?0.0347 ?0.0350 CPT (w/o butterfly) ?0.0102 ?0.0079 CPT (w/o masking) ?0.0095 ?0.0137 CPT ?0.0078 ?0.0112</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Standard deviations of the corresponding metrics of the proposed CPT system and the ablations. DEMIX (Gururangan et al., 2021) is a recent model to adapt a pre-trained LM with new domains. It adds a new adapter once a new domain arrives (network expansion is needed) and initializes the new adapter with the parameters of the previous trained adapter nearest to the new domain data. They use the perplexity on held-out samples to choose the most probable adapter.</figDesc><table><row><cell>post-training, following the setting in (Lester et al.,</cell></row><row><cell>2021).</cell></row><row><cell>(6)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We will use the term domain in this paper to be consistent with the post-training literature 4 CL has two other settings: class-incremental learning and domain-incremental learning (van de Ven and Tolias, 2019).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For example, fine-tuning an end restaurant sentiment classification task achieves macro-F1 (MF1) of 0.64 right after post-training the restaurant domain but its fine-tuning MF1 drops to 0.44 after post-training three more domains.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Forgetting rate is computed as as follows(Liu et al.,  2020a), 1 t?1 t?1 i=1 Ai,i ? At,i,where Ai,i is the end-task performance right after its domain i is post-trained, and At,i is the performance of the end-task of domain i after posttraining the last domain. We average over all end-tasks except the last one as the last domain has no forgetting.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work of Zixuan Ke and Bing Liu was supported in part by two National Science Foundation (NSF) grants (IIS-1910424 and IIS-1838770).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Defining benchmarks for continual few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Ochal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">One-shot unsupervised cross domain translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sam Mc-Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020b. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner; NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
	<note>virtual</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dark experience for general continual learning: a strong</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Buzzega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Boschini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07211</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">simple baseline. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lifelong machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Sung</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02123</idno>
		<title level="m">Lifelong language knowledge distillation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pathnet: Evolution channels gradient descent in super neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yori</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Online</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Online</meeting>
		<imprint>
			<date type="published" when="2021-04-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Continual learning in generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Beatson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting with hard attention to the task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didac</forename><surname>Suris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A progressive model to enable continual learning for semantic slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Jung Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Using deepspeed and megatron to train megatron-turing NLG 530b</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaden</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zerveas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elton</forename><surname>Zheng</surname></persName>
		</author>
		<editor>Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. 2022</editor>
		<imprint/>
	</monogr>
	<note>A large-scale generative language model. CoRR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How to fine-tune bert for text classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">China national conference on Chinese computational linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lamol: Language modeling is all you need for lifelong language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan-Keng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Hao</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Few-shot learning through an information retrieval lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Three scenarios for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Van De Ven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tolias</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1904.07734" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient meta lifelonglearning with limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnab?s</forename><surname>Sanket Vaibhav Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Incremental few-shot text classification with multi-round new classes: Formulation, dataset and system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</meeting>
		<imprint>
			<date type="published" when="2021-06-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">BERT post-training for review reading comprehension and aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contintin: Continual learning from task instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022-05-22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><forename type="middle">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
