<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Global Registration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Global Registration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Deep Global Registration, a differentiable framework for pairwise registration of real-world 3D scans. Deep global registration is based on three modules: a 6-dimensional convolutional network for correspondence confidence prediction, a differentiable Weighted Procrustes algorithm for closed-form pose estimation, and a robust gradient-based SE(3) optimizer for pose refinement. Experiments demonstrate that our approach outperforms stateof-the-art methods, both learning-based and classical, on real-world data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A variety of applications, including 3D reconstruction, tracking, pose estimation, and object detection, invoke 3D registration as part of their operation <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b33">33]</ref>. To maximize the accuracy and speed of 3D registration, researchers have developed geometric feature descriptors <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b9">9]</ref>, pose optimization algorithms <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b50">50]</ref>, and endto-end feature learning and registration pipelines <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>In particular, recent end-to-end registration networks have proven to be effective in relation to classical pipelines. However, these end-to-end approaches have some drawbacks that limit their accuracy and applicability. For example, PointNetLK <ref type="bibr" target="#b1">[2]</ref> uses globally pooled features to encode the entire geometry of a point cloud, which decreases spatial acuity and registration accuracy. Deep closest point <ref type="bibr" target="#b41">[41]</ref> makes strong assumptions on the distribution of points and correspondences, which do not hold for partially overlapping 3D scans.</p><p>In this work, we propose three modules for robust and accurate registration that resolve these drawbacks: a 6-dimensional convolutional network for correspondence confidence estimation, a differentiable Weighted Procrustes method for scalable registration, and a robust SE(3) optimizer for fine-tuning the final alignment.</p><p>The first component is a 6-dimensional convolutional network that analyzes the geometry of 3D correspondences and estimates their accuracy. Our approach is inspired * indicates equal contribution (a) RANSAC <ref type="bibr" target="#b35">[35]</ref> (b) FGR <ref type="bibr" target="#b50">[50]</ref> (c) DCP <ref type="bibr" target="#b41">[41]</ref> (d) Ours <ref type="figure">Figure 1</ref>: Pairwise registration results on the 3DMatch dataset <ref type="bibr" target="#b47">[47]</ref>. Our method successfully aligns a challenging 3D pair (left), while RANSAC <ref type="bibr" target="#b35">[35]</ref>, FGR <ref type="bibr" target="#b50">[50]</ref>, and DCP <ref type="bibr" target="#b41">[41]</ref> fail. On an easier pair (right), our method achieves finer alignment. by a number of learning-based methods for estimating the validity of correspondences in 2D <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b34">34]</ref> and 3D <ref type="bibr" target="#b31">[31]</ref>. These methods stack coordinates of correspondence pairs, forming a vector [x; y] ? R 2?D for each correspondence x, y ? R D . Prior methods treat these 2 ? D-dimensional vectors as a set, and apply global set processing models for analysis. Such models largely disregard local geometric structure. Yet the correspondences are embedded in a metric space (R 2?D ) that induces distances and neighborhood relationships. In particular, 3D correspondences form a geometric structure in 6-dimensional space <ref type="bibr" target="#b8">[8]</ref> and we use a high-dimensional convolutional network to analyze the 6D structure formed by correspondences and estimate the likelihood that a given correspondence is correct (i.e., an inlier).</p><p>The second component we develop is a differentiable Weighted Procrustes solver. The Procrustes method <ref type="bibr" target="#b15">[15]</ref> provides a closed-form solution for rigid registration in SE <ref type="bibr" target="#b3">(3)</ref>. A differentiable version of the Procrustes method by Wang et al. <ref type="bibr" target="#b41">[41]</ref> has been used for end-to-end registration. However, the differentiable Procrustes method passes gradients through coordinates, which requires O(N 2 ) time and memory for N keypoints, limiting the number of keypoints that can be processed by the network. We use the inlier probabilities predicted by our first module (the 6D convolutional network) to guide the Procrustes method, thus forming a differentiable Weighted Procrustes method. This method passes gradients through the weights associated with correspondences rather than correspondence coordinates. The computational complexity of the Weighted Procrustes method is linear in the number of correspondences, allowing the registration pipeline to use dense correspondence sets rather than sparse keypoints. This substantially increases registration accuracy.</p><p>Our third component is a robust optimization module that fine-tunes the alignment produced by the Weighted Procrustes solver. This optimization module minimizes a differentiable loss via gradient descent on the continuous SE(3) representation space <ref type="bibr" target="#b52">[52]</ref>. The optimization is fast since it does not require neighbor search in the inner loop <ref type="bibr" target="#b48">[48]</ref>.</p><p>Experimentally, we validate the presented modules on a real-world pairwise registration benchmark <ref type="bibr" target="#b47">[47]</ref> and large-scale scene reconstruction datasets <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b32">32]</ref>.</p><p>We show that our modules are robust, accurate, and fast in comparison to both classical global registration algorithms <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b45">45]</ref> and recent end-toend approaches <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b1">2]</ref>. All training and experiment scripts are available at https://github.com/ chrischoy/DeepGlobalRegistration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We divide the related work into three categories following the stages of standard registration pipelines that deal with real-world 3D scans: feature-based correspondence matching, outlier filtering, and pose optimization. Feature-based correspondence matching. The first step in many 3D registration pipelines is feature extraction. Lo-cal and global geometric structure in 3D is analyzed to produce high-dimensional feature descriptors, which can then be used to establish correspondences.</p><p>Traditional hand-crafted features commonly summarize pairwise or higher-order relationships in histograms <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b35">35]</ref>. Recent work has shifted to learning features via deep networks <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b22">22]</ref>. A number of recent methods are based on global pooling models <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b49">49]</ref>, while others use convolutional networks <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b9">9]</ref>.</p><p>Our work is agnostic to the feature extraction mechanism. Our modules primarily address subsequent stages of the registration pipeline and are compatible with a wide variety of feature descriptors.</p><p>Outlier filtering. Correspondences produced by matching features are commonly heavily contaminated by outliers. These outliers need to be filtered out for robust alignment. A widely used family of techniques for robust model fitting is based on RANdom SAmple Consensus (RANSAC) <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b19">19]</ref>, which iteratively samples small sets of correspondences in the hope of sampling a subset that is free from outliers. Other algorithms are based on branch-and-bound <ref type="bibr" target="#b45">[45]</ref>, semi-definite programming <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b25">25]</ref>, and maximal clique selection <ref type="bibr" target="#b44">[44]</ref>. These methods are accurate, but commonly require longer iterative sampling or more expensive computation as the signalto-noise ratio decreases. One exception is TEASER <ref type="bibr" target="#b44">[44]</ref>, which remains effective even with high outlier rates. Other methods use robust loss functions to reject outliers during optimization <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b4">4]</ref>.</p><p>Our work uses a convolutional network to identify inliers and outliers. The network needs only one feed-forward pass at test time and does not require iterative optimization.</p><p>Pose optimization. Pose optimization is the final stage that minimizes an alignment objective on filtered correspondences. Iterative Closest Points (ICP) <ref type="bibr" target="#b3">[3]</ref> and Fast Global Registration (FGR) <ref type="bibr" target="#b50">[50]</ref> use second-order optimization to optimize poses. Makadia et al. <ref type="bibr" target="#b26">[26]</ref> propose an iterative procedure to minimize correlation scores. Maken et al. <ref type="bibr" target="#b27">[27]</ref> propose to accelerate this process by stochastic gradient descent.</p><p>Recent end-to-end frameworks combine feature learning and pose optimization. Aoki et al. <ref type="bibr" target="#b1">[2]</ref> combine Point-Net global features with an iterative pose optimization method <ref type="bibr" target="#b24">[24]</ref>. Wang et al. <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42]</ref> train graph neural network features by backpropagating through pose optimization.</p><p>We further advance this line of work. In particular, our Weighted Procrustes method reduces the complexity of optimization from quadratic to linear and enables the use of dense correspondences for highly accurate registration of real-world scans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Global Registration</head><p>3D reconstruction systems typically take a sequence of partial 3D scans as input and recover a complete 3D model of the scene. These partial scans are scene fragments, as shown in <ref type="figure">Fig. 1</ref>. In order to reconstruct the scene, reconstruction systems often begin by aligning pairs of fragments <ref type="bibr" target="#b6">[6]</ref>. This stage is known as pairwise registration. The accuracy and robustness of pairwise registration are critical and often determine the accuracy of the final reconstruction.</p><p>Our pairwise registration pipeline begins by extracting pointwise features. These are matched to form a set of putative correspondences. We then use a high-dimensional convolutional network (ConvNet) to estimate the veracity of each correspondence. Lastly, we use a Weighted Procrustes method to align 3D scans given correspondences with associated likelihood weights, and refine the result by optimizing a robust objective.</p><p>The following notation will be used throughout the paper. We consider two point clouds,</p><formula xml:id="formula_0">X = [x 1 , ..., x Nx ] ? R 3?Nx and Y = [y 1 , ..., y Ny ] ? R 3?Ny , with N x and N y points respectively, where x i , y j ? R 3 . A correspondence between x i and y j is denoted as x i ? y j or (i, j).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Extraction</head><p>To prepare for registration, we extract pointwise features that summarize geometric context in the form of vectors in metric feature space. Our pipeline is compatible with many feature descriptors. We use Fully Convolutional Geometric Features (FCGF) <ref type="bibr" target="#b9">[9]</ref>, which have recently been shown to be both discriminative and fast. FCGF are also compact, with dimensionality as low as 16 to 32, which supports rapid neighbor search in feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Correspondence Confidence Prediction</head><p>Given the features F x = {f x1 , ..., f x Nx } and F y = {f y1 , ..., f y Nx } of two 3D scans, we use the nearest neighbor in the feature space to generate a set of putative correspondences or matches</p><formula xml:id="formula_1">M = {(i, argmin j f xi ? f yj )|i ? [1, ..., N x ]}.</formula><p>This procedure is deterministic and can be hand-crafted to filter out noisy correspondences with ratio or reciprocity tests <ref type="bibr" target="#b50">[50]</ref>. However, we propose to learn this heuristic filtering process through a convolutional network that learns to analyze the underlying geometric structure of the correspondence set.</p><p>We first provide a 1-dimensional analogy to explain the geometry of correspondences. Let A be a set of 1-dimensional points A = {0, 1, 2, 3, 4} and B be another such set B = {10, 11, 12, 13, 14}. Here B is a translation of A: B = {a i + 10|a i ? A}. If an algorithm returns a set of possible correspondences {(0, 10), <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b11">11)</ref>, <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b12">12)</ref>, <ref type="bibr" target="#b3">(3,</ref><ref type="bibr" target="#b13">13)</ref>, <ref type="bibr" target="#b4">(4,</ref><ref type="bibr" target="#b14">14)</ref>, (0, 14), (4, 10)}, then the set of correct correspondences (inliers) will form a Figure 2: 6-dimensional convolutional network architecture for inlier likelihood prediction (Sec. 3.2). The network has a U-net structure with residual blocks between strided convolutions. Best viewed on the screen. line (first 5 pairs), whereas incorrect correspondences (outliers) will form random noise outside the line (last 2 pairs). If we extend this to 3D scans and pointclouds, we can also represent a 3D correspondence x i ? y j as a point in</p><formula xml:id="formula_2">6-dimensional space [x T i , y T j ] T ? R 6 .</formula><p>The inlier correspondences will be distributed on a lower-dimensional surface in this 6D space, determined by the geometry of the 3D input.</p><formula xml:id="formula_3">We denote P = {(i, j)| T * (x i ) ? y j &lt; ?, (i, j) ? M}</formula><p>as a set of inliers or a set of correspondences (i, j) that align accurately up to the threshold ? under the ground truth transformation T * . Meanwhile, the outliers N = P C ? M will be scattered outside the surface P. To identify the inliers, we use a convolutional network. Such networks have been proven effective in related dense prediction tasks, such as 3D point cloud segmentation <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b7">7]</ref>. The convolutional network in our setting is in 6-dimensional space <ref type="bibr" target="#b8">[8]</ref>. The network predicts a likelihood for each correspondence, which is a point in 6D space [x T i , y T j ] T . The prediction is interpreted as the likelihood that the correspondence is true: an inlier.</p><p>Note that the convolution operator is translation invariant, thus our 6D ConvNet will generate the same output regardless of the absolute position of inputs in 3D. We use a similar network architecture to Choy et al. <ref type="bibr" target="#b9">[9]</ref> to create a 6D convolutional network with skip connections within the spatial resolution across the network. The architecture of the 6D ConvNet is shown in <ref type="figure">Fig. 2</ref>. During training, we use the binary cross-entropy loss between the likelihood prediction that a correspondence (i, j) is an inlier, p (i,j) ? [0, 1], and the ground-truth correspondences P to optimize the network parameters:</p><formula xml:id="formula_4">L bce (M, T * ) = 1 |M| (i,j)?P log p (i,j) + (i,j)?N log p C (i,j) ,<label>(1)</label></formula><p>where p C = 1 ? p and |M| is the cardinality of the set of putative correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Weighted Procrustes for SE(3)</head><p>The inlier likelihood estimated by the 6D ConvNet provides a weight for each correspondence. The original Procrustes method <ref type="bibr" target="#b15">[15]</ref> minimizes the mean squared error between corresponding points 1 N (i,j)?M x i ? y j 2 and thus gives equal weight to all correspondences. In contrast, we minimize a weighted mean squared error</p><formula xml:id="formula_5">(i,j)?M w (i,j) x i ? y j 2 .</formula><p>This change allows us to pass gradients through the weights, rather than through the position <ref type="bibr" target="#b41">[41]</ref>, and enables the optimization to scale to dense correspondence sets.</p><p>Formally, Weighted Procrustes analysis minimizes:</p><formula xml:id="formula_6">e 2 = e 2 (R, t; w, X, Y ) (2) = (i,j)?Mw (i,j) (y j ? (Rx i + t)) 2 (3) = Tr (Y ? RX ? t1 T )W (Y ? RX ? t1 T ) T , (4) where 1 = (1, ..., 1) T , X = [x 1 , ..., x |M| ], and Y = [y J1 , ..., y J |M| ]. J is a list of indices that defines the corre- spondences x i ? y Ji . w = [w 1 , ? ? ? , w |M| ] is the weight vector andw = [w 1 , ? ? ? ,w |M| ] ?(w)</formula><p>||?(w)||1 denotes the normalized weight after a nonlinear transformation ? that applies heuristic prefiltering. W = diag(w) forms the diagonal weight matrix.</p><p>Theorem 1 : The R and t that minimize the squared error e 2 (R, t)</p><formula xml:id="formula_7">= (i,j) w (i,j) (y j ? Rx i ? t) 2 aret = (Y ? RX)W 1 andR = U SV T where U ?V T = SVD(? xy ), ? xy = Y KW KX T , K = I ? ?w?w T , and S = diag (1, ? ? ? , 1, det(U )det(V )).</formula><p>Sketch of proof. First, we differentiate e 2 w.r.t. t and equate the partial derivative to 0. This gives ust = (Y ? RX)W 1. Next, we substitute X = KX + X ?w?w T on Eq. 4 and do the same for Y . Then, we substitute t =t into the squares. This yields e 2 = Tr (Y ? RX)KW K T (Y ? RX) T and expanding the term results in two squares plus ?2Tr(Y KW K T X T R T ). We maximize the last negative term whose maximum is the sum of all singular values, which leads toR. The full derivation is in the supplement.</p><p>We can easily extend the above theorem to incorporate a scaling factor c ? R + , or anisotropic scaling for tasks such as scan-to-CAD registration, but in this paper we assume that partial scans of the same scene have the same scale.</p><p>The Weighted Procrustes method generates rotationR and translationt as outputs that depend on the weight vector w. In our current implementation,R andt are directly sent to the robust registration module in Section 4 as an initial pose. However, we briefly demonstrate that they can also be embedded in an end-to-end registration pipeline, since Weighted Procrustes is differentiable. From a top-level loss function L ofR andt, we can pass the gradient through the closed-form solver, and update parameters in downstream modules:</p><formula xml:id="formula_8">? ?w L(R,t) = ?L(R,t) ?R ?R ?? + ?L(R,t) ?t ?t(R,?) ?? ,<label>(5)</label></formula><p>where L(R,t) can be defined as the combination of differentiable rotation error (RE) and translation error (TE) between predictionsR,t and ground-truth R * , t * :</p><formula xml:id="formula_9">L rot (R) = arccos Tr(R T R * ) ? 1 2 ,<label>(6)</label></formula><formula xml:id="formula_10">L trans (t) =||t ? t * || 2 2 ,<label>(7)</label></formula><p>or the Forbenius norm of relative transformation matrices defined in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b41">41]</ref>. The final loss is the weighted sum of L rot , L trans , and L bce .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Robust Registration</head><p>In this section, we propose a fine-tuning module that minimizes a robust loss function of choice to improve the registration accuracy. We use a gradient-based method to refine poses, where a continuous representation <ref type="bibr" target="#b52">[52]</ref> for rotations is adopted to remove discontinuities and construct a smooth optimization space. This module initializes the pose from the prediction of the Weighted Procrustes method. During iterative optimization, unlike Maken et al. <ref type="bibr" target="#b27">[27]</ref>, who find the nearest neighbor per point at each gradient step, we rely on the correspondence likelihoods from the 6D Con-vNet, which is estimated only once per initialization.</p><p>In addition, our framework naturally offers a failure detection mechanism. In practice, Weighted Procrustes may generate numerically unstable solutions when the number of valid correspondences is insufficient due to small overlaps or noisy correspondences between input scans. By computing the ratio of the sum of the filtered weights to the total number of correspondences, i.e. i ?(w i )/|M|, we can easily approximate the fraction of valid correspondences and predict whether an alignment may be unstable. When this fraction is low, we resort to a more time-consuming but accurate registration algorithm such as RANSAC <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b35">35]</ref> or a branch-and-bound method <ref type="bibr" target="#b45">[45]</ref> to find a numerically stable solution. In other words, we can detect when our system might fail before it returns a result and fall back to a more accurate but time-consuming algorithm, unlike previous end-to-end methods that use globally pooled latent features <ref type="bibr" target="#b1">[2]</ref> or a singly stochastic matrix <ref type="bibr" target="#b41">[41]</ref> -such latent representations are more difficult to interpret.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">SE(3) Representation and Initialization</head><p>We use the 6D representation of 3D rotation proposed by Zhou et al. <ref type="bibr" target="#b52">[52]</ref>, rather than Euler angles or quaternions.</p><p>The new representation uses 6 parameters a 1 , a 2 ? R 3 and can be transformed into a 3 ? 3 orthogonal matrix by</p><formula xml:id="formula_11">f ? ? ? ? | | a 1 a 2 | | ? ? ? ? = ? ? | | | b 1 b 2 b 3 | | | ? ? ,<label>(8)</label></formula><formula xml:id="formula_12">where b 1 , b 2 , b 3 ? R 3 are b 1 = N (a 1 ), b 2 = N (a 2 ? (b 1 ? a 2 )b 1 ), and b 3 = b 1 ? b 2 ,</formula><p>and N (?) denotes L2 normalization. Thus, the final representation that we use is a 1 , a 2 , t which are equivalent to R, t using Eq. 8.</p><p>To initialize a 1 , a 2 , we simply use the first two columns of the rotation matrix R, i.e., b 1 , b 2 . For convenience, we define f ?1 as f ?1 (f (R)) = R though this inverse function is not unique as there are infinitely many choices of a 1 , a 2 that map to the same R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Energy Minimization</head><p>We use a robust loss function to fine-tune the registration between predicted inlier correspondences. The general form of the energy function is</p><formula xml:id="formula_13">E(R, t) = n i=1 ?(w (i,Ji) )L(y Ji , Rx i + t),<label>(9)</label></formula><p>wherew i and J i are defined as in Eq. 3 and ?(?) is a prefiltering function. In the experiments, we use ?(w) = I[w &gt; ? ]w, which clips weights below ? elementwise as neural network outputs bounded logit scores. L(x, y) is a pointwise loss function between x and y; we use the Huber loss in our implementation. The energy function is parameterized by R and t which in turn are represented as a 1 , a 2 , t. We can apply first-order optimization algorithms such as SGD, Adam, etc. to minimize the energy function, but higher-order optimizers are also applicable since the number of parameters is small. The complete algorithm is described in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We analyze the proposed model in two registration scenarios: pairwise registration where we estimate an SE(3) transformation between two 3D scans or fragments, and multi-way registration which generates a final reconstruction and camera poses for all fragments that are globally consistent. Here, pairwise registration serves as a critical module in multi-way registration.</p><p>For pairwise registration, we use the 3DMatch benchmark <ref type="bibr" target="#b47">[47]</ref> which consists of 3D point cloud pairs from various real-world scenes with ground truth transformations estimated from RGB-D reconstruction pipelines <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b10">10]</ref>. We follow the train/test split and the standard procedure to generate pairs with at least 30% overlap for training and testing <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b9">9]</ref>. For multi-way registration, we use the simulated Augmented ICL-NUIM dataset <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b18">18]</ref> for quantitative</p><formula xml:id="formula_14">Algorithm 1: Deep Global Registration Input: X ? R n?3 , Y ? R m?3 Output: R ? SO(3), t ? R 3?1 1 F x ? Feature(X) // ? 3.1 2 F y ? Feature(Y ) 3 J x?y ? NearestNeighbor(F x , F y ) // ? 3.2 4 M ? {(i, J x?y,i ) | i ? [1, ..., n]} 5 w ? InlierProbability(M) 6 if E i ?(w i ) &lt; ? s then 7 return SafeGuardRegistration(X, Y ) // ?4 8 else 9R,t ? argmin R,t e 2 (R, t; w, X, Y ) // ? 3.3 10 a ? f ?1 (R), t ?t // ? 4.1 11 while not converging do 12 ? (i,j)?M ?(w (i,j) )L(Y j , f (a)X i + t) 13 a ? Update(a, ? ?a (a, t))</formula><p>trajectory results, and Indoor LiDAR RGB-D dataset <ref type="bibr" target="#b32">[32]</ref> and Stanford RGB-D dataset <ref type="bibr" target="#b6">[6]</ref> for qualitative registration visualizations. Note in this experiment we use networks trained on the 3DMatch training set and do not fine-tune on the other datasets. This illustrates the generalization abilities of our models. Lastly, we use KITTI LIDAR scans <ref type="bibr" target="#b13">[13]</ref> for outdoor pairwise registration. As the official registration splits do not have labels for pairwise registration, we follow Choy et al. <ref type="bibr" target="#b9">[9]</ref> to create pairwise registration train/val/test splits. For all indoor experiments, we use 5cm voxel downsampling <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b51">51]</ref>, which randomly subsamples a single point within each 5cm voxel to generate point clouds with uniform density. For safeguard registration, we use RANSAC and the safeguard threshold ? s = 0.05, which translates to 5% of the correspondences should be valid. We train learning-based state-of-the-art models and our network on the training split of the 3DMatch benchmark. During training, we augment data by applying random rotations varying from ?180 to 180 degrees around a random axis. Groundtruth pointwise correspondences are found using nearest neighbor search in 3D space. We train the 6-dimensional ConvNet on a single Titan XP with batch size 4. SGD is used with an initial learning rate 10 ?1 and an exponential learning rate decay factor 0.99.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Pairwise Registration</head><p>In this section, we report the registration results on the test set of the 3DMatch benchmark <ref type="bibr" target="#b47">[47]</ref>, which contains 8 different scenes as depicted in <ref type="figure" target="#fig_0">Fig. 3</ref>. We measure translation error (TE) defined in Eq. 6, rotation error (RE) defined  Our approach outperforms baseline methods for all thresholds while being 6.5? faster than the most accurate baseline.</p><p>in Eq. 7, and recall. Recall is the ratio of successful pairwise registrations and we define a registration to be successful if its rotation error and translation error are smaller than predefined thresholds. Average TE and RE are computed only on these successfully registered pairs since failed registrations return poses that can be drastically different from the ground truth, making the error metrics unreliable.</p><p>We compare our methods with various classical methods <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b45">45]</ref> and state-of-the-art learning based methods <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b31">31]</ref>. All the experiments are evaluated on an Intel i7-7700 CPU and a GTX 1080Ti graphics card except for Go-ICP <ref type="bibr" target="#b45">[45]</ref> tested on an Intel i7-5820K CPU.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we measure recall with the TE threshold 30cm which is typical for indoor scene relocalization <ref type="bibr" target="#b30">[30]</ref>, and RE threshold 15 degrees which is practical for partially overlapping scans from our experiments. In <ref type="figure" target="#fig_1">Fig. 4</ref>, we plot the sensitivity of recall on both thresholds by changing one threshold and setting the other to infinity. <ref type="figure">Fig. 5</ref> includes detailed statistics on separate test scenes. Our system outperforms all the baselines on recall by a large margin and achieves the lowest translation and rotation error consistently on most scenes. Classical methods. To compare with classical methods, we evaluate point-to-point ICP, Point-to-plane ICP, RANSAC <ref type="bibr" target="#b35">[35]</ref>, and FGR <ref type="bibr" target="#b50">[50]</ref>, all implemented in Open3D <ref type="bibr" target="#b51">[51]</ref>. In addition, we test the open-source Python bindings of Go-ICP <ref type="bibr" target="#b45">[45]</ref> and Super4PCS <ref type="bibr" target="#b29">[29]</ref>. For RANSAC and FGR, we extract FPFH from voxeldownsampled point clouds. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>ICP variants mostly fail as the dataset contains challenging 3D scan sequences with small overlap and large camera viewpoint change. Super4PCS, a sampling-based algorithm, performs similarly to Go-ICP, an ICP variant with branch-and-bound search.</p><p>Feature-based methods, FGR and RANSAC, perform better. When aligning 5cm-voxel-downsampled point clouds, RANSAC achieves recall as high as 70%, while FGR reaches 40%. <ref type="table" target="#tab_0">Table 1</ref> also shows that increasing the number of RANSAC iterations by a factor of 2 only im-  <ref type="figure">Figure 5</ref>: Analysis of 3DMatch registration results per scene. Row 1: recall rate (higher is better). Row 2-3: TE and RE measured on successfully registered pairs (lower is better). Our method is consistently better on all scenes, which were not seen during training. Note: a missing bar corresponds to zero successful alignments in a scene. proves performance marginally. Note that our method is about twice as fast as RANSAC with 2M iterations while achieving higher recall and registration accuracy.</p><p>Learning-based methods. We use 3DRegNet <ref type="bibr" target="#b31">[31]</ref>, Deep Closest Point (DCP) <ref type="bibr" target="#b41">[41]</ref>, PRNet <ref type="bibr" target="#b42">[42]</ref>, and PointNetLK <ref type="bibr" target="#b1">[2]</ref> as our baselines. We train all the baselines on 3DMatch with the same setup and data augmentation as ours for all experiments.</p><p>For 3DRegNet, we follow the setup outlined in <ref type="bibr" target="#b31">[31]</ref>, except that we do not manually filter outliers with ground truth, and train and test with the standard realistic setup. We find that the registration loss of 3DRegNet does not converge during training and the rotation and translation errors are consistently above 30 degrees and 1m during test.</p><p>We train Deep Closest Point (DCP) with 1024 randomly sampled points for each point cloud for 150 epochs <ref type="bibr" target="#b41">[41]</ref>. We initialize the network with the pretrained weights provided by the authors. Although the training loss converges, DCP fails to achieve reasonable performance for point clouds with partial overlap. DCP uses a singly stochastic matrix to find correspondences, but this formulation assumes that all points in point cloud X have at least one corresponding point in the convex hull of point cloud Y . This assumption fails when some points in X have no corresponding points in Y , as is the case for partially overlapping fragments. We also tried to train PRNet <ref type="bibr" target="#b42">[42]</ref> on our setup, but failed to get reasonable results due to random crashes and high-variance training losses. Lastly, we fine-tune PointNetLK <ref type="bibr" target="#b1">[2]</ref> on 3DMatch for 400 epochs, starting from the pretrained weights provided by the authors. PointNetLK uses a single feature that is globally pooled for each point cloud and regresses the relative pose between objects, and we suspect that a globally pooled feature fails to capture complex scenes such as 3DMatch.</p><p>In conclusion, while working well on object-centric synthetic datasets, current end-to-end registration approaches fail on real-world data. Unlike synthetic data, real 3D point cloud pairs contain multiple objects, partial scans, selfocclusion, substantial noise, and may have only a small degree of overlap between scans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Multi-way Registration</head><p>Multi-way registration for RGB-D scans proceeds via multiple stages. First, the pipeline estimates the camera pose via off-the-shelf odometry and integrates multiple 3D scans to reduce noise and generate accurate 3D fragments of a scene. Next, a pairwise registration algorithm roughly aligns all fragments, followed by multi-way registration <ref type="bibr" target="#b6">[6]</ref> which optimizes fragment poses with robust pose graph optimization <ref type="bibr" target="#b23">[23]</ref>.</p><p>We use a popular open-source implementation of this registration pipeline <ref type="bibr" target="#b51">[51]</ref> and replace the pairwise registration stage in the pipeline with our proposed modules. Note that we use the networks trained on the 3DMatch training set and test on the multi-way registration datasets <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b6">6]</ref>; this demonstrates cross-dataset generalization.</p><p>We test the modified pipeline on the Augmented ICL-NUIM dataset <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b18">18]</ref> for quantitative trajectory results, and Indoor LiDAR RGB-D dataset <ref type="bibr" target="#b32">[32]</ref> and Stanford RGB-D dataset <ref type="bibr" target="#b6">[6]</ref> for qualitative registration visualizations. We measure the absolute trajectory error (ATE) on the Augmented ICL-NUIM dataset with simulated depth noise. As shown in <ref type="table" target="#tab_1">Table 2</ref>, compared to state-of-the-art online SLAM <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b39">39]</ref> and offline reconstruction methods <ref type="bibr" target="#b50">[50]</ref>, our approach yields consistently low error across scenes.</p><p>For qualitative results, we compare pairwise fragment registration on these scenes against FGR and RANSAC in <ref type="table">Table 3</ref>: Registration on the KITTI test split <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b9">9]</ref>. We use thresholds of 0.6m and 5 degrees. 'Ours + ICP' refers to our method followed by ICP for fine-grained pose adjustment. The runtime includes feature extraction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Outdoor LIDAR Registration</head><p>We use outdoor LIDAR scans from the KITTI dataset <ref type="bibr" target="#b13">[13]</ref> for registration, following <ref type="bibr" target="#b9">[9]</ref>. The registration split of Choy et al. <ref type="bibr" target="#b9">[9]</ref> uses GPS-IMU to create pairs that are at least 10m apart and generated ground-truth transformation using GPS followed by ICP to fix errors in GPU readings. We use FCGF features <ref type="bibr" target="#b9">[9]</ref> trained on the training set of the registration split to find the correspondences and trained the 6D ConvNet for inlier confidence prediction similar to how we trained the system for indoor registration. We use voxel size 30cm for downsampling point clouds for all experiments. Registration results are reported in Tab. 3 and visualized in <ref type="figure">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented Deep Global Registration, a learningbased framework that robustly and accurately aligns realworld 3D scans. To achieve this, we used a 6D convolutional network for inlier detection, a differentiable Weighted Procrustes algorithm for scalable registration, and a gradient-based optimizer for pose refinement. Experiments show that our approach outperforms both classical and learning-based registration methods, and can serve as a ready-to-use plugin to replace alternative registration methods in off-the-shelf scene reconstruction pipelines.  <ref type="figure" target="#fig_3">Figure 6</ref>: Fragment registrations on <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b32">32]</ref>. From left to right: FGR <ref type="bibr" target="#b50">[50]</ref>, RANSAC <ref type="bibr" target="#b35">[35]</ref>, Ours. Row 1-3: our method succeeds on scenes with small overlaps or ambiguous geometry structures while other methods fail. Row 4-6: by combining Weighted Procrustes and gradient-based refinement, our method outputs more accurate registrations in one pass, leading to better aligned details.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Global registration results of our method on all 8 different test scenes in 3DMatch<ref type="bibr" target="#b47">[47]</ref>. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Overall pairwise registration recall (y-axis) on the 3DMatch benchmark with varying rotation (left image) and translation (right image) error thresholds (x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>-to-point) ICP (Point-to-plane) Super4PCS Go-ICP FGR RANSAC Ours w/o safeguard Ours</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Full scene reconstruction results are shown in the supplement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Real-world: Apartment(b) Real-world: Boardroom (c) Synthetic: Office (d) Real-world: Copyroom (e) Real-world: Loft (f) Synthetic: Livingroom</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Row 1-6: registration results of our method and classical global registration methods on point clouds voxelized with 5cm voxel size. Our method outperforms RANSAC and FGR while being as fast as FGR. Row 7-10: results of ICP variants. Row 11 -12: results of learningbased methods. The learning-based methods generally fail on real-world scans. Time includes feature extraction.</figDesc><table><row><cell></cell><cell cols="4">Recall TE (cm) RE (deg) Time (s)</cell></row><row><cell>Ours w/o safeguard</cell><cell>85.2%</cell><cell>7.73</cell><cell>2.58</cell><cell>0.70</cell></row><row><cell>Ours</cell><cell>91.3%</cell><cell>7.34</cell><cell>2.43</cell><cell>1.21</cell></row><row><cell>FGR [50]</cell><cell>42.7%</cell><cell>10.6</cell><cell>4.08</cell><cell>0.31</cell></row><row><cell>RANSAC-2M [35]</cell><cell>66.1%</cell><cell>8.85</cell><cell>3.00</cell><cell>1.39</cell></row><row><cell>RANSAC-4M</cell><cell>70.7%</cell><cell>9.16</cell><cell>2.95</cell><cell>2.32</cell></row><row><cell>RANSAC-8M</cell><cell>74.9%</cell><cell>8.96</cell><cell>2.92</cell><cell>4.55</cell></row><row><cell>Go-ICP [45]</cell><cell>22.9%</cell><cell>14.7</cell><cell>5.38</cell><cell>771.0</cell></row><row><cell>Super4PCS [29]</cell><cell>21.6%</cell><cell>14.1</cell><cell>5.25</cell><cell>4.55</cell></row><row><cell>ICP (P2Point) [51]</cell><cell>6.04%</cell><cell>18.1</cell><cell>8.25</cell><cell>0.25</cell></row><row><cell>ICP (P2Plane) [51]</cell><cell>6.59%</cell><cell>15.2</cell><cell>6.61</cell><cell>0.27</cell></row><row><cell>DCP [41]</cell><cell>3.22%</cell><cell>21.4</cell><cell>8.42</cell><cell>0.07</cell></row><row><cell>PointNetLK [2]</cell><cell>1.61%</cell><cell>21.3</cell><cell>8.04</cell><cell>0.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ATE (cm) error on the Augmented ICL-NUIM dataset with simulated depth noise. For InfiniTAM, the loop closure module is disabled since it fails in all scenes. For BAD-SLAM, the loop closure module only succeeds in 'Living room 2'.</figDesc><table><row><cell></cell><cell cols="3">ElasticFusion [43] InfiniTAM [21] BAD-SLAM [39]</cell><cell cols="3">Multi-way + FGR [50] Multi-way + RANSAC [51] Multi-way + Ours</cell></row><row><cell>Living room 1</cell><cell>66.61</cell><cell>46.07</cell><cell>fail</cell><cell>78.97</cell><cell>110.9</cell><cell>21.06</cell></row><row><cell>Living room 2</cell><cell>24.33</cell><cell>73.64</cell><cell>40.41</cell><cell>24.91</cell><cell>19.33</cell><cell>21.88</cell></row><row><cell>Office 1</cell><cell>13.04</cell><cell>113.8</cell><cell>18.53</cell><cell>14.96</cell><cell>14.42</cell><cell>15.76</cell></row><row><cell>Office 2</cell><cell>35.02</cell><cell>105.2</cell><cell>26.34</cell><cell>21.05</cell><cell>17.31</cell><cell>11.56</cell></row><row><cell>Avg. Rank</cell><cell>3</cell><cell>5</cell><cell>5</cell><cell>3.5</cell><cell>2.5</cell><cell>2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">t ? Update(t, ? ?t (a, t)) 15 return f (a), t</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">4-points congruent sets for robust pairwise surface registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dror</forename><surname>Aiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">(a) KITTI registration test pair 1 (b) KITTI registration test pair 2 (c) KITTI registration test pair 3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hunter</forename><surname>Goforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rangaprasad Arun Srivatsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>PointNetLK: Robust &amp; efficient point cloud registration using PointNet</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Figure 7: KITTI registration results. Pairs of scans are at least 10m apart</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Bes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sparse iterative closest point</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofien</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3D deformable face tracking with a commodity depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust reconstruction of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">4D spatio-temporal ConvNets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High-dimensional convolutional networks for geometric pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junha</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully convolutional geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BundleFusion: Real-time globally consistent 3D reconstruction using on-the-fly surface re-integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zoll?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PPF-FoldNet: Unsupervised learning of rotation invariant 3D local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PPFNet: Global context aware local features for robust 3D point matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The perfect match: 3D point cloud matching with smoothed densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifa</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Dirk</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generalized procrustes analysis. Psychometrika</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gower</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3D semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fine-to-coarse global registration of RGB-D scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Registration with the point cloud library: A modular framework for aligning in 3-D. RA-L</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Ichim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Behnke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Using spin images for efficient object recognition in cluttered 3D scenes. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Realtime large-scale dense 3D reconstruction with loop closure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning compact geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">g2o: A general framework for graph optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>K?mmerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Grisetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hauke</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DARPA Image Understanding Workshop</title>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A global solution to sparse correspondence problems. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Maciel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">Paulo</forename><surname>Costeira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully automatic registration of 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Speeding up iterative closest point using stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Fahira Afzal Maken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Point registration via efficient convex relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Dym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Kezurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Kovalsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Super 4PCS fast global pointcloud registration via smart indexing. Computer Graphics Forum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Mellado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dror</forename><surname>Aiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy J</forename><surname>Mitra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ORB-SLAM: A versatile and accurate monocular SLAM system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tard?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dias Pais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Miraldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikumar</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacinto</forename><forename type="middle">C</forename><surname>Nascimento</surname></persName>
		</author>
		<title level="m">Venu Madhav Govindu, and Rama Chellappa. 3DRegNet: A deep neural network for 3D point registration. arXiv</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Colored point cloud registration revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Realtime and robust hand tracking from depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep fundamental matrix estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (FPFH) for 3D registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Aligning point cloud views using persistent feature histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SHOT: Unique signatures of histograms for surface and texture description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Di Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient RANSAC for point-cloud shape detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruwen</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">BAD SLAM: Bundle adjusted direct RGB-D SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unique shape context for 3D data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Workshop on 3D Object Retrieval</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep closest point: Learning representations for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">PRNet: Self-supervised learning for partial-to-partial registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ElasticFusion: Dense SLAM without a pose graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">F</forename><surname>Salas-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Robotics: Science and Systems</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A polynomial-time solution for robust registration with extreme outlier rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Carlone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Go-ICP: A globally optimal solution to 3D ICP pointset registration. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning to find good correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Kwang Moo Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3DMatch: Learning local geometric descriptors from RGB-D reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Iterative point matching for registration of free-form curves and surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">NM-Net: Mining reliable neighbors for robust feature correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast global registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Open3D: A modern library for 3D data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
