<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ExpandNet: A Deep Convolutional Neural Network for High Dynamic Range Expansion from Low Dynamic Range Content</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018">2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marnerides</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Warwick Centre for Predictive Modelling (WCPM)</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">WMG</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bashford-Rogers</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Creative Technologies</orgName>
								<orgName type="institution">University of the West of England</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hatchett</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">WMG</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Debattista</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">WMG</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ExpandNet: A Deep Convolutional Neural Network for High Dynamic Range Expansion from Low Dynamic Range Content</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">37</biblScope>
							<biblScope unit="issue">2</biblScope>
							<date type="published" when="2018">2018</date>
						</imprint>
					</monogr>
					<note>EUROGRAPHICS 2018 / D. Gutierrez and A. Sheffer (Guest Editors)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>CCS Concepts ?Computing methodologies ? Neural networks; Image processing;</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High dynamic range (HDR) imaging provides the capability of handling real world lighting as opposed to the traditional low dynamic range (LDR) which struggles to accurately represent images with higher dynamic range. However, most imaging content is still available only in LDR. This paper presents a method for generating HDR content from LDR content based on deep Convolutional Neural Networks (CNNs) termed ExpandNet. ExpandNet accepts LDR images as input and generates images with an expanded range in an end-to-end fashion. The model attempts to reconstruct missing information that was lost from the original signal due to quantization, clipping, tone mapping or gamma correction. The added information is reconstructed from learned features, as the network is trained in a supervised fashion using a dataset of HDR images. The approach is fully automatic and data driven; it does not require any heuristics or human expertise. ExpandNet uses a multiscale architecture which avoids the use of upsampling layers to improve image quality. The method performs well compared to expansion/inverse tone mapping operators quantitatively on multiple metrics, even for badly exposed inputs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>High dynamic range (HDR) imaging provides the capability to capture, manipulate and display real-world lighting, unlike traditional, low dynamic range (LDR) imaging. HDR has found many applications in photography, physically-based rendering, gaming, films, medical and industrial imaging and recent displays support HDR content [SHS * 04, <ref type="bibr" target="#b39">MdPVA16]</ref>. While HDR imaging has seen many advances, LDR remains the status quo, and the majority of both current and legacy content is predominantly LDR. In order to gain an improved viewing experience [AFR * 07], or to use this content in future HDR pipelines, LDR content needs to be converted to HDR.</p><p>A number of methods which can retarget LDR to HDR content have been presented [BADC17]. These methods make it possible to utilise and manipulate the vast amounts of LDR content within HDR pipelines and visualise them on HDR displays. However, such methods are primarily model-driven, use various parameters which make them difficult to use by non-experts, and are not suitable for all types of content.</p><p>Recent machine learning advances for applications in image processing provide data driven solutions for imaging problems, bypassing reliance on human expertise and heuristics. CNNs are the current de-facto approach used for many imaging tasks, due to their high learning capacity as well as their architectural qualities which make them highly suitable for image processing <ref type="bibr" target="#b51">[Sch14]</ref>. The networks allow for abstract representations to be acquired directly from data, surpassing simplistic pixelwise processing. This acquisition of abstractness is especially strong when the networks are of sufficient depth <ref type="bibr" target="#b22">[HZRS15]</ref>. This paper presents a method for HDR expansion based on deep Convolutional Neural Networks (CNNs).</p><p>In this work, a novel multiscale CNN architecture, called Ex-pandNet, is presented. On a local scale, one branch of the network learns how to maintain and expand high frequency detail, while a dilation branch learns information on larger pixel neighbourhoods. A final third branch provides overall information by learning the global context of the input. The architecture is designed to avoid upsampling of downsampled features, in an attempt to reduce blocking and/or haloing artefacts that may arise from more straightforward approaches, for example autoencoder architectures <ref type="bibr" target="#b4">[Ben09]</ref>. Results demonstrate an improvement in quality over all other previous approaches that were tested, including some other CNN architectures.</p><p>In summary, the primary contributions of this work are:</p><p>? A fully automatic, end-to-end, parameter free method for the expansion of LDR content based on a novel CNN architecture which improves image quality for HDR expansion. ? Results which are competitive with the other approaches tested, including other CNN architectures applied to single exposure LDR to HDR. ? Data augmentation for limited HDR content via different exposure and position selection to obtain more LDR-HDR training pairs. ? A comprehensive quantitative comparison of LDR to HDR expansion methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>A number of methods to expand LDR to HDR have been presented in the literature. Furthermore, deep learning methods have been used for similar problems in the past. The following subsections discuss these topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">LDR to HDR</head><p>Expansion operators (EOs), also known as inverse or reverse tone mapping operators, attempt to generate HDR content from LDR content. EOs can generally be expressed as:</p><formula xml:id="formula_0">Le = f (L d ), where f : [0, 255] ? R + (1)</formula><p>where Le corresponds to the expanded HDR content, L d to the LDR input and f (?) is the EO. In this context f (?) could be considered as an ill-posed function. However, a variety of methods have emerged that attempt to tackle this issue. The majority of EOs can be broadly divided into two categories: global and local methods [BADC17].</p><p>The global methods use a straightforward function to expand the content equally across all pixels. One of the first of such methods was the technique presented by Landis <ref type="bibr" target="#b35">[Lan02]</ref>   <ref type="bibr" target="#b12">[DMHS08]</ref>, which operate on different parts of the image by classifying these parts accordingly.</p><p>Banterle et al. [BDA * 09] provide a broader view of these methods. With most of the above, the added information is derived from heuristics that may produce sufficient results for well behaved inputs, but are not data driven. Most importantly, most existing EOs find it difficult to handle under/over-exposed LDR content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Learning for Image Processing</head><p>Deep learning has been extensively used for image processing problems recently. In image-to-image translation <ref type="bibr" target="#b27">[IZZE16b]</ref> the authors present a method based on Generative Adversarial Networks [GPAM * 14] and the U-Net [RFB15] architecture that transforms images from one domain to another (e.g. maps to satellite). Many approaches have also been developed for other kinds of ill-posed or inverse problems, including image super-resolution and upsampling [DLHT16, KLL15, YKK17] as well as inpainting/hallucination of missing information <ref type="bibr" target="#b25">[ISSI17]</ref>. Automatic colorization <ref type="bibr" target="#b24">[ISSI16]</ref> converts grey scale to color images using a CNN which uses two routes of computation, fusing local and global context for improved image quality.</p><p>In visualization, graphics and HDR imaging, neural networks have been used for predicting sky illumination for rendering [SBRCD17, HGSH * 17], denoising Monte Carlo renderings [KBS15, CKS * 17, BVM * 17], predicting HDR environment maps <ref type="bibr" target="#b58">[ZL17a]</ref>, reducing artefacts such as ghosting when fusing multiple LDR exposures to create HDR content <ref type="bibr" target="#b32">[KR17]</ref> and for tone mapping <ref type="bibr" target="#b18">[HDQ17]</ref>.</p><p>Concurrently to this work, two other deep learning approaches that expand LDR content to HDR have been developed. Eilertsen et al. [EKD * 17], use a U-Net like architecture to predict values for saturated areas of badly exposed content, whereas non-saturated areas are linearised by applying an inverse camera response curve. Endo et al.</p><p>[EKM17] use a modified U-Net architecture that predicts multiple exposures from a single exposure which are then used to generate an HDR image using standard merging algorithms.</p><p>The first method does not suffer greatly from artefacts produced from upsampling that are common with U-Net and similar architectures <ref type="bibr" target="#b43">[ODO16]</ref> since only areas of badly exposed content are expanded by the network. In the latter, the authors mention the appearance of tiling artefacts in some cases. There are other examples in literature when fully converged U-Net like networks exhibit artefacts, for example in image-to-image translation tasks <ref type="bibr" target="#b26">[IZZE16a]</ref>, or semantic segmentation <ref type="bibr" target="#b59">[ZL17b]</ref>. Our approach differs from these methods methods as it presents a dedicated architecture for and end-to-end image expansion, without using upsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ExpandNet</head><p>This section describes the ExpandNet architecture in detail. The network is designed to tackle the problem directly via a novel three  branch architecture. <ref type="figure" target="#fig_1">Figure 1</ref> presents an overview of the architecture. The three branches of computation are a local, a dilation and a global one. Each branch is itself a CNN that accepts an RGB LDR image as input. Each one of the three branches is responsible for a particular aspect, with the local branch handling local detail, the dilation branch for medium level detail, and a global branch accounting for higher level image-wide features.</p><p>The local and dilation branches avoid any use of downsampling and upsampling, which is a common approach in the design of CNNs, and the global branch only downsamples. In image processing CNNs it is common to downsample the width and height of the input image, while expanding the channel dimension. This forms a set of more abstract features after a few layers of downsampling. The features are then upsampled to the original dimensions, for example in autoencoders. As also mentioned in the previous section, it is argued [ODO16] that upsampling, especially the frequently used deconvolutional layers [SCT * 16], cause checkerboard artefacts. Furthermore, upsampling may cause unwanted information bleeding in areas where context is missing, for example large overexposed areas. <ref type="figure" target="#fig_1">Figure 11</ref> and <ref type="figure" target="#fig_1">Figure 12</ref>  The outputs of the three branches are fused and further processed by a small final convolutional layer that produces the predicted HDR image. The input LDR and the predicted HDR are both in the [0, 1] range.</p><p>The following subsection briefly introduces CNNs, followed by a detailed overview of the three branches of the ExpandNet architecture, including design characteristics for feature fusion, activation functions and the loss function used for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Convolutional Neural Networks</head><p>A feed-forward neural network (NN) is a function composed of multiple layers of non-linear transformations. Given an input vector x, a network of M layers (with no skip connections) can be expressed as follows:</p><formula xml:id="formula_1">f NN (x) = (l M ? l M?1 ? ? ? ? ? l 2 ? l 1 )(x)<label>(2)</label></formula><p>where l i is the i th hidden layer of the network and ? is the composition operator. Each layer accepts the output of the previous layer, o i?1 , and applies a linear map followed by a non-linear transformation:</p><formula xml:id="formula_2">o i = l i (o i?1 ) = ?(W i o i?1 ) (3)</formula><p>where W i is a matrix of learnable parameters (weights), o N is the network output and o 0 = x. ?(z) is a non-linear (scalar) activation function, applied to each value of the resulting vector independently. A learnable bias term exists in the linear map as well, but is folded in W i (and x) for ease of notation.</p><p>A convolutional layer, c i , uses sparse parameter matrices with repeated values. The sparsity and repetition structure is such, so that the linear product can be expressed as a convolution, * , between a learnable parameter filterw and the input to the layer.</p><formula xml:id="formula_3">c i (o i?1 ) = ?(w i * o i?1 )<label>(4)</label></formula><p>This formulation is analogous for higher dimensions. In the scope of this work, images are three dimensional objects (width ? height ? channels / features), thus the parameter matrices become four-dimensional tensors. For image processing CNNs, the convolutions are usually only in the width and height dimensions, while the third dimension is fully connected (dense tensor dimension).</p><p>The convolutional architecture is extremely suitable for images since it exploits spatial correlations and symmetries, and dramatically reduces the number of learnable parameters compared to fully connected networks. It also allows for efficient implementations on GPUs as well as more stable training of deeper models <ref type="bibr" target="#b51">[Sch14]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Branches</head><p>The three branches play different roles in expanding the dynamic range of the input LDR. The global branch seeks to reduce the dimensionality of the input and capture abstract features. It has a sufficiently large receptive field that covers the whole image. It accepts the entire LDR image as input, re-sized to 256 ? 256, and eventually downsamples it to 1 ? 1 over a total of seven layers. Each layer has 64 feature maps and uses stride 2 convolutions which consecutively downsample the spatial dimensions by a factor of 2. All the global branch layers use a convolutional kernel of size 3 ? 3, with padding 1 except the last layer which uses a 4 ? 4 kernel with no padding, essentially densely connecting the previous layer, which consists of 4 ? 4 features, with the last layer, creating a vector of 1 ? 1 features.</p><p>The other two branches provide localized processing without downsampling that captures higher frequencies and neighbouring features. The local branch has a receptive field of 5 ? 5 pixels and consists of two layers with 3 ? 3 convolutions of stride 1 and padding 1, with 64 and 128 feature maps respectively. The small receptive field of the local branch provides learning at the pixel level, preserving high frequency detail.</p><p>The dilation branch has a wider receptive field of 17 ? 17 pixels and uses dilated convolutions <ref type="bibr" target="#b56">[YK15]</ref> of dilation size 2, kernel 3?3, stride 1, and padding 2. Dilated convolutions are large, sparse convolutional kernels, used to quickly increase the receptive field of CNNs. A total of four dilation layers are used each with 64 features. With an increased receptive field, the dilation network captures local features with medium range frequencies otherwise missed by the other two branches whose focus is on the two extremes of the frequency spectrum.</p><p>The effects of each individual branch are presented in <ref type="figure">Figure 3</ref>. Masking the input to an individual branch causes the output appearance to change, depending on which branch was masked, highlighting its role. The local branch produces high frequency features, while the dilation branch adds medium range frequencies.</p><p>The global branch changes the overall appearance of the output by adding low frequencies and adjusting the overall sharpness of the image. Results, shown in Section 5.3, further help to illustrate the advantages posed by the three distinct branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fusion</head><p>The outputs of the three branches are merged in a manner similar to the fusion layer by Iizuka et al. <ref type="bibr" target="#b24">[ISSI16]</ref>. The local and dilation outputs, which have the same height and width as the input, are concatenated along the feature map dimension. The output of the global network is a vector of 64 features which is replicated along the width and height dimensions to match the dimensions of the other two outputs. The replication superposes the vector over each pixel of the predictions of the other two branches. It is then concatenated with the rest of the outputs along the feature map dimension resulting in a total of 256 features. The concatenation is followed by a convolution of kernel size 1 ? 1 which fuses the global feature vector with each individual pixel of the local and dilated features, thus combining context from multiple scales. The output of the fusion layer is further processed by a final convolutional layer with 3 ? 3 kernels, stride 1 and padding 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Activations</head><p>All the layers, besides the output layer, use the Scaled Exponential Linear Unit (SELU) activation function [KUMH17], a variation of the Exponential Linear Unit (ELU).</p><formula xml:id="formula_4">SELU(z) = ? z if z &gt; 0 ?e z ? ? if z ? 0<label>(5)</label></formula><p>where ? ? 1.05070 and ? ? 1.67326. SELU was recently introduced for the creation of self normalizing neural networks and it ensures that the distributions of the activations at each layer have a mean of zero and unit variance. It provides a solution to the internal covariate shift problem during training at a lower memory cost compared to the frequently used batch normalization technique <ref type="bibr" target="#b23">[IS15]</ref>. The SELU unit also preserves all the properties of the ELU, which in its turn improves on the Rectified Linear Unit  </p><formula xml:id="formula_5">?(z) = 1 1 + e ?z<label>(6)</label></formula><p>which maps the output to the [0, 1] range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss function</head><p>The Loss function, L, used for optimizing the network is the L 1 distance between the predicted image,?, and real HDR image, I, from the dataset. The L 1 distance is chosen for this problem since the more frequently used L 2 distance was found to cause blurry results for images <ref type="bibr" target="#b37">[MCL15]</ref>. An additional cosine similarity term is added to ensure color correctness of the RGB vectors of each pixel.</p><formula xml:id="formula_6">L i = ? i ? I i 1 + ? 1 ? 1 K K ? j=i? j i ? I j i ? j i 2 I j i 2 (7)</formula><p>where L i is the loss contribution of the i th image of the dataset, ? is a constant factor that adjusts the contribution of the cosine similarity term, I j i is the j th RGB pixel vector of image I i and K is the total number of pixels of the image.</p><p>Cosine similarity measures how close two vectors are by comparing the angle between them, not taking magnitude into account. For the context of this work, it ensures that each pixel points in the same direction of the three dimensional RGB space. It provides improved color stability, especially for low luminance values, which are frequent in HDR images, since slight variations in any of the RGB components of these low values do not contribute much to the L 1 loss, but they may however cause noticeable color shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training and Implementation</head><p>This section presents the implementation details used for Expand-Net, including the dataset used and how it was augmented, and implementation and optimization details. Results are presented in Section 5. <ref type="figure" target="#fig_3">Figure 2</ref> gives an overview of the training and testing methodology employed.  <ref type="bibr" target="#b15">[Fai07]</ref>. All the images contained linear RGB values. The 50 test images used for evaluation in Section 5 were selected randomly from the Fairchild images with calibrated absolute luminance. LDR content for training was generated on-the-fly, directly from the dataset, and was augmented in a number of ways as outlined below.</p><p>At every epoch each HDR image from the training set is used as input in the network once after preprocessing. Preprocessing consists of randomly selecting a position for a sub image, cropping, and having its dynamic range reduced using one of a set of operators. The randomness entails that at every epoch a different LDR-HDR pair is generated from a single HDR image in the training set.</p><p>Initially, the HDR image has its cropping position selected. The position is drawn from a spatial Gaussian distribution such that the most frequently selected regions are towards the center of the image. The crop size is drawn from an exponential distribution such that smaller crops are more frequent than larger ones, with a minimum crop size of 384 ? 384. Randomly cropping the images is a standard technique for data augmentation. Choosing the crop size at random adds another layer of augmentation, since the likelihood of picking the same crop is reduced, but it also aids in how well the model generalizes since it provides different sized content for similar scenes.</p><p>The cropped image is resized to 256 ? 256 and linearly mapped to the [0, 1] range to create the output. Since only a small fraction of the dataset images contain absolute luminance values, the network was trained to predict relative luminance values in the [0, 1] range.</p><p>A tone mapping operator (TMO) <ref type="bibr" target="#b54">[TR93]</ref> or single exposure operator is applied to form the input LDR from the output HDR, chosen uniformly from a list of five operators: dynamic range reduction inspired by photoreceptor physiology (Photoreceptor) <ref type="bibr" target="#b45">[RD05]</ref>, Adaptive Logarithmic Mapping (ALM) <ref type="bibr" target="#b11">[DMAC03]</ref>, Display Adaptive Tone Mapping (display) <ref type="bibr" target="#b38">[MDK08]</ref>, Bilateral <ref type="bibr" target="#b9">[DD02]</ref> and Exposure. The OpenCV3 implementations of the TMOs were used. The Exposure operator was implemented for this work and clamps the top and bottom percentiles of the image and adds a gamma curve. In addition to using a random operator for each input-output pair, the parameters of the operators are also randomized. The parameters of the functions used are summarized in <ref type="table" target="#tab_2">Table 1</ref>. The TMO parameter randomization was done to ensure that the model performs well under a variety of inputs when tested with real LDR inputs and does not just learn to invert specific TMOs. It acts as yet another layer of data augmentation. Results shown in the following  section only use single exposures for generating HDR; the TMOs are just used for data augmentation during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Optimization</head><p>The network parameters are optimized to minimize the loss given in Equation 7, with ? = 5, using mini-batch gradient descent and the backpropagation algorithm <ref type="bibr" target="#b47">[RHW86]</ref>. The Adam optimizer was used <ref type="bibr" target="#b28">[KB14]</ref>, with an initial learning rate of 7e ? 5 and a batch size of 12. After the first 10, 000 epochs, the learning rate was reduced by a factor of 0.8 whenever the loss reached a plateau, until the learning rate reached values less than 1e ? 7 for a total of 1, 600 epochs extra. L 2 regularization (weight decay) was used to reduce the chance of overfitting. All experiments were implemented using the PyTorch library <ref type="bibr">[pyt]</ref>. Training time took a total of 130 hours on an Nvidia P100.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>This section presents an evaluation of ExpandNet compared to other EOs and deep learning architectures. <ref type="figure" target="#fig_3">Figure 2 (right)</ref> shows an overview of the evaluation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Quantitative</head><p>For a quantitative evaluation of the work, four metrics are considered, Peak Signal to Noise Ratio (PSNR), Structural Similarity (SSIM), Multi-Scale Structural Similarity (MS-SSIM), and HDR-VDP-2.2 [NMDSLC15]. For the first three metrics, a perceptual uniformity (PU) encoding [AMS08] is applied to the prediction and reference images to make them suitable for HDR comparisons. HDR-VDP-2.2 includes the PU-encoding in its implementation. The values from HDR-VDP-2.2 are those of the VDP-Q quality score. <ref type="figure" target="#fig_11">Figure 8</ref>: HDR-VDP-2.2 visibility probability maps for predictions of (culling) M3 Middle Pond using all methods. Blue indicates imperceptible differences, red indicates perceptible differences. Four CNN architectures are compared, including the proposed ExpandNet method (EXP). Two other network architectures that have been used for similar problems have been adopted and trained in the same way as EXP. The first network is based on U-Net [RFB15] (UNT), an architecture that has shown strong results with image translation tasks between domains. The second network is an architecture first used for colorization <ref type="bibr" target="#b24">[ISSI16]</ref> (COL), which uses two branches and a fusion layer similar to the one used for ExpandNet. These three are implemented using the same pyTorch framework and trained on the same training dataset. The recent network architecture used for LDR to HDR conversion [EKD * 17] (EIL) is also included. The predictions from this method were created using the trained network which was made available online by the authors, applied on the same test dataset used for all the other methods.</p><formula xml:id="formula_7">(a) LDR (b) AKY (c) LAN (d) BNT (e) HUO (f) REM (g) MAS (h) KOV (i) COL (j) UNT (k) EIL (l) EXP</formula><formula xml:id="formula_8">(a) LDR (b) AKY (c) LAN (d) BNT (e) HUO (f) REM (g) MAS (h) KOV (i) COL (j) UNT (k) EIL (l) EXP</formula><p>The inputs to the methods are single exposure LDR images of the 50 full HD (1920 ? 1080) images in the HDR test dataset. The single exposures are obtained using two methods. The first method (optimal) finds the optimal/automatic exposure [DBRS * 15] using the HDR image histogram, resulting in minimal clipping at the two ends of the luminance range. The second method (culling) simply clips the top and bottom 10% of the values of the images, resulting in more information loss and distortion of the input LDR. The resulting test LDR input images are saved with JPEG encoding before testing. When compared to the 10 th percentile loss for the images generated using culling, on average, the number of pixels over the test dataset that are over-exposed when using optimal is 3.89% and the number of pixels under-exposed is 0.35%.</p><p>The outputs of the methods are in the [0, 1] range, predicting relative luminance. The scaling permits evaluation for scene-referred and display-referred output. Hence, the predicted HDR images are scaled to match the original HDR content (scene-referred) and a 1,000 cd/m 2 display (display-referred), which represents current commercial HDR display technology. The scaling is done to match the 0.1 and 99.9 percentiles of the predictions with the corresponding percentiles of the HDR test images. Furthermore, scaling is useful as the PU-encoded HDR metrics are dependent on absolute luminance values in cd/m 2 . By scaling the prediction outputs, the PU-encoded metrics can be used to quantify the ability of the network to reconstruct the original signal. <ref type="table" target="#tab_3">Table 2</ref> and <ref type="table" target="#tab_4">Table 3</ref> summarize the results of the four metrics applied on all the methods, using the optimal and culling, for scenereferred and display-referred respectively. Box plots for the distribution of the four metrics are presented in <ref type="figure" target="#fig_5">Figure 4</ref> and <ref type="figure" target="#fig_6">Figure 5</ref> for the scene-referred results of optimal and culling respectively. Similarly <ref type="figure" target="#fig_7">Figure 6</ref> and <ref type="figure" target="#fig_8">Figure 7</ref> show the display-referred results of optimal and culling respectively. Box plots are sorted by ascending order of median value. When analysed for significant differences amongst all the methods, a significance is found for all tests (at p &lt; 0.001) using Friedman's test. Pairwise comparisons ranked EXP in the top group, consisting of the group of methods that cannot be significantly differentiated, in 13 of the 16 results (these consist of all four metrics for both optimal and culling and for both scenereferred and display referred). The conditions where EXP was not in the top group were: pu-SSIM (in the cases of scene-referred and display-referred) and pu-MMSIM (for scene-referred only); in all three cases this occurred for the optimal condition.</p><formula xml:id="formula_9">(a) LDR (b) AKY (c) LAN (d) BNT (e) HUO (f) REM (g) MAS (h) KOV (i) COL (j) UNT (k) EIL (l) EXP</formula><p>As can be seen in the overall, EXP performs reasonably well. In particular for the culling case when a significant number of pixels are over or under-exposed EXP appears to reproduce HDR better than the other methods. For optimal, EIL performs very well also, and this is expected as in such cases the number of pixels that are required to be predicted from the CNN are smaller. Similarly, the non deep learning based expansion methods such as MAS perform well especially for SSIM which quantifies structural similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Visual Inspection</head><p>This section presents some qualitative aspects of the results. HDR-VDP-2.2 visibility probability maps for all the methods are pre-sented, as well as images from the CNN predictions exhibiting effects such as hallucination, blocking and information bleeding.  <ref type="figure" target="#fig_1">Figure 10</ref> show the HDR-VDP-2.2 probability maps for the predictions of all the methods from the test set. The HDRs are predicted from culling LDRs with scene-referred scaling. The HDR-VDP-2.2 visibility probability map describes how likely it is for a difference to be noticed by the average observer, at each pixel. Red values indicate high probability, while blue values indicate low probability. Results show EXP performs better than most other methods for these scenes. EIL also performs well, particularly for the challenging scenario in <ref type="figure" target="#fig_1">Figure 10</ref>. <ref type="figure" target="#fig_1">Figure 11</ref> and <ref type="figure" target="#fig_1">Figure 12</ref> show single exposure slices (both these cases are from low exposure slices) from the predicted HDRs for the four CNN architectures. The input LDRs were created with culling and are shown in the respective sub figure (f). It is clear that UNT and COL have issues with blocking or banding and information bleeding, and this can be observed, to a certain extent, for EIL as well, but to a much lesser degree. <ref type="figure" target="#fig_1">Figure 14</ref> presents predictions at multiple exposures comparing EXP and EIL. The images contain saturated areas of different sizes as well as different combinations of saturated channels. <ref type="figure" target="#fig_1">Figure 14a</ref> contains blue pixels which after  <ref type="figure" target="#fig_1">Figure 14b</ref> contains saturated purple pixels, where both the R and B channels are clipped. <ref type="figure" target="#fig_1">Figure 14d</ref> contains a saturated colour chart. It can be noticed that EXP tries to minimize the bleeding of information into large overexposed areas, recovering high frequency contrast, for example around text. It is also worth noting that artefacts around sharp edges are not completely eliminated, but are much less pronounced and with a much smaller extend. <ref type="figure" target="#fig_1">Figure 13</ref>: Training convergence for all the possible combinations of branches. Each point is an average of 10,000 gradient steps for a total of 254,000 steps, the equivalent of 10,000 epochs (each epoch has 254 mini-batches). Axes are logarithmic. To further investigate the effects of data augmentation, a network was trained using Camera Response Functions (CRFs) in addition to the TMOs used for EXP reported in the previous section. Following the Deep Reverse Tone Mapping [EKM17], the same database of CRFs was used <ref type="bibr" target="#b16">[GN03]</ref>, and the same method of obtaining five representative CRFs by k-means clustering was adopted. The results do not show any improvement and are almost identical to EXP on all metrics (within 1%). This might be because CRFs are monotonically increasing functions, which can be approximated in many cases by the randomized exposure and gamma TMO used in the initial set of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Further Investigation</head><p>Branches: To gain insight on the effect of the individual branches and further motivate the three-branch architecture, different branch combinations were trained from scratch. <ref type="figure" target="#fig_1">Figure 13</ref> shows the training convergence for 10,000 epochs. It is evident that the global branch which is fused with each pixel makes the largest contribution. On average, the full ExpandNet architecture is the quickest to converge and has the lowest loss. The combination of the local and dilation branches improves the performance of each one individually. We can further understand the architecture by comparing figures 3 and 13. The performance of Dilated + Global is comparable to that of Local + Global, even though figure 3b is visually much better than 3c. This is because the images from figure 3 are predictions from an ExpandNet with all branches (some zeroed out when predicting), where the local and dilated branches have acquired separate scales of focus during training (high and medium frequencies respectively). In <ref type="figure" target="#fig_1">figure 13</ref>, where each one is trained individually, these scales are not separated; each branch tries to learn all the scales simultaneously. Separating scales in the architecture leads to improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This paper has introduced a method of expanding single exposure LDR content to HDR via the use of CNNs. The novel three branch architecture provides a dedicated solution for this type of problem as each of the branches account for different aspects of the expansion. Via a number of metrics it was shown that Ex-pandNet mostly outperforms the traditional expansion operators. Furthermore, it performs better than non-dedicated CNN architectures based on UNT and COL. Compared to other dedicated CNN methods [EKD * 17, EKM17] it does well in certain cases, exhibiting fewer artefacts, particularly for content which is heavily under and over exposed. On the whole, ExpadNet is complementary to EIL which is designed to expand the saturated areas and does very well in such cases. Furthermore, EIL has a smaller memory footprint. ExpandNet has shown that a dedicated architecture can be employed without the need of upsampling to convert HDR to LDR, however, further challenges remain. To completely remove artefacts further investigation is required, for example in the receptive fields of the networks. Dynamic methods may require further careful design to maintain temporal coherence and Long Short Term Memory networks <ref type="bibr" target="#b20">[HS97]</ref> might provide the solution for such content.</p><p>Revisions v2: Results in <ref type="figure" target="#fig_6">figure 5</ref> and LAN (culling) in <ref type="table" target="#tab_3">Table 2</ref> was corrected. The changes are minor and do not alter the outcomes and conclusions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>c</head><label></label><figDesc>2018 The Author(s) Computer Graphics Forum c 2018 The Eurographics Association and John Wiley &amp; Sons Ltd.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>ExpandNet architecture. The LDR input is propagated through the the local and dilation branches, while a resized input (256?256) is propagated through the global branch. The output of the global branch is superposed over each pixel of the outputs of the other two branches. The resulting features are fused using 1 ? 1 convolutions to form the last feature layer which then gives an RGB HDR prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b) and (c), discussed further in Section 5, provide examples where such artefacts can arise in upsampling networks, seen as blocking in (b) due to deconvolutions, and banding in (c) due to nearest-neighbour upsampling. ExpandNet avoids the use of upsampling layers to reduce such artefacts and improves the quality of the predicted HDR images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>General overview of the workflow. (left) The training dataset is sampled and preprocessed on-the-fly to form 256 ? 256 resolution input-output pairs, which are then used to optimize the network weights. (right) For testing, the images are full-HD (1,920 ? 1, 080). The luminance of the predictions of all methods is scaled either to match the original HDR image (scene-referred) or that of a 1,000 cd/m 2 display (display-referred).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( a )Figure 3 :</head><label>a3</label><figDesc>Local + Dilated + Global (b) Local + Global (c) Dilated + Global (d) Local + Dilated (e) Local (f) Dilated Illustration of the contribution of each of the three branches of ExpandNet. These images were obtained by masking one or more branches with zero inputs. The bottom row is produced with the global branch masked. This causes the overall appearance of the images to be darker and sharper, since there are low frequencies missing. The middle column masks the dilation branch, resulting in sharp high-frequency images. The right column masks the local branch which causes most of the fine details to be lost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Box plots for scene-referred HDR obtained from LDR via optimal exposure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Box plots for scene-referred HDR obtained from LDR via culling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Box plots for display-referred HDR obtained from LDR via optimal exposure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Box plots for display-referred HDR obtained from LDR via culling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>HDR-VDP-2.2 visibility probability maps for predictions of (culling) Devils Bathtub using all methods. Blue indicates imperceptible differences, red indicates perceptible differences. The ExpandNet architecture is compared against seven other previous methods for dynamic range expansion/inverse tone mapping. The chosen methods were the methods of: Landis [Lan02] (LAN), Banterle et al. [BLDC06] (BNT), Aky?z et al. [AFR * 07] (AKY), Rempel et al. [RTS * 07] (REM), Masia et al. [MAF * 09] (MAS), Kovaleski and Oliveira [KO14] (KOV) and Huo et al. [HYDB14] (HUO). The Matlab implementations from the HDR toolbox [BADC17] were used to obtain these results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>HDR-VDP-2.2 visibility probability maps for predictions of (culling) Tunnel View using all methods. Blue indicates imperceptible differences, red indicates perceptible differences.(a) Input LDR (culling) (b) UNT (c) COL (d) Exposure of original HDR (e) EIL (f) EXP Figure 11: (a) LDR input image created using culling from the Balanced Rock HDR image. (d) Low exposure of the original HDR image. (b,c,e,f) Low exposure slices of the predictions from methods that use CNN architectures showing artefacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 ,</head><label>8</label><figDesc>Figure 9 and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>( a )</head><label>a</label><figDesc>Input LDR (culling) (b) UNT (c) COL (d) Exposure of original HDR (e) EIL (f) EXP Figure 12: (a) LDR input image created using culling from The Grotto HDR image. (d) Low exposure of the original HDR image. (b,c,e,f) Low exposure slices of the predictions from methods that use CNN architectures showing artefacts. exposure (scaling and clipping at 255) only have their B channel saturated (e.g. a pixel [x, x, 243] becomes [x+y, x+y, 255] where B is clipped at 255).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Examples of expanded images using EXP and EIL at three different exposures. The examples are cropped from larger images, showing under various lighting conditions and from different scenes. The top row of each sub-figure shows the input LDR created with culling. The second row of each sub-figure shows the exposures of the original HDR. The following row shows exposures of predicted HDR using EIL. The last row shows exposures of predicted HDR using EXP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>which expands content based on power functions. A straightforward method that uses a linear transformation combined with gamma correction was presented by Aky?z et al. [AFR * 07] and evaluated using a subjective experiment. Masia et al. [MAF * 09, MSG17] also presented a global method which expands the content based on image attributes defined by an image key.Local methods typically expand LDR content to HDR through the use of an analytical function combined with an expand map. The inverse tone mapping method<ref type="bibr" target="#b5">[BLDC06]</ref> initially expands the content using an inverted photographic tone reproduction tone mapper [RSSF02], although this could be applied to other tone mappers that are invertible. An expand map is generated by selecting a constellation of bright points and expanding them via density estimation. This is subsequently used in conjunction with the in-verse tone mapping equation to map LDR values to HDR values to avoid quantization errors that would arise via inverse tone mapping only. Rempel et al. [RTS * 07] also used an expand map, however this was computed through the use of a Gaussian filter in conjunction with an edge-stopping function to maintain contrast. Kovaleski and Oliviera [KO14] extended the work of Rempel et al. via the use of a cross bilateral filter. Subsequently, Huo et al. [HYDB14] further extended this work to remove the thresholding used by Kovaleski and Oliviera. Other methods include inpainting as used by Wang et al. [WWZ * 07] which is partially user-based, and classification based methods such as by Meylan et al. [MDS06] and Didyk et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>TMO</cell><cell>Parameters</cell></row><row><cell></cell><cell>Intensity: [?1.0, 1.0]</cell></row><row><cell>Photoreceptor</cell><cell>Light adaptation: [0.8, 1.0]</cell></row><row><cell></cell><cell>Color adaptation: [0.0, 0.2]</cell></row><row><cell>ALM</cell><cell>Saturation: 1.0, Bias: [0.7, 0.9]</cell></row><row><cell cols="2">Display Adaptive Saturation: 1.0, Scale: [0.65, 0.85]</cell></row><row><cell>Bilateral</cell><cell>Saturation: 1.0, Contrast: [3, 5] ?space : 8, ? color : 4</cell></row><row><cell>Exposure</cell><cell>Percentile: [0, 15] to [85, 100]</cell></row></table><note>Parameters used for tone mapping. All images are fol- lowed by a gamma correction curve with ? ? [1.8, 2.2]. Values given within ranges are sampled from a uniform distribution.(ReLU). ReLUs alleviate the vanishing/exploding gradient prob- lem [KSH17] that was frequent with the traditional Sigmoid acti- vations (when stacked), while ELUs improve the sparse activation problem of the ReLUs by providing negative activation values. The final layer of the network uses a Sigmoid activation,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Average values of the four metrics for all methods for scene-referred scaling. Bold values indicate the best value. Method SSIM MS-SSIM PSNR HDR-VDP-2.2</figDesc><table><row><cell></cell><cell></cell><cell>optimal</cell><cell></cell><cell></cell></row><row><cell>LAN</cell><cell>0.72</cell><cell>0.78</cell><cell>22.21</cell><cell>39.01</cell></row><row><cell>AKY</cell><cell>0.72</cell><cell>0.78</cell><cell>22.70</cell><cell>39.11</cell></row><row><cell>MAS</cell><cell>0.75</cell><cell>0.80</cell><cell>23.29</cell><cell>38.98</cell></row><row><cell>BNT</cell><cell>0.70</cell><cell>0.73</cell><cell>19.56</cell><cell>37.63</cell></row><row><cell>KOV</cell><cell>0.74</cell><cell>0.80</cell><cell>25.03</cell><cell>38.39</cell></row><row><cell>HUO</cell><cell>0.74</cell><cell>0.78</cell><cell>19.71</cell><cell>38.04</cell></row><row><cell>REM</cell><cell>0.68</cell><cell>0.64</cell><cell>15.68</cell><cell>33.61</cell></row><row><cell>COL</cell><cell>0.58</cell><cell>0.69</cell><cell>23.21</cell><cell>31.23</cell></row><row><cell>UNT</cell><cell>0.68</cell><cell>0.71</cell><cell>20.52</cell><cell>34.88</cell></row><row><cell>EIL</cell><cell>0.72</cell><cell>0.78</cell><cell>22.90</cell><cell>39.06</cell></row><row><cell>EXP</cell><cell>0.74</cell><cell>0.79</cell><cell>25.54</cell><cell>39.27</cell></row><row><cell></cell><cell></cell><cell>culling</cell><cell></cell><cell></cell></row><row><cell>LAN</cell><cell>0.72</cell><cell>0.64</cell><cell>17.15</cell><cell>30.47</cell></row><row><cell>AKY</cell><cell>0.72</cell><cell>0.64</cell><cell>17.08</cell><cell>30.75</cell></row><row><cell>MAS</cell><cell>0.72</cell><cell>0.63</cell><cell>16.87</cell><cell>30.59</cell></row><row><cell>BNT</cell><cell>0.74</cell><cell>0.66</cell><cell>18.91</cell><cell>32.03</cell></row><row><cell>KOV</cell><cell>0.75</cell><cell>0.68</cell><cell>18.60</cell><cell>31.92</cell></row><row><cell>HUO</cell><cell>0.75</cell><cell>0.64</cell><cell>16.27</cell><cell>29.95</cell></row><row><cell>REM</cell><cell>0.63</cell><cell>0.49</cell><cell>13.55</cell><cell>27.34</cell></row><row><cell>COL</cell><cell>0.63</cell><cell>0.69</cell><cell>22.08</cell><cell>29.74</cell></row><row><cell>UNT</cell><cell>0.77</cell><cell>0.70</cell><cell>19.66</cell><cell>34.65</cell></row><row><cell>EIL</cell><cell>0.52</cell><cell>0.53</cell><cell>17.92</cell><cell>28.14</cell></row><row><cell>EXP</cell><cell>0.81</cell><cell>0.79</cell><cell>22.58</cell><cell>35.04</cell></row><row><cell>4.1. Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">A dataset of HDR images was created consisting of 1,013 training</cell></row><row><cell cols="5">images and 50 test images, with resolutions ranging from 800?800</cell></row><row><cell>up to 4,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>916 ? 3, 273. The images were collected from various sources, including in-house images, frames from HDR videos and the web. Only 100 of the images contained calibrated luminance values, sourced from the Fairchild database</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Average values of the four metrics for all methods for display-referred scaling. Bold values indicate the best value. Method SSIM MS-SSIM PSNR HDR-VDP-2.2</figDesc><table><row><cell></cell><cell></cell><cell>optimal</cell><cell></cell><cell></cell></row><row><cell>LAN</cell><cell>0.76</cell><cell>0.80</cell><cell>19.89</cell><cell>41.01</cell></row><row><cell>AKY</cell><cell>0.76</cell><cell>0.80</cell><cell>20.37</cell><cell>40.89</cell></row><row><cell>MAS</cell><cell>0.79</cell><cell>0.82</cell><cell>21.03</cell><cell>40.83</cell></row><row><cell>BNT</cell><cell>0.74</cell><cell>0.75</cell><cell>17.22</cell><cell>39.99</cell></row><row><cell>KOV</cell><cell>0.80</cell><cell>0.83</cell><cell>23.01</cell><cell>40.00</cell></row><row><cell>HUO</cell><cell>0.77</cell><cell>0.77</cell><cell>17.83</cell><cell>38.58</cell></row><row><cell>REM</cell><cell>0.66</cell><cell>0.59</cell><cell>14.60</cell><cell>33.74</cell></row><row><cell>COL</cell><cell>0.63</cell><cell>0.71</cell><cell>21.00</cell><cell>31.41</cell></row><row><cell>UNT</cell><cell>0.72</cell><cell>0.73</cell><cell>18.23</cell><cell>35.68</cell></row><row><cell>EIL</cell><cell>0.77</cell><cell>0.80</cell><cell>20.66</cell><cell>41.01</cell></row><row><cell>EXP</cell><cell>0.79</cell><cell>0.82</cell><cell>23.43</cell><cell>40.81</cell></row><row><cell></cell><cell></cell><cell>culling</cell><cell></cell><cell></cell></row><row><cell>LAN</cell><cell>0.31</cell><cell>0.17</cell><cell>9.12</cell><cell>18.01</cell></row><row><cell>AKY</cell><cell>0.74</cell><cell>0.66</cell><cell>15.00</cell><cell>31.39</cell></row><row><cell>MAS</cell><cell>0.73</cell><cell>0.64</cell><cell>14.77</cell><cell>31.11</cell></row><row><cell>BNT</cell><cell>0.36</cell><cell>0.27</cell><cell>9.61</cell><cell>24.51</cell></row><row><cell>KOV</cell><cell>0.77</cell><cell>0.69</cell><cell>16.54</cell><cell>31.78</cell></row><row><cell>HUO</cell><cell>0.74</cell><cell>0.64</cell><cell>14.85</cell><cell>30.57</cell></row><row><cell>REM</cell><cell>0.59</cell><cell>0.46</cell><cell>12.81</cell><cell>27.96</cell></row><row><cell>COL</cell><cell>0.66</cell><cell>0.70</cell><cell>19.99</cell><cell>30.26</cell></row><row><cell>UNT</cell><cell>0.78</cell><cell>0.69</cell><cell>17.02</cell><cell>35.27</cell></row><row><cell>EIL</cell><cell>0.54</cell><cell>0.55</cell><cell>15.96</cell><cell>27.58</cell></row><row><cell>EXP</cell><cell>0.83</cell><cell>0.79</cell><cell>19.93</cell><cell>36.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Data Augmentation: The method used to generate input-output pairs significantly affects the end result. To demonstrate, the Ex-pandNet architecture was trained on LDR inputs generated using only the Photoreceptor TMO (EXP-Photo). In this case it consistently underperforms when tested against EXP trained with all the TMOs mentioned in Section 4.1, giving an average PSNR of 19.93 for display-referred culling. However, if the testing is done on LDR images produced not by culling, but instead Photoreceptor, then EXP-Photo produces significantly better results (PSNR of 24.28 vs 21.52 for EXP) since it was specialized to invert the Photoreceptor TMO. This can be useful if, for example, to convert images captured by commercial mobile phones which are stored as tone mapped images using a particular tone mapper back to HDR.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">c 2018 The Author(s) Computer Graphics Forum c 2018 The Eurographics Association and John Wiley &amp; Sons Ltd.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Debattista is partially supported by a Royal Society Industrial Fellowship (IF130053). Marnerides is funded by the EPSRC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Do HDR displays support LDR content?: A psychophysical evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">O</forename><surname>Aky?z A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fleming R</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><forename type="middle">E E</forename><surname>Riecke B</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>B?lthoff</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/1276377.1276425</idno>
		<ptr target="http://doi.acm.org/10.1145/1276377.1276425.1" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extending quality metrics to full luminance range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aydin T</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mantiuk R</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seidel H.-P</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="68060" to="68060" />
		</imprint>
	</monogr>
	<note>Electronic Imaging</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Advanced High Dynamic Range Imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Banterle F</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artusi A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chalmers A</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High dynamic range imaging and low dynamic range expansion for generating HDR content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Banterle F</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artusi A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ledda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chalmers A</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2343" to="2367" />
		</imprint>
	</monogr>
	<note>BDA * 09</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI. Found</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000006</idno>
		<idno>doi:10.1561/2200000006. 1</idno>
		<ptr target="http://dx.doi.org/10.1561/2200000006" />
	</analytic>
	<monogr>
		<title level="j">Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inverse tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Banterle F</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ledda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chalmers A</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/1174429.1174489</idno>
		<ptr target="http://doi.acm.org/10.1145/1174429.1174489.2,8" />
	</analytic>
	<monogr>
		<title level="m">GRAPHITE &apos;06: Proceedings of the 4th International Conference on Computer Graphics and Interactive Techniques in Australasia and Southeast Asia</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kernelpredicting convolutional networks for denoising monte carlo renderings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bako S</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vogels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcwilliams B</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meyer M</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nov?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harvill A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Derose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rousselle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>BVM * 17</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interactive reconstruction of monte carlo image sequences using a recurrent denoising autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R A</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">S</forename><surname>Kaplanyan A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schied</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lefohn A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CKS * 17</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimal exposure compression for high dynamic range content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Debattista K</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bashford-Rogers T</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Selmanovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mukherjee R</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chalmers A</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1089" to="1099" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>DBRS * 15</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast bilateral filtering for the display of high-dynamic-range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Durand</forename><forename type="middle">F</forename><surname>Dorsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.1145/566654.566574</idno>
		<idno>doi:10.1145/566654.566574. 6</idno>
		<ptr target="http://doi.acm.org/10.1145/566654.566574" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="266" />
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang X</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2015.2439281</idno>
		<idno>doi:10.1109/TPAMI.2015.2439281. 2</idno>
		<ptr target="http://dx.doi.org/10.1109/TPAMI.2015.2439281" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive Logarithmic Mapping For Displaying High Contrast Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drago F</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Annen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chiba</surname></persName>
		</author>
		<idno type="DOI">10.1111/1467-8659.00689</idno>
		<idno>doi:10.1111/1467-8659.00689. 6</idno>
		<ptr target="http://dx.doi.org/10.1111/1467-8659.00689" />
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="419" to="426" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Enhancement of bright video features for HDR displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Didyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mantiuk R</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seidel H.-P</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1265" to="1274" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HDR image reconstruction from a single exposure using deep CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Eilertsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kronander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Denes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">K</forename><surname>Mantiuk R</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Unger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Reverse Tone Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kanamori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH ASIA 2017</title>
		<meeting>of SIGGRAPH ASIA 2017</meeting>
		<imprint>
			<date type="published" when="2017-11" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The HDR photographic survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">D</forename><surname>Fairchild M</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Society for Imaging Science and Technology</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="233" to="238" />
		</imprint>
	</monogr>
	<note>Color and Imaging Conference</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What is the space of camera response functions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">D</forename><surname>Grossberg M</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2003.1211522</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="602" to="611" />
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow I</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu B</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Warde-Farley D</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bengio Y. ; Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.2" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<editor>Weinberger K. Q.,</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
	<note>GPAM * 14</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep feature consistent deep image transformations: Downscaling, decolorization and HDR tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hou X</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<idno>abs/1707.09482</idno>
		<ptr target="http://arxiv.org/abs/1707.09482.2" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep outdoor illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hold-Geoffroy Y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gam-Baretto E</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lalonde J.-F</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>HGSH * 17</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochreiter S</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<idno>doi:10.1162/neco.1997.9.8.1735. 11</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Physiological inverse tone mapping based on retina response. The Visual Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><forename type="middle">F</forename><surname>Dong L</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brost V</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He K</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang X</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<ptr target="http://arxiv.org/abs/1512.03385.1" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ioffe S</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR Workshop and Conference Proceedings, JMLR.org</title>
		<editor>Bach F. R., Blei D. M.,</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iizuka S</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simo-Serra E</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<idno>110:1-110:11. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH 2016)</title>
		<meeting>of SIGGRAPH 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iizuka S</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simo-Serra E</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073659</idno>
		<idno>doi:10.1145/3072959.3073659. 2</idno>
		<ptr target="http://doi.acm.org/10.1145/3072959.3073659" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Image-to-Image Translation with Conditional Adversarial Nets. Supplementary Material</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou T</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<ptr target="https://phillipi.github.io/pix2pix/images/cityscapes_cGAN_AtoB/latest_net_G_val/index.html" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou T</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>abs/1611.07004</idno>
		<ptr target="http://arxiv.org/abs/1611.07004.2" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">P</forename><surname>Kingma D</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980.7</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A machine learning approach for filtering monte carlo noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">K</forename><surname>Kalantari N</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bako</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="122" to="123" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K M</forename><surname>Lee K</surname></persName>
		</author>
		<idno>CoRR abs/1511.04491</idno>
		<ptr target="http://arxiv.org/abs/1511.04491.2" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">High-quality reverse tone mapping for a wide range of exposures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">P</forename><surname>Kovaleski R</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th SIBGRAPI Conference on Graphics, Patterns and Images</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014-08" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep high dynamic range imaging of dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">K</forename><surname>Kalantari N</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramamoorthi R</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073609</idno>
		<idno>doi:10.1145/3072959.3073609. 2</idno>
		<ptr target="http://doi.acm.org/10.1145/3072959.3073609" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krizhevsky A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever I</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3065386</idno>
		<idno>doi:10.1145/3065386. 5</idno>
		<ptr target="http://doi.acm.org/10.1145/3065386" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mayr A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochre-Iter S</surname></persName>
		</author>
		<idno>abs/1706.02515</idno>
		<ptr target="http://arxiv.org/abs/1706.02515.4" />
		<title level="m">Self-Normalizing Neural Networks. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Production-ready global illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Landis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH Course Notes</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="87" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evaluation of reverse tone mapping through varying exposure conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agustin</forename><forename type="middle">S</forename><surname>Masia B</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">W</forename><surname>Fleming R</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gutierrez D</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/1618452.1618506</idno>
		<ptr target="http://doi.acm.org/10.1145/1618452.1618506.2,8" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>MAF * 09</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mathieu M</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1511.05440</idno>
		<ptr target="http://arxiv.org/abs/1511.05440.5" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Display adaptive tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mantiuk R</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly S</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kerofsky</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/1360612.1360667</idno>
		<ptr target="http://doi.acm.org/10.1145/1360612.1360667.6" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Clinical evaluation of a medical high dynamic range display</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marchessoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Paepe L</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vanovermeire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Albani</surname></persName>
		</author>
		<idno type="DOI">10.1118/1.4953187</idno>
		<idno>doi:10.1118/1.4953187</idno>
		<ptr target="http://dx.doi.org/10.1118/1.4953187" />
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="4023" to="4031" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Reproduction of Specular Highlights on High Dynamic Range Displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meylan L</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly S</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IST/SID 14th Color Imaging Conference</title>
		<meeting><address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="333" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dynamic range expansion based on image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Masia B</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serrano A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gutierrez D</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="631" to="648" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">HDR-VDP-2.2: a calibrated method for objective quality prediction of high-dynamic range and standard images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narwaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">K</forename><surname>Mantiuk R</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">P</forename><surname>Da Silva M</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le Callet P</surname></persName>
		</author>
		<idno>010501-010501. 7</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deconvolution and checkerboard artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Odena A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00003</idno>
		<ptr target="http://distill.pub/2016/deconv-checkerboard" />
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<ptr target="http://pytorch.org/.7" />
		<title level="m">PyTorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic range reduction inspired by photoreceptor physiology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><forename type="middle">E</forename><surname>Devlin K</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="13" to="24" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ronneberger O</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox T</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1505.04597.2,8" />
		<title level="m">U-Net: Convolutional Networks for Biomedical Image Segmentation. CoRR abs/1505</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4597</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">E</forename><surname>Rumelhart D</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">E</forename><surname>Hinton G</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">J</forename><surname>Williams R</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="533" to="538" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Photographic tone reproduction for digital images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><forename type="middle">E</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shirley P</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ferwerda</surname></persName>
		</author>
		<idno type="DOI">10.1145/566654.566575</idno>
		<idno>doi:10.1145/566654.566575. 2</idno>
		<ptr target="http://doi.acm.org/10.1145/566654.566575" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">LDR2HDR: Onthe-fly reverse tone mapping of legacy video and photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">G</forename><surname>Rempel A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trentacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seetzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whitehead L</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ward G</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/1276377.1276426</idno>
		<ptr target="http://doi.acm.org/10.1145/1276377.1276426.2,8" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>RTS * 07</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A machine-learning-driven sky model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satilmis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bashford-Rogers T</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chalmers A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Debattista</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCG.2016.67</idno>
		<idno>doi:10. 1109/MCG.2016.67. 2</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="91" />
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1404.7828</idno>
		<ptr target="http://arxiv.org/abs/1404.7828" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Is the deconvolution layer the same as a convolutional layer?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi W</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theis L</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huszar F</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">P</forename><surname>Aitken A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang Z</surname></persName>
		</author>
		<idno>CoRR abs/1609.07009</idno>
		<ptr target="http://arxiv.org/abs/1609.07009.3" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>SCT * 16</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">High dynamic range display systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seetzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stuerzlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ward G</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whitehead L</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trentacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghosh A</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voroz-Covs A</surname></persName>
		</author>
		<idno type="DOI">10.1145/1186562.1015797</idno>
		<idno>doi:10.1145/1186562.1015797. 1</idno>
		<ptr target="http://doi.acm.org/10.1145/1186562.1015797" />
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;04, ACM</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="760" to="768" />
		</imprint>
	</monogr>
	<note>Papers</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tone reproduction for realistic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rushmeier</surname></persName>
		</author>
		<idno type="DOI">10.1109/38.252554</idno>
		<idno>doi:10.1109/ 38.252554. 6</idno>
		<ptr target="http://dx.doi.org/10.1109/38.252554" />
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="42" to="48" />
			<date type="published" when="1993-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">High dynamic range image hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><forename type="middle">L</forename><surname>Wei L.-Y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou K</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shum H.-Y</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/1278780.1278867</idno>
		<ptr target="http://doi.acm.org/10.1145/1278780.1278867.2" />
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;07: ACM SIGGRAPH 2007 Sketches</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page">72</biblScope>
		</imprint>
	</monogr>
	<note>WWZ * 07</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">F</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<idno>abs/1511.07122</idno>
		<ptr target="http://arxiv.org/abs/1511.07122.4" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Fast and accurate image super resolution by deep CNN with skip connection and network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yamanaka</forename><forename type="middle">J</forename><surname>Kuwashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kurita T</surname></persName>
		</author>
		<idno>abs/1707.05425</idno>
		<ptr target="http://arxiv.org/abs/1707.05425.2" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning high dynamic range from outdoor panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lalonde J.-F</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Learning High Dynamic Range from Outdoor Panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lalonde J.-F</surname></persName>
		</author>
		<ptr target="http://vision.gel.ulaval.ca/~jflalonde/projects/learningHDR/supp_mat/index.html" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Supplementary Material</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
