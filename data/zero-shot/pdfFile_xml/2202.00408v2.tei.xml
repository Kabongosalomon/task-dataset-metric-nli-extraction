<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dimensionality Reduction Meets Message Passing for Graph Node Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Sadowski</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Szarmach</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddie</forename><surname>Mattia</surname></persName>
						</author>
						<title level="a" type="main">Dimensionality Reduction Meets Message Passing for Graph Node Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have become a popular approach for various applications, ranging from social network analysis to modeling chemical properties of molecules. While GNNs often show remarkable performance on public datasets, they can struggle to learn long-range dependencies in the data due to over-smoothing and over-squashing tendencies. To alleviate this challenge, we propose PCAPass, a method which combines Principal Component Analysis (PCA) and message passing for generating node embeddings in an unsupervised manner and leverages gradient boosted decision trees for classification tasks. We show empirically that this approach provides competitive performance compared to popular GNNs on node classification benchmarks, while gathering information from longer distance neighborhoods. Our research demonstrates that applying dimensionality reduction with message passing and skip connections is a promising mechanism for aggregating long-range dependencies in graph structured data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning on graphs has become a common building block <ref type="bibr" target="#b26">Wu et al., 2021)</ref> for applications like social network analysis <ref type="bibr" target="#b17">(Li et al., 2021c)</ref>, recommendation systems <ref type="bibr" target="#b5">(Gao et al., 2021)</ref>, fraud detection mechanisms  and predicting chemical properties <ref type="bibr" target="#b15">(Li et al., 2021b)</ref>. Unlike previous approaches that involved converting a graph into a sequence of tensors, GNNs use its structure to learn more expressive representations. <ref type="bibr" target="#b33">Zhou et al., 2021</ref>  <ref type="figure">Figure 1</ref>. Illustration of the process performed in the PCAPass layer. Our method keeps the meaningful properties from the aggregated neighborhood and node embeddings from the previous algorithm step by using dimensionality reduction.</p><p>Most GNN architectures distinguish themselves from other neural network families by the concept of messaging passing <ref type="bibr" target="#b6">(Gilmer et al., 2017)</ref>. The essence of this operation is that the node collects messages from its neighbors, i.e. other nodes connected by the edge. The entire process can be described as follows:</p><formula xml:id="formula_0">m k v = u N (?) M k (h k?1 u , h k?1 v , e uv )<label>(1)</label></formula><formula xml:id="formula_1">h k v = U k (h k?1 v , m k v ),<label>(2)</label></formula><p>where h is node feature embedding, k is an algorithm step and N denotes nodes in graph. Messages are computed as a message function M of the source node u, destination node v , and the edge connecting them e (Equation 1). Then, aggregated messages m by permutation invariant function are used to create new embedding of destination node by update function U (Equation 2). Repeating this process k times causes the node to incrementally receive information from further k-hop neighborhoods.</p><p>As the receptive field is enlarged, the number of neighbors increases exponentially leading to a problem known as arXiv:2202.00408v2 [cs.</p><p>LG] 2 Feb 2022 neighborhood explosion which leads to high memory utilization. For tasks such as predicting molecular properties, where GNNs operate on many small graphs, it is possible to batch them to fit into memory. In the case of training on one large graph, such as a social network, many solutions related to sampling of neighbors or graph partitioning were created <ref type="bibr" target="#b4">Chiang et al., 2019;</ref><ref type="bibr" target="#b8">Hamilton et al., 2018)</ref>. However, this process can potentially introduce bias to the model and is computationally expensive as it is typically performed for each layer of the neural network.</p><p>Stacking multiple layers of message passing is necessary to train long-range dependencies in a graph but can also lead to issues known as over-smoothing and over-squashing. Due to the aggregation function in message passing as more and more layers are added, adjacent nodes begin to have increasingly similar representations . We define the problem of over-smoothing by the difficulty of the predictive tasks in distinguishing between nodes. The term over-squashing has been introduced to describe a similar problem with GNNs not propagating long-range graph signals. As the information is aggregated from an exponentially growing number of neighbors, it is propagated through a constant size vector, leading to a bottleneck like that found in Recurrent Neural Networks .</p><p>Typically, GNNs use a multilayer perceptron (MLP) to make predictions. In the case of our method, we decide to use the gradient boosted decision trees classifier (GBDT) implemented with the XGBoost algorithm <ref type="bibr" target="#b3">(Chen &amp; Guestrin, 2016)</ref>. Gradient boosting is a widely used machine learning technique. An ensemble of weak models, usually decision trees, is used to create a stronger predictive model. Like other boosting methods, GBDT is built iteratively but allows optimization of any differentiable loss function.</p><p>In this paper we present PCAPass <ref type="figure">(Figure 1</ref>), a node embedding method that addresses the problems of learning information from distant neighborhoods in graph structured data. Our contributions are as follows:</p><p>? We introduce resistant to over-smoothing method for generating node embeddings in unsupervised manner that leverages graph structure and input features. We empirically prove that our method benefits over message passing itself in gathering information from longrange signals</p><p>? By using message passing in preprocessing our method is more computationally efficient than GNNs which use this process during training. Additionally, therefore, it can be used by the GBDT classifier to make use of the graph structure or other GNNs using message passing in preprocessing. Our experiments show that PCAPass gives a better generalization of XGBoost to unseen data than message passing itself.</p><p>? We empirically demonstrate that PCAPass combined with the XGBoost classifier gives competitive results compared to popular GNNs on three benchmark datasets: ogbn-arxiv, ogbn-products and Reddit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>When training GNNs, the goal is to make predictions at the node, edge, or graph level. The structure of the graph on which the model operates determines the patterns that must be learned. In order to aggregate long-range signals efficiently, many solutions have been developed that address the problem of over-smoothing and over-squashing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Learning Distant Relationships</head><p>Recently, several solutions have been developed to avoid over-smoothing using the skip connection mechanism known from Convolutional Neural Networks <ref type="bibr" target="#b7">Gong et al., 2020;</ref><ref type="bibr" target="#b28">Xu et al., 2021)</ref>. Instead of summing node embeddings from the previous layer along with the aggregated information from neighbors, GCNII uses the initial residual connection to propagate the input features across all layers <ref type="bibr" target="#b2">(Chen et al., 2020)</ref>. JKNet leverages this mechanism to combine the node embeddings from the various message passing steps to get better structure aware representations <ref type="bibr" target="#b27">(Xu et al., 2018)</ref>. These solutions use multiple layers of message passing while training, and therefore consume a lot of memory. To improve the scalability of deep GNNs, RevGNN <ref type="bibr" target="#b14">(Li et al., 2021a)</ref> extends this mechanism using reversible residual connections and applies additional approaches such as group convolutions and weight tying. This model achieves a state-of-the-art result on the ogbnproteins dataset.</p><p>Over-squashing is implicitly resolved through solutions that employ an attention mechanism. Self-attentional layers used in GAT <ref type="bibr" target="#b23">(Veli?kovi? et al., 2018)</ref> and GATv2 <ref type="bibr" target="#b1">(Brody et al., 2021)</ref> alleviate over-squashing by weighting messages can ignore irrelevant edges, which means that fewer of them have to pass through the bottleneck . The initial residual approach has been combined with the concept of attention in FDGATII, while mitigating both of the mentioned problems related to learning long-range dependencies <ref type="bibr" target="#b11">(Kulatilleke et al., 2021)</ref>. Another recent work discusses the relation between the geometric concept of Ricci Curvature and over-squashing and proposes a graph rewiring method to alleviate this problem <ref type="bibr" target="#b22">(Topping et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Message Passing in Preprocessing</head><p>To avoid high memory consumption and enable training without node sampling, SIGN uses message passing in preprocessing <ref type="bibr" target="#b21">(Rossi et al., 2020)</ref>. The success and scalability of this method have inspired other solutions to use this approach. The NARS authors used a similar mechanism to train the model on heterogeneous graphs, i.e. graphs with multiple relations . Receptive field attention introduced in GAMLP enable nodes to flexibly use aggregated messages during precomputation from different neighborhood sizes . The methods using neighborhood aggregation in preprocessing yield state-ofthe-art results on Open Graph Benchmark node classification datasets .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Gradient Boosted Decision Trees on Graph Data</head><p>Motivated by the observation that GBDT are usually better at making predictions from heterogeneous features than other machine learning methods, BGNN authors use this method for graph data <ref type="bibr" target="#b10">(Ivanov &amp; Prokhorenkova, 2021)</ref>. This solution can be viewed as an inversion of ours because instead of using a graph structure to prepare node embeddings, it uses GBDT to process the features that are passed to a GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs</head><p>Message passing PCA XGBoost Repeat k-hop times <ref type="figure">Figure 2</ref>. End-to-end architecture of PCAPass method combined with XGBoost classifier. We use to denote concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PCAPass</head><p>In this work we propose PCAPass, a method for generating node embeddings in an unsupervised manner. The key building block of the architecture is the Principal Component Analysis (PCA) dimensionality reduction process combined with the message passing mechanism. PCA is based on selecting only the first principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. Drawing inspiration from the SIGN architecture, our method uses message passing in preprocessing. The success of this model suggests that GNNs are using message passing to smooth node features over local neighborhoods rather than learning non-linear feature hierarchies. Moreover, giving up stacking message passing with MLP allows GBDT to leverage graph structure without any additional mechanism that requires gradient backpropagation. PCAPass method <ref type="figure">(Figure 2)</ref> can be described as follows:</p><formula xml:id="formula_2">h k ? = AGG k (h k?1 u , ? u N (?))<label>(3)</label></formula><formula xml:id="formula_3">h k N (?) = P CA(h k v h k?1 v ),<label>(4)</label></formula><p>where AGG is local neighborhood feature aggregation function and is concatenation. At each iteration, our algorithm gathers node information from local neighbors (Equation 3). The architecture design allows the choice of any aggregation functions suitable to graph structure and features. After gathering information from each node's neighborhood, PCAPass uses skip connections to provide node embeddings from the previous step and concatenates both vectors <ref type="bibr">(Equation 4</ref>). The use of concatenation for skip connections instead of sum as usual is also seen in GraphSAGE. At each iteration, nodes aggregate information from more distant neighbors, and the size of information used to compute new embeddings increases exponentially (2 k ). We apply dimensionality reduction (Equation 4) at each step of the algorithm to overcome large memory consumption and high computational costs. In addition, this process retains essential properties from aggregated neighborhood and of destination node state from the previous iteration. It is worth noting that PCAPass is not using node sampling or subgraph partitioning which could introduce bias into training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate PCAPass performance on three benchmarks: predicting Amazon co-purchasing network categories in a multiclass configuration (ogbn-products), classifying scientific articles into different subject areas using the citation network (ogbn-arxiv), and classifying Reddit posts as belonging to different communities (Reddit). In all these experiments, we test the proposed method on the node classification task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baselines</head><p>The effectiveness of our method is compared with the popular GNNs, that have a similar approach to the problems related to learning long-range signals. DeeperGCN uses skip connections and graph normalization to alleviate oversmoothing. Similar to GraphSAGE, PCAPass combines aggregated messages and node embedding from the previous step of the algorithm by concatenation. GAT addresses the tendency of over-smoothing <ref type="bibr" target="#b18">(Min et al., 2022)</ref> and over-squashing  with an attention mechanism that weights incoming messages. SIGN performs message passing in a preprocessing step that generates an individual node embedding for each k-hop step. Therefore, it requires k -1 times more memory to store preprocessed input features than PCAPass with the same embedding size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Setup</head><p>In all experiments, we use graphs with bidirectional edges and self-loops. Early stopping is used during classifier training with a patience of 10 with respect to the calculation of cross-entropy loss on the validation dataset split. We experimented with mean aggregation and the symmetric normalized adjacency matrix as aggregation rules. Empirically, we found that the mean aggregation is better suited to the PCAPass method on the tested datasets. An attempt to standardize the features at each k-hop step before applying the PCA process to the concatenated embeddings (Equation 4) has shown that on the tested datasets this approach is not beneficial. The architectural and optimization hyperparameters were sampled using SigOpt 1 for Bayesian optimization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>Results of our experiments, performed over 10 different random seeds, are summarized in the <ref type="table" target="#tab_1">Table 1</ref>. PCAPass works best compared to other methods on ogbn-products, which can be viewed as a semi-supervised task as it contains 90% of nodes in the test data split. On the smaller ogbn-arxiv and Reddit graphs, our method outperforms GraphSAGE and GAT and provides competitive results to DeeperGCN and SIGN. Combining PCAPass with XGBoost gives results at least 15% better than GBDT alone on all tested datasets.</p><p>Additionally, in <ref type="table" target="#tab_2">Table 2</ref> we show how big is the receptive field of methods that use message passing. GBDT classifier trained on PCAPass node embeddings leverages longerrange dependencies than other methods that are not using skip connections. DeeperGCN gathers information from more distant neighborhoods than PCAPass on ogbn-arxiv. 1 app.sigopt.com</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation</head><p>In our method, message passing is implemented using Deep Graph Library <ref type="bibr" target="#b25">(Wang et al., 2020)</ref> with PyTorch <ref type="bibr" target="#b19">(Paszke et al., 2019)</ref> as backend. We use PCA from scikit-learn <ref type="bibr" target="#b20">(Pedregosa et al., 2018)</ref> with the Intel scikit-learn extension 2 to leverage software and hardware stack. Our choice for GBDT classifier is XGBoost, a popular open-source library which is designed to be flexible, portable, and efficient solution.</p><p>All experiments were run on Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz with 128 GB RAM using Ubuntu 18.04 operating system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis</head><p>In early experiments, we tested the simple preprocessing message passing solution. Therefore, we decided to make additional analyzes to show our method in the context of generalization to unseen data and gathering information from a large receptive field.  . XGBoost classifier performance trained on node embeddings during 50 runs of hyperparameter optimization. The best result in terms of validation loss is marked with a for the specific method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Generalization to Unseen Data</head><p>We perform additional analysis with 50 runs of hyperparameter optimization applied to both preprocessing and the XGBoost classifier <ref type="figure" target="#fig_0">(Figure 3</ref>). To evaluate how solutions generalize we choose a validation cross-entropy loss to indicate model metric evaluated during optimization and test accuracy to show performance on unseen data. Our results show that classifier achieves the best results with PCAPass node embeddings and generalizes better than other methods due to the strongest Pearson correlation between evaluated metrics ( <ref type="figure">Figure 5</ref>). Interestingly, message passing and its variant with skip connections yield similar results, suggesting the latter may be suffering from over-squashing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Long-range Aggregation</head><p>To test the resistance of our method to over-smoothing, we performed an analysis using the k-means clustering algorithm ( <ref type="figure" target="#fig_1">Figure 4</ref>). We chose this approach because it is a simple solution and has no hyperparameters that would introduce the bias in the results.  <ref type="figure">Figure 5</ref>. Absolute value of the correlation between validation loss and test accuracy. We show that XGBoost, along with our node embedding method, best generalizes to unseen data.</p><p>We explored PCAPass, message passing, and message passing with skip connections across the 50 k-hop neighborhood aggregation step. As a result of over-smoothing, the nodes become more similar to each other, which makes clustering more difficult. We normalized the result of v-measure per method to show how resistant it is to this problem with respect to the size of the receptive field. For each of the methods, we chose the mean as the aggregation function and we did not use the possibility of PCAPass to resize node embeddings to have a fair comparison. Additionally, we standardize node embeddings for each of the methods before applying k-means. PCAPass and message passing with skip connections have similar trend lines on ogbn-products and ogbn-arxiv datasets which shows that PCA does not spoil the effect of skip connections. Although our method performance drops faster on Reddit than message passing, it still has its best result with a larger receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this paper, we presented PCAPass, an approach for generating graph node embeddings. Our method uses skip connections and dimensionality reduction to retain information about the node and its neighborhood. The use of these techniques provides a mechanism for navigating oversmoothing and over-squashing, allowing information to be gathered from distant neighborhoods. PCAPass, which uses message passing during the preprocessing step, allows the GBDT classifier to use the graph topology efficiently. Our method combined with XGBoost gives competitive results compared to popular GNNs, which we demonstrated on 3 benchmark datasets. We have empirically demonstrated that our solution creates embeddings that lead to better generalization than with message passing alone. Additionally, PCAPass can be used by other GNNs that use message passing in preprocessing, such as SIGN, GAMLP and NARS, giving the possibility to aggregate information from distant neighborhoods.</p><p>We use mean aggregation in our experiments, but PCAPass can potentially benefit from using more specialized aggregation functions suited for graph structure and performed task. In addition, our architecture allows the use of other dimesionality reduction algorithms to create more informative embeddings. Our results shows that dimensionality reduction could be a promising mechanism for alleviating over-squashing in the message passing process. It can be included in neighborhood aggregation mechanism to leave as much relevant information as possible.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3</head><label>3</label><figDesc>Figure 3. XGBoost classifier performance trained on node embeddings during 50 runs of hyperparameter optimization. The best result in terms of validation loss is marked with a for the specific method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Analysis of resistance to over-smoothing of node embeddings performed with k-means clustering. Normalized v-measure score per method shows how each of them behaves while changing the size of receptive field. The trend lines and best results denoted by indicate that PCAPass and message passing with skip connections are least affected by over-smoothing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Intel 2 SigOpt (Intel). Correspondence to: Krzysztof Sadowski &lt;krzysztof.sadowski@intel.com&gt;, Eddie Mattia &lt;eddie.mattia@intel.com&gt;, Micha? Szarmach &lt;michal.szarmach@intel.com&gt;.</figDesc><table><row><cell></cell><cell>Skip</cell></row><row><cell>Message</cell><cell>connection</cell></row><row><cell>passing</cell><cell>PCA</cell></row><row><cell></cell><cell>Concatenation</cell></row><row><cell>).</cell><cell></cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Classification accuracy on tested datasets. PCAPass combined with XGBoost achieves the best results from the compared methods on ogbn-products and gives competitive results on the others.</figDesc><table><row><cell>MODEL</cell><cell>OGBN-ARXIV</cell><cell>OGBN-PRODUCTS</cell><cell>REDDIT</cell></row><row><cell>DEPEERGCN</cell><cell>71.92?0.16</cell><cell>80.98?0.20</cell><cell>N/A</cell></row><row><cell>GAT</cell><cell>71.59?0.38</cell><cell>79.45?0.57</cell><cell>N/A</cell></row><row><cell>GRAPHSAGE</cell><cell>71.74?0.29</cell><cell>78.70?0.36</cell><cell>95.40?0.22</cell></row><row><cell>SIGN</cell><cell>71.95?0.06</cell><cell>80.52?0.16</cell><cell>96.80?0.00</cell></row><row><cell>PCAPASS + XGBOOST</cell><cell>71.87?0.03</cell><cell>81.15?0.02</cell><cell>96.26?0.02</cell></row><row><cell>XGBOOST</cell><cell>54.56?0.07</cell><cell>61.83?0.02</cell><cell>74.88?0.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Number of k-hop neighborhoods aggregated during training. PCAPass along with DeeperGCN that also uses skip connections have a larger receptive field than other methods.</figDesc><table><row><cell>MODEL</cell><cell>OGBN-ARXIV</cell><cell>OGBN-PRODUCTS</cell><cell>REDDIT</cell></row><row><cell>DEEPERGCN</cell><cell>28</cell><cell>14</cell><cell>N/A</cell></row><row><cell>GAT</cell><cell>3</cell><cell>3</cell><cell>N/A</cell></row><row><cell>GRAPHSAGE</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>SIGN</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell>PCAPASS + XGBOOST</cell><cell>13</cell><cell>24</cell><cell>21</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/intel/scikit-learn-intelex</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">How attentive are graph attention networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahav</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xgboost</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<idno>doi: 10.1145/ 2939672.2939785</idno>
		<ptr target="http://dx.doi.org/10.1145/2939672.2939785" />
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cluster-Gcn</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330925</idno>
		<ptr target="http://dx.doi.org/10.1145/3292500" />
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Graph neural networks for recommender systems: Challenges, methods, and directions</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Geometrically principled connections in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Boost then convolve: Gradient boosting meets graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prokhorenkova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fast dynamic graph attention with initial residual and identity mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Kulatilleke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Portmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fdgatii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Training graph neural networks with 1000 layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Graph representation learning in biomedicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Relevance-aware anomalous users detection in social network via graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scattering gcn: Overcoming oversmoothness in graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wenkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>An imperative style, high-performance deep learning library</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duchesnay</forename><surname>And?douard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Scikit-learn: Machine learning in python</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sign: Scalable inception graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Understanding over-squashing and bottlenecks on graphs via curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Topping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Graph attention networks</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A review on graph neural network methods in financial applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep graph library: A graphcentric, highly-performant package for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2020.2978386</idno>
		<ptr target="http://dx.doi.org/10.1109/TNNLS.2020.2978386" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ichi Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Optimization of graph neural networks: Implicit acceleration by skip connections and more depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219890</idno>
		<ptr target="http://dx.doi.org/10.1145/3219819.3219890" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Scalable graph neural networks for heterogeneous graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Graph attention multi-layer perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
