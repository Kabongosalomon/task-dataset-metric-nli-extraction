<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Time-Series Representation Learning via Temporal and Contextual Contrasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emadeldeen</forename><surname>Eldele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ragab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<region>A*STAR</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wu</surname></persName>
							<email>wumin@i2r.a-star.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<region>A*STAR</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keong</forename><surname>Chee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwoh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
							<email>xlli@i2r.a-star.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<region>A*STAR</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuntai</forename><surname>Guan</surname></persName>
							<email>ctguan@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Time-Series Representation Learning via Temporal and Contextual Contrasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>This article has been published in the International Joint Conferences on Artificial Intelligence (IJCAI-21). 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning decent representations from unlabeled time-series data with temporal dynamics is a very challenging task. In this paper, we propose an unsupervised Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC), to learn time-series representation from unlabeled data. First, the raw timeseries data are transformed into two different yet correlated views by using weak and strong augmentations. Second, we propose a novel temporal contrasting module to learn robust temporal representations by designing a tough cross-view prediction task. Last, to further learn discriminative representations, we propose a contextual contrasting module built upon the contexts from the temporal contrasting module. It attempts to maximize the similarity among different contexts of the same sample while minimizing similarity among contexts of different samples. Experiments have been carried out on three real-world time-series datasets. The results manifest that training a linear classifier on top of the features learned by our proposed TS-TCC performs comparably with the supervised training. Additionally, our proposed TS-TCC shows high efficiency in few-labeled data and transfer learning scenarios. The code is publicly available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Time-series data are being incrementally collected on daily basis from IoT and wearable devices for various applications in healthcare, manufacturing, etc. However, they generally do not have human recognizable patterns and require specialists for annotation/labeling. Therefore, it is much harder to label time-series data than images, and little time-series data have been labeled in real-world applications <ref type="bibr" target="#b3">[Ching et al., 2018]</ref>. Given that deep learning methods usually require a massive amount of labeled data for training, it is thus very challenging to apply them on time-series data with these labeling limitations. * Corresponding Author Self-supervised learning gained more attention recently to extract effective representations from unlabeled data for downstream tasks. Compared with models trained on full labeled data (i.e., supervised models), self-supervised pretrained models can achieve comparable performance with limited labeled data . Various selfsupervised approaches relied on different pretext tasks to train the models and learn representations from unlabeled data, such as solving puzzles <ref type="bibr" target="#b7">[Noroozi and Favaro, 2016]</ref> and predicting image rotation <ref type="bibr" target="#b5">[Gidaris et al., 2018]</ref>. However, the pretext tasks can limit the generality of the learned representations. For example, classifying the different rotation angles of an image may deviate the model from learning features about the color or orientation of objects <ref type="bibr" target="#b8">[Oord et al., 2018]</ref>.</p><p>Contrastive learning has recently shown its strong ability for self-supervised representation learning in computer vision domain because of its ability to learn invariant representation from augmented data <ref type="bibr" target="#b6">[Hjelm et al., 2019;</ref><ref type="bibr">He et al., 2020;</ref>. It explores different views of the input images by first applying data augmentation techniques and then learns the representations by maximizing the similarity of different views from the same sample and minimizing the similarity with the views from different samples. However, these image-based contrastive learning methods are not able to work well on time-series data for the following reasons. First, they may not be able to address the temporal dependencies of data, which are key characteristics of time-series <ref type="bibr" target="#b4">[Franceschi et al., 2019]</ref>. Second, some augmentation techniques used for images such as color distortion, generally cannot fit well with time-series data. So far, few works on contrastive learning have been proposed for time-series data. For example, <ref type="bibr">[Mohsenvand et al., 2020;</ref><ref type="bibr" target="#b2">Cheng et al., 2020]</ref> developed contrastive learning methods for bio-signals such as EEG and ECG. However, the above two methods are proposed for specific applications and they are not generalizable to other time-series data.</p><p>To address the above issues, we propose a Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC). Our framework employs simple yet efficient data augmentations that can fit any time-series data to create two different, but correlated views of the input data. Next, we propose a novel temporal contrasting module to learn robust representations by designing a tough cross-view prediction task, which for a certain timestep, it utilizes the past latent features of one augmentation to predict the future of another augmentation. This novel operation will force the model to learn robust representation by a harder prediction task against any perturbations introduced by different timesteps and augmentations. Furthermore, we propose a contextual contrasting module in TS-TCC to further learn discriminative representations upon the robust representations learned by the temporal contrasting module. In this contextual contrasting module, we aim to maximize the similarity among different contexts of the same sample while minimizing similarity among contexts of different samples.</p><p>In summary, the main contributions of this work are as follows.</p><p>? A novel contrastive learning framework is proposed for unsupervised time-series representation learning. ? Simple yet efficient augmentations are designed for time-series data in the contrastive learning framework. ? We propose a novel temporal contrasting module to learn robust representations from time series data by designing a tough cross-view prediction task. In addition, we propose a contextual contrasting module to further learn discriminative representations upon the robust representations. ? We perform extensive experiments on our proposed TS-TCC framework using three datasets. Experimental results show that the learned representations are effective for downstream tasks under supervised learning, semisupervised learning and transfer learning settings.</p><p>2 Related Works</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Self-supervised Learning</head><p>The recent advances in self-supervised learning started with applying pretext tasks on images to learn useful representations, such as solving jigsaw puzzles <ref type="bibr" target="#b7">[Noroozi and Favaro, 2016]</ref>, image colorization <ref type="bibr" target="#b10">[Zhang et al., 2016]</ref> and predicting image rotation <ref type="bibr" target="#b5">[Gidaris et al., 2018]</ref>. Despite the good results achieved by these pretext tasks, they relied on heuristics that might limit the generality of the learned representations. On the other hand, contrastive methods started to shine via learning invariant representations from augmented data.   to EEG data. Existing approaches used either temporal or global features. Differently, we first construct different views for input data by designing time-series specific augmentations. Additionally, we propose a novel cross-view temporal and contextual contrasting modules to improve the learned representations for time-series data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>This section describes our proposed TS-TCC in details. As shown in <ref type="figure">Figure 1</ref>, we first generate two different yet correlated views of the input data based on strong and weak augmentations. Then, a temporal contrasting module is proposed to explore the temporal features of the data with an autoregressive model. These models perform a tough cross-view prediction task by predicting the future of one view using the past of the other. We further maximize the agreement be-tween the contexts of the autoregressive models by a contextual contrasting module. Next, we will introduce each component in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Time-Series Data Augmentation</head><p>Data augmentation is a key part in the success of the contrastive learning methods <ref type="bibr">Grill et al., 2020]</ref>. Contrastive methods try to maximize the similarity among different views of the same sample, while minimizing its similarity with other samples. It is thus important to design proper data augmentations for contrastive learning <ref type="bibr">Mohsenvand et al., 2020]</ref>. Usually, contrastive learning methods use two (random) variants of the same augmentation. Given a sample x, they produce two views x 1 and x 2 sampled from the same augmentations family T , i.e.,</p><p>x 1 ? T and x 2 ? T . However, we argue that using different augmentations can improve the robustness of the learned representations. Consequently, we propose applying two separate augmentations, such that one augmentation is weak and the other is strong. In this paper, weak augmentation is a jitter-and-scale strategy. Specifically, we add random variations to the signal and scale up its magnitude. For strong augmentation, we apply permutation-and-jitter strategy, where permutation includes splitting the signal into a random number of segments with a maximum of M and randomly shuffling them. Next, a random jittering is added to the permuted signal. Notably, the augmentation hyperparameters should be chosen carefully according to the nature of the time-series data. For example, the value of M in a time-series data with longer sequences should be greater than its value in those with shorter sequences when applying permutation. Similarly, the jittering ratio for normalized time-series data should be much less than the ratio for unnormalized data. For each input sample x, we denote its strongly augmented view as x s , and its weakly augmented view as x w , where x s ? T s and x w ? T w . These views are then passed to the encoder to extract their high dimensional latent representations. In particular, the encoder has a 3-block convolutional architecture as proposed in <ref type="bibr" target="#b9">[Wang et al., 2017]</ref>. For an input x, the encoder maps x into a high-dimensional latent representation z = f enc (x). We define z = [z 1 , z 2 , . . . z T ], where T is the total timesteps, z i ? R d , where d is the feature length. Thus, we get z s for the strong augmented views, and z w for the weak augmented views, which are then fed into the temporal contrasting module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Temporal Contrasting</head><p>The Temporal Contrasting module deploys a contrastive loss to extract temporal features in the latent space with an autoregressive model. Given the latent representations z, the autoregressive model f ar summarizes all z ?t into a context vector c t = f ar (z ?t ), c t ? R h , where h is the hidden dimension of f ar . The context vector c t is then used to predict the timesteps from z t+1 until z t+k (1 &lt; k ? K). To predict future timesteps, we use log-bilinear model that would preserve the mutual information between the input x t+k and c t , such that f k (x t+k , c t ) = exp((W k (c t )) T z t+k ), where W k is a linear function that maps c t back into the same dimension as z, i.e. W k : R h?d . In our approach, the strong augmentation generates c s t and the weak augmentation generates c w t . We propose a tough cross-view prediction task by using the context of the strong augmentation c s t to predict the future timesteps of the weak augmentation z w t+k and vice versa. The contrastive loss tries to minimize the dot product between the predicted representation and the true one of the same sample, while maximizing the dot product with the other samples N t,k within the minibatch. Accordingly, we calculate the two losses L s T C and L w T C as follows:</p><formula xml:id="formula_0">L s T C = ? 1 K K k=1 log exp((W k (c s t )) T z w t+k ) n?N t,k exp((W k (c s t )) T z w n ) (1) L w T C = ? 1 K K k=1 log exp((W k (c w t )) T z s t+k ) n?N t,k exp((W k (c w t )) T z s n )<label>(2)</label></formula><p>We use Transformer as the autoregressive model because of its efficiency and speed <ref type="bibr" target="#b9">[Vaswani et al., 2017]</ref>. The architecture of the Transformer model is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. It mainly consists of successive blocks of multi-headed attention (MHA) followed by an MLP block. The MLP block is composed of two fully-connected layers with a non-linearity ReLU function and dropout in between. Pre-norm residual connections, which can produce more stable gradients <ref type="bibr" target="#b9">[Wang et al., 2019]</ref>, are adopted in our Transformer. We stack L identical layers to generate the final features. Inspired by BERT model <ref type="bibr">[Devlin et al., 2019]</ref>, we add a token c ? R h to the input whose state acts as a representative context vector in the output. The operation of the Transformer starts by applying the features z ?t to a linear projection W T ran layer that maps the features into the hidden dimension, i.e. W T ran : R d?h . The output of this linear projection is then sent to the Transformer i.e.z = W T ran (z ?t ),z ? R h . Next, we attach the context vector into the features vectorz such that the input features become ? 0 = [c;z], where the subscript 0 denotes being the input to the first layer. Next, we pass ? 0 through Transformer layers as in the following equations:</p><formula xml:id="formula_1">? = MHA(Norm(? ?1 )) + ? ?1 , 1 ? ? L; (3) ? = MLP(Norm(? )) +? , 1 ? ? L. (4)</formula><p>Finally, we re-attach the context vector from the final output such that c t = ? 0 L . This context vector will be the input of the following contextual contrasting module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contextual Contrasting</head><p>We further propose a contextual contrasting module that aims to learn more discriminative representations. It starts with applying a non-linear transformation to the contexts using a non-linear projection head as in . The projection head maps the contexts into the space of where the contextual contrasting is applied.</p><p>Given a batch of N input samples, we will have two contexts for each sample from its two augmented views, and thus have 2N contexts. For a context c i t , we denote c i + t as the positive sample of c i t that comes from the other augmented view of the same input, and hence, (c i t , c i + t ) are considered to be a positive pair. Meanwhile, the remaining (2N ? 2) contexts from other inputs within the same batch are considered as the negative samples of c i t , i.e., c i t can form (2N ? 2) negative pairs with its negative samples. Therefore, we can derive a contextual contrasting loss to maximize the similarity between the positive pair and minimizing the similarity between negative pairs. As such, the final representations can be discriminative.</p><p>Eq. 5 defines the contextual contrasting loss function L CC . Given a context c i t , we divide its similarity with its positive sample c i + t by its similarity with all the other (2N ? 1) samples, including the positive pair and (2N ? 2) negative pairs, to normalize the loss.</p><formula xml:id="formula_2">L CC = ? N i=1 log exp sim c i t , c i + t /? 2N m=1 1 [m =i] exp sim c i t , c m t /? ,<label>(5)</label></formula><p>where sim(u, v) = u T v/ u v denotes the dot product between 2 normalized u and v (i.e., cosine similarity), 1 [m =i] ? {0, 1} is an indicator function, evaluating to 1 iff m = i, and ? is a temperature parameter.</p><p>The overall self-supervised loss is the combination of the two temporal contrasting losses and the contextual contrasting loss as follows.</p><formula xml:id="formula_3">L = ? 1 ? (L s T C + L w T C ) + ? 2 ? L CC ,<label>(6)</label></formula><p>where ? 1 and ? 2 are fixed scalar hyperparameters denoting the relative weight of each loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To evaluate our model, we adopted three publicly available datasets for human activity recognition, sleep stage classification and epileptic seizure prediction, respectively. Additionally, we investigated the transferability of our learned features on a fault diagnosis dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Activity Recognition (HAR)</head><p>We use UCI HAR dataset [Anguita et al., 2013] which contains sensor readings for 30 subjects performing 6 activities (i.e. walking, walking upstairs, downstairs, standing, sitting, and lying down). They collected the data using a mounted Samsung Galaxy S2 device on their waist, with a sampling rate of 50 Hz.  <ref type="table" target="#tab_3">HAR  7352  2947  128  9  6  Sleep-EDF 25612  8910  3000  1  5  Epilepsy  9200  2300  178  1  2  FD  8184  2728  5120  1  3   Table 1</ref>: Description of datasets used in our experiments. The details of FD is the same for all the 4 working conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sleep Stage Classification</head><p>In this problem, we aim to classify the input EEG signal into one of five classes: Wake (W), Non-rapid eye movement (N1, N2, N3) and Rapid Eye Movement (REM). We downloaded Sleep-EDF dataset from the PhysioBank <ref type="bibr" target="#b6">[Goldberger et al., 2000]</ref>. Sleep-EDF includes whole-night PSG sleep recordings, where we used a single EEG channel (i.e., Fpz-Cz) with a sampling rate of 100 Hz, following previous studies <ref type="bibr">[Eldele et al., 2021]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Epilepsy Seizure Prediction</head><p>The Epileptic Seizure Recognition dataset <ref type="bibr" target="#b0">[Andrzejak et al., 2001]</ref> consists of EEG recordings from 500 subjects, where the brain activity was recorded for each subject for 23.6 seconds. Note that the original dataset is labeled with five classes. As four of them do not include epileptic seizure, so we merged them into one class and treat it as a binary classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fault Diagnosis (FD)</head><p>We conducted the transferability experiment on a real-world fault diagnosis dataset <ref type="bibr" target="#b7">[Lessmeier et al., 2016]</ref>. This dataset was collected under four different working conditions. Each working condition can be considered as a separate domain as it has different characteristics from the other working conditions <ref type="bibr" target="#b9">[Ragab et al., 2020]</ref>. Each domain has three classes, namely, two fault classes (i.e., inner fault and outer fault) and one healthy class. <ref type="table">Table 1</ref> summarizes the details of each dataset, e.g., the number of training samples (# Train) and testing samples (# Test), the length of the sample, the number of sensor channels (# Channel) and the number of classes (# Class).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We split the data into 60%, 20%, 20% for training, validation and testing, with considering subject-wise split for Sleep-EDF dataset to avoid overfitting. Experiments were repeated for 5 times with 5 different seeds, and we reported the mean and standard deviation. The pretraining and downstream tasks were done for 40 epochs, as we noticed that the performance does not improve with further training. We applied a batch size of 128 (which was reduced to 32 in few-labeled data experiments as data size may be less than 128). We used Adam optimizer with a learning rate of 3e-4, weight decay of 3e-4, ? 1 = 0.9, and ? 2 = 0.99. For the strong augmentation, we set M HAR = 10, M Ep = 12 and M EDF = 20, while for the weak augmentation, we set the scaling ratio to 2 for all the datasets. We set ? 1 = 1, while we achieved good performance when ? 2 ? 1. Particularly, we set it as 0.7 in our experiments on the four datasets. In  the Transformer, we set the L = 4, and the number of heads as 4. We tuned h ? {32, 50, 64, 100, 128, 200, 256} and set h HAR,Ep = 100, h EDF = 64. We also set its dropout to 0.1. In contextual contrasting, we set ? = 0.2. Lastly, we built our model using PyTorch 1.7 and trained it on a NVIDIA GeForce RTX 2080 Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>To show the efficacy of our proposed TS-TCC, we test it on three different training settings, including linear evaluation, semi-supervised training and transfer learning. We evaluate the performance using two metrics namely the accuracy and the macro-averaged F1-score (MF1) to better evaluate the imbalanced datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with Baseline Approaches</head><p>We compare our proposed approach against the following baselines.</p><p>(1) Random Initialization: training a linear classifier on top of randomly initialized encoder; (2) Supervised: supervised training of both encoder and classifier model; (3) SSL-ECG [P. <ref type="bibr" target="#b9">Sarkar, 2020]</ref>; (4) CPC <ref type="bibr" target="#b8">[Oord et al., 2018]</ref>; <ref type="formula" target="#formula_2">(5)</ref> SimCLR . It is worth noting that, we use time-series specific augmentations to adapt SimCLR to our application as it was originally designed for images.</p><p>To evaluate the performance of our TS-TCC model, we follow the standard linear benchmarking evaluation scheme <ref type="bibr" target="#b8">[Oord et al., 2018;</ref>. Particularly, we train a linear classifier (single MLP layer) on top of a frozen self-supervised pretrained encoder model. <ref type="table" target="#tab_3">Table 2</ref> shows the linear evaluation results of our approach against the baseline methods. Overall, our proposed TS-TCC outperforms all the three state-of-the-art methods. Furthermore, TS-TCC, with only linear classifier, performs best on two out of three datasets while achieving comparable performance to the supervised approach on the third dataset. This demonstrates the powerful representation learning capability of our TS-TCC model. Notably, contrastive methods (e.g., CPC, SimCLR and our TS-TCC) generally achieve better results than the pretext-based method (i.e., SSL-ECG), which reflects the power of invariant features learned by contrastive methods. Additionally, CPC method shows better results than SimCLR, indicating that temporal features are more important than general features in time-series data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Semi-supervised Training</head><p>We investigate the effectiveness of our TS-TCC under the semi-supervised settings, by training the model with 1%, 5%, 10%, 50%, and 75% of randomly selected instances of the training data. <ref type="figure" target="#fig_1">Figure 3</ref> shows the results of our TS-TCC along with the supervised training under the aforementioned settings. In particular, TS-TCC fine-tuning (i.e., red curves in <ref type="figure" target="#fig_1">Figure 3</ref>) means that we fine-tuned the pretrained encoder with few labeled samples.</p><p>We observe that supervised training performs poorly with limited labeled data, while our TS-TCC fine-tuning achieves significantly better performance than supervised training with only 1% of labeled data. For example, TS-TCC fine-tuning can still achieve around 70% and 90% for HAR and Epilepsy datasets respectively. Furthermore, our TS-TCC fine-tuning with only 10% of labeled data can achieve comparable performance with the supervised training with 100% of labeled data in the three datasets, demonstrating the effectiveness of our TS-TCC method under the semi-supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Transfer Learning Experiment</head><p>We further examine the transferability of the learned features by designing a transfer learning experiment. We use Fault Diagnosis (FD) dataset introduced in <ref type="table">Table 1</ref> for the evaluation under the transfer learning setting. Here, we train the model on one condition (i.e., source domain) and test it on another condition (i.e., target domain). In particular, we adopt two training schemes on the source domain, namely, (1) supervised training and <ref type="formula" target="#formula_0">(2)</ref>    fine-tuned our pretrained encoder using the labeled data in the source domain. <ref type="table" target="#tab_5">Table 3</ref> shows the performance of the two training schemes under 12 cross-domain scenarios. Clearly, our pretrained TS-TCC model with fine-tuning (FT) consistently outperforms the supervised pretraining in 8 out of 12 cross-domain scenarios. TS-TCC model can achieve at least 7% improvement in 7 out of 8 winning scenarios (except for D?B scenario). Overall, our proposed approach can improve the transferability of learned representations over the supervised training by about 4% in terms of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>We study the effectiveness of each component in our proposed TS-TCC model. Specifically, we derive different model variants for comparison as follows. First, we train the Temporal Contrasting module (TC) without the cross-view prediction task, where each branch predicts the future timesteps of the same augmented view. This variant is denoted as 'TC only'. Second, we train the TC module with adding the cross-view prediction task, which is denoted as 'TC + X-Aug'. Third, we train the whole proposed TS-TCC model, which is denoted as 'TC + X-Aug + CC'. We also study the effect of using a single augmentation in TS-TCC. In particular, for an input x, we generate two different views x 1 and x 2 from the same augmentation type, i.e., x 1 ? T w and x 2 ? T w when using the weak augmentation. <ref type="table" target="#tab_6">Table 4</ref> shows this ablation study on the three datasets. Clearly, the proposed cross-view prediction task generates robust features and thus improves the performance by more than 5% on HAR datasets, and ?1% on Sleep-EDF and Epilepsy datasets. Additionally, the contextual contrasting module further improves the performance, as it helps the features to be more discriminative. Studying the augmentations effect, we find that generating different views from the same augmentation type is not helpful with HAR and Sleep-EDF datasets. On the other hand, Epilepsy dataset can achieve comparable performance with only one augmentation. Overall, our proposed TS-TCC method using both types of augmentations achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Sensitivity Analysis</head><p>We perform sensitivity analysis on HAR dataset to study three parameters namely, the number of predicted future timesteps K in the temporal contrasting module, besides ? 1 and ? 2 in Eq. 5. <ref type="figure" target="#fig_2">Figure 4a</ref> shows the effect of K on the overall performance, where x-axis is the percentage K/d, d is the length of the features. Clearly, increasing the percentage of the predicted future timesteps improves the performance. However, larger percentages can harm the performance as it reduces the amount of past data used for training the autoregressive model. We observe that predicting 40% of the total feature length performs the best, and thus we set K as d?40% in our experiments. <ref type="figure" target="#fig_2">Figures 4b and 4c</ref> show the results of varying ? 1 and ? 2 in a range between 0.001 and 1000 respectively. We fix ? 1 = 1 and change the values of ? 2 in <ref type="figure" target="#fig_2">Figure 4c</ref>. We observe that our model achieves good performance when ? 2 ? 1, where the model performs best with ? 2 = 0.7. Consequently, we fix ? 2 = 0.7 and tune the value of ? 1 as in <ref type="figure" target="#fig_2">Figure 4b</ref>, where we find that our model achieves the best performance when ? 1 = 1. We also find that as ? 1 &lt; 10, our model is less sensitive to its value, while it is more sensitive to different values of ? 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We propose a novel framework called TS-TCC for unsupervised representation learning from time-series data. The proposed TS-TCC framework first creates two views for each sample by applying strong and weak augmentations. Then the temporal contrasting module learns robust temporal features by applying a tough cross-view prediction task. We further propose a contextual contrasting module to learn discriminative features upon the learned robust representations. The experiments show that a linear classifier trained on top the features learned by our TS-TCC performs comparably with supervised training. In addition, our proposed TS-TCC shows high efficiency on few-labeled data and transfer learning scenarios, e.g., our TS-TCC by using only 10% of the labeled data can achieve close performance to the supervised training with full labeled data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of Transformer model used in Temporal Contrasting module. The token c in the output is sent next to the Contextual Contrasting module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparison between supervised training vs. TS-TCC fine-tuning for different few-labeled data scenarios in terms of MF1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Three sensitivity analysis experiments on HAR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>89?5.13 55.45?5.49 35.61?6.96 23.80?7.96 90.26?1.77 81.12?4.22 Supervised 90.14?2.49 90.31?2.24 83.41?1.44 74.78?0.86 96.66?0.24 94.52?0.43 SSL-ECG [P. Sarkar, 2020] 65.34?1.63 63.75?1.37 74.58?0.60 65.44?0.97 93.72?0.45 89.15?0.93 CPC [Oord et al., 2018] 83.85?1.51 83.27?1.66 82.82?1.68 73.94?1.75 96.61?0.43 94.44?0.69 SimCLR [Chen et al., 2020] 80.97?2.46 80.19?2.64 78.91?3.11 68.60?2.71 96.05?0.34 93.53?0.63 TS-TCC (ours) 90.37?0.34 90.38?0.39 83.00?0.71 73.57?0.74 97.23?0.10 95.54?0.08</figDesc><table><row><cell></cell><cell>HAR</cell><cell></cell><cell cols="2">Sleep-EDF</cell><cell>Epilepsy</cell><cell></cell></row><row><cell>Baseline</cell><cell>ACC</cell><cell>MF1</cell><cell>ACC</cell><cell>MF1</cell><cell>ACC</cell><cell>MF1</cell></row><row><cell>Random Initialization</cell><cell>57.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison between our proposed TS-TCC model against baselines using linear classifier evaluation experiment.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>TS-TCC fine-tuning where we A?B A?C A?D B?A B?C B?D C?A C?B C?D D?A D?B D?C AVG Supervised 34.38 44.94 34.57 52.93 63.67 99.82 52.93 84.02 83.54 53.15 99.56 62.43 63.83 TS-TCC (FT) 43.15 51.50 42.74 47.98 70.38 99.30 38.89 98.31 99.38 51.91 99.96 70.31 67.82</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Cross-domains transfer learning experiment applied on Fault Diagnosis dataset in terms of accuracy. (FT stands for fine-tuning) 76?1.50 82.17?1.64 80.55?0.39 70.99?0.86 94.39?1.19 90.93?1.41 TC + X-Aug 87.86?1.33 87.91?1.09 81.58?1.70 71.88?1.71 95.56?0.24 92.57?0.29 TS-TCC (TC + X-Aug + CC) 90.37?0.34 90.38?0.39 83.00?0.71 73.57?0.74 97.23?0.10 95.54?0.08</figDesc><table><row><cell></cell><cell>HAR</cell><cell></cell><cell cols="2">Sleep-EDF</cell><cell>Epilepsy</cell><cell></cell></row><row><cell>Component</cell><cell>ACC</cell><cell>MF1</cell><cell>ACC</cell><cell>MF1</cell><cell>ACC</cell><cell>MF1</cell></row><row><cell cols="7">TC only 82.TS-TCC (Weak only) 76.55?3.59 75.14?4.66 80.90?1.87 72.51?1.74 97.18?0.17 95.47?0.31</cell></row><row><cell>TS-TCC (Strong only)</cell><cell cols="6">60.23?3.31 56.15?4.14 78.55?2.94 68.05?1.87 97.14?0.23 95.39?0.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of each component in TS-TCC model performed with linear classifier evaluation experiment.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is supported by the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funds (Grant No. A20H6b0151) and Career Development Award (Grant No. C210112046).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Indications of nonlinear deterministic and finite-dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESANN</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>A public domain dataset for human activity recognition using smartphones</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<idno type="arXiv">arXiv:2011.10566</idno>
		<title level="m">and He, 2020] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<idno type="arXiv">arXiv:2007.04871</idno>
	</analytic>
	<monogr>
		<title level="m">Oncel Tuzel, and Erdrin Azemi. Subjectaware contrastive learning for biosignals</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An attention-based deep learning approach for sleep stage classification with single-channel eeg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<editor>Emadeldeen Eldele, Zhenghua Chen, Chengyu Liu, Min Wu, Chee-Keong Kwoh, Xiaoli Li, and Cuntai Guan</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised scalable representation learning for multivariate time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Franceschi</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gidaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spyros Gidaris, Praveer Singh, and Nikos Komodakis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">physiotoolkit, and physionet: components of a new research resource for complex physiologic signals. circulation</title>
		<editor>CVPR, 2020. [Hjelm et al.</editor>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">101</biblScope>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Condition monitoring of bearing damage in electromechanical drive systems by using motor current signals of electric motors: A benchmark data set for data-driven classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lessmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference of the prognostics and health management society</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Aaron van den Oord, Yazhe Li, and Oriol Vinyals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
	</analytic>
	<monogr>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Time series classification from scratch with deep neural networks: A strong baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Etemad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ragab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Affective Computing</title>
		<editor>Wang et al., 2017] Zhiguang Wang, Weizhong Yan, and Tim Oates</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
