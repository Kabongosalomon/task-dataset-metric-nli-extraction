<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TrackFormer: Multi-Object Tracking with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TrackFormer: Multi-Object Tracking with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The challenging task of multi-object tracking (MOT) requires simultaneous reasoning about track initialization, identity, and spatio-temporal trajectories. We formulate this task as a frame-to-frame set prediction problem and introduce TrackFormer, an end-to-end trainable MOT approach based on an encoder-decoder Transformer architecture. Our model achieves data association between frames via attention by evolving a set of track predictions through a video sequence. The Transformer decoder initializes new tracks from static object queries and autoregressively follows existing tracks in space and time with the conceptually new and identity preserving track queries. Both query types benefit from self-and encoder-decoder attention on global frame-level features, thereby omitting any additional graph optimization or modeling of motion and/or appearance. TrackFormer introduces a new tracking-byattention paradigm and while simple in its design is able to achieve state-of-the-art performance on the task of multiobject tracking (MOT17 and MOT20) and segmentation (MOTS20). The code is available at https://github. com/timmeinhardt/trackformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans need to focus their attention to track objects in space and time, for example, when playing a game of tennis, golf, or pong. This challenge is only increased when tracking not one, but multiple objects, in crowded and real world scenarios. Following this analogy, we demonstrate the effectiveness of Transformer <ref type="bibr" target="#b50">[51]</ref> attention for the task of multi-object tracking (MOT) in videos.</p><p>The goal in MOT is to follow the trajectories of a set of objects, e.g., pedestrians, while keeping their identities discriminated as they are moving throughout a video sequence. Due to the advances in image-level object detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39]</ref>, most approaches follow the two-step tracking-by-detection paradigm: (i) detecting objects in individual video frames, and (ii) associating sets of detections between frames and thereby creating individual object tracks over time. Traditional tracking-by-detection methods associate detections via temporally sparse <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref> or dense <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref> graph optimization, or apply convolutional neural networks to predict matching scores between detections <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b68">69]</ref> suggest a variation of the traditional paradigm, coined tracking-by-regression <ref type="bibr" target="#b11">[12]</ref>. In this approach, the object detector not only provides frame-wise detections, but replaces the data association step with a continuous regression of each track to the changing position of its object. These approaches achieve track association implicitly, but provide top performance only by relying either on additional graph optimization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29]</ref> or motion and appearance models <ref type="bibr" target="#b3">[4]</ref>. This is largely due to the isolated and local bounding box regression which lacks any notion of object identity or global communication between tracks.</p><p>In this work, we introduce the tracking-by-attention paradigm which not only applies attention for data association <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b69">70]</ref> but jointly performs tracking and detection. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, this is achieved by evolving a set of tracks from frame to frame forming trajectories over time.</p><p>We present a first straightforward instantiation of tracking-by-attention, TrackFormer, an end-to-end trainable Transformer <ref type="bibr" target="#b50">[51]</ref> encoder-decoder architecture. It encodes frame-level features from a convolutional neural network (CNN) <ref type="bibr" target="#b17">[18]</ref> and decodes queries into bounding boxes associated with identities. The data association is performed through the novel and simple concept of track queries. Each query represents an object and follows it in space and time over the course of a video sequence in an autoregressive fashion. New objects entering the scene are detected by static object queries as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b70">71]</ref> and subsequently transform to future track queries. At each frame, the encoder-decoder computes attention between the input image features and the track as well as object queries, and outputs bounding boxes with assigned identities. Thereby, TrackFormer performs tracking-by-attention and achieves detection and data association jointly without relying on any additional track matching, graph optimization, or explicit modeling of motion and/or appearance. In contrast to tracking-by-detection/regression, our approach detects and associates tracks simultaneously in a single step via attention (and not regression). TrackFormer extends the recently proposed set prediction objective for object detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b70">71]</ref> to multi-object tracking.</p><p>We evaluate TrackFormer on the MOT17 <ref type="bibr" target="#b29">[30]</ref> and MOT20 <ref type="bibr" target="#b12">[13]</ref> benchmarks where it achieves state-of-the-art performance for public and private detections. Furthermore, we demonstrate the extension with a mask prediction head and show state-of-the-art results on the Multi-Object Tracking and Segmentation (MOTS20) challenge <ref type="bibr" target="#b51">[52]</ref>. We hope this simple yet powerful baseline will inspire researchers to explore the potential of the tracking-by-attention paradigm.</p><p>In summary, we make the following contributions:</p><p>? An end-to-end trainable multi-object tracking approach which achieves detection and data association in a new tracking-by-attention paradigm.</p><p>? The concept of autoregressive track queries which embed an object's spatial position and identity, thereby tracking it in space and time.</p><p>? New state-of-the-art results on three challenging multiobject tracking (MOT17 and MOT20) and segmentation (MOTS20) benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In light of the recent trend in MOT to look beyond tracking-by-detection, we categorize and review methods according to their respective tracking paradigm.</p><p>Tracking-by-detection approaches form trajectories by associating a given set of detections over time.</p><p>Graphs have been used for track association and longterm re-identification by formulating the problem as a maximum flow (minimum cost) optimization <ref type="bibr" target="#b2">[3]</ref> with distance based <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b63">64]</ref> or learned costs <ref type="bibr" target="#b24">[25]</ref>. Other methods use association graphs <ref type="bibr" target="#b45">[46]</ref>, learned models <ref type="bibr" target="#b22">[23]</ref>, and motion information <ref type="bibr" target="#b21">[22]</ref>, general-purpose solvers <ref type="bibr" target="#b62">[63]</ref>, multicuts <ref type="bibr" target="#b48">[49]</ref>, weighted graph labeling <ref type="bibr" target="#b18">[19]</ref>, edge lifting <ref type="bibr" target="#b19">[20]</ref>, or trainable graph neural networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b54">55]</ref>. However, graphbased approaches suffer from expensive optimization routines, limiting their practical application for online tracking.</p><p>Appearance driven methods capitalize on increasingly powerful image recognition backbones to track objects by relying on similarity measures given by twin neural networks <ref type="bibr" target="#b23">[24]</ref>, learned reID features <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b41">42]</ref>, detection candidate selection <ref type="bibr" target="#b7">[8]</ref> or affinity estimation <ref type="bibr" target="#b9">[10]</ref>. Similar to reidentification, appearance models struggle in crowded scenarios with many object-object-occlusions.</p><p>Motion can be modelled for trajectory prediction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">43]</ref> using a constant velocity assumption (CVA) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> or the social force model <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b58">59]</ref>. Learning a motion model from data <ref type="bibr" target="#b24">[25]</ref> accomplishes track association between frames <ref type="bibr" target="#b64">[65]</ref>. However, the projection of non-linear 3D motion <ref type="bibr" target="#b49">[50]</ref> into the 2D image domain still poses a challenging problem for many models.</p><p>Tracking-by-regression refrains from associating detections between frames but instead accomplishes tracking by regressing past object locations to their new positions in the current frame. Previous efforts <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref> use regression heads on region-pooled object features. In <ref type="bibr" target="#b68">[69]</ref>, objects are represented as center points which allow for an association by a distance-based greedy matching algorithm. To overcome their lacking notion of object identity and global track reasoning, additional re-identification and motion models <ref type="bibr" target="#b3">[4]</ref>, as well as traditional <ref type="bibr" target="#b28">[29]</ref> and learned <ref type="bibr" target="#b5">[6]</ref> graph methods have been necessary to achieve top performance.</p><p>Tracking-by-segmentation not only predicts object masks but leverages the pixel-level information to mitigate issues with crowdedness and ambiguous backgrounds. Prior attempts used category-agnostic image segmentation <ref type="bibr" target="#b30">[31]</ref>, applied Mask R-CNN <ref type="bibr" target="#b16">[17]</ref> with 3D convolutions <ref type="bibr" target="#b51">[52]</ref>, mask pooling layers <ref type="bibr" target="#b37">[38]</ref>, or represented objects as unordered point clouds <ref type="bibr" target="#b57">[58]</ref> and cost volumes <ref type="bibr" target="#b56">[57]</ref>. However, the scarcity of annotated MOT segmentation data makes modern approaches still rely on bounding boxes.</p><p>Attention for image recognition correlates each element of the input with respect to the others and is used in Transformers <ref type="bibr" target="#b50">[51]</ref> for image generation <ref type="bibr" target="#b33">[34]</ref> and object detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b70">71]</ref>. For MOT, attention has only been used to associate a given set of object detections <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b69">70]</ref>, not tackling the detection and tracking problem jointly.</p><p>In contrast, TrackFormer casts the entire tracking objective into a single set prediction problem, applying attention not only for the association step. It jointly reasons about track initialization, identity, and spatio-temporal trajectories. We only rely on feature-level attention and avoid additional graph optimization and appearance/motion models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TrackFormer</head><p>We present TrackFormer, an end-to-end trainable multiobject tracking (MOT) approach based on an encoderdecoder Transformer <ref type="bibr" target="#b50">[51]</ref> architecture. This section describes how we cast MOT as a set prediction problem and introduce the new tracking-by-attention paradigm. Furthermore, we explain the concept of track queries and their application for frame-to-frame data association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">MOT as a set prediction problem</head><p>Given a video sequence with K individual object identities, MOT describes the task of generating ordered tracks T k = (b k t1 , b k t2 , . . . ) with bounding boxes b t and track identities k. The subset (t 1 , t 2 , . . . ) of total frames T indicates the time span between an object entering and leaving the the scene. These include all frames for which an object is occluded by either the background or other objects.</p><p>In order to cast MOT as a set prediction problem, we leverage an encoder-decoder Transformer architecture. Our model performs online tracking and yields per-frame object bounding boxes and class predictions associated with identities in four consecutive steps: (i) Frame-level feature extraction with a common CNN backbone, e.g., ResNet-50 <ref type="bibr" target="#b17">[18]</ref>.</p><p>(ii) Encoding of frame features with self-attention in a Transformer encoder <ref type="bibr" target="#b50">[51]</ref>.</p><p>(iii) Decoding of queries with self-and encoder-decoder attention in a Transformer decoder <ref type="bibr" target="#b50">[51]</ref>.</p><p>(iv) Mapping of queries to box and class predictions using multilayer perceptrons (MLP).</p><p>Objects are implicitly represented in the decoder queries, which are embeddings used by the decoder to output bounding box coordinates and class predictions. The decoder alternates between two types of attention: (i) self-attention over all queries, which allows for joint reasoning about the objects in a scene and (ii) encoder-decoder attention, which gives queries global access to the visual information of the encoded features. The output embeddings accumulate bounding box and class information over multiple decoding layers. The permutation invariance of Transformers requires additive feature and object encodings for the frame features and decoder queries, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Tracking-by-attention with queries</head><p>The total set of output embeddings is initialized with two types of query encodings: (i) static object queries, which allow the model to initialize tracks at any frame of the video, and (ii) autoregressive track queries, which are responsible for tracking objects across frames.</p><p>The simultaneous decoding of object and track queries allows our model to perform detection and tracking in a unified way, thereby introducing a new tracking-by-attention paradigm. Different tracking-by-X approaches are defined by their key component responsible for track generation. For tracking-by-detection, the tracking is performed by computing/modelling distances between frame-wise object detections. The tracking-by-regression paradigm also performs object detection, but tracks are generated by regressing each object box to its new position in the current frame. Technically, our TrackFormer also performs regression in the mapping of object embeddings with MLPs. However, the actual track association happens earlier via attention in the Transformer decoder. A detailed architecture overview which illustrates the integration of track and object queries into the Transformer decoder is shown in the appendix.</p><p>Track initialization. New objects appearing in the scene are detected by a fixed number of N object output embeddings each initialized with a static and learned object encoding referred to as object queries <ref type="bibr" target="#b6">[7]</ref>. Intuitively, each object query learns to predict objects with certain spatial properties, such as bounding box size and position. The decoder self-attention relies on the object encoding to avoid duplicate detections and to reason about spatial and categorical relations of objects. The number of object queries is ought to exceed the maximum number of objects per frame.</p><p>Track queries. In order to achieve frame-to-frame track generation, we introduce the concept of track queries to the decoder. Track queries follow objects through a video sequence carrying over their identity information while adapting to their changing position in an autoregressive manner.</p><p>For this purpose, each new object detection initializes a track query with the corresponding output embedding of the previous frame. The Transformer encoder-decoder performs attention on frame features and decoder queries continuously updating the instance-specific representation of an object's identity and location in each track query embedding. Self-attention over the joint set of both query types allows for the detection of new objects while simultaneously avoiding re-detection of already tracked objects.</p><p>In <ref type="figure" target="#fig_3">Figure 2</ref>, we provide a visual illustration of the track query concept. The initial detections in frame t = 0 spawn new track queries following their corresponding objects to frame t and beyond. To this end, N object ob-     ject queries (white) are decoded to output embeddings for potential track initializations. Each valid object detection {b 0 0 , b 1 0 , . . . } with a classification score above ? object , i.e., output embedding not predicting the background class (crossed), initializes a new track query embedding. Since not all objects in a sequence appear on the first frame, the track identities K t=0 = {0, 1, . . . } only represent a subset of all K. For the decoding step at any frame t &gt; 0, track queries initialize additional output embeddings associated with different identities (colored). The joint set of N object +N track output embeddings is initialized by (learned) object and (temporally adapted) track queries, respectively.</p><formula xml:id="formula_0">/ c G g f H b d V n E o C L R K z W H Y D r I B R A S 1 N N Y N u I g H z g E E n G N / m f u c R p K K x a O p J A j 7 H Q 0 E j S r A 2 0 s C + 7 B M Q G m T e n z U l F i q K J Q c 5 L S 8 a d 4 L E o R E H d s W t u j M 4 q 8 Q r S A U V a A z s 7 3 4 Y k 5 S b r w j D S v U 8 N 9 F + h q W m h I E Z k i p I M B n j I f Q M F Z i D 8 r P Z W V P n 3 C i h Y / Y x T 2 h n p i 5 2 Z J g r N e G B q e R Y j 9 S y l 4 v / e b 1 U R z d + R k W S a h B k P i h K m a N j J 8 / I C a k E o t n E E E w k N b s 6 Z I Q l J i Y P V T Y h e M</formula><formula xml:id="formula_1">L i q 0 D b s E 5 U F W k S 4 B Z 5 8 = " &gt; A A A C F n i c b V D L S s N A F J 3 4 r P U V d e k m W A Q 3 l q Q g u i z o w m W F v q A N Z T K 5 a Y f O T M L M R C i h X + H G X 3 H j Q h G 3 4 s 6 / c d J m U V s P D B z O u X f u v S d I G F X a d X + s t f W N</formula><formula xml:id="formula_2">L i q 0 D b s E 5 U F W k S 4 B Z 5 8 = " &gt; A A A C F n i c b V D L S s N A F J 3 4 r P U V d e k m W A Q 3 l q Q g u i z o w m W F v q A N Z T K 5 a Y f O T M L M R C i h X + H G X 3 H j Q h G 3 4 s 6 / c d J m U V s P D B z O u X f u v S d I G F X a d X + s t f W N</formula><formula xml:id="formula_3">L i q 0 D b s E 5 U F W k S 4 B Z 5 8 = " &gt; A A A C F n i c b V D L S s N A F J 3 4 r P U V d e k m W A Q 3 l q Q g u i z o w m W F v q A N Z T K 5 a Y f O T M L M R C i h X + H G X 3 H j Q h G 3 4 s 6 / c d J m U V s P D B z O u X f u v S d I G F X a d X + s t f W N</formula><p>The Transformer decoder transforms the entire set of output embeddings at once and provides the input for the subsequent MLPs to predict bounding boxes and classes for frame t. The number of track queries N track changes between frames as new objects are detected or tracks removed. Tracks and their corresponding query can be removed either if their classification score drops below ? track or by non-maximum suppression (NMS) with an IoU threshold of ? NMS . A comparatively high ? NMS only removes strongly overlapping duplicate bounding boxes which we found to not be resolvable by the decoder self-attention.</p><p>Track query re-identification. The ability to decode an arbitrary number of track queries allows for an attentionbased short-term re-identification process. We keep decoding previously removed track queries for a maximum number of T track-reid frames. During this patience window, track queries are considered to be inactive and do not contribute to the trajectory until a classification score higher than ? track-reid triggers a re-identification. The spatial information embedded into each track query prevents their application for long-term occlusions with large object movement, but, nevertheless, allows for a short-term recovery from track loss. This is possible without any dedicated re-identification training; and furthermore, cements TrackFormer's holistic approach by relying on the same attention mechanism as for track initialization, identity preservation and trajectory forming even through short-term occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">TrackFormer training</head><p>For track queries to work in interaction with object queries and follow objects to the next frame, TrackFormer requires dedicated frame-to-frame tracking training. As indicated in <ref type="figure" target="#fig_3">Figure 2</ref>, we train on two adjacent frames and optimize the entire MOT objective at once. The loss for frame t measures the set prediction of all output embeddings N = N object + N track with respect to the ground truth objects in terms of class and bounding box prediction.</p><p>The set prediction loss is computed in two steps:</p><p>(i) Object detection on frame t ? 1 with N object object queries (see t = 0 in <ref type="figure" target="#fig_3">Figure 2</ref>).</p><p>(ii) Tracking of objects from (i) and detection of new objects on frame t with all N queries.</p><p>The number of track queries N track depends on the number of successfully detected objects in frame t?1. During training, the MLP predictions? = {? j } N j=1 of the output embeddings from step (iv) are each assigned to one of the ground truth objects y or the background class. Each y i represents a bounding box b i , object class c i and identity k i .</p><p>Bipartite matching. The mapping j = ?(i) from ground truth objects y i to the joint set of object and track query predictions? j is determined either via track identity or costs based on bounding box similarity and object class. For the former, we denote the subset of ground truth track identities at frame t with K t ? K. Each detection from step (i) is assigned to its respective ground truth track identity k from the set K t?1 ? K. The corresponding output embeddings, i.e., track queries, inherently carry over the identity information to the next frame. The two ground truth track identity sets describe a hard assignment of the N track track query outputs to the ground truth objects in frame t:</p><formula xml:id="formula_4">K t ? K t?1 : Match by track identity k. K t?1 \ K t : Match with background class. K t \ K t?1 : Match by minimum cost mapping.</formula><p>The second set of ground truth track identities K t?1 \ K t includes tracks which either have been occluded or left the scene at frame t. The last set K object = K t \ K t?1 of previously not yet tracked ground truth objects remains to be matched with the N object object queries. To achieve this, we follow <ref type="bibr" target="#b6">[7]</ref> and search for the injective minimum cost mapping? in the following assignment problem,</p><formula xml:id="formula_5">? = arg min ? ki?Kobject C match (y i ,? ?(i) ),<label>(1)</label></formula><p>with index ?(i) and pair-wise costs C match between ground truth y i and prediction? i . The problem is solved with a combinatorial optimization algorithm as in <ref type="bibr" target="#b47">[48]</ref>. Given the ground truth class labels c i and predicted class probabilitie? p i (c i ) for output embeddings i, the matching cost C match with class weighting ? cls is defined as</p><formula xml:id="formula_6">C match = ?? clsp?(i) (c i ) + C box (b i ,b ?(i) ).<label>(2)</label></formula><p>The authors of <ref type="bibr" target="#b6">[7]</ref> report better performance without logarithmic class probabilities. The C box term penalizes bounding box differences by a combination of ? 1 distance and generalized intersection over union (IoU) <ref type="bibr" target="#b39">[40]</ref> cost C iou ,</p><formula xml:id="formula_7">C box = ? ?1 ||b i ?b ?(i) || 1 + ? iou C iou (b i ,b ?(i) ),<label>(3)</label></formula><p>with weighting parameters ? ?1 , ? iou , ? ?. In contrast to ? 1 , the scale-invariant IoU term provides similar relative errors for different box sizes. The optimal cost mapping? determines the corresponding assignments in ?(i).</p><p>Set prediction loss. The final MOT set prediction loss is computed over all N = N object + N track output predictions:</p><formula xml:id="formula_8">L MOT (y,?, ?) = N i=1 L query (y,? i , ?).<label>(4)</label></formula><p>The output embeddings which were not matched via track identity or? are not part of the mapping ? and will be assigned to the background class c i = 0. We indicate the ground truth object matched with prediction i by y ?=i and define the loss per query</p><formula xml:id="formula_9">L query = ?? cls logp i (c ?=i ) + L box (b ?=i ,b i ), if i ? ? ?? cls logp i (0), if i / ? ?.</formula><p>The bounding box loss L box is computed in the same fashion as <ref type="formula" target="#formula_7">(3)</ref>, but we differentiate its notation as the cost term C box is generally not required to be differentiable.</p><p>Track augmentations. The two-step loss computation, see (i) and (ii), for training track queries represents only a limited range of possible tracking scenarios. Therefore, we propose the following augmentations to enrich the set of potential track queries during training. These augmentations will be verified in our experiments. We use three types of augmentations similar to <ref type="bibr" target="#b68">[69]</ref> which lead to perturbations of object location and motion, missing detections, and simulated occlusions.</p><p>1. The frame t ? 1 for step (i) is sampled from a range of frames around frame t, thereby generating challenging frame pairs where the objects have moved substantially from their previous position. Such a sampling allows for the simulation of camera motion and low frame rates from usually benevolent sequences.</p><p>2. We sample false negatives with a probability of p FN by removing track queries before proceeding with step (ii). The corresponding ground truth objects in frame t will be matched with object queries and trigger a new object detection. Keeping the ratio of false positives sufficiently high is vital for a joined training of both query types.</p><p>3. To improve the removal of tracks, i.e., by background class assignment, in occlusion scenarios, we complement the set of track queries with additional false positives. These queries are sampled from output embeddings of frame t?1 that were classified as background. Each of the original track queries has a chance of p FP to spawn an additional false positive query. We chose these with a large likelihood of occluding with the respective spawning track query.</p><p>Another common augmentation for improved robustness, is to applying spatial jittering to previous frame bounding boxes or center points <ref type="bibr" target="#b68">[69]</ref>. The nature of track queries, which encode object information implicitly, does not allow for such an explicit perturbation in the spatial domain. We believe our randomization of the temporal range provides a more natural augmentation from video data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present tracking results for TrackFormer on two MOTChallenge benchmarks, namely, MOT17 <ref type="bibr" target="#b29">[30]</ref> and MOTS20 <ref type="bibr" target="#b51">[52]</ref>. Furthermore, we verify individual contributions in an ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">MOT benchmarks and metrics</head><p>Benchmarks. MOT17 <ref type="bibr" target="#b29">[30]</ref> consists of a train and test set, each with 7 sequences and persons annotated with full-body bounding boxes. To evaluate only tracking, the public detection setting provides DPM <ref type="bibr" target="#b15">[16]</ref>, Faster R-CNN <ref type="bibr" target="#b38">[39]</ref> and SDP <ref type="bibr" target="#b60">[61]</ref> detections varying in quality.</p><p>The MOT20 <ref type="bibr" target="#b12">[13]</ref> benchmark follows MOT17 but provides 4 train and 4 test sequences with crowded scenarios.</p><p>MOTS20 <ref type="bibr" target="#b51">[52]</ref> provides mask annotations for 4 train and test sequences of MOT17 but without annotations for small objects. The corresponding bounding boxes are not fullbody, but based on the visible segmentation masks.</p><p>Metrics. Different aspects of MOT are evaluated by a number of individual metrics <ref type="bibr" target="#b4">[5]</ref>. The community focuses on Multiple Object Tracking Accuracy (MOTA) and Identity F1 Score (IDF1) <ref type="bibr" target="#b40">[41]</ref>. While the former focuses on object coverage, the identity preservation is measured by the latter. For MOTS, we report MOTSA which evaluates predictions with a ground truth matching based on mask IoU.</p><p>Public detections. The MOT17 <ref type="bibr" target="#b29">[30]</ref> benchmark is evaluated in a private and public detection setting. The latter allows for a comparison of tracking methods independent of the underlying object detection performance. MOT17 provides three sets of public detections with varying quality. In contrast to classic tracking-by-detection methods, TrackFormer is not able to directly produce tracking outputs from detection inputs. Therefore, we report the results of TrackFormer and CenterTrack <ref type="bibr" target="#b68">[69]</ref> in <ref type="table" target="#tab_8">Table 1</ref> by filtering the initialization of tracks with a minimum IoU requirement. For more implementation details and a discussion on the fairness of such a filtering, we refer to the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>TrackFormer follows the ResNet50 <ref type="bibr" target="#b17">[18]</ref> CNN feature extraction and Transformer encoder-decoder architecture presented in Deformable DETR <ref type="bibr" target="#b70">[71]</ref>. For track queries, the deformable reference points for the current frame are dynamically adjusted to the previous frame bounding box centers. Furthermore, for the decoder we stack the feature maps of the previous and current frame and compute cross-attention with queries over both frames. Queries are able to discriminate between features from the two frames by applying a temporal feature encoding as in <ref type="bibr" target="#b55">[56]</ref>. For more detailed hyperparameters, we refer to the appendix.</p><p>Decoder Queries. By design, TrackFormer can only detect a maximum of N object objects. To detect the maximum number of 52 objects per frame in MOT17 <ref type="bibr" target="#b29">[30]</ref>, we train TrackFormer with N object = 500 learned object queries. For optimal performance, the total number of queries must exceed the number of ground truth objects per frame by a large margin. The number of possible track queries is adaptive and only practically limited by the abilities of the decoder.</p><p>Simulate MOT from single images. The encoderdecoder multi-level attention mechanism requires substantial amounts of training data. Hence, we follow a similar approach as in <ref type="bibr" target="#b68">[69]</ref> and simulate MOT data from the Crowd-Human <ref type="bibr" target="#b44">[45]</ref> person detection dataset. The adjacent training frames t ? 1 and t are generated by applying random spatial augmentations to a single image. To generate challenging tracking scenarios, we randomly resize and crop of up to 20% with respect to the original image size.</p><p>Training procedure. All trainings follow <ref type="bibr" target="#b70">[71]</ref> and apply a batch size of 2 with initial learning rates of 0.0002 and 0.00002 for the encoder-decoder and backbone, respectively. For public detections, we initialize with the model weights from <ref type="bibr" target="#b70">[71]</ref> pretrained on COCO <ref type="bibr" target="#b27">[28]</ref> and then finetune on MOT17 for 50 epochs with a learning rate drop after 10 epochs. The private detections model is trained from scratch for 85 epochs on CrowdHuman <ref type="bibr" target="#b44">[45]</ref> with simulated adjacent frames and we drop the initial learning rates after 50 epochs. To avoid overfitting to the small MOT17 dataset, we then fine-tune for additional 40 epochs on the combined CrowdHuman and MOT17 datasets. The fine-tuning starts with the initial learning rates which are dropped after 10 epochs. By the nature of track queries each sample has a different number of total queries N = N object + N track . In order to stack samples to a batch, we pad the samples with additional false positive queries. The training of the private detections model takes around 2 days on 7 ? 32GB GPUs.</p><p>Mask training. TrackFormer predicts instance-level object masks with a segmentation head as in <ref type="bibr" target="#b6">[7]</ref> by generating spatial attention maps from the encoded image features and decoder output embeddings. Subsequent upscaling and convolution operations yield mask predictions for all output embeddings. We adopt the private detection training pipeline from MOT17 but retrain TrackFormer with the original DETR <ref type="bibr" target="#b6">[7]</ref> attention. This is due to the reduced memory consumption for single scale feature maps and inferior segmentation masks from sparse deformable attention maps. Furthermore, the benefits of deformable attention vanish on MOTS20 as it excludes small objects. After training on MOT17, we freeze the model and only train the segmentation head on all COCO images containing persons. Finally, we fine-tune the entire model on MOTS20.  <ref type="bibr" target="#b65">[66]</ref>, JTA <ref type="bibr" target="#b13">[14]</ref>, M=Market1501 <ref type="bibr" target="#b66">[67]</ref> and C=CUHK03 <ref type="bibr" target="#b26">[27]</ref>. Runtimes (FPS) are self-measured.</p><formula xml:id="formula_10">Method Data FPS ? MOTA ? IDF1 ? MT ? ML ? FP ? FN ? ID</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Benchmark results</head><p>MOT17. Following the training procedure described in Section 4.2, we evaluate TrackFormer on the MOT17 <ref type="bibr" target="#b29">[30]</ref> test set and report results in <ref type="table" target="#tab_8">Table 1</ref>.</p><p>First of all, we isolate the tracking performance and compare results in a public detection setting by applying a track initialization filtering similar to <ref type="bibr" target="#b68">[69]</ref>. However to improve fairness, we filter not by bounding box center distance as in <ref type="bibr" target="#b68">[69]</ref> but a minimum IoU as detailed in the appendix. TrackFormer performs on-par with state-of-the-art results in terms of MOTA without pretraining on Crowd-Human <ref type="bibr" target="#b44">[45]</ref>. Our identity preservation performance is only surpassed by <ref type="bibr" target="#b46">[47]</ref> and offline methods which benefit from the processing of entire sequences at once.</p><p>On private detections, we achieve a new state-of-the-art both in terms of MOTA (+5.0) and IDF1 (1.7) for methods only trained on CrowdHuman <ref type="bibr" target="#b44">[45]</ref>. Only <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref> which follow <ref type="bibr" target="#b65">[66]</ref> and pretrain on 6 additional tracking datasets (6M) surpass our performance. In contrast to our public detection model not only the detection but tracking performance are greatly improved. This is due to the additional tracking data required by Transformers and provided via adjacent frame simulation on CrowdHuman.</p><p>MOT20. On MOT20 <ref type="bibr" target="#b12">[13]</ref>, we introduce the first method only pretrained on CrowdHuman <ref type="bibr" target="#b44">[45]</ref> (CH). However, we surpass or perform on-par with several modern methods <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b67">68]</ref> that were trained on significantly more data, i.e., 6 additional tracking datasets (6M).</p><p>Our tracking-by-attention approach achieves top performance via global attention without relying on additional motion <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref> or appearance models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>. Furthermore, the frame association with track queries avoids postprocessing with heuristic greedy matching procedures <ref type="bibr" target="#b68">[69]</ref> or additional graph optimization <ref type="bibr" target="#b28">[29]</ref>. Our proposed TrackFormer represents the first application of Transformers to the MOT problem and could work as a blueprint for future research. In particular, we expect great potential for methods going beyond our two-frame training/inference. MOTS20. In addition to detection and tracking, Track-Former is able to predict instance-level segmentation masks. As reported in <ref type="table" target="#tab_2">Table 2</ref>, we achieve state-of-the-art object coverage (MOTSA) and identity preservation (IDF1) results for MOTS. All methods are evaluated in a private setting. A MOTS20 <ref type="bibr" target="#b51">[52]</ref> test set submission is only recently possible, hence we also provide the 4-fold cross-validation evaluation established in <ref type="bibr" target="#b51">[52]</ref> reporting the mean best epoch results. TrackFormer surpasses all previous methods without relying on a dedicated mask tracking formulation as in <ref type="bibr" target="#b57">[58]</ref>. In <ref type="figure" target="#fig_6">Figure 3</ref>, we qualitatively compare TrackFormer and Track R-CNN <ref type="bibr" target="#b51">[52]</ref> on two test sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>The ablation study on the MOT17 and MOTS20 training sequences are evaluated in a private detection setting with a 50-50 frame and 4-fold cross-validation split, respectively.</p><p>TrackFormer components. We ablate the impact of different TrackFormer components on the tracking performance in <ref type="table" target="#tab_3">Table 3</ref>. Our full pipeline including pretraining on the CrowdHuman dataset provides a MOTA and IDF1 of 71.3 and 73.4, respectively. The baseline without (w\o) pretraining reduces this by -2.0 and -1.6 points, an effect expected to even more severe for the generalization to test. The attention-based track query re-identification has a negligible effect on MOTA but improves IDF1 by 1.4 points.</p><p>The ablation of false positive (FP) and frame range track augmentations yields another drop of -5.2 MOTA and -11.2  <ref type="table" target="#tab_2">Table 2</ref> can be clearly observed by the difference in pixel mask accuracy.     greedy center distance matching as in <ref type="bibr" target="#b68">[69]</ref> resulting in a huge drop of -3.0 MOTA and -14.1 IDF1. This version represents previous heuristic matching methods and demonstrates the benefit of jointly addressing track initialization and association in a unified TrackFormer formulation.</p><p>Mask information improves tracking. This ablation studies the synergies between segmentation and tracking training. <ref type="table" target="#tab_4">Table 4</ref> only evaluates bounding box tracking performance and shows a +1.2 IDF1 improvement when trained jointly with mask prediction. The additional mask information does not improve track coverage (MOTA) but resolves ambiguous occlusion scenarios during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a unified tracking-by-attention paradigm for detection and multi-object tracking with Transformers. As an example of said paradigm, our endto-end trainable TrackFormer architecture applies autoregressive track query embeddings to follow objects over a sequence. We jointly tackle track initialization, identity and trajectory forming with a Transformer encoder-decoder architecture and not relying on additional matching, graph optimization or motion/appearance modeling. Our approach achieves state-of-the-art results for multi-object tracking as well as segmentation. We hope that this paradigm will foster future work in Transformers for multi-object tracking.</p><p>This section provides additional material for the main paper: ?A contains further implementation details for Track-Former ( ?A.1), a visualization of the Transformer encoderdecoder architecture ( ?A.3), and parameters for multiobject tracking ( ?A.4). ?B contains a discussion related to public detection evaluation ( ?B.1), and detailed persequence results for MOT17 and MOTS20 ( ?B.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details A.1. Backbone and training</head><p>We provide additional hyperparameters for TrackFormer.</p><p>This supports our implementation details reported in Section 4.2 of the main paper. The Deformable DETR <ref type="bibr" target="#b70">[71]</ref> encoder and decoder both apply 6 individual layers with multi-headed self-attention <ref type="bibr" target="#b50">[51]</ref> with 8 attention heads. We do not use the "DC5" (dilated conv 5 ) version of the backbone as this will incur a large memory requirement related to the larger resolution of the last residual stage. We expect that using "DC5" or any other heavier, or higher-resolution, backbone to provide better accuracy and leave this for future work. Furthermore, we also apply the refinement of deformable reference point coined as bounding box refinement in <ref type="bibr" target="#b70">[71]</ref>.</p><p>Our training hyperparameters follow deformable DETR <ref type="bibr" target="#b70">[71]</ref>. The weighting parameters of the cost and their corresponding loss terms are set to ? cls = 2, ? ?1 = 5 and ? iou = 2. The probabilities for the track augmentation at training time are p FN = 0.4 and p FP = 0.1 Furthermore, every MOT17 <ref type="bibr" target="#b29">[30]</ref> frame is jittered by 1% with respect to the original image size similar to the adjacent frame simulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Dataset splits</head><p>All experiments evaluated on dataset splits (ablation studies and MOTS20 training set in <ref type="table" target="#tab_2">Table 2</ref> ) apply the same private training pipeline presented in Section 4.2 to each split. For our ablation on the MOT17 <ref type="bibr" target="#b29">[30]</ref> training set, we separate the 7 sequences into 2 splits and report results from training on the first 50% and evaluating on the last 50% of frames. For MOTS20 we average validation metrics over all splits and report the results from a single epoch (which yields the best mean MOTA / MOTSA) over all splits, i.e., we do not take the best epoch for each individual split. Before training each of the 4 MOTS20 [52] splits, we pre-train the model on all MOT17 sequences excluding the corresponding split of the validation sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Transformer encoder-decoder architecture</head><p>To foster the understanding of TrackFormer's integration of track queries within the decoder self-attention block, we provide a simplified visualization of the encoder-decoder architecture in <ref type="figure" target="#fig_0">Figure A.1</ref>. In comparison to the original illustration in <ref type="bibr" target="#b6">[7]</ref>, we indicate track identities instead of spatial encoding with color-coded queries. The frame features (indicated in grey) are the final output of the CNN feature extractor and have the same number of channels as both query types. The entire Transformer architecture applies N = 6 and M = 6 independently supervised encoder and decoder layers, with feature and object encoding as in <ref type="bibr" target="#b6">[7]</ref>. To improve, tracking consistency we stack the feature maps of the previous and current frame and apply a spatial positional and temporal encoding as in <ref type="bibr" target="#b55">[56]</ref> Track queries are fed autoregressively from the previous frame output embeddings of the last decoding layer (before the final feedforward class and bounding box networks (FFN)). The object encoding is achieved by adding the initial object queries to the key (K) and query (Q) of the corresponding embeddings at each decoder layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Multi-object tracking parameters</head><p>In Section 3.2 , we explain the process of track initialization and removal over a sequence. The corresponding hyperparameters were optimized by a grid search on the MOT17 validation split. The grid search yielded track initialization and removal thresholds of ? detection = 0.4 and ? track = 0.4, respectively. TrackFormer benefits from an NMS operation for the removal of strong occlusion cases with an intersection over union larger than ? NMS = 0.9.</p><p>For the track query re-identification, our search proposed an optimal inactive patience and score of T track-reid = 5 and ? track-reid = 0.4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Public detections and track filtering</head><p>TrackFormer implements a new tracking-by-attention paradigm which requires track initializations to be filtered for an evaluation with public detections. Here, we provide a discussion on the comparability of TrackFormer with earlier methods and different filtering schemes.</p><p>Common tracking-by-detection methods directly process the MOT17 public detections and report their mean tracking performance over all three sets. This is only possible for methods that perform data association on a bounding box level. However, TrackFormer and point-based methods such as CenterTrack <ref type="bibr" target="#b68">[69]</ref> require a procedure for filtering track initializations by public detections in a comparable manner. Unfortunately, MOT17 does not provide a standardized protocol for such a filtering. The authors of Cen-terTrack <ref type="bibr" target="#b68">[69]</ref> filter detections based on bounding box center distances (CD). Each public detection can possibly initialize a single track but only if its center point falls in the bounding box area of the corresponding track.</p><p>In <ref type="table" target="#tab_8">Table A</ref>.1, we revisit our MOT17 test set results but with this public detections center distance (CD) filtering, while also inspecting the CenterTrack per-sequence results in <ref type="table" target="#tab_8">Table A</ref>. <ref type="bibr" target="#b5">6</ref>. We observe that this filtering does not reflect the quality differences in each set of public detections, i.e., DPM <ref type="bibr" target="#b15">[16]</ref> and SDP <ref type="bibr" target="#b60">[61]</ref> results are expected to be the worst and best, respectively, but their difference is small.</p><p>We hypothesize that a center distance filtering is not in accordance with the common public detection setting and propose a filtering based on Intersection over Union (IoU). For IoU filtering, public detections only initialize a track if they have an IoU larger than 0.5. The results in <ref type="table" target="#tab_8">Table A.1</ref> show that for TrackFormer and CenterTrack IoU filtering performs worse compared to the CD filtering which is expected as this is a more challenging evaluation protocol. We believe IoU-based filtering (instead of CD-based) provides a fairer comparison to previous MOT methods which directly process public detections as inputs (IN). This is validated by the per-sequence results in <ref type="table" target="#tab_8">Table A.</ref>3, where IoU filtering shows differences across detectors that are more meaningfully correlated with detector performance, compared to the relatively uniform performance across detections with the CD based method in <ref type="table" target="#tab_8">Table A</ref>.6 (where DPM, FRCNN and SDP show very similar performance).</p><p>Consequently, we follow the IoU-based filtering protocol to compare with CenterTrack in our main paper. While our gain over CenterTrack seems similar across the two filtering techniques for MOTA (see <ref type="table" target="#tab_8">Table A</ref>.1), the gain in IDF1 is significantly larger under the more challenging IoU-based protocol, which suggests that CenterTrack benefits from the less challenging CD-based filtering protocol, while Track-Former does not rely on the filtering for achieving its high IDF1 tracking accuracy.  <ref type="bibr">(IoU)</ref>. We report mean results over the three sets of public detections provided by <ref type="bibr" target="#b29">[30]</ref> and separate between online and offline approaches. The arrows indicate low or high optimal metric values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. MOT17, MOT20 and MOTS20 results</head><p>In Evaluation metrics In Section 4.1 we explained two compound metrics for the evaluation of MOT results, namely, Multi-Object Tracking Accuracy (MOTA) and Identity F1 score (IDF1). However, the MOTChallenge benchmark implements all CLEAR MOT <ref type="bibr" target="#b4">[5]</ref> evaluation metrics. In addition to MOTA and IDF1, we report the following CLEAR MOT metrics:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MT:</head><p>Ground truth tracks covered for at least 80%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML:</head><p>Ground truth tracks covered for at most 20%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FP:</head><p>False positive bounding boxes not corresponding to any ground truth.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>TrackFormer jointly performs object detection and tracking-by-attention with Transformers. Object and autoregressive track queries reason about track initialization, identity, and spatiotemporal trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " b W c A v I s k I F H 6 M / / q 7 B F 2 2 u y 7 w 4 o = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k l B 9 F j o x V O p a D + g D W W z 3 b R L N 5 u w O x F K 6 E / w 4 k E R r / 4 i b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k Q K g 6 7 7 7 W x s b m 3 v 7 B b 2 i v s H h 0 f H p Z P T t o l T z X i L x T L W 3 Y A a L o X i L R Q o e T f R n E a B 5 J 1 g U p / 7 n S e u j Y j V I 0 4 T 7 k d 0 p E Q o G E U r P d Q b j U G p 7 F b c B c g 6 8 X J S h h z N Q e m r P 4 x Z G n G F T F J j e p 6 b o J 9 R j Y J J P i v 2 U 8 M T y i Z 0 x H u W K h p x 4 2 e L U 2 f k 0 i p D E s b a l k K y U H 9 P Z D Q y Z h o F t j O i O D a r 3 l z 8 z + u l G N 7 6 m V B J i l y x 5 a I w l Q R j M v + b D I X m D O X U E s q 0 s L c S N q a a M r T p F G 0 I 3 u r L 6 6 R d r X j X F f e + W q 6 5 e R w F O I c L u A I P b q A G d 9 C E F j A Y w T O 8 w p s j n R f n 3 f l Y t m 4 4 + c w Z / I H z + Q P I S o 1 n &lt; / l a t e x i t &gt; CNN &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b W c A v I s k I F H 6 M / / q 7 B F 2 2 u y 7 w 4 o = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k l B 9 F j o x V O p a D + g D W W z 3 b R L N 5 u w O x F K 6 E / w 4 k E R r / 4 i b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k Q K g 6 7 7 7 W x s b m 3 v 7 B b 2 i v s H h 0 f H p Z P T t o l T z X i L x T L W 3 Y A a L o X i L R Q o e T f R n E a B 5 J 1 g U p / 7 n S e u j Y j V I 0 4 T 7 k d 0 p E Q o G E U r P d Q b j U G p 7 F b c B c g 6 8 X J S h h z N Q e m r P 4 x Z G n G F T F J j e p 6 b o J 9 R j Y J J P i v 2 U 8 M T y i Z 0 x H u W K h p x 4 2 e L U 2 f k 0 i p D E s b a l k K y U H 9 P Z D Q y Z h o F t j O i O D a r 3 l z 8 z + u l G N 7 6 m V B J i l y x 5 a I w l Q R j M v + b D I X m D O X U E s q 0 s L c S N q a a M r T p F G 0 I 3 u r L 6 6 R d r X j X F f e + W q 6 5 e R w F O I c L u A I P b q A G d 9 C E F j A Y w T O 8 w p s j n R f n 3 f l Y t m 4 4 + c w Z / I H z + Q P I S o 1 n &lt; / l a t e x i t &gt; CNN &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b W c A v I s k I F H 6 M / / q 7 B F 2 2 u y 7 w 4 o = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k l B 9 F j o x V O p a D + g D W W z 3 b R L N 5 u w O x F K 6 E / w 4 k E R r / 4 i b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k Q K g 6 7 7 7 W x s b m 3 v 7 B b 2 i v s H h 0 f H p Z P T t o l T z X i L x T L W 3 Y A a L o X i L R Q o e T f R n E a B 5 J 1 g U p / 7 n S e u j Y j V I 0 4 T 7 k d 0 p E Q o G E U r P d Q b j U G p 7 F b c B c g 6 8 X J S h h z N Q e m r P 4 x Z G n G F T F J j e p 6 b o J 9 R j Y J J P i v 2 U 8 M T y i Z 0 x H u W K h p x 4 2 e L U 2 f k 0 i p D E s b a l k K y U H 9 P Z D Q y Z h o F t j O i O D a r 3 l z 8 z + u l G N 7 6 m V B J i l y x 5 a I w l Q R j M v + b D I X m D O X U E s q 0 s L c S N q a a M r T p F G 0 I 3 u r L 6 6 R d r X j X F f e + W q 6 5 e R w F O I c L u A I P b q A G d 9 C E F j A Y w T O 8 w p s j n R f n 3 f l Y t m 4 4 + c w Z / I H z + Q P I S o 1 n &lt; / l a t e x i t &gt; CNN &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V / W H V s l 4 A Y M V K s 6 i h 4 8 A z u g x W + w = " &gt; A A A C F n i c b V D L S s N A F J 3 4 r P U V d e k m W A Q 3 l q Q g u i y I 4 L J C X 9 C G M p n c t E N n J m F m I p T Q r 3 D j r 7 h x o Y h b c e f f O G m z q K 0 H B g 7 n 3 D v 3 3 h M k j C r t u j / W 2 v r G 5 t Z 2 a a e 8 u 7 d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>s n r 5 J 2 r e p d V d 2 H W q X u F n G U 0 C k 6 Q x f I Q 9 e o j u 5 R A 7 U Q Q U / o B b 2 h d + v Z e r U + r M 9 5 6 Z p V 9 J y g P 7 C + f g F m j K C 0 &lt; / l a t e x i t &gt; Transformer Encoder &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V / W H V s l 4 A Y M V K s 6 i h 4 8 A z u g x W + w = " &gt; A A A C F n i c b V D L S s N A F J 3 4 r P U V d e k m W A Q 3 l q Q g u i y I 4 L J C X 9 C G M p n c t E N n J m F m I p T Q r 3 D j r 7 h x o Y h b c e f f O G m z q K 0 H B g 7 n 3 D v 3 3 h M k j C r t u j / W 2 v r G 5 t Z 2 a a e 8 u 7 d / c G g f H b d V n E o C L R K z W H Y D r I B R A S 1 N N Y N u I g H z g E E n G N / m f u c R p K K x a O p J A j 7 H Q 0 E j S r A 2 0 s C + 7 B M Q G m T e n z U l F i q K J Q c 5 L S 8 a d 4 L E o R E H d s W t u j M 4 q 8 Q r S A U V a A z s 7 3 4 Y k 5 S b r w j D S v U 8 N 9 F + h q W m h I E Z k i p I M B n j I f Q M F Z i D 8 r P Z W V P n 3 C i h Y / Y x T 2 h n p i 5 2 Z J g r N e G B q e R Y j 9 S y l 4 v / e b 1 U R z d + R k W S a h B k P i h K m a N j J 8 / I C a k E o t n E E E w k N b s 6 Z I Q l J i Y P V T Y h e M s n r 5 J 2 r e p d V d 2 H W q X u F n G U 0 C k 6 Q x f I Q 9 e o j u 5 R A 7 U Q Q U / o B b 2 h d + v Z e r U + r M 9 5 6 Z p V 9 J y g P 7 C + f g F m j K C 0 &lt; / l a t e x i t &gt; Transformer Encoder &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V / W H V s l 4 A Y M V K s 6 i h 4 8 A z u g x W + w = " &gt; A A A C F n i c b V D L S s N A F J 3 4 r P U V d e k m W A Q 3 l q Q g u i y I 4 L J C X 9 C G M p n c t E N n J m F m I p T Q r 3 D j r 7 h x o Y h b c e f f O G m z q K 0 H B g 7 n 3 D v 3 3 h M k j C r t u j / W 2 v r G 5 t Z 2 a a e 8 u 7 d / c G g f H b d V n E o C L R K z W H Y D r I B R A S 1 N N Y N u I g H z g E E n G N / m f u c R p K K x a O p J A j 7 H Q 0 E j S r A 2 0 s C + 7 B M Q G m T e n z U l F i q K J Q c 5 L S 8 a d 4 L E o R E H d s W t u j M 4 q 8 Q r S A U V a A z s 7 3 4 Y k 5 S b r w j D S v U 8 N 9 F + h q W m h I E Z k i p I M B n j I f Q M F Z i D 8 r P Z W V P n 3 C i h Y / Y x T 2 h n p i 5 2 Z J g r N e G B q e R Y j 9 S y l 4 v / e b 1 U R z d + R k W S a h B k P i h K m a N j J 8 / I C a k E o t n E E E w k N b s 6 Z I Q l J i Y P V T Y h e M s n r 5 J 2 r e p d V d 2 H W q X u F n G U 0 C k 6 Q x f I Q 9 e o j u 5 R A 7 U Q Q U / o B b 2 h d + v Z e r U + r M 9 5 6 Z p V 9 J y g P 7 C + f g F m j K C 0 &lt; / l a t e x i t &gt; Transformer Encoder &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 H S o 0 U S 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>z a 3 t 0 k 5 5 d 2 /</head><label>2</label><figDesc>/ 4 N A + O m 6 r O J U E W i R m s e w G W A G j A l q a a g b d R A L m A Y N O M L 7 N / c 4 j S E V j 0 d S T B H y O h 4 J G l G B t p I F 9 2 S c g N M i 8 P 2 t K L F Q U S w 5 y W l 4 0 7 o D E o R E H d s W t u j M 4 q 8 Q r S A U V a A z s 7 3 4 Y k 5 S b r w j D S v U 8 N 9 F + h q W m h I E Z k i p I M B n j I f Q M F Z i D 8 r P Z W V P n 3 C i h Y / Y x T 2 h n p i 5 2 Z J g r N e G B q e R Y j 9 S y l 4 v / e b 1 U R z d + R k W S a h B k P i h K m a N j J 8 / I C a k E o t n E E E w k N b s 6 Z I Q l J i Y P V T Y h e M s n r 5 J 2 r e p d V d 2 H W q X u F n G U 0 C k 6 Q x f I Q 9 e o j u 5 R A 7 U Q Q U / o B b 2 h d + v Z e r U + r M 9 5 6 Z p V 9 J y g P 7 C + f g F X J 6 C q &lt; / l a t e x i t &gt; Transformer Decoder &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 H S o 0 U S 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>z a 3 t 0 k 5 5 d 2 / / 4 N A + O m 6 r O J U E W i R m s e w G W A G j A l q a a g b d R A L m A Y N O M L 7 N / c 4 j S E V j 0 d S T B H y O h 4 J G l G B t p I F 9 2 S c g N M i 8 P 2 t K L F Q U S w 5 y W l 4 0 7 o D E o R E H d s W t u j M 4 q 8 Q r S A U V a A z s 7 3 4 Y k 5 S b r w j D S v U 8 N 9 F + h q W m h I E Z k i p I M B n j I f Q M F Z i D 8 r P Z W V P n 3 C i h Y / Y x T 2 h n p i 5 2 Z J g r N e G B q e R Y j 9 S y l 4 v / e b 1 U R z d + R k W S a h B k P i h K m a N j J 8 / I C a k E o t n E E E w k N b s 6 Z I Q l J i Y P V T Y h e M s n r 5 J 2 r e p d V d 2 H W q X u F n G U 0 C k 6 Q x f I Q 9 e o j u 5 R A 7 U Q Q U / o B b 2 h d + v Z e r U + r M 9 5 6 Z p V 9 J y g P 7 C + f g F X J 6 C q &lt; / l a t e x i t &gt; Transformer Decoder &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 H S o 0 U S 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 .</head><label>2</label><figDesc>z a 3 t 0 k 5 5 d 2 / / 4 N A + O m 6 r O J U E W i R m s e w G W A G j A l q a a g b d R A L m A Y N O M L 7 N / c 4 j S E V j 0 d S T B H y O h 4 J G l G B t p I F 9 2 S c g N M i 8 P 2 t K L F Q U S w 5 y W l 4 0 7 o D E o R E H d s W t u j M 4 q 8 Q r S A U V a A z s 7 3 4 Y k 5 S b r w j D S v U 8 N 9 F + h q W m h I E Z k i p I M B n j I f Q M F Z i D 8 r P Z W V P n 3 C i h Y / Y x T 2 h n p i 5 2 Z J g r N e G B q e R Y j 9 S y l 4 v / e b 1 U R z d + R k W S a h B k P i h K m a N j J 8 / I C a k E o t n E E E w k N b s 6 Z I Q l J i Y P V T Y h e M s n r 5 J 2 r e p d V d 2 H W q X u F n G U 0 C k 6 Q x f I Q 9 e o j u 5 R A 7 U Q Q U / o B b 2 h d + v Z e r U + r M 9 5 6 Z p V 9 J y g P 7 C + f g F X J 6 C q &lt; / l a t e x i t &gt;TransformerDecoder TrackFormer casts multi-object tracking as a set prediction problem performing joint detection and tracking-by-attention. The architecture consists of a CNN for image feature extraction, a Transformer<ref type="bibr" target="#b50">[51]</ref> encoder for image feature encoding and a Transformer decoder which applies self-and encoder-decoder attention to produce output embeddings with bounding box and class information. At frame t = 0, the decoder transforms Nobject object queries (white) to output embeddings either initializing new autoregressive track queries or predicting the background class (crossed). On subsequent frames, the decoder processes the joint set of Nobject + Ntrack queries to follow or remove (blue) existing tracks as well as initialize new tracks (purple).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>We compare TrackFormer segmentation results with the popular Track R-CNN [52] on selected MOTS20 [52] test sequences. The superiority of TrackFormer in terms of MOTSA in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>--w\o -----------------Pretraining on CrowdHuman 69.3 -2.0 71.8 -1.6 Track query re-identification 69.2 -0.1 70.4 -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure A. 1 .</head><label>1</label><figDesc>The TrackFormer encoder-decoder architecture. We indicate the tensor dimensions in squared brackets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>MethodTbD sMOTSA ? IDF1 ? FP ? FN ? ID Sw. ?</figDesc><table><row><cell cols="5">Train set (4-fold cross-validation)</cell><cell></cell><cell></cell></row><row><cell>MHT DAM [23]</cell><cell>?</cell><cell>48.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FWT [19]</cell><cell>?</cell><cell>49.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MOTDT [8]</cell><cell>?</cell><cell>47.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>jCC [22]</cell><cell>?</cell><cell>48.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TrackRCNN [52]</cell><cell></cell><cell>52.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MOTSNet [38]</cell><cell></cell><cell>56.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PointTrack [58]</cell><cell></cell><cell>58.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TrackFormer</cell><cell></cell><cell>58.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Test set</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Track R-CNN [52]</cell><cell></cell><cell>40.6</cell><cell cols="4">42.4 1261 12641 567</cell></row><row><cell>TrackFormer</cell><cell></cell><cell>54.9</cell><cell cols="4">63.6 2233 7195 278</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of multi-object tracking and segmentation methods evaluated on the MOTS20<ref type="bibr" target="#b51">[52]</ref> train and test sets.</figDesc><table><row><cell>We</cell></row></table><note>Method MOTA ? ? IDF1 ? ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on TrackFormer components.</figDesc><table><row><cell>We report</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>We demonstrate the effect of jointly training for tracking and segmentation on a 4-fold split on the MOTS20 [52] train set. We evaluate with regular MOT metrics, i.e., matching to ground truth with bounding boxes instead of masks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>IN IoU CD MOTA ? IDF1 ?Table A.1. Comparison of modern multi-object tracking methods evaluated on the MOT17 [30] test set for different public detection processing. Public detections are either directly processed as input (IN) or applied for filtering of track initializations by center distance (CD) or intersection over union</figDesc><table><row><cell></cell><cell></cell><cell>Offline</cell><cell></cell><cell></cell></row><row><cell>MHT DAM [23] jCC [22] FWT [19] eHAF [46] TT [65] MPNTrack [6] Lif T [20]</cell><cell>? ? ? ? ? ? ?</cell><cell></cell><cell></cell><cell>50.7 51.2 51.3 51.8 54.9 58.8 60.5</cell><cell>47.2 54.5 47.6 54.7 63.1 61.7 65.6</cell></row><row><cell></cell><cell></cell><cell>Online</cell><cell></cell><cell></cell></row><row><cell cols="2">MOTDT [8] FAMNet [10] Tracktor++ [4] GSM Tracktor [29] ? ? ? ? TMOH [47] ? CenterTrack [69] TrackFormer CenterTrack [69] TrackFormer</cell><cell>? ?</cell><cell>? ?</cell><cell>50.9 52.0 56.3 56.4 62.1 60.5 62.3 61.5 63.4</cell><cell>52.7 48.7 55.1 57.8 62.8 55.7 57.6 59.6 60.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table A.2 and and Table A.3, we provide per-sequence MOT17 [30] test set results for private and public detection filtering via Intersection over Union (IoU), respectively. For per-sequence results on MOT20 , we refer to Table A.4. Furthermore, we present per-sequence Track-Former results on the MOTS20 [52] test set in Table A.5.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Public detection MOTA ? IDF1 ? MT ? ML ? FP ? FN ? ID Sw. ?Table A.2. We report private TrackFormer results on each individual sequence evaluated on the MOT17 [30] test set. To follow the official MOT17 format, we display the same results per public detection set. The arrows indicate low or high optimal metric values. Sequence Public detection MOTA ? IDF1 ? MT ? ML ? FP ? FN ? ID Sw. ? Table A.3. We report TrackFormer results on each individual sequence and set of public detections evaluated on the MOT17 [30] test set. We apply our minimum Intersection over Union (IoU) public detection filtering. The arrows indicate low or high optimal metric values. Sequence MOTA ? IDF1 ? MT ? ML ? FP ? FN ? ID Sw. ? Table A.4. We report private TrackFormer results on each individual sequence evaluated on the MOT20 [13] test set. The arrows indicate low or high optimal metric values. Sequence sMOTSA ? IDF1 ? MOTSA ? FP ? FN ? ID Sw. ?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FN:</cell><cell></cell><cell cols="3">False negative ground truth boxes not cov-</cell></row><row><cell cols="3">MOT17-01 DPM [16]</cell><cell>57.9</cell><cell>49.7</cell><cell>11</cell><cell>4</cell><cell cols="3">ered by any bounding box. 477 2191 45</cell></row><row><cell cols="3">MOT17-03 DPM MOT20-04 MOT17-06 DPM MOT17-07 DPM MOT20-06 MOT17-08 DPM MOT20-07 MOT17-12 DPM MOT20-08 MOT17-01 FRCNN [39] 82.7 55.9 56.2 46.0 MOT17-14 DPM ALL 68.6</cell><cell>88.6 75.6 59.8 65.5 53.5 54.5 59.0 51.8 48.3 57.9 47.4 65.7</cell><cell>79.6 490 60.8 49.5 96 42.5 41 63.0 39 49.7 54.9 666</cell><cell cols="5">ID Sw.: 124 28 sMOTSA: Mask-based Multi-Object Tracking Accu-3 2469 9365 122 9639 37165 566 Bounding boxes switching the correspond-ing ground truth identity. 104 27 1791 2775 173 24 5 1030 4671 72 5582 52440 545 118 24 9 1461 7861 20 547 13856 92 279 43 14 1880 2258 61 4580 36912 329 42 racy (MOTA) which counts true positives in-11 4 477 2191 45 stead of only masks with IoU larger than 0.5. 41 20 2426 7138 164 181 20348 140373 1532</cell></row><row><cell cols="3">MOT17-03 FRCNN</cell><cell>88.6</cell><cell>79.6</cell><cell>124</cell><cell>3</cell><cell></cell><cell>2469</cell><cell>9365</cell><cell>122</cell></row><row><cell cols="3">MOT17-06 FRCNN</cell><cell>59.8</cell><cell>60.8</cell><cell>104</cell><cell>27</cell><cell></cell><cell>1791</cell><cell>2775</cell><cell>173</cell></row><row><cell cols="3">MOT17-07 FRCNN</cell><cell>65.5</cell><cell>49.5</cell><cell>24</cell><cell>5</cell><cell></cell><cell>1030</cell><cell>4671</cell><cell>118</cell></row><row><cell cols="3">MOT17-08 FRCNN</cell><cell>54.5</cell><cell>42.5</cell><cell>24</cell><cell>9</cell><cell></cell><cell>1461</cell><cell>7861</cell><cell>279</cell></row><row><cell cols="3">MOT17-12 FRCNN</cell><cell>51.8</cell><cell>63.0</cell><cell>43</cell><cell>14</cell><cell></cell><cell>1880</cell><cell>2258</cell><cell>42</cell></row><row><cell cols="3">MOT17-14 FRCNN</cell><cell>47.4</cell><cell>54.9</cell><cell>41</cell><cell>20</cell><cell></cell><cell>2426</cell><cell>7138</cell><cell>164</cell></row><row><cell cols="3">MOT17-01 SDP [61]</cell><cell>57.9</cell><cell>49.7</cell><cell>11</cell><cell>4</cell><cell></cell><cell>477</cell><cell>2191</cell><cell>45</cell></row><row><cell cols="2">MOT17-03 SDP</cell><cell>MOTS20-01</cell><cell>88.6 59.8</cell><cell>79.6 68.0</cell><cell>124 79.6</cell><cell>3 255 364</cell><cell></cell><cell>2469 16</cell><cell>9365</cell><cell>122</cell></row><row><cell cols="2">MOT17-06 SDP</cell><cell>MOTS20-06</cell><cell>59.8 63.9</cell><cell>60.8 65.1</cell><cell>104 78.7</cell><cell cols="3">27 595 1335 158 1791</cell><cell>2775</cell><cell>173</cell></row><row><cell cols="2">MOT17-07 SDP MOT17-08 SDP</cell><cell>MOTS20-07 MOTS20-12</cell><cell>65.5 43.2 54.5 62.0</cell><cell>49.5 53.6 42.5 76.8</cell><cell>24 58.5 24 74.6</cell><cell cols="2">5 834 4433 9 549 1063</cell><cell>1030 75 1461 29</cell><cell>4671 7861</cell><cell>118 279</cell></row><row><cell cols="2">MOT17-12 SDP</cell><cell>ALL</cell><cell>51.8 54.9</cell><cell>63.0 63.6</cell><cell>43 69.9</cell><cell cols="3">14 2233 7195 278 1880</cell><cell>2258</cell><cell>42</cell></row><row><cell cols="2">MOT17-14 SDP</cell><cell></cell><cell>47.4</cell><cell>54.9</cell><cell>41</cell><cell>20</cell><cell></cell><cell>2426</cell><cell>7138</cell><cell>164</cell></row><row><cell>All</cell><cell>All</cell><cell></cell><cell>74.1</cell><cell>68.0</cell><cell>1113</cell><cell cols="4">246 34602 108777</cell><cell>2829</cell></row><row><cell cols="3">MOT17-01 DPM [16]</cell><cell>49.9</cell><cell>43.0</cell><cell>5</cell><cell>8</cell><cell></cell><cell>258</cell><cell>2932</cell><cell>40</cell></row><row><cell cols="2">MOT17-03 DPM</cell><cell></cell><cell>74.0</cell><cell>66.5</cell><cell>85</cell><cell>18</cell><cell></cell><cell>1389</cell><cell>25396</cell><cell>374</cell></row><row><cell cols="2">MOT17-06 DPM</cell><cell></cell><cell>53.6</cell><cell>51.8</cell><cell>63</cell><cell>75</cell><cell></cell><cell>711</cell><cell>4575</cell><cell>180</cell></row><row><cell cols="2">MOT17-07 DPM</cell><cell></cell><cell>52.6</cell><cell>48.1</cell><cell>12</cell><cell>16</cell><cell></cell><cell>258</cell><cell>7663</cell><cell>88</cell></row><row><cell cols="2">MOT17-08 DPM</cell><cell></cell><cell>32.5</cell><cell>31.9</cell><cell>10</cell><cell>32</cell><cell></cell><cell>288</cell><cell>13838</cell><cell>128</cell></row><row><cell cols="2">MOT17-12 DPM</cell><cell></cell><cell>51.3</cell><cell>57.7</cell><cell>21</cell><cell>31</cell><cell></cell><cell>606</cell><cell>3565</cell><cell>53</cell></row><row><cell cols="2">MOT17-14 DPM</cell><cell></cell><cell>38.1</cell><cell>42.0</cell><cell>15</cell><cell>63</cell><cell></cell><cell>627</cell><cell>10505</cell><cell>314</cell></row><row><cell cols="3">MOT17-01 FRCNN [39]</cell><cell>50.9</cell><cell>42.3</cell><cell>8</cell><cell>6</cell><cell></cell><cell>308</cell><cell>2813</cell><cell>48</cell></row><row><cell cols="3">MOT17-03 FRCNN</cell><cell>75.3</cell><cell>67.0</cell><cell>84</cell><cell>16</cell><cell></cell><cell>1434</cell><cell>24040</cell><cell>335</cell></row><row><cell cols="3">MOT17-06 FRCNN</cell><cell>57.2</cell><cell>54.8</cell><cell>73</cell><cell>48</cell><cell></cell><cell>960</cell><cell>3856</cell><cell>226</cell></row><row><cell cols="3">MOT17-07 FRCNN</cell><cell>52.4</cell><cell>47.9</cell><cell>12</cell><cell>11</cell><cell></cell><cell>499</cell><cell>7437</cell><cell>106</cell></row><row><cell cols="3">MOT17-08 FRCNN</cell><cell>31.1</cell><cell>31.7</cell><cell>10</cell><cell>36</cell><cell></cell><cell>285</cell><cell>14166</cell><cell>102</cell></row><row><cell cols="3">MOT17-12 FRCNN</cell><cell>47.7</cell><cell>56.7</cell><cell>19</cell><cell>32</cell><cell></cell><cell>702</cell><cell>3785</cell><cell>45</cell></row><row><cell cols="3">MOT17-14 FRCNN</cell><cell>37.8</cell><cell>41.8</cell><cell>17</cell><cell>56</cell><cell></cell><cell>1300</cell><cell>9795</cell><cell>406</cell></row><row><cell cols="3">MOT17-01 SDP [61]</cell><cell>53.7</cell><cell>45.3</cell><cell>10</cell><cell>5</cell><cell></cell><cell>556</cell><cell>2386</cell><cell>47</cell></row><row><cell cols="2">MOT17-03 SDP</cell><cell></cell><cell>79.6</cell><cell>65.8</cell><cell>95</cell><cell>13</cell><cell></cell><cell>2134</cell><cell>18632</cell><cell>545</cell></row><row><cell cols="2">MOT17-06 SDP</cell><cell></cell><cell>56.4</cell><cell>54.0</cell><cell>82</cell><cell>57</cell><cell></cell><cell>1017</cell><cell>3889</cell><cell>228</cell></row><row><cell cols="2">MOT17-07 SDP</cell><cell></cell><cell>54.6</cell><cell>47.8</cell><cell>16</cell><cell>11</cell><cell></cell><cell>590</cell><cell>6965</cell><cell>121</cell></row><row><cell cols="2">MOT17-08 SDP</cell><cell></cell><cell>35.0</cell><cell>33.0</cell><cell>12</cell><cell>27</cell><cell></cell><cell>443</cell><cell>13152</cell><cell>144</cell></row><row><cell cols="2">MOT17-12 SDP</cell><cell></cell><cell>48.9</cell><cell>57.5</cell><cell>22</cell><cell>28</cell><cell></cell><cell>850</cell><cell>3527</cell><cell>54</cell></row><row><cell cols="2">MOT17-14 SDP</cell><cell></cell><cell>40.4</cell><cell>42.4</cell><cell>17</cell><cell>49</cell><cell></cell><cell>1376</cell><cell>9206</cell><cell>434</cell></row><row><cell>ALL</cell><cell>ALL</cell><cell></cell><cell>62.3</cell><cell>57.6</cell><cell>688</cell><cell cols="4">638 16591 192123</cell><cell>4018</cell></row></table><note>Sequence</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A .</head><label>A</label><figDesc>5. We present TrackFormer tracking and segmentation results on each individual sequence of the MOTS20 [52] test set. MOTS20 is evaluated in a private detections setting. The arrows indicate low or high optimal metric values.SequencePublicdetection MOTA ? IDF1 ? MT ? ML ? FP ? FN ? ID Sw. ?Table A.6. We report the original per-sequence CenterTrack [69] MOT17 [30] test set results with Center Distance (CD) public detection filtering. The results do not reflect the varying object detection performance of DPM, FRCNN and SDP, respectively. The arrows indicate low or high optimal metric values.</figDesc><table><row><cell cols="2">MOT17-01 DPM [16]</cell><cell>41.6</cell><cell>44.2</cell><cell>5</cell><cell>8</cell><cell>496</cell><cell>3252</cell><cell>22</cell></row><row><cell cols="2">MOT17-03 DPM</cell><cell>79.3</cell><cell>71.6</cell><cell>94</cell><cell>8</cell><cell>1142</cell><cell>20297</cell><cell>191</cell></row><row><cell cols="2">MOT17-06 DPM</cell><cell>54.8</cell><cell>42.0</cell><cell>54</cell><cell>63</cell><cell>314</cell><cell>4839</cell><cell>175</cell></row><row><cell cols="2">MOT17-07 DPM</cell><cell>44.8</cell><cell>42.0</cell><cell>11</cell><cell>16</cell><cell>1322</cell><cell>7851</cell><cell>147</cell></row><row><cell cols="2">MOT17-08 DPM</cell><cell>26.5</cell><cell>32.2</cell><cell>11</cell><cell>37</cell><cell>378</cell><cell>15066</cell><cell>88</cell></row><row><cell cols="2">MOT17-12 DPM</cell><cell>46.1</cell><cell>53.1</cell><cell>16</cell><cell>45</cell><cell>207</cell><cell>4434</cell><cell>30</cell></row><row><cell cols="2">MOT17-14 DPM</cell><cell>31.6</cell><cell>36.6</cell><cell>13</cell><cell>78</cell><cell>636</cell><cell>11812</cell><cell>196</cell></row><row><cell cols="2">MOT17-01 FRCNN [39]</cell><cell>41.0</cell><cell>42.1</cell><cell>6</cell><cell>9</cell><cell>571</cell><cell>3207</cell><cell>25</cell></row><row><cell cols="2">MOT17-03 FRCNN</cell><cell>79.6</cell><cell>72.7</cell><cell>93</cell><cell>7</cell><cell>1234</cell><cell>19945</cell><cell>180</cell></row><row><cell cols="2">MOT17-06 FRCNN</cell><cell>55.6</cell><cell>42.9</cell><cell>57</cell><cell>59</cell><cell>363</cell><cell>4676</cell><cell>190</cell></row><row><cell cols="2">MOT17-07 FRCNN</cell><cell>45.5</cell><cell>41.5</cell><cell>13</cell><cell>15</cell><cell>1263</cell><cell>7785</cell><cell>156</cell></row><row><cell cols="2">MOT17-08 FRCNN</cell><cell>26.5</cell><cell>31.9</cell><cell>11</cell><cell>36</cell><cell>332</cell><cell>15113</cell><cell>89</cell></row><row><cell cols="2">MOT17-12 FRCNN</cell><cell>46.1</cell><cell>52.6</cell><cell>15</cell><cell>45</cell><cell>197</cell><cell>4443</cell><cell>30</cell></row><row><cell cols="2">MOT17-14 FRCNN</cell><cell>31.6</cell><cell>37.6</cell><cell>13</cell><cell>77</cell><cell>780</cell><cell>11653</cell><cell>202</cell></row><row><cell cols="2">MOT17-01 SDP [61]</cell><cell>41.8</cell><cell>44.3</cell><cell>7</cell><cell>8</cell><cell>612</cell><cell>3112</cell><cell>27</cell></row><row><cell cols="2">MOT17-03 SDP</cell><cell>80.0</cell><cell>72.0</cell><cell>93</cell><cell>8</cell><cell>1223</cell><cell>19530</cell><cell>181</cell></row><row><cell cols="2">MOT17-06 SDP</cell><cell>55.5</cell><cell>43.8</cell><cell>56</cell><cell>61</cell><cell>354</cell><cell>4712</cell><cell>181</cell></row><row><cell cols="2">MOT17-07 SDP</cell><cell>45.2</cell><cell>42.4</cell><cell>13</cell><cell>15</cell><cell>1332</cell><cell>7775</cell><cell>147</cell></row><row><cell cols="2">MOT17-08 SDP</cell><cell>26.6</cell><cell>32.3</cell><cell>11</cell><cell>36</cell><cell>350</cell><cell>15067</cell><cell>91</cell></row><row><cell cols="2">MOT17-12 SDP</cell><cell>46.0</cell><cell>53.0</cell><cell>16</cell><cell>45</cell><cell>221</cell><cell>4426</cell><cell>30</cell></row><row><cell cols="2">MOT17-14 SDP</cell><cell>31.7</cell><cell>37.1</cell><cell>13</cell><cell>76</cell><cell>749</cell><cell>11677</cell><cell>205</cell></row><row><cell>All</cell><cell>All</cell><cell>61.5</cell><cell>59.6</cell><cell>621</cell><cell cols="3">752 14076 200672</cell><cell>2583</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>We are grateful for discussions with Jitendra Malik, Karttikeya Mangalam, and David Novotny.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-target tracking by continuous energy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Andriyenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Engin Turetken, and Pascal Fua. Multiple object tracking using k-shortest paths optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keni</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a neural solver for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Bras?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time multiple people tracking with deeply learned candidate selection and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiple target tracking in world coordinate with single, minimally calibrated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online multi-object tracking using cnn-based single object tracker with spatial-temporal attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4836" to="4845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Motchallenge: A benchmark for single-camera multiple target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MOT20: A benchmark for multi object tracking in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to detect and track visible and occluded body joints in a virtual world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Lanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Palazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improvements to frank-wolfe optimization for multi-detector multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lifted disjoint paths with application in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Hornakova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Swoboda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A linear programming approach for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidney</forename><forename type="middle">S</forename><surname>Fels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Motion segmentation &amp; multiple object tracking by correlation co-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arridhana</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning by tracking: siamese cnn for robust target association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning an image-based motion context for multiple people tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Fenzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Everybody needs somebody: Modeling social and grouping behavior on a linear programming multiple people tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis. Workshops</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<title level="m">Microsoft coco: Common objects in context</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gsm: Graph similarity model for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiankun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Joint Conf. Art. Int</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">Mot16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Track, then decide: Category-agnostic visionbased multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljo?a</forename><surname>O?ep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Mehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Rob. Aut</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tubetk: Adopting tubes to track multi-object in a one-step training model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Quasi-dense similarity learning for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Image transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">You&apos;ll never walk alone: modeling social behavior for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Globally-optimal greedy algorithms for tracking a variable number of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning multiobject tracking and segmentation from automatic annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hofinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idoia</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">S</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Features for multi-target multi-camera tracking and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning social etiquette: Human trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning pedestrian dynamics from the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Heterogeneous association graph fusion for target association in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving multiple pedestrian tracking by track management and occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Stadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Beyerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to track with object permanence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis., 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mots: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berin</forename><surname>Balachandar Gnana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multiple object tracking with correlation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
		<idno>2021. 7</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A general recurrent tracking framework without real data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis., 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Joint object detection and multi-object tracking with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Rob. Aut</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Endto-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Track to detect and segment: An online multi-object tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Segment as points for efficient online multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liusheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">Who are you with and where are you going? IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Remot: A model-agnostic refinement for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
		<idno>106:104091, 2021. 7</idno>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2129" to="2137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multiple target tracking using spatio-temporal markov chain monte carlo data association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Global data association for multi-object tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Long-term tracking with deep tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Fairmot: On the fairness of detection and re-identification in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Improving multiple object tracking with single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2453" to="2462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with dual matching attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
