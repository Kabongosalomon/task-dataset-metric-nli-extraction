<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Alignment and Restoration for Domain Generalization and Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
							<email>wezeng@microsoft.comchenzhibo@ustc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Alignment and Restoration for Domain Generalization and Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For domain generalization (DG) and unsupervised domain adaptation (UDA), cross domain feature alignment has been widely explored to pull the feature distributions of different domains in order to learn domain-invariant representations. However, the feature alignment is in general task-ignorant and could result in degradation of the discrimination power of the feature representation and thus hinders the high performance. In this paper, we propose a unified framework termed Feature Alignment and Restoration (FAR) to simultaneously ensure high generalization and discrimination power of the networks for effective DG and UDA. Specifically, we perform feature alignment (FA) across domains by aligning the moments of the distributions of attentively selected features to reduce their discrepancy. To ensure high discrimination, we propose a Feature Restoration (FR) operation to distill task-relevant features from the residual information and use them to compensate for the aligned features. For better disentanglement, we enforce a dual ranking entropy loss constraint in the FR step to encourage the separation of task-relevant and taskirrelevant features. Extensive experiments on multiple classification benchmarks demonstrate the high performance and strong generalization of our FAR framework for both domain generalization and unsupervised domain adaptation. Align Common feature: Shape horse or sheep? Source-1:Painting Source-2:Real Source-3:Sketch Align Common feature: Shape Figure 1: Motivation: only depending on the aligned feature seems not enough for clear classification.</p><p>aligning second order correlation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, moment matching <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b13">14]</ref>, adversarial domain confusion <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12]</ref>, and GAN-based alignment <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13]</ref>. They intend to drive the DNNs to learn domain-invariant features for improving its generalization and transferability <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>However, the aligning in general overlooks a side effect, i.e., the loss of task-relevant discriminative information. The data of different domains has some domain-shared characteristics and also some domain-specific characteristics. Aligning the features is to obtain a shared sub-space for all these domains, where the domain-specific information tends to be excluded to assure the alignment.</p><p>Aligning the features across domains through some constraints drives the features to lie in a shared space. However, since the purpose of alignment is not task-specific, it could lose some information, including task-relevant information. As illustrated by the toy example in <ref type="figure">Figure 1</ref>, the common space after domain alignment of the three source domains (including painting, real, sketch) could be that related with shape. However, besides the shape, the texture information is also discriminative to distinguish between the 'horse' and 'sheep' for the domain of real and painting. Among the previous works, the restoration of discriminative information is overlooked even though important.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNNs) have advanced the state-of-the-art for a wide variety of machinelearning problems and applications. The trained models typically perform well when the test data follows a similar distribution as the training data <ref type="bibr" target="#b0">[1]</ref>, but suffer from significant performance degradation (poor generalization capability) when testing on a previously unseen data distribution <ref type="bibr" target="#b1">[2]</ref>. The data may be captured in different environments or obtained by different manners (e.g., photo, sketch, painting), which exhibit domain shifts. To address such domain shift problem, two related fields have been studied intensively: domain generalization (DG) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> and unsupervised domain adaptation (UDA) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. DG and UDA both aim to bridge the source and target domains by learning domain-invariant feature representations, where DG can only exploit source domain data while UDA can also exploit the unlabeled training data of the target domain for training/fine-tuning.</p><p>Feature regularization based methods have been widely investigated to mitigate the domain gap. Several methods incorporate the distribution distance metric, such as Maximum Mean Discrepancy (MMD), into model as loss to diminish the domain discrepancy for DG and UDA <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Some other methods introduce different learning schemes to align the source and target domains, including</p><p>In this paper, we propose a novel unified framework termed Feature Alignment and Restoration (FAR) to perform feature aligning and restoration to simultaneously enhance cross domain generalization and preserve feature discrimination for effective DG and UDA classification. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the pipeline. It consists of four parts in order: feature extraction, feature alignment (FA), feature restoration (FR), and classification. In the FA phase, similar to <ref type="bibr" target="#b13">[14]</ref>, we align the feature distributions across different domains by matching the moments of the multiple distributions. Different from <ref type="bibr" target="#b13">[14]</ref>, we enforce the alignment on the adaptively selected features rather than on the original features in order to reserve the comprehensive original information and find a subspace (selected features) that is suitable for alignment. Since the alignment is task-ignorant, the aligned features lose some discriminative information. We propose a Feature Restoration (FR) to further distill task-relevant feature from the residual to compensate for the aligned feature. Moreover, within FR, for better disentanglement, we enforce a dual ranking entropy loss constraint to encourage the separation of task-relevant and task-irrelevant features. Finally, a domain-shared classifier is used to classify the restored features, trained with the guidance of an expert of each domain. We validate the effectiveness of the FAR framework on multiple widely-used benchmarks for both the DG and UDA settings. FAR significantly outperforms the state-of-the-art DG and UDA approaches. We summarize our main contributions as follows:</p><p>? We introduce the concept of feature alignment and restoration (FAR) to the cross-domain classification task for effective DG or UDA, by ensuring high generalization and discrimination power of the learned feature within a unified framework. ? In the FR phase, we introduce a dual ranking entropy loss constraint to facilitate the restoration of the task-relevant information and better disentanglement. ? To keep the simplicity, as a minor insight, we avoid the use of multiple classifiers for the inference by training a shared classifier with the guidance of the expert teacher of each domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Unsupervised Domain Adaptation (UDA). Unsupervised domain adaptation is a target domain annotation-free transfer learning task, where there are domain shifts/gaps between the training source dataset and the testing target dataset. Motivated by the seminal theory work <ref type="bibr" target="#b29">[30]</ref>, many advanced UDA methods seek to learn domain-invariant features by reducing the distribution discrepancy between source and target features. Some methods minimize distribution divergence with the losses of maximum mean discrepancy (MMD) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18]</ref>, second order correlation CORAL <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Some others learn a domain discriminator in an adversarial manner to achieve domain confusion <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. For better feature alignment, Maximum Classifier Discrepancy (MCD) <ref type="bibr" target="#b11">[12]</ref> maximizes the discrepancy between two classifiers while minimizing it with respect to the feature extractor. Similarly, Minimax Entropy (MME) <ref type="bibr" target="#b32">[33]</ref> maximizes the conditional entropy on unlabeled target data with respect to the classifier and minimizes it with respect to the feature encoder. M3SDA <ref type="bibr" target="#b13">[14]</ref> is one of the latest multi-source UDA methods and it minimizes the moment distance among the source and target domains and per-domain classifier is used and optimized as in MCD to enhance alignment.</p><p>Domain Generalization (DG). DG considers a more challenging yet attractive setting where the target data is unavailable during training. Many DG methods share the similar ideas of distribution alignment with UDA to learn domain-invariant features by minimizing distances among the multiple source domains <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b4">5]</ref>. Some other DG methods attempt to explore optimization/training strategies, e.g., through meta-learning <ref type="bibr" target="#b5">[6]</ref>, episodic training <ref type="bibr" target="#b7">[8]</ref>, and adaptive ensemble learning <ref type="bibr" target="#b35">[36]</ref>.</p><p>For UDA or DG classification, these alignment-based methods typically consider to use feature regularizations to achieve domain aligning but inevitably lose some discriminative information. How to fully exploit all the discriminative information in both aligned and un-aligned features remains under-explored. Jin et al. propose a domain generalization method for person re-identification <ref type="bibr" target="#b36">[37]</ref>, which reduces the style discrepancy by instance normalization and restitutes the identity-relevant features. Different from it, we address the more general domain gaps in classification and propose a unified framework with feature alignment and restoration to support both the domain generalization and adaptation tasks. The general classification is more challenging than re-identification where the latter is a fine-granularity retrieval task and the former needs to handle larger intra-class variations.</p><p>Feature Disentanglement. DNNs are known to extract features where multiple hidden factors are highly entangled <ref type="bibr" target="#b37">[38]</ref>. Some works <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> explore the learning of interpretable representations, by leveraging generative adversarial networks (GANs) <ref type="bibr" target="#b42">[43]</ref> or variational autoencoders (VAEs) <ref type="bibr" target="#b43">[44]</ref>. In this paper, different from prior works, we aim to disentangle the task-relevant features from the residual features (the difference between the original features and the aligned features) and return them for effective classification. We propose a dual ranking entropy loss constraint in the feature restoration phase, by enforcing higher discrimination of the feature after the restoration than before.</p><p>3 Proposed Unified Feature Alignment and Restoration Framework Problem definition. Given K source domain D s = {D s1 , ? ? ? , D s K } of labeled images and a target domain D t of unlabeled images. Due to the domain shifts/gaps, models trained in source domains in general suffer from performance degradation when tested on a target domain. The goal of this work is to design a unified framework for both domain generalization (DG) and unsupervised domain adaptation (UDA) classification.</p><p>Overview. <ref type="figure" target="#fig_0">Figure 2</ref> shows the pipeline of our feature alignment and restoration (FAR) framework when applied to the classification task. We split the classification pipeline into four parts: feature extraction, feature alignment (FA), feature restoration (FR), and classification. In particular, FA and FR aim to boost the generalization and discrimination capability of the network. In the FA phase, we first leverage spatial and channel attention to adaptively select features from the extracted feature map F of the backbone network and enforce across domain alignment constraints on them. Then, the FR step distills task-relevant (discriminative) features from the residual to compensate for the aligned features. Moreover, in the FR phase, we design a dual ranking entropy loss constraint to promote the distillation of the task-relevant features from the residual. Finally, a domain-shared classifier is used to classify the restored features, trained with the guidance of an expert of each domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Alignment and Restoration</head><p>In the fields of domain generalization and adaptation, some theoretical analyses <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref> show that reducing the dissimilarity among domains improves the generalization ability. Ben-David et al. <ref type="bibr" target="#b29">[30]</ref> pioneer this direction by introducing an H?H-divergence among the source domains and target domain. The recent work <ref type="bibr" target="#b13">[14]</ref> extends the prior theoretical analysis to the case of momentbased divergence between source and target domains, which matches the moments of the distributions across domains to achieve feature alignment. However, these works focus only on aligning domains. They overlook the side effect of the loss of task-relevant information due to the alignment.</p><p>To address this problem, we propose to restore the discriminative features from the discarded domainspecific information, by disentangling it into task-relevant features and task-irrelevant features with a dual ranking entropy loss constraint.</p><p>Feature Alignment (FA) Phase. FA aims to reduce domain discrepancy and get rid of domainspecific information to makes feature domain-invariant. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, for the FA phase, we denote the input feature map (obtained by the Feature Extraction step from the backbone network) by F ? R h?w?c with width w, height h, and the number of channels c. We denote the aligned feature map by A ? R h?w?c . We use different subscripts to represent the domain of data, e.g., F s1 denotes a feature map extracted from data of the first source domain.</p><p>To allow the feature F to preserve comprehensive information, instead of on F , we enforce the alignment constraints on the adaptively selected feature A, which is obtained by gating the comprehensive feature F with the tool of spatial and temporal attention in parallel <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> (see Supplementary about the formulation). Such attentive gating facilitates the adaptive selection of feature sub-spaces that are suitable for alignment.</p><p>On the attentively selected feature A, similar to <ref type="bibr" target="#b13">[14]</ref>, we add the constraints L align which align the moments of the feature distributions across different domains (see Supplementary for more details).</p><p>Feature Restoration (FR) Phase. The feature alignment constraints are task-ignorant, this inevitably discards some task-relevant information. We propose to compensate the network with such taskrelevant feature by distilling it from the residual features R, which is defined as the difference between the comprehensive original feature F and the aligned feature A as R = F ? A.</p><p>Given the residual R, we further disentangle it into task-relevant feature R + ? R h?w?c and taskirrelevant feature R ? ? R h?w?c . To enable the content-adaptive disentanglement, we use a spatial and channel attention module (similar to the attention in the FA, see <ref type="figure">Supplementary)</ref> as the gate to obtain task-relevant feature R + and the remaining task-irrelevant feature R ? as</p><formula xml:id="formula_0">R + = Gate(R) ? R, R ? = (1 ? Gate(R)) ? R,<label>(1)</label></formula><p>where Gate(R) represents the spatial and channel attention response on R. By adding the disentangled task-relevant feature R + to the aligned feature A, we obtain the restoration feature A + R + .</p><p>Dual Ranking Entropy Loss Constraint. In order to facilitate the disentanglement into task-relevant and task-irrelevant features, we design a dual ranking entropy loss constraint.</p><p>Intuitively, after adding the task-relevant feature R + to the aligned feature A, we expect the enhanced feature becomes more discriminative and thus the entropy of predicted class likelihood would become smaller, with less ambiguity. On the other hand, after adding the task-irrelevant feature (interference) R ? to the aligned feature A, the contaminated feature becomes less discriminative and thus the entropy would become larger. Therefore, we design a dual ranking entropy loss</p><formula xml:id="formula_1">L DRE = L + DRE + L ? DRE .</formula><p>We pass the spatially average pooled enhanced feature vector f + = pool(A + R + ) ? R c through the final classifier C and calculate its entropy. We denote the entropy functions as E(?) = ?p(?) log p(?). Similarly, the contaminated feature vector can be obtained by f ? = pool(A + R ? ). The reference feature vector f = pool(A). L + DRE and L ? DRE are defined as:</p><formula xml:id="formula_2">L + DRE = Sof tplus(E(C(f + )) ? E(C(f )), L ? DRE = Sof tplus(E(C(f )) ? E(C(f ? ))).</formula><p>(2) Here Sof tplus(?) = ln(1 + exp(?)) is a monotonically increasing function that aims to reduce the optimization difficulty by avoiding negative loss values.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dog Guitar Horse House Person</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>Our FAR framework is trained in an end-to-end fashion. For DG, the total loss L is a weighted summation of the alignment loss L align , dual ranking entropy loss L DRE and the basic classification loss L cls . The training data is all from the source domains. For UDA, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the target domain data without labels is also used and the loss for such data consists of L align and L DRE . Please see our Supplementary for the weight determination on each loss and pseudo code.</p><p>Note that the network parameters are all shared for different domains. To enable fast convergence and learning of the knowledge from the feature F which contains comprehensive information (both domain invariant and domain specific), as a design choice, we assign a classifier for each source domain on the feature F and let them teach the final shared classifier by enforcing L1-distance consistency L consist on the predicted probability from the classifiers. In the inference phase, the classifiers for individual source domains are not needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We describe the datasets and implementation details in Sec. 4.1. We validate the effectiveness and superiority of our FAR method in Sec. 4.2. We study some design choices in Sec. 4.3. Moreover, Sec. 4.4 shows the visualization analysis and Sec. 4.5 presents the comparisons with the state-of-the-art approaches for both DG and UDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Implementation Details</head><p>We test our method on the four classification datasets of multiple domains: Digit-Five, DomainNet <ref type="bibr" target="#b13">[14]</ref>, PACS, Office-Home <ref type="bibr" target="#b56">[57]</ref>. <ref type="figure">Figure 3</ref> shows some samples and please see more details in Supplementary. We follow prior works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b7">8]</ref> to use the leave-one-domain-out protocol. Following <ref type="bibr" target="#b13">[14]</ref>, we build the backbone using three Conv-layers and two FC-layers for Digit-Five. We use ResNet18 <ref type="bibr" target="#b58">[59]</ref> as the backbone for mini-DomainNet, PACS, and Office-Home. More training details are presented in Supplementary. where we add the moment alignment constraint on the extracted feature F . Baseline-att-align: we add Spatial&amp;Channel attention (abbr., SCA) on feature F and add the moment alignment constraint on A. FAR: our final scheme where the proposed FAR is added after Baseline (see <ref type="figure" target="#fig_0">Figure 2</ref>). <ref type="table" target="#tab_1">Table 1</ref> shows the results. We have the following observations:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>1) Baseline-align outperforms Baseline by 4.75% in accuracy on Digit-Five, which demonstrates the effectiveness of moment alignment for UDA.</p><p>2) Baseline-att-align further improves over Baseline-align by 3.39% on Digit-Five, and 1.11% on mini-DomainNet. The attentive gating could facilitate the adaptive selection of feature sub-spaces that are suitable for alignment.</p><p>3) By compensating the task-relevant information by the proposed restoration, our final scheme FAR has high generalization and discrimination capability. It significantly outperforms all the baseline schemes. In particular, FAR outperforms Baseline-align by 7.93% and 3.22% on Digit-Five and mini-DomainNet, respectively. Such large improvements demonstrate that feature alignment is not enough, and the proposed restoration is critical. Effectiveness of Dual Ranking Entropy Loss. We perform ablation study on the proposed dual ranking entropy loss L DRE . <ref type="table" target="#tab_2">Table 2</ref> shows that our final scheme FAR outperforms the scheme without the dual ranking entropy loss (i.e., scheme FAR w/o L DRE ) by 2.07% and 1.15% on Digit-Five and mini-DomainNet, respectively. Besides, both the constraint on the enhanced feature L + DRE and that on the contaminated feature L ? DRE contribute to the good feature disentanglement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Design Choices of FAR</head><p>Analysis on Attention Design. As described in Sec. 3.1, we use spatial and channel attention gates to adaptively select features for alignment, and to conduct feature disentanglement within FR. We study the influence of different designs and show the results in <ref type="table" target="#tab_3">Table 3</ref>. FAR conv : map the feature F using 1?1 convolutional layer followed by non-liner ReLU activation to obtain feature A. FAR Gate C and FAR Gate S : use channel attention gate alone or spatial attention gate alone.</p><p>We observe that 1) our spatial&amp;channel attention scheme FAR outperforms FAR conv by 4.67% on Digit-Five, demonstrating the benefit of attention design; (2) FAR outperforms FAR Gate S /FAR Gate C by 1.99%/2.59% on Digit-Five, demonstrating the complementary and comprehensiveness of the two types of attention in our FAR framework.</p><p>Is 'Ranking' Necessary for L DRE ? Here 'ranking' in L DRE refers to the operation of comparing the entropy of the predicted class likelihood of features before and after the feature restoration.</p><p>To verify the effectiveness this strategy, we compare it with the scheme without ranking, which minimizes the entropy loss of the predicted class distribution of the enhanced feature f + and maximizes the entropy loss of the predicted class distribution of the contaminated feature f ? , i.e., without comparison with before. The results in <ref type="table" target="#tab_3">Table 3</ref> reveal that our scheme FAR with the ranking idea outperforms the above-mentioned scheme w/o ranking by 1.72% on Digit-Five.   Teacher-Student Learning for Classifier. We compare the case without using the teacher-student (TS) strategy (i.e., constraint L consist ) which is denoted as w/o TS learning. As shown in the <ref type="table" target="#tab_3">Table 3</ref>, this strategy introduces 0.82% gain on average on Digit-Five.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization</head><p>Feature Map Visualization. To better understand how the FAR method works, we visualize the intermediate feature maps in different phases. We get each activation map by summarizing the feature maps along channels followed by a spatial 2 normalization. <ref type="figure" target="#fig_3">Figure 4</ref> (a) shows the activation maps of the comprehensive basic feature F , aligned feature (i.e., after FA) A, and the final enhanced feature after FR A + R + . We see that the aligned features A typically focus on a more compact region of object to achieve distribution alignment. Since it is taskirrelevant, there is some loss of fine-grained details. Our proposed feature restoration (FR) provides compensation of such task-relevant feature R + and leads to better captured discriminative regions for the final classification. Note that the four images are from different domains of DomainNet.</p><p>Feature Divergence Analysis. Following <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref>, we use the symmetric KL divergence of features between two domains (here we calculate each of the two domains' KL divergence and average them) as the metric to measure their feature divergence. We train the models in clp,pnt,rel?skt/mm,mt,sv,syn?up settings for mini-DomainNet/Digit-Five, and evaluate the feature divergences among all domains (4*250 and 5*200 test samples are randomly selected for mini-DomainNet and Ditgit-Five). We calculate the feature divergence of the basic feature F , aligned feature (i.e., after FA) A, and final enhanced feature after FR A + R + . <ref type="figure" target="#fig_3">Figure 4 (b)</ref> shows the results.</p><p>We observe that the feature divergence (FD) is relatively large for basic feature F . The feature alignment operation significantly reduces the FD among domains. Moreover, the FD after feature restoration operation (A + R + ) becomes higher than that of A, which is because the compensation of some task-relevant features increases the discrimination capability and thus increases the FD.</p><p>Visualization of Feature Distributions. In <ref type="figure" target="#fig_4">Figure 5</ref>, we visualize the distribution of the features using t-SNE <ref type="bibr" target="#b61">[62]</ref> in mm,mt,sv,syn?up setting. We compare the feature distribution of our (c) FAR with two baselines (a) Baseline and (b) Baseline-align. We observe that: 1) feature alignment indeed pulls the different domains closer and makes the distribution compact for each cluster; 2) however, the alignment seems to easily hurt the feature discrimination because it is task-irrelevant, making the different classes closer (indistinguishable); 3) FAR features obtained by our method not only make the domains compact and aligned, but also preserve the clear boundaries of different classes/digits.  Results on Unsupervised Domain Adaptation (UDA). Following the leave-one-domain-out protocol <ref type="bibr" target="#b13">[14]</ref>, the classification accuracy on the target domain test set is reported. Table (4a)(4b) show the results on the two UDA datasets. We have the following observations. 1) For the overall performance (in the Avg column), FAR achieves the best results on both datasets, outperforming the second-best methods by large margins: 2.74% on Digit-Five, 1.51% on mini-DomainNet. 2) On the small Digit-Five dataset, SVHN and SYN are the two relatively difficult domains as shown in <ref type="figure">Fig. 3</ref>. SVHN contains blurred and cluttered digits while SYN has complex backgrounds. Such large differences of the two domains with other domains make the adaptation harder. Nonetheless, FAR achieves the best accuracy, outperforming M 3 SDA-? <ref type="bibr" target="#b13">[14]</ref> (2nd best) by 5.20% on SVHN and 5.86% on SYN, which further confirms the efficacy of FAR. 3) For the large-scale mini-DomainNet, our FAR also consistently shows clear improvements over the second-best MME <ref type="bibr" target="#b32">[33]</ref> on mini-DomainNet. Particularly, in the hardest domain 'sketch', FAR achieves the best 50%, which further validates that feature restoration improves the feature generalization ability, especially for the extreme situation.</p><p>Results on Domain Generalization (DG). <ref type="table" target="#tab_5">Table 5</ref> shows the comparison with the state-of-the-art for DG. We can see that FAR achieves the best results on both PACS and Office-Home with clear margins against all competitors. In particular, FAR is clearly better than the distribution alignment methods, i.e., CCSA <ref type="bibr" target="#b34">[35]</ref> and MMD-AAE <ref type="bibr" target="#b28">[29]</ref>, with 2.3%/4.7% improvement on PACS and 1.1%/3.3% improvement on Office-Home. Besides, compared with the recent self-supervised method JiGen <ref type="bibr" target="#b6">[7]</ref>, data augmentation method CrossGrad <ref type="bibr" target="#b4">[5]</ref>, and the episodic training method Epi-FCR <ref type="bibr" target="#b7">[8]</ref>, FAR is clearly superior thanks to the effectiveness of feature alignment and restoration learning. Furthermore, in comparison with the above methods that typically require multiple feature extractors or classifiers, our FAR incurs less computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we study a crucial but overlooked issue that existed in the prior cross-domain generalization/adaptation works, and propose a new concept of Feature Alignment and Restoration (FAR) to help build a unified framework for effective DG and UDA classification. FAR performs feature aligning by moment distributions matching and feature restoration by compensating with task-relevant features to simultaneously enhance cross domain generalization and preserve feature discrimination. For better distilling task-relevant information, we enforce a dual ranking entropy loss constraint to encourage the better disentanglement of task-relevant/-irrelevant feature. Extensive experiments on several settings demonstrate the effectiveness of FAR, which achieves the state-of-the-art performance for both domain generalization and unsupervised domain adaptation.</p><p>Moreover, we believe that our FAR is a general framework and can be extended to more general and practical fields by equipping with the corresponding task heads, such as image segmentation and object detection. This would be part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary 1 More Details of Feature Alignment and Restoration</head><p>More Details of the Feature Alignment (FA) Phase. To allow the feature F to preserve comprehensive information, instead of on F , we enforce the alignment constraints on the adaptively selected feature A, which is obtained by gating the comprehensive feature F with the tool of spatial and temporal attention in parallel <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>. We formulate this as</p><formula xml:id="formula_3">A(i, j, :) = S i,j A (i, j, :), A (:, :, k) = a k F (:, :, k),<label>(3)</label></formula><p>where F (:, :, k) ? R h?w denotes the k th channel of feature F , k = 1, 2, ? ? ? , c, and F (i, j, :) ? R c denotes the spatial position of (i, j) of feature F , i = 1, 2, ? ? ? , h; j = 1, 2, ? ? ? , w. The channel attention <ref type="bibr" target="#b48">[49]</ref> response vector a = [a 1 , a 2 , ? ? ? , a c ] ? R c and the spatial attention <ref type="bibr" target="#b49">[50]</ref> response map S ? R h?w with S i,j denoting the spatial attention value at the position (i, j). They are obtained based on the same input F (i.e., in parallel instead of in sequential) as</p><formula xml:id="formula_4">a = Gate c (F ) = ?(W c 2 * ?(W c 1 * pool(F ))), S = Gate s (F ) = ?(W s 2 * ?(W s 1 * channel_pool(F ))),<label>(4)</label></formula><p>where the channel attention gate Gate c (?) consists of a global spatial average pooling layer pool(?) followed by two FC layers with parameters of W c 2 ? R (c/r)?c and W c 1 ? R c?(c/r) which are followed by ReLU activation function ?(?) and sigmoid activation function ?(?), respectively. For the spatial attention gate Gate s (?), it consists of a cross-channel pooling layer, two convolutional layers followed by ReLU and sigmoid activation functions, respectively. To reduce the number of parameters, a dimension reduction ratio r is used and is set to 8 for both channel and spatial gates.</p><p>On the attentively selected feature A, similar to <ref type="bibr" target="#b13">[14]</ref>, we add the constraint L align which aligns the moments of the feature distributions across different domains. We assume that A s1 , A s2 , ..., A s N , A t are the collections of aligned features (originated from i.i.d. samples from the domains D s1 , D s2 , ..., D s N , D t ). The alignment loss L align in terms of the Moment Distance (MD) between them is defined as</p><formula xml:id="formula_5">L align = M D 2 (D s1 , D s2 , ..., D s N , D t ) = 1 N N i=1 ? k i ? ? k t 2 + 1 C 2 N N ?1 i=1 N j=i+1 ? k i ? ? k j 2 + 1 N N i=1 ? k i ? ? k t 2 + 1 C 2 N N ?1 i=1 N j=i+1 ? k i ? ? k j 2 ,<label>(5)</label></formula><p>where (? i , ? i ) denotes (mean, variance) of the distributions of aligned features A si of source i, and (? t , ? t ) denotes (mean, variance) of the distribution of aligned features A t of the target domain.</p><p>More Details of the Dual Ranking Entropy Loss. Here we would like to further explain the physical meaning of the proposed dual ranking entropy loss constraint L DRE within FR phase from a more high-level perspective. As illustrated in <ref type="figure" target="#fig_5">Figure 6</ref>, the main idea is that: after compensating the task-relevant feature R + to the aligned feature A, the feature becomes more discriminative and its predicted class likelihood becomes sharper with smaller entropy, enabling less ambiguous; on the other hand, after compensating the task-irrelevant feature R ? to the aligned feature A, the feature should become less discriminative (get larger entropy of the predicted class likelihood).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Min. entropy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Training</head><p>Our FAR framework is trained in an end-to-end manner. Here, we introduce more details about the loss for domain generalization and domain adaptation, respectively. The pseudo code of FAR is given in Algorithm 1.</p><p>Domain Generalization. For DG, the training data is all from the source domains, and the total training loss L total is a weighted summation of the alignment loss L align , dual ranking entropy loss L DRE , and the basic classification loss (i.e., cross-entropy loss) L cls :</p><formula xml:id="formula_6">L total = ? align L align + ? DRE L DRE + ? cls L cls ,<label>(6)</label></formula><p>where ? align , ? DRE , ? cls denote the hyper-parameters for balancing the three losses. Experimentally, we finally set ? align : ? DRE : ? cls = 0.5 : 0.1 : 1. We follow the common practices to set them. We initially set all of them to 1, and then coarsely determine each one based on the value of each loss and their gradients observed during the training. The principle is to set the balancing weight values to make the weighted loss values/gradients of the three lie in a similar range. Grid search within a small range of the derived value is further employed to get better parameters.</p><p>In addition, as we described in the main manuscripts, to enable fast convergence and learning of the knowledge from the feature F which contains comprehensive information (both domain invariant and domain specific), as a design choice, we assign a classifier on the feature F for each source domain and let them teach the final shared classifier (on the compensated feature A + R + ) by enforcing L1-distance consistency L consist on the predicted probability from the classifiers. Due to the relative smaller loss values in comparison with the other three losses, we experimentally set the balance weight of such sub-loss as ? consist = 100.</p><p>Domain Adaptation. For DA, the target domain data without labels is also used, but the loss for such data only consists of L align and L DRE . The loss balance also follows the same rule with DG setting and we finally set them as ? align : ? DRE : ? cls : ? consist = 0.5 : 0.1 : 1 : 100.</p><p>Pseudo Code of FAR. The pseudo code of FAR is roughly presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">More Details about Datasets and Implementation</head><p>Datasets. Digit-5 consists of five different digit recognition datasets: MNIST <ref type="bibr" target="#b52">[53]</ref>, MNIST-M <ref type="bibr" target="#b53">[54]</ref>, USPS <ref type="bibr" target="#b54">[55]</ref>, SVHN <ref type="bibr" target="#b55">[56]</ref> and SYN <ref type="bibr" target="#b53">[54]</ref>. We follow the same split setting as <ref type="bibr" target="#b13">[14]</ref> to utilize the dataset. DomainNet is a recently introduced benchmark for large-scale multi-source domain adaptation <ref type="bibr" target="#b13">[14]</ref>, which includes six domains (Clipart, Infograph, Painting, Quickdraw, Real and Sketch) and 600k images with 345 classes. Considering the required considerable computation resources, we takes a subset of DomainNet (i.e., mini-DomainNet) following <ref type="bibr" target="#b35">[36]</ref> for performing experiments. PACS <ref type="bibr" target="#b3">[4]</ref> and Office-Home <ref type="bibr" target="#b64">[65]</ref> are both commonly used DG dataset with four domains, where the former one has seven object categories and the latter one has 65 categories.</p><p>Implementation Details. In all experiments, SGD with momentum is used as the optimizer and the cosine annealing rule <ref type="bibr" target="#b65">[66]</ref> is adopted for learning rate decay.</p><p>For Digit-5, we build the backbone with three convolution layers and two fully connected layers, following <ref type="bibr" target="#b13">[14]</ref>. For each mini-batch, we sample from each domain of 64 images. The model is trained with an initial learning rate of 0.05 for 30 epochs. For mini-DomainNet, we use ResNet18 <ref type="bibr" target="#b58">[59]</ref> as the CNN backbone. We sample 42 images from each domain to form a mini-batch and train the model for 60 epochs with an initial learning rate of 0.005.</p><p>For PACS and Office-Home, similar to <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b7">8]</ref>, we use ResNet18 as the CNN backbone. We train the model for 40 epochs with an initial learning rate of 0.002. Each mini-batch contains 30 images (10 per source domain). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Complexity</head><p>The model size of our final scheme FAR is similar to that of Baseline (e.g., ResNet18 as backbone, 11.57 M vs. 11.23 M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Loss Curves Comparison for the Expert Teaching</head><p>As we have described in the main manuscript, to enable fast convergence and learning of the knowledge from the original feature F which contains comprehensive information (both domain invariant and domain specific), we assign a classifier for each source domain on the feature F and let them teach the final shared classifier by enforcing L1-distance consistency L consist on the predicted probability from the classifiers. Here we show the loss curves of our FAR without L consist and our FAR with L consist in <ref type="figure">Figure 7</ref>. We can see that the loss curve of FAR with L consist converges faster.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Flowchart of the Feature Alignment and Restoration (FAR) framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Photo Cartoon Art</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>(a) Activation feature maps in different phases within our FAR framework. They show that the feature alignment (FA) operation may cause the loss of some information that benefits the task, and the feature restoration (FR) could compensate the aligned features with such informative features. (b) Analysis of the feature divergence among different domains in different phases within FAR.(a) Baseline features (b) Baseline-align features (c) FAR features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of t-SNE distributions. We compare our FAR with two baseline schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Illustration of the proposed dual ranking entropy loss constraint. (a) L + DRE : compensating task-relevant feature benefits classification and then the predicted class likelihood becomes sharper (smaller entropy), enabling less ambiguous. (b) L ? DRE : compensating task-irrelevant feature results in larger uncertainty on the predicted class (larger entropy of the predicted class likelihood).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1 Feature Alignment and Restoration 1 :Figure 7 :</head><label>117</label><figDesc>Input: source domains Ds1, Ds2...Ds N , target domain Dt, learning rate ?, batch size M , loss balance weights ? align : ?DRE : ? cls : ?consist = 0.5 : 0.1 : 1 : 100, maximum iterations N . The entire network F AR(?) consists of the feature extractor f ? , feature alignment module h ? , feature restoration module r ? , final classifier c?, and individual classifier for each source domain c? i . For DG, there is no target domain Dt and the related items can be ignored. 2: Output: predicted class from classifier c?.3: for n = 1 to N do 4: Sample M samples (x1, y1, d1), ? ? ? , (xM , yM , dM ) // Randomly sample several samples from each domain to form a batch of M samples, where (xi, yi, di) denotes input sample xi, its class label yi, and domain label di 5: L total = ? align L align + ?DRELDRE + ? cls L cls + ?consistLconsist // For each sample in the batch, calculate each loss and the total loss based on the network F AR(?) 6: (?, ?, ?, ?, ?i) = (?, ?, ?, ?, ?i) ? ?? (?,?,?,?,? i ) ? cls L cls // Use the classification loss L cls to update the entire network 7: ? = ? ? ?? ? ? align L align // Use the alignment loss L align to update feature alignment module h ? 8: ? = ? ? ?? ? ?DRELDRE // Use the proposed dual ranking entropy loss LDRE to update feature restoration module r ? 9: ? = ? ? ????consistLconsist // Use the proposed consist loss Lconsist to update feature classifier c? in a teacher-student learning manner 10: end for FAR w/o ? FAR w ? Loss Iterations Loss curves comparison about the expert teaching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Sktech (skt). Considering the required huge computation resources, we use a subset of DomainNet (i.e., mini-DomainNet) following<ref type="bibr" target="#b35">[36]</ref> for experiments. (c) PACS, includes Sketch, Photo, Cartoon, and Art. (d) Office-Home, includes Real-world (Real), Product, Clipart, and Art.</figDesc><table><row><cell>Figure 3: Four classification datasets (first two for UDA and last two for DG). (a) Digit-Five, includes</cell></row><row><cell>MNIST [53] (mt), MNIST-M [54] (mm), USPS [55] (up), SVHN [56] (sv), and Synthetic [54]</cell></row><row><cell>(syn). (b) DomainNet, includes Clipart (clp), Infograph (inf ), Painting (pnt), Quickdraw (qdr), Real</cell></row><row><cell>(rel), and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Effectiveness of our FAR. Note that the italics denotes the left-out target domain. 90.50 88.71 63.54 82.44 77.71 62.86 47.94 58.73 43.02 53.14 Baseline-align 65.01 93.18 93.12 76.41 84.59 82.46 63.17 49.21 58.10 44.76 53.81 Baseline-att-align 68.17 96.86 94.23 78.11 91.89 85.85 65.56 47.62 57.78 48.73 54.92 FAR 72.27 99.12 98.60 86.52 95.44 90.39 66.51 50.48 61.11 50.00 57.</figDesc><table><row><cell>Method</cell><cell>mm</cell><cell>mt</cell><cell>Digit-Five up sv</cell><cell>syn</cell><cell>Avg</cell><cell>clp</cell><cell>mini-DomainNet pnt rel skt</cell><cell>Avg</cell></row><row><cell>Baseline</cell><cell>63.37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>03 Effectiveness of our FAR. We compare several schemes. Baseline: combines all source domains for training with standard supervised learning. Baseline-align: a naive model that builds upon Baseline,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on the dual ranking entropy loss L DRE . 90.50 88.71 63.54 82.44 77.71 62.86 47.94 58.73 43.02 53.14 FAR w/o LDRE 69.82 98.09 98.33 84.63 93.37 88.85 63.51 50.24 59.88 49.87 55.88 FAR w/o L + DRE 70.98 98.57 98.38 84.95 93.62 89.30 64.29 50.00 60.48 50.16 56.23 FAR w/o L ? DRE 71.55 98.78 98.16 85.05 94.21 89.55 65.40 51.43 59.68 50.00 56.63 FAR 72.27 99.12 98.60 86.52 95.44 90.39 66.51 50.48 61.11 50.00 57.03</figDesc><table><row><cell>Method</cell><cell>mm</cell><cell>mt</cell><cell>Digit-Five up sv</cell><cell>syn</cell><cell>Avg</cell><cell>clp</cell><cell>mini-DomainNet pnt rel skt</cell><cell>Avg</cell></row><row><cell>Baseline</cell><cell>63.37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Study on different design choices of FAR.</figDesc><table><row><cell>o TS learning</cell></row></table><note>Source Target Baseline FARconv FARGate C FARGate S FAR (ours) w/o ranking w/</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance (%) comparisons with the state-of-the-art approaches for UDA. (a) Comparison results on the Digit-Five. 96.31 94.24 62.45 85.43 80.44 CORAL [19] 62.53 97.21 93.45 64.40 82.77 80.07 72.82 98.43 96.14 81.32 89.58 87.65 Baseline 63.37 90.50 88.71 63.54 82.44 77.71 FAR 72.27 99.12 98.60 86.52 95.44 90.39 (b) Comparison results on the mini-DomainNet.</figDesc><table><row><cell cols="2">Method DAN [11] 63.78 DANN [23] mm 71.30 97.60 92.33 63.48 85.34 82.01 Digit-Five mt up sv syn Avg JAN [18] 65.88 97.21 95.42 75.27 86.55 84.07 ADDA [24] 71.57 97.89 92.83 75.48 86.45 84.84 DCTN [63] 70.53 96.23 92.81 77.61 86.77 84.79 MEDA [64] 71.31 96.47 97.01 78.45 84.62 85.60 MCD [12] 72.50 96.21 95.33 78.89 87.47 86.10 M3SDA [14] 69.76 98.58 95.23 78.56 87.56 86.13 M3SDA-? [14] Method MCD [12] DCTN [63] DANN [23] M3SDA [14] 64.18 49.05 57.70 49.21 55.03 mini-DomainNet clp pnt rel skt Avg 62.91 45.77 57.57 45.88 53.03 62.06 48.79 58.85 48.25 54.49 65.55 46.27 58.68 47.88 54.60 MME [33] 68.09 47.14 63.33 43.50 55.52 Baseline 62.86 47.94 58.73 43.02 53.14</cell></row><row><cell>FAR</cell><cell>66.51 50.48 61.11 50.00 57.03</cell></row><row><cell>4.5 Comparison with State-of-the-Arts</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance (%) comparisons with the state-of-the-art approaches for DG.</figDesc><table><row><cell>Method</cell><cell>Art</cell><cell>Cartoon</cell><cell>PACS Photo</cell><cell>Sketch</cell><cell>Avg</cell><cell>Art</cell><cell>Clipart</cell><cell>Office-Home Product</cell><cell>Real</cell><cell>Avg</cell></row><row><cell>MMD-AAE [29]</cell><cell>75.2</cell><cell>72.7</cell><cell>96.0</cell><cell>64.2</cell><cell>77.0</cell><cell>56.5</cell><cell>47.3</cell><cell>72.1</cell><cell>74.8</cell><cell>62.7</cell></row><row><cell>CCSA [35]</cell><cell>80.5</cell><cell>76.9</cell><cell>93.6</cell><cell>66.8</cell><cell>79.4</cell><cell>59.9</cell><cell>49.9</cell><cell>74.1</cell><cell>75.7</cell><cell>64.9</cell></row><row><cell>JiGen [7]</cell><cell>79.4</cell><cell>75.3</cell><cell>96.2</cell><cell>71.6</cell><cell>80.5</cell><cell>53.0</cell><cell>47.5</cell><cell>71.5</cell><cell>72.8</cell><cell>61.2</cell></row><row><cell>CrossGrad [5]</cell><cell>79.8</cell><cell>76.8</cell><cell>96.0</cell><cell>70.2</cell><cell>80.7</cell><cell>58.4</cell><cell>49.4</cell><cell>73.9</cell><cell>75.8</cell><cell>64.4</cell></row><row><cell>Epi-FCR [8]</cell><cell>82.1</cell><cell>77.0</cell><cell>93.9</cell><cell>73.0</cell><cell>81.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline</cell><cell>77.0</cell><cell>75.9</cell><cell>96.1</cell><cell>69.2</cell><cell>79.5</cell><cell>58.9</cell><cell>49.4</cell><cell>74.3</cell><cell>76.2</cell><cell>64.7</cell></row><row><cell>FAR</cell><cell>79.3</cell><cell>77.7</cell><cell>95.3</cell><cell>74.7</cell><cell>81.7</cell><cell>61.4</cell><cell>52.9</cell><cell>74.5</cell><cell>75.4</cell><cell>66.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5542" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio D&amp;apos;</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<title level="m">Learning transferable features with deep adaptation networks. ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Cycada: Cycle-consistent adversarial domain adaptation. ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1426" to="1435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transferable normalization: Towards improving transferability of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1951" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Synthetic to real adaptation with generative correlation alignment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1982" to="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Central moment discrepancy (cmd) for domain-invariant representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Natschl?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Saminger-Platz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient learning of domain-invariant image representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by domain invariant projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mehrtash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Collaborative and adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3801" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial multiple source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">P</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8559" to="8570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation via minimax entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8050" to="8058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scatter component analysis: A unified framework for domain adaptation and domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1414" to="1430" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5715" to="5725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Domain adaptive ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07325</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Style normalization and restitution for generalizable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11037</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation in deep representation using adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><forename type="middle">Jake</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5040" to="5048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Adversarial autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A unified feature disentangler for multi-domain image translation and manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2590" to="2599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Diverse imageto-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Domain adaptation with multiple sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21</title>
		<editor>D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1041" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning from multiple sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Wortman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1757" to="1774" />
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adversarial multiple source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">P</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8568" to="8579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Algorithms and theory for multiple-source adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningshan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8246" to="8256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adaptive transfer network for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7202" to="7211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<editor>NeurIPS-W</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Antonio D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="464" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Revisiting batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04779</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep cocktail network: Multi-source unsupervised domain adaptation with category shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3964" to="3973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Visual domain adaptation with manifold embedded distribution alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="402" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
