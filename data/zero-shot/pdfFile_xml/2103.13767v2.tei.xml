<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Patch Craft: Video Denoising by Deep Modeling and Patch Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Vaksman</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
							<email>melad@google.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">CS Department</orgName>
								<orgName type="institution">The Technion Technion City</orgName>
								<address>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Google Research Mountain-View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Peyman Milanfar Google Research Mountain-View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Patch Craft: Video Denoising by Deep Modeling and Patch Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The non-local self-similarity property of natural images has been exploited extensively for solving various image processing problems. When it comes to video sequences, harnessing this force is even more beneficial due to the temporal redundancy. In the context of image and video denoising, many classically-oriented algorithms employ selfsimilarity, splitting the data into overlapping patches, gathering groups of similar ones and processing these together somehow. With the emergence of convolutional neural networks (CNN), the patch-based framework has been abandoned. Most CNN denoisers operate on the whole image, leveraging non-local relations only implicitly by using a large receptive field. This work proposes a novel approach for leveraging self-similarity in the context of video denoising, while still relying on a regular convolutional architecture. We introduce a concept of patch-craft framesartificial frames that are similar to the real ones, built by tiling matched patches. Our algorithm augments video sequences with patch-craft frames and feeds them to a CNN. We demonstrate the substantial boost in denoising performance obtained with the proposed approach.</p><p>Several recent denoisers have combined the patch-based</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper we put our emphasis on the denoising task, removing white additive Gaussian noise of known variance from visual content, focusing on video sequences. Image and video denoising is a rich and heavily studied topic, with numerous classically oriented methods and ideas that span decades of extensive research activity. The recent emergence of deep learning has brought a further boost to this field, with better performing solutions. Our goal in this paper to propose a novel video denoising strategy that builds on a synergy between the classics and deep neural networks.</p><p>A key feature we build upon is the evident spatio-temporal self-similarity existing in video sequences.</p><p>Natural images are known to have a spatial self-similarity property -local image components tend to repeat them-selves inside the same picture <ref type="bibr" target="#b33">[33]</ref>. Imagine an image split into overlapping patches of small size (e.g. 7 ? 7). Many of these are likely to have several similar twins in different locations in the same image. This property has been exploited extensively by classically oriented algorithms for solving various image processing problems -denoising and many other tasks. These algorithms usually split the processed image into fully overlapping patches and arrange them into some structure according to their similarity. For example, the well-known Non-Local-Means algorithm <ref type="bibr" target="#b2">[3]</ref> filters each patch by averaging it with similar ones. The methods reported in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref> group the similar patches and denoise them jointly. Alternatively, the authors of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref> chain all the patches into a shortest-path, using this as a regularization for solving various inverse problems. Other methods go even farther and construct more complicated structures, such as binary trees <ref type="bibr" target="#b16">[17]</ref> or graphs <ref type="bibr" target="#b26">[27]</ref>, and use these structures for solving image reconstruction tasks.</p><p>In recent years convolutional neural networks (CNN) entered the image restoration field and took the lead, showing impressive results (e.g., <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">32]</ref>). With this trend in place, the patch-based framework has been nearly abandoned, despite its success and popularity in classical algorithms. Most CNN based schemes work globally, operating on the whole image rather than splitting it into patches, leveraging self-similarity only implicitly by using a large receptive field. This trend has two origins: First, the reconstructed patches tend to be inconsistent on their overlaps. This undesirable and challenging phenomenon is referred to in the literature as the local-global gap, handled typically by plain averaging <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">34]</ref>. The second reason for abandoning the patch-based framework is the difficulty of combining it with CNNs. The convolutional architecture has been shown to be a very successful choice, achieving state-of-the-art (SOTA) results in many image restoration tasks (e.g., <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">32]</ref>). However, such an architecture implies working on the whole image uniformly, and thus combining it with a patch-based framework is not straightforward. point of view within a deep-learning solution (e.g. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24]</ref>). Without diving into their details, these algorithms split a noisy image into fully overlapping patches, augment each with a group of similar ones and feed these groups to a denoising network. All three algorithms have managed to achieve near SOTA results while using a small number of trainable parameters. However, their performance is still challenged by leading convolutional networks, such as DnCNN and other networks <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>When turning to video processing, self similarity is further amplified due to the temporal redundancy. Thus, harnessing non-local processing in video is expected to be even more effective and beneficial. Many classical algorithms have successfully exploited spatio-temporal self-similarity by working on 2D or even 3D patches. For example, the V-BM4D <ref type="bibr" target="#b11">[12]</ref> groups similar 3D patches and denoises them by a joint transform and thresholding, extending the wellknown BM3D to video <ref type="bibr" target="#b4">[5]</ref>. VNLB <ref type="bibr" target="#b0">[1]</ref> also relies on groups of such patches, employing a joint empirical Bayes estimation for each group, under a Gaussianity modeling.</p><p>In contrast to the activity in image denoising, where many CNN-based schemes surpass classical algorithms, only a few video denoising networks have been shown to be competitive with classical methods. The recently published DVDnet <ref type="bibr" target="#b21">[22]</ref> and FastDVDnet <ref type="bibr" target="#b22">[23]</ref> are such networks, obtaining SOTA results, the first leveraging motion compensation, and the second combining several U-Net <ref type="bibr" target="#b18">[19]</ref> networks for increasing its receptive field. Other CNN-based video denoisers in recent literature are <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7]</ref> While CNN-based algorithms for video denoising, such as DVDnet and FastDVDnet, choose to work on whole frames rather than on patches, neural network based video denoising may still exploit self-similarity. For example, VNLnet <ref type="bibr" target="#b5">[6]</ref>, the first good-performing such denoiser, combines the patch-based framework with DnCNN <ref type="bibr" target="#b30">[30]</ref> architecture, augmenting noisy frames with auxiliary feature maps that consist of central pixels taken from similar patches. While VNLnet's strategy introduces a non-locality flavor into the denoising process, it is limited in its effectiveness due to the use of central pixels instead of full neighboring patches, as evident from its performance.</p><p>Our work proposes a novel, intuitive, and highly effective way to leverage non-local self-similarity within a CNN architecture. We introduce the concept of patch-craft frames and use these as feature maps within the denoising process. For constructing the patch-craft frames, we split each video frame into fully overlapping patches. For each patch we find its n nearest neighbors in a spatio-temporal window, and those are used to build f (patch size) groups of corresponding n patch-craft frames. These are augmented to the real video frames and fed into a spatio-temporal denoising network. This way, self-similarity is fully leveraged, while preserving the CNN's nature of operating on whole frames, and overcoming the local-global gap.</p><p>Augmenting video sequences with patch-craft frames requires processing large amounts of data in producing each output frame. To overcome this difficulty, we use a CNN composed of multidimensional separable convolutional (SepConv) layers. A SepConv layer applies a series of convolutional filters, each working on a sub-group of dimensions while referring to the rest as independent tensors. Such layers implement a multidimensional separable convolution, which allows reducing the number of trainable parameters and expediting the inference.</p><p>The processing pipeline of our proposed augmentation scheme for video denoising is composed of two stages. First, we augment a noisy video sequence with patch-craft frames and feed the augmented sequence into a CNN built of SepConv layers. This stage functions mostly as a spatial filtering. At the second stage, we apply a temporal filtering, using a 3D extension of the DnCNN <ref type="bibr" target="#b30">[30]</ref> architecture. This filter works in a sliding window manner, getting as input the outcome of the first stage with the original noisy video and producing reconstructed video at its output. Through extensive experiments, we show that the proposed method leads to a substantial boost in video denoising performance compared with leading SOTA algorithms.</p><p>To summarise, the contributions of this work are the following: We propose a neural network based video denoising scheme that consists of an augmentation of patch-craft frames, followed by a spatial and a temporal filtering. The proposed augmentation leverages non-local self-similarity using the patch-based framework, while allowing the denoising network to operate on whole frames. The deployed SepConv layers, which are used as building blocks of the spatial filtering CNN, allow reasonable memory and computational complexities for inference and learning, despite the large number of the patch-craft frames. The proposed method shows SOTA video denoising performance when compared to leading alternative algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Patch-Craft Frames</head><p>Let us start by motivating our approach. Consider a video frame that should be denoised by a neural network. Imagine that one could construct an artificial frame identical to the real one but with a different noise realization. Such a synthetic frame would be beneficial for denoising because it holds additional information about the processed frame. More specifically, this synthetic frame could be used as an additional feature map representing the real frame. Following this motivation, we define the patch-craft frames as such auxiliary artificial frames built of patches taken from the current and surrounding frames.</p><p>Constructing of the patch-craft frames is carried out as follows: We start by extracting all possible overlapping patches of size ? F ? ? F from the processed frame, which we refer to as the current frame, and find n nearest neighbors (most similar patches) for each extracted patch. We use the L 2 norm as distance metrics and limit the nearest neighbor search to a spatio-temporal 3D box of size B ? B ? (2T s + 1), where B refers to spatial axes and 2T s + 1 stands for the temporal window used -T s backward and T s forward. The n found neighbor patches are used for building the patch-craft frames where we utilize only their central parts of size ? f ? ? f , as shown in <ref type="figure">Figure 1</ref>. The patch-craft frames are created by stitching nonoverlapping patches together. More specifically, we build f groups of n + 1 frames. Each group contains a copy of the processed frame and n patch-craft ones. The first patch-craft frame is built by stitching the first nearest neighbors together, the second frame is constructed from the second nearest neighbors, and so on, till the last (n-th) nearest neighbor. The f groups differ by the patches' offsets. For building the first group, we use the neighbors of patches with no offset (i. e., with offset [0, 0]). The second group is constructed using the neighbors of patches with offset [0, 1], and so on, till an offset</p><formula xml:id="formula_0">[ ? f ? 1, ? f ? 1].</formula><p>For handling boundary pixels, we extrapolate the frame with a mirror reflection of itself and cut the leftovers after stitching the neighbors. Splitting of a frame to non-overlapping patches with different offsets is shown schematically in <ref type="figure">Figure 2</ref>.</p><p>Clearly, stitching patches with no overlaps to form an image may lead to the block boundary artifacts. Such artifacts appear, for example, in heavy JPEG compression. Therefore, the reader may wonder how do we avoid these artifacts in the patch-craft frames? Indeed, a naive attempt to construct patch-craft frames from a clean video sequence may lead to a significant block boundary artifacts. These are clearly seen in Figures 3b and 3f, which show an example of stitching together clean fifth nearest neighbors.</p><p>In order to explain why this problem is avoided in our case, we draw intuition from dithering methods. It is wellknown that adding random noise before quantization causes a reduction in visual artifacts. More generally, adding random noise to a signal can help to combat structural noise. An adaptation of this idea for our case is immediate: the fact that the handled video sequence is already noisy leads to reduced artifacts, as can be seen in an example in <ref type="figure" target="#fig_1">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A comparison between Figures 3b, 3f and Figures 3d, 3h</head><p>exposes the benefit of the added random noise.</p><p>In addition to the f groups of n + 1 frames, we provide the denoising network with n+1 feature maps of scores that indicate the reliability of the patch-craft frames. A natural measure for this reliability is a patchwise squared distance between the processed and the patch-craft frames. If we denote the processed frame by y, and the jth patch-craft frame in group i by? ij , then the patchwise squared distance d ij can be calculated by subtracting the frames, computing the pointwise square of the difference, and convolving the result with a uniform kernel,</p><formula xml:id="formula_1">d ij = conv2d (y ?? ij ) * * 2, ones f , f . (1)</formula><p>Since the neural network can learn and absorb convolution kernels, we omit the last convolution. Thus we build the feature maps of scores by calculating average pointwise squared distances between the processed and patch-craft frames. More specifically, these feature maps is a group of n + 1 frames {d j } n j=0</p><formula xml:id="formula_2">1 , where d j = 1 f f ?1 i=0 (y ?? ij ) * * 2 .<label>(2)</label></formula><p>We concatenate these feature maps of scores with the f groups of n + 1 frames along the f dimension, passing the denoising network f + 1 groups of n + 1 frames for each processed original frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Algorithm</head><p>In this section we present the proposed architecture covering the separable spatial and the temporal filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Separable Convolutional Neural Network</head><p>As described in the previous section, the spatial denoising network gets as input f + 1 groups of n + 1 feature maps for each processed frame. For instance, if the patch  The proposed SepConv layer is shown schematically in <ref type="figure" target="#fig_2">Figure 4</ref>. It is a separable convolutional layer composed of three convolutional filters, conv vh, conv f , and conv n. Each filter works on a sub-group of dimensions and refers to the rest as independent tensors. The input and output of SepConv are five dimensional tensors of sizes</p><formula xml:id="formula_3">n in ? f in ? c ? v ? h and n out ? f out ? c ? v ? h respectively, where [v, h]</formula><p>is the frame size, c is the num-ber of color layers, f is the patch-size, and n is the number of neighbors to be used.</p><p>The conv vh filter applies 2D convolutions with kernels of size m?m referring to dimension c as input channels and dimensions n and f as independent ones. This filter represents a local spatial prior, having n in f in groups of trainable convolution kernels. The conv f filter applies 2D convolutions with 1 ? 1 kernels referring to dimensions c and f as input channels and n as independent. This filter represents a weighted patch averaging, having n in groups of trainable kernels. conv n applies 2D convolutions with 1 ? 1 kernels referring to n as input channels, while c and f are referred to as independent. This kernel represents a weighted neighbor averaging, having f out c groups of trainable kernels.</p><p>The spatial denoising network (S-CNN) is composed of blocks as shown in <ref type="figure" target="#fig_3">Figure 5</ref>. The first block includes the SepConv layer followed by ReLU, the middle blocks are similar to the first but with a Batch Normalization (BN) between SepConv and ReLU, and the last block consists of a single SepConv layer. Each SepConv layer reduces the number of neighbors by a factor of 2, i.e., n out = n in /2 . The network operates in the residual domain predicting the noise z s . The output frame? is obtained by subtracting the predicted noise from the corrupted frame y.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal Filtering</head><p>Although S-CNN processes information from adjacent frames due to the augmentation, it does not guarantee temporal continuity. More generally, S-CNN does not impose an explicit temporal prior on the denoised video sequence. Thus, we apply temporal post-filtering, T-CNN, on the S-CNN output. The architecture of T-CNN is shown in <ref type="figure" target="#fig_4">Figure 6</ref>, working in a sliding window -getting as input 2T t + 1 frames for any output one. Each T-CNN input frame is a concatenation (along the color dimension) of the S-CNN input and output frames y and?. Similar to S-CNN, T-CNN works in the residual domain, predicting the noise z t . The output framex is obtained by subtracting the predicted noise from the partially denoised frame?. The network architecture should remind the reader of DnCNN <ref type="bibr" target="#b30">[30]</ref>. The first part of it, Temporal Filter 3D (Tf3D), is composed of T t blocks consisting of a 3D convolutions with 3 ? 3 ? 3 kernels followed by Leaky ReLUs (LReLU). The second part, Temporal Filter 2D (Tf2D), consists of 2D convolutions with 3 ? 3 kernels followed by LReLU. Each 3D kernel of Tf3D applies no padding in the temporal dimension, while padding with zeros spatially. The kernels of Tf2D apply padding with zeros as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Video Denoising</head><p>In this section we report the denoising performance of our scheme, while comparing it with leading algorithms. We refer hereafter to our method as Patch-Craft Network (PaCNet). For quantitative comparisons, we use the PSNR metric, which is a commonly used measure of distortion for reconstructed video. In addition, we present qualitative comparisons between the reconstructed videos. Among classical video denoisers, we compare with V-BM4D <ref type="bibr" target="#b11">[12]</ref> and VNLB <ref type="bibr" target="#b0">[1]</ref> as they are the best performing classical schemes in terms of PSNR. In comparisons with CNNbased denoisers, we include VNLnet <ref type="bibr" target="#b5">[6]</ref> since it has a flavor of non-locality, and the two SOTA networks, DVDnet <ref type="bibr" target="#b21">[22]</ref> and FastDVDnet <ref type="bibr" target="#b22">[23]</ref>.</p><p>We test our network with additive white Gaussian noise of known variance, a standard and common scenario. The algorithm parameters are as follows: T s = 3, B = 89, ? F = 15, ? f = 7, n = 14. S-CNN has 5 blocks (3 inner SepConv + BN + ReLU blocks) with m = 7. For the first SepConv layer n in = n + 1, and f in = f + 1. For all layers except the last f out = f in , while for the last f out = 1. T-CNN has 17 Conv2D layers with 96 channels each, Conv3D layers have 48 channels, and T t = 3. Our network is trained on 90 video sequences at 480p resolution -the DAVIS dataset <ref type="bibr" target="#b14">[15]</ref>. The spatial and the temporal CNNs are trained separately, both using the Mean Squared Error (MSE) loss. We start by training the spatial CNN alone, and then fix its parameters and train the temporal CNN. Both networks are trained using Lamb optimizer <ref type="bibr" target="#b27">[28]</ref> with a decreasing learning rate, starting from 5?10 ?3 for the spatial and 2 ? 10 ?3 for the temporal CNNs. Our network has in total 2.87 ? 10 6 trainable parameters, where 1.34 ? 10 6 are S-CNN parameters and 1.53 ? 10 6 are T-CNN parameters. The inference time for video resolution of 854 ? 480 pixels is about 0.5 minute per frame on Nvidia Quadro RTX 8000 GPU or about 5.5 minutes per frame on CPU. <ref type="table">Table 1</ref> reports the average PSNR performance per noise level for 30 test video sequences from the DAVIS dataset (Test-Dev 2017) at 480p resolution. As can be seen, PaC-Net shows a substantial boost in denoising performance of 0.5 dB -1.2 dB, compared with the existing SOTA algorithms. When compared to FastDVDnet and DVDnet, the PSNR benefit decreases with the increase in noise level. This behavior can be explained by the deterioration of the nearest neighbor search for higher noise levels. <ref type="figure" target="#fig_5">Figures 7 and 8</ref> present qualitative comparisons of our algorithm with leading alternatives. As can be seen, our method reconstructs video frames more faithfully than the competing algorithms. For example, in <ref type="figure" target="#fig_5">Figure 7</ref>, PaCNet manages to recover the eyes and preserves more details in the background trees. The comparison with VNLB <ref type="bibr" target="#b0">[1]</ref> and FastDVDnet <ref type="bibr" target="#b22">[23]</ref> shows that PaCNet tends to produce sharper frames with more details. The strength of the Fast-DVDnet is its reliance on a plain CNN architecture, but its weakness is lack of explicitly harnessing non-local selfsimilarity. In contrast, while VNLB leverages non-local redundancy, it is still inferior to a supervised trained CNN. PaCNet enjoys both worlds, as it combines a CNN processing with leveraging of non-local self-similarity. The synergy between these two leads to SOTA denoising performance both visually and in terms of PSNR.</p><p>We also compare PaCNet with VNLnet <ref type="bibr" target="#b5">[6]</ref>, which combines nearest neighbor search with CNN for video denois-   ing. Although VNLnet performs non-local filtering, its effectiveness is limited due to the restricted use of central pixels of patches. As can be seen in <ref type="figure" target="#fig_5">Figure 7j</ref>, VNLnet creates a sharp frame with a good recovery, but suffers from artifacts along edges, as reflected by a 2.5dB drop compared to PaCNet in <ref type="figure" target="#fig_5">Figure 7l</ref>. In <ref type="figure" target="#fig_5">Figure 7j</ref>, these artifacts can be seen on the man's cap. We bring more qualitative comparisons in the supplementary material. Beyond high PSNR and the sharp reconstructed frames, our method produces video sequences with low flickering -this can be seen in video sequences in the supplementary material. In addition to the above, we evaluate the denoising performance of PaCNet in experiments with a clipped Gaussian noise (i.e., truncation of noisy pixels to [0, 1]) and provide a comprehensive comparison to recent SOTA algorithms for this type of distortion, ViDeNN <ref type="bibr" target="#b3">[4]</ref> and FastDVDnet <ref type="bibr" target="#b22">[23]</ref>. In this experiment we use the same network parameters and the same training and test sets as above. Average PSNR results are reported in <ref type="table" target="#tab_1">Table 2</ref>, exposing a similar trend to previous experiments. More specifically, PaCNet shows con-2 FastDVDnet <ref type="bibr" target="#b22">[23]</ref> PSNR values are obtained from the released code. The rest of the values reported in <ref type="table" target="#tab_1">Tables 1 and 2</ref> are taken from <ref type="bibr" target="#b22">[23]</ref>. <ref type="bibr" target="#b2">3</ref> The PSNR value for VNLnet <ref type="bibr" target="#b5">[6]</ref> with ? = 30 is missing as <ref type="bibr" target="#b5">[6]</ref> did not provide a model for this noise level. siderable improvement in PSNR of 0.8 dB -1.4 dB, while the improvement decreases with an increase of noise level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Single Image Denoising</head><p>The proposed algorithm can be easily reduced to a single image denoiser by omitting the temporal denoising network and setting T s = 0. We refer to this as S-CNN-0. Among existing image denoisers, this configuration is most similar to LIDIA <ref type="bibr" target="#b23">[24]</ref>, as both methods perform nearest neighbor search as an augmentation for denoising. <ref type="table" target="#tab_2">Table 3</ref> shows that these two denoisers have very similar performance. In order to demonstrate the impact of each component of PaC-Net, we add two columns to <ref type="table">Table 1</ref>: S-CNN-0 and S-CNN-3. S-CNN-0 reports the performance of our scheme in an intra-frame denoising configuration, in which the video is denoised frame by frame independently. S-CNN-3 shows the PaCNet performance without the temporal filtering. In this scenario, we set T s = 3, extending the nearest neighbor search in 7 adjacent frames. As can be seen, extending the nearest neighbor search to nearby frames gains more than 1 dB in PSNR, compared to a frame by frame denoising. Temporal filtering adds 0.15-0.8 dB, where this benefit increases with the increase in noise level. In addition, the temporal filter plays a key role in the reduction of flickering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work presents a novel algorithm for video denoising. Our method augments the processed video with patchcraft frames and applies spatial and temporal filtering on the enlarged sequence. The augmentation leverages non-local redundancy, similar to the way the patch-based framework operates. The spatial denoising network consists of separable convolutional layers, which allow for reasonable memory and computational complexities. The temporal CNN reduces flickering by imposing temporal continuity. We demonstrate the proposed method in extensive tests. <ref type="bibr" target="#b3">4</ref>   The PSNR values appearing in 8c, 8d, 8e and 8f refer to the whole frame, whereas those in 8i, 8j, 8k and 8l refer to the cropped area. As can be seen, PaCNet leads to better reconstructed results -see the face and the details in the background shrubs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Difference Between Patch-Craft Frames</head><p>As described in Section 2, our algorithm augments each processed frame with nf patch-craft frames. We emphasize that all these are different, not identical nor shifted versions of each other. Thus, each brings an important additional information for the denoising to leverage. Here is an illustrative example to clarify this point. Assume for simplicity that n = 1, i.e., only one nearest neighbor is used. Consider two patch-craft frames with two different offsets,  Consider the blue patches in these frames and their overlap red area. Each blue patch is a nearest neighbor (NN) of a corresponding patch in the processed frame, which means that the blue patches come from different locations in the video (perhaps even different frames). As such, their red regions are different, holding each additional information about the corresponding area of the processed frame. More broadly, all patch-craft frames are similar to each other but not identical, thus enriching the denoising process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Details Regarding Training</head><p>Figures 10a and 10b present graphs of PSNR versus number of epochs during training of our networks. The values shown in the graphs are a rough estimation of training PSNR obtained by evaluating the networks on a small set of short videos randomly cropped from the training set. The spatial network, S-CNN, is trained using spatio-temporal 3D boxes of size 150 ? 150 ? 7, applying denoising on the central frame of size 64?64?1, where the rest of the box is used for nearest neighbor search. The boxes are randomly cropped from the training video seqiences. We use batches of size 10 and train the network for 7000 epochs. For training the temporal network, T-CNN, we use batches of 10 randomly cropped spatial-temporal boxes of size 64 ? 64 ? 7 and run training for 500 epochs. <ref type="table" target="#tab_4">Table 4</ref> reports the average PSNR performance per noise level for 8 video sequences from the Set8 <ref type="bibr" target="#b21">[22]</ref> dataset. ure 12 presents graphs showing PSNR versus frame number for several test video sequences comparing PaCNet performance with VNLB <ref type="bibr" target="#b0">[1]</ref>, VNLnet <ref type="bibr" target="#b5">[6]</ref>, and FasDVDnet <ref type="bibr" target="#b22">[23]</ref>. <ref type="figure" target="#fig_1">Figures 13, 14 and 15</ref> show visual comparisons of our method versus leading algorithms. In addition to these figures, we attach to our paper several video (AVI) files that show comparisons of video sequences. Each file simultaneously plays the outcomes of four denoising algorithms: VNLB <ref type="bibr" target="#b0">[1]</ref>, VNLnet <ref type="bibr" target="#b5">[6]</ref>, FastDVDnet <ref type="bibr" target="#b22">[23]</ref>, and PaC-Net (ours), along with the clean and the noisy sequences. These sequences are arranged according to the chart shown in <ref type="figure" target="#fig_10">Figure 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Results</head><p>Files salsa s40 merge rect.avi and skatejump s20 merge rect.avi show the video sequences salsa and skate-jump contaminated by noise with ? = 40 and ? = 20 respectively. There are two rectangles, red and green, in each video. The rest four files show zoom-in on the area in these rectangles:</p><p>? The green rectangle in salsa is shown in salsa s40 merge zoom g.avi.</p><p>As can be seen, our result is sharper than the VNLB outcome and less noisy than the outputs of FastDVDnet and VNLnet -see for example the floor. Also, observe that the VNLnet has noticeable artifacts around the legs.</p><p>? The red rectangle in salsa is shown in salsa s40 merge zoom r.avi. As can be seen, PaCNet leads to better reconstruction -see for example the   brick wall. Our output is sharper and less noisy than the competitors' results.</p><p>? The green and the red rectangles of skate-jump are shown in skate-jump s20 merge zoom g.avi and skatejump s20 merge zoom r.avi respectively. As can be seen here as well, our algorithm leads to better reconstruction -e.g. see the trees.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Patches of size ? F ? ? F are used for nearest neighbor search. Their central ? f ? ? f part are used for constructing patch-craft frames.(a) Offset [0, 0] (b) Offset [v of f s , h of f s ] Splitting a frame to non-overlapping patches with different offsets. Figure 2a shows a splitting to patches without an offset ([0, 0]), while 2b shows a splitting to patches with an offset [v of f s , h of f s ]. The white rectangle represents the processed frame, where the blue area represents a mirror reflection of the frame pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(a) Clean frame (b) Clean, fifth neighbor (c) Noisy frame with ? = 25 (d) Noisy, fifth neighbor (e) Clean frame (f) Clean, fifth neighbor (g) Noisy frame with ? = 25 (h) Noisy, fifth neighbor An example of the block boundary artifacts and the influence of the additive random Gaussian noise. This figure shows the fifth patch-craft frame, i.e., built of fifth neighbors, for frame 6 of the sequence mallard-water. As can be seen by comparing 3b, 3f and 3d, 3h, the patch-craft frame built using a clean sequence suffers from block boundary artifacts while the noisy data leads to artifact reduction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The SepConv layer. size is 7 ? 7 (i.e., f = 49) and n = 14, the number of feature maps pushed into the network for each processed frame (3 color channels) is (49 + 1) ? (14 + 1) ? 3 = 2250. A regular convolutional neural network would be inapplicable for processing such an amount of data, as even using the smallest possible filters of size 3 ? 3 requires learning kernels of size 3 ? 3 ? 2250. To overcome this difficulty, we use separable convolutional layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Our spatial denoising network S-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Our temporal filtering network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Noisy with ? = 20 (c) VNLB [1], PSNR = 36.39dB (d) VNLnet [6], PSNR = 33.41dB (e) FastDVDnet [23], PSNR = 35.15dB (f) PaCNet (ours), PSNR = 37.31dB (g) Original (h) Noisy with ? = 20 (i) VNLB [1], PSNR = 35.21dB (j) VNLnet [6], PSNR = 33.61dB (k) FastDVDnet [23], PSNR = 35.00dB (l) PaCNet (ours), PSNR = 36.11dB Denoising example with ? = 20. The figure shows frame 61 of the sequence skate-jump.The PSNR values appearing in 7c, 7d, 7e and 7f refer to the whole frame, whereas those in 7i, 7j, 7k and 7l refer to the cropped area. As can be seen, PaCNet leads to better reconstructed result -see the eyes and the details in the background trees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Noisy with ? = 40 (c) VNLB [1], PSNR = 28.66dB (d) VNLnet [6], PSNR = 29.03dB (e) FastDVDnet [23], PSNR = 29.27dB (f) PaCNet (ours), PSNR = 29.73dB (g) Original (h) Noisy with ? = 40 (i) VNLB [1], PSNR = 27.92dB (j) VNLnet [6], PSNR = 27.95dB (k) FastDVDnet [23], PSNR = 28.23dB (l) PaCNet (ours), PSNR = 29.07dB Denoising example with ? = 40. The figure shows frame 48 of the sequence horsejump-stick.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>[0, 0] and [h of f s , v of f s ], as shown in Figure 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Inconsistency of patch overlaps in patch-craft frames with different offsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>PSNR vs. the number of epochs for the validation set during training of the spatial and the temporal denoising networks, S-CNN and T-CNN, for noise level ? = 30. (We use different validation sets for each network).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Video chart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Denoising for clipped Gaussian noise.</figDesc><table><row><cell>Method</cell><cell>15</cell><cell>Noise ? 25</cell><cell>50</cell><cell>Average</cell></row><row><cell>LIDIA [24]</cell><cell cols="3">34.03 31.31 27.99</cell><cell>31.11</cell></row><row><cell cols="4">S-CNN-0 (ours) 33.95 31.22 27.93</cell><cell>31.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Single image denoising performance comparison.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>DVDnet [22] 36.08 33.49 31.79 30.55 29.56 32.29 FastDVDnet [23] 36.44 33.43 31.68 30.46 29.53 32.31 PaCNet (ours) 37.06 33.94 32.05 30.70 29.66 32.68</figDesc><table><row><cell>Method</cell><cell>10</cell><cell>20</cell><cell>Noise ? 30</cell><cell>40</cell><cell>50</cell><cell>Average</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Video denoising performance on Set8<ref type="bibr" target="#b21">[22]</ref>.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We set? 0j = y, thus d 0 is zero, used for preserving tensor size.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The code reproducing the results of this paper is available at https://github.com/grishavak/PaCNet-denoiser.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The videos are better seen in repeat mode.     The PSNR values appearing in 15c, 15d, 15e and 15f refer to the whole frame, whereas those in 15i, 15j, 15k and 15l refer to the cropped area. As can be seen, PaCNet leads to better reconstructed results -see the pattern on wheels.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video denoising via empirical bayesian estimation of space-time patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the global-local dichotomy in sparsity modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Batenkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Compressed Sensing and its Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Videnn: Deep blind video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transformdomain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A non-local cnn for video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaud</forename><surname>Ehret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Facciolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model-blind video denoising via frame-to-frame training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaud</forename><surname>Ehret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Facciolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11369" to="11378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Non-local color image denoising with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Lefkimmiatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3587" to="3596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal denoising networks: a novel cnn architecture for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Lefkimmiatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3204" to="3213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02919</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-level wavelet-cnn for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengju</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="773" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video denoising using separable 4d nonlocal spatiotemporal transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing: Algorithms and Systems IX</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7870</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Non-local sparse models for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2272" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jiao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Bin</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09056</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Patch-orderingbased wavelet frame and its use in inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israel</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2779" to="2792" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalized treebased wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israel</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4199" to="4209" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Patch-disagreement as away to improve k-svd denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1280" to="1284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Expected patch log likelihood with a sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremias</forename><surname>Sulam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="99" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dvdnet: A fast network for deep video denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Tassano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fastdvdnet: Towards real-time deep video denoising without flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Tassano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lidia: Lightweight learned image denoising with instance adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Vaksman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Patch ordering as a regularization for inverse problems in image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Vaksman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zibulevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="287" to="319" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video enhancement with taskoriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1106" to="1125" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dual graph regularized dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Yankelevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal and Information Processing over Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="611" to="624" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<imprint>
			<pubPlace>Xiaodan Song, James Dem</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00962</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Supervised raw video denoising with a benchmark dataset on dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanjing</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghe</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2301" to="2310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ffdnet: Toward a fast and flexible solution for cnn-based image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4608" to="4622" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Residual dense network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Internal statistics of a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Zontak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="977" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
