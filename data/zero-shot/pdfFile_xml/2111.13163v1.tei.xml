<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic-Aware Generation for Self-Supervised Visual Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjie</forename><surname>Tian</surname></persName>
							<email>tianyunjie19@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohang</forename><surname>Xu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
							<email>jiaojb@ucas.ac.cntian.qi1@huawei.comqxye@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic-Aware Generation for Self-Supervised Visual Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a self-supervised visual representation learning approach which involves both generative and discriminative proxies, where we focus on the former part by requiring the target network to recover the original image based on the mid-level features. Different from prior work that mostly focuses on pixel-level similarity between the original and generated images, we advocate for Semantic-aware Generation (SaGe 1 ) to facilitate richer semantics rather than details to be preserved in the generated image. The core idea of implementing SaGe is to use an evaluator, a deep network that is pre-trained without labels, for extracting semantic-aware features. SaGe complements the target network with view-specific features and thus alleviates the semantic degradation brought by intensive data augmentations. We execute SaGe on ImageNet-1K and evaluate the pre-trained models on five downstream tasks including nearest neighbor test, linear classification, and fine-scaled image recognition, demonstrating its ability to learn stronger visual representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Self-supervised visual representation learning has attracted increasing attentions of the community, arguably due to its potentials of extracting general and transferable features that apply to various downstream tasks. Compared to supervised learning where manual annotations are naturally used as learning objectives, the key of self-supervised learning is to design some type of proxies so that extracted features satisfy annotation-free constraints. In the past years, the community has witnessed the evolution from geometry-based proxies (e.g., solving jigsaw puzzles <ref type="bibr" target="#b40">[40]</ref> and predicting rotations <ref type="bibr" target="#b16">[16]</ref>) to contrastive <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">18]</ref> or pre- <ref type="bibr" target="#b0">1</ref> The code is at https://github.com/sunsmarterjie/SaGe Method discrimination generation instance level pixel level semantic level MoCo <ref type="bibr" target="#b18">[18]</ref>, SimCLR <ref type="bibr" target="#b4">[5]</ref>, BYOL <ref type="bibr" target="#b17">[17]</ref>, SwAV <ref type="bibr" target="#b2">[3]</ref>, etc.</p><p>Auto-encoder <ref type="bibr" target="#b23">[23]</ref>, inpainting <ref type="bibr" target="#b42">[42]</ref>, AET <ref type="bibr" target="#b54">[54]</ref>, Colorization <ref type="bibr" target="#b55">[55]</ref>, etc.</p><p>PCRL <ref type="bibr" target="#b57">[57]</ref>, RCL <ref type="bibr" target="#b33">[33]</ref>, GenRep <ref type="bibr" target="#b26">[26]</ref>, etc.</p><p>SaGe (ours) <ref type="table">Table 1</ref>. A comparison between the proposed framework, SaGe, and prior self-supervised learning approaches. SaGe not only takes both discrimination and generation into consideration, but also enhances the generation branch with semantic awareness. dictive <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">17]</ref> proxies. Despite the substantial progress in terms of downstream performance, we note that the above approaches mostly focused on the discriminative ability yet somewhat undervalued the generative ability -as a reference, generation-based approaches <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b54">54]</ref> reported inferior performance in downstream tasks. In this paper, we construct a self-supervised learning framework that requires the target network to gain the abilities of both discrimination and generation. Specifically, the framework consists of three parts, (i) an encoder (i.e., the target network that is transferred to downstream tasks) that extracts visual features into a compact vector; (ii) a decoder (i.e., a complementary network) that tries to recover the original image based on the compact vector; and (iii) an evaluator that measures the quality of the encoder-decoder system. The evaluator itself outputs two scores, where a discrimination score is computed by feeding the compact vector into either contrastive or predictive learning, and a generation score is obtained from a standalone module that takes both the original and recovered images as inputs.</p><p>Based on the above framework, we offer a new insight to the aforementioned phenomenon that discrimination outperforms generation. The secret lies in how the generation score is computed. Existing approaches mostly used pixel-level similarity <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b42">42]</ref>, but we argue that it is not an ideal objective because pursuing pixel-level similarity may neglect semantic information, but such information is important for a wide range of downstream tasks (e.g., object detection and instance segmentation). To facilitate richer semantic information to be learned, we advocate for Semantic-aware Generation (SaGe) that measures the generation score at the semantic level. This is done by introducing a self-supervised network that extracts features and thus measures the similarity between the original and generated images. As we would see in the experiments, equipping the evaluator with pre-learned semantics brings a major benefit and thus facilitates the encoder-decoder system to arrive at a better tradeoff between image recovery quality and semantic sensitivity. The comparison between SaGe and prior works is summarized in <ref type="table">Table 1</ref>.</p><p>We evaluate SaGe-based visual representation learning (in short, SaGe) by pre-training the encoder-decoder system on ImageNet-1K and then transferring the encoder to a series of downstream tasks, including the linear classification test, nearest neighbor test, and semi-supervised test on ImageNet-1K, object detection and instance segmentation on MS-COCO, and semantic segmentation on Cityscapes. SaGe shows favorable performance compared to the existing approaches, and extensive ablation studies verify that the improvement indeed comes from the newly introduced proxy. More importantly, SaGe has the potential of replacing the contrastive/predictive learning, where the usage of intensive data augmentations can risk semantic inconsistency that harms the pre-trained models.</p><p>Overall, the contributions of this work are two-fold. First, we build a self-supervised learning framework that involves both discriminative and generative abilities, and validate its superior performance in a series of downstream tasks. Second, with extensive diagnostic and ablative experiments, we verify that the effectiveness of generationbased proxy depends on a semantic-aware evaluator, which we hope tp inspire future research in this routine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Self-supervised visual representation learning is attracting increasing attentions, arguably because the increasing amount of unlabeled image data and the expensiveness of data annotation. The key to self-supervised learning is to design a pretext task, or equivalently a proxy, so that unlabeled images naturally satisfy. Below, we categorize prior work into three types of proxies.</p><p>The geometry-based proxies refer to the constraints based on the spatial relationship among image patches and/or the feature-level consistency across geometric transformations. Typical examples include <ref type="bibr" target="#b40">[40]</ref> that required the network to solve a jigsaw puzzle so as to learn the spatial relationship between image patches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b48">48]</ref>, <ref type="bibr" target="#b16">[16]</ref> that randomly rotated the image and asked the network to predict the angle of rotation, and <ref type="bibr" target="#b55">[55]</ref> that received a grayscale image and tried to recover the RGB version. Since geometries are sometimes predictable by low-level features, these approaches share a common weakness of learning effective high-level features, e.g., in the linear test on ImageNet, best performance usually appears upon mid-level features.</p><p>The contrastive proxy went one step further by generating different (most often, two) views of an image and assuming that the target network has the ability of extracting highly consistent features for these views. Intensive data augmentations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b39">39]</ref> are often added to avoid the target network from learning naive parameters. There are mainly two paths to measure feature consistency. The first path is named contrastive learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b58">58]</ref>, where view #1 is put into a pool that contains a large number of distractors, and view #2 is used as the query with the goal being to distinguishing view #1 from all others -this mechanism often appears as a classification task where each instance forms an independent class <ref type="bibr" target="#b13">[14]</ref>, and the so-called memory bank <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b49">49]</ref> significantly enlarged the pool size in a practical manner. There exist follow-up works that discussed various aspects of improving the learning performance, including encouraging locality-sensitive feature matching to avoid the conflict between data augmentation and imagelevel consistency <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b50">50]</ref>, constructing prototypes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">32]</ref> or inter-sample contrastive pairs <ref type="bibr" target="#b4">[5]</ref>, etc. The second path is named predictive learning <ref type="bibr" target="#b17">[17]</ref>, where the goal is to predict the feature extracted from view #1 using that from view #2. An important mechanism that avoids the model from collapsing is to use online and moving-averaged versions of the target network and perform the stop-gradient operator <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">17]</ref>. The predictive learning approaches enjoy elegance (e.g., the memory bank is no longer needed) but also suffer heavier computational overheads. Similarly, there are many efforts that tried to improve predictive learning from different aspects <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b53">53]</ref>.</p><p>The generation proxy assumes that the target network (which mainly encodes the input image into a compact vector) should have the ability of recovering the original image. The early efforts date back to research on generic data, where autoencoders <ref type="bibr" target="#b23">[23]</ref> where an auxiliary module named decoder is trained to recover the input data, and the variational version <ref type="bibr" target="#b28">[28]</ref> that added constraints on the distribution of encoded data. The computer vision community later leveraged these ideas to learn visual representations <ref type="bibr" target="#b22">[22]</ref>, inheriting the encoder-decoder (a.k.a., discriminator-generator <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>) system. Examples include <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b46">46]</ref> that added noise to the input image and stop grad <ref type="bibr" target="#b0">(1)</ref> (2) For brevity, we only show the loss terms related to the encoding-decoding procedure of x <ref type="bibr" target="#b1">(2)</ref> , while there is a symmetric part that involves the encoding-decoding procedure of x <ref type="bibr" target="#b0">(1)</ref> .</p><formula xml:id="formula_0">(1) (2) ? D (1) (2) (2) EMA ? G ? G ? G D (?) E (?)</formula><p>trained the network to recover it, <ref type="bibr" target="#b56">[56]</ref> that reconstructed a part of input image from another part with the cross-channel features, <ref type="bibr" target="#b54">[54]</ref> that minimized the input and output transformations in an end-to-end manner, etc. Recently, generationbased learning shows inferior performance to contrastive or predictive learning -in this paper, we reveal the reason behind this phenomenon to be the evaluation of generation quality neglecting semantics but mostly focusing on pixellevel similarity. As a side note, the generation proxy works better in the natural language processing community, i.e., the masked language modeling task <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b37">37]</ref>, arguably because the basic linguistic elements are clear and discrete, unlike the image pixels or patches being continuous yet may not reflect to complete semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>In this section, we present the SaGe approach. We start with introducing the setting of self-supervised visual representation learning in Section 3.1. Next, in Section 3.2-Section 3.4, we elaborate the idea that considers both discrimination and generation, the semantic-aware generation, and design choices to wrap up the entire framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Setting and Notations</head><p>The goal of self-supervised visual representation learning is to learn a computational model z = f (x; ?) from an unlabeled dataset D = {x n } N n=1 . In the deep learning era, f (x; ?) often appears as a deep neural network with ? denoting the learnable parameters. Different from supervised settings, the output, z, does not correspond to some particular semantics (e.g., the class label of x), but serves as a compact representation (a.k.a., features) of x. Hence, the downstream tasks can inherit these features for an efficient fine-tuning procedure to perform visual recognition.</p><p>Since no annotations are provided, most self-supervised visual representation learning involves defining a proxy (i.e., an annotation-free condition that all images shall satisfy) P and transforming it into a loss function L(?) to optimize f (x; ?). An example comes from the learningby-predicting-rotation approach <ref type="bibr" target="#b16">[16]</ref> where the proxy, denoted by P rot , demonstrates that the orientation of an image is always predictable regardless how it was rotated (e.g., clockwise by 90 ? ). To use P rot in self-supervised learning, one can add a random angle of rotation r to any sample x n from D and train the network to predict r, namely, r = h(z n ; ? ) = h(f (x; ?); ? ), where h(z n ; ? ) is an auxiliary head that is used during the self-supervised learning procedure but discarded thereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">A Discrimination-Generation Framework</head><p>We construct a self-supervised visual representation learning framework that considers the abilities of both discrimination and generation, so that we can further diagnose and compare these two kinds of proxies. Here, by the discrimination proxy, P D , we refer to the ability of extracting consistent features for different views of an image; correspondingly, by the generation proxy, P G , we require the network to recover the original image from the compact features. The overall framework is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Mathematically, let x be an input image sampled from the unlabeled dataset, D. Without loss of generality, we apply independent data augmentations to extract two views of x, namely, x (1) = t (1) (x) and x (2) = t (2) (x), where t (1) (?) and t (2) (?) are sampled from a pre-defined set of transformation functions, T . Data augmentation is crucial for constructing sufficiently different views of x <ref type="bibr" target="#b0">(1)</ref> and x <ref type="bibr" target="#b1">(2)</ref> , otherwise extracting consistent features for them becomes a naive task. Both x (1) and x <ref type="bibr" target="#b1">(2)</ref> are fed to an encoder (i.e., the target network) to obtain corresponding mid-level features, denoted by z <ref type="bibr" target="#b0">(1)</ref> </p><formula xml:id="formula_1">||? G ? ? G ( )|| 2 &lt; ||? G ? ? G ( )|| 2 &lt; ||? G ? ? G ( )|| 2 - - x || ? || = || ? || = || ? ||</formula><formula xml:id="formula_2">= f (x (1) ; ? E ) and z (2) = f (x (2) ; ? E ),</formula><p>where the subscript E stands for the 'encoder'. z <ref type="bibr" target="#b0">(1)</ref> and z <ref type="bibr" target="#b1">(2)</ref> lay the foundation for discrimination, but to facilitate generation, we further feed z <ref type="bibr" target="#b0">(1)</ref> and z <ref type="bibr" target="#b1">(2)</ref> into a decoder to recover the input images. This is denoted byx <ref type="bibr" target="#b0">(1)</ref> </p><formula xml:id="formula_3">= g(z (1) ; ? D ) andx (2) = g(z (2) ; ? D ),</formula><p>where the subscript D stands for the 'decoder'. Note that the decoder together with the parameters are not used in the downstream tasks, so we can refer to it as an auxiliary module that assists optimizing the encoder.</p><p>Based on the basic elements, namely, <ref type="bibr" target="#b1">(2)</ref> , the key is to define two proxies for evaluating the abilities of discrimination and generation, respectively. We denote the loss functions for discrimination and generation as L D (z <ref type="bibr" target="#b0">(1)</ref> , z <ref type="bibr" target="#b1">(2)</ref> ) and L G (x (1) , x (2) ,x (1) ,x (2) ), respectively. Recent research offers efficient examples for L D (z <ref type="bibr" target="#b0">(1)</ref> , z <ref type="bibr" target="#b1">(2)</ref> ) include putting z <ref type="bibr" target="#b1">(2)</ref> into an instance pool B (each instance forms a class) and compute the classification loss <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">18]</ref>, or using an auxiliary module h D (?) that transforms z <ref type="bibr" target="#b0">(1)</ref> to h D (z <ref type="bibr" target="#b0">(1)</ref> ) and measuring the distance between h D (z <ref type="bibr" target="#b0">(1)</ref> ) and z <ref type="bibr" target="#b1">(2)</ref> attaching with stop gradient mechanism <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">17]</ref>. In comparison, existing examples for L G (x (1) , x (2) ,x (1) ,x <ref type="bibr" target="#b1">(2)</ref> ) mostly involves accumulating pixel-level inconsistency, e.g.,</p><formula xml:id="formula_4">x (1) , x (2) , z (1) , z (2) ,x (1) ,x</formula><formula xml:id="formula_5">L G (x (1) , x (2) ,x (1) ,x (2) ) = x (1) ?x (1) 2 + x (2) ?x (2) 2 ,<label>(1)</label></formula><p>where the 2 -norm can be replaced by other (e.g., p -norm) metrics.</p><p>However, we point out that pixel-level inconsistency has the drawback of neglecting semantics and hence it is not a perfect objective to optimize. <ref type="figure" target="#fig_1">Figure 2</ref> shows an example, where the original image x is encoded into z and then decoded into three candidates that are equivalent in terms of 2 -norm distance, but one of them preserves almost complete semantics of the original image but other two does not. Producing such unsatisfying results implies that the encoder-decoder system fails to efficiently encode semantics into z 2 . Note that the goal is to transfer the encoder into downstream visual recognition tasks, it would be better to force it to preserve semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Towards Semantic-Aware Generation</head><p>The above analysis motivates us to introduce semantics into the generation proxy, or more specifically, into the loss function of L G (x <ref type="bibr" target="#b0">(1)</ref> , x (2) ,x (1) ,x <ref type="bibr" target="#b1">(2)</ref> ). This is done by using another auxiliary module, h G (?), that extracts semanticaware features from both the original and generated images. We name h G (?) as a semantic-aware evaluator. Equipped with h G (?), Eqn (1) becomes</p><formula xml:id="formula_6">L G (x (1) , x (2) ,x (1) ,x (2) ) = h G (x (1) ) ? h G (x (1) ) 2 + h G (x (2) ) ? h G (x (2) ) 2 . (2)</formula><p>Equivalently, the aim is to measure the distance between x (1) andx <ref type="bibr" target="#b0">(1)</ref> after projecting them to the semantic space. The projector, h G (?), is the key component -when h G (?) is an identity mapping function, Eqn (2) degenerates to Eqn (1). In this paper, we instantiate h G (?) as a pre-trained model without using labels (e.g., the model obtained from the BYOL algorithm <ref type="bibr" target="#b17">[17]</ref> on ImageNet-1K). As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the candidates that are not distinguishable at the pixel level vary significantly at the semantic level, and the one that preserves most semantics has the lowest loss.</p><p>A discrimination-generation framework that contains h G (?) for semantic-aware evaluation is referred to as Semantic-aware Generation (SaGe).</p><p>Before entering design principles and experiments, we understand SaGe from the complementariness of its two branches, or equivalently, the discriminative and generative loss functions, L D (z <ref type="bibr" target="#b0">(1)</ref> , z <ref type="bibr" target="#b1">(2)</ref> ) and L G (x (1) , x (2) ,x (1) ,x <ref type="bibr" target="#b1">(2)</ref> ). L D (z <ref type="bibr" target="#b0">(1)</ref> , z <ref type="bibr" target="#b1">(2)</ref> ) pulls the features from two views together, aiming to eliminate the effect of t (1) (?) and t (2) (?). This proxy may cause the target network insensitive to image transformations. Although this strategy, in average, brings the benefit of learning image-level semantics, it can also cause the conflict between intensive data augmentation and image-level consistency, especially for complicated images <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b52">52]</ref>. We point out that the conflict is essentially the loss of view-specific information, and SaGe encourages z <ref type="bibr" target="#b0">(1)</ref> and z <ref type="bibr" target="#b1">(2)</ref> to preserve the view-specific information (so that the decoder can recover the original image), hence alleviating the conflict and improving the learning quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Design Choices and Discussions</head><p>We instantiate SaGe upon the baseline of BYOL <ref type="bibr" target="#b17">[17]</ref>, yet this does not obstacle its application to other methods, e.g., MoCo <ref type="bibr" target="#b18">[18]</ref>. The overall loss function is written as:</p><formula xml:id="formula_7">L overall = L D (z (1) , z (2) ) + ? ? L G (x (1) , x (2) ,x (1) ,x (2) ), (3) where ? is the balancing coefficient. Since h G (?)</formula><p>is a hierarchical function, we compute the distance between x (1) andx <ref type="bibr" target="#b0">(1)</ref> , and similarly, between x (2) andx <ref type="bibr" target="#b1">(2)</ref> , on multiple levels, including the pixel level. We borrow h G (?) from the pre-trained model of BYOL that runs on ImageNet for 300 epochs, and freeze its parameters during the SaGe training procedure. After the pre-training, all of g(?), h D (?) and h G (?) are discarded and f (?) is preserved and transferred to downstream tasks.</p><p>Below, we briefly discuss some design choices.</p><formula xml:id="formula_8">? Why not minimizing h G (x (1) ) ? h G (x (2) ) 2 or h G (x (2) ) ? h G (x (1) ) 2 ?</formula><p>This is mainly due to the potential difference between x (1) and x <ref type="bibr" target="#b1">(2)</ref> , caused by intensive data augmentations. A straightforward example lies in random crop, where x (1) and x (2) may correspond to different regions of x, and thus the above task of minimization is not reasonable. In addition, as elaborated in the previous part, we hope the encoder to preserve view-specific information, which aligns with intra-view learning, i.e., minimizing h G (x <ref type="bibr" target="#b0">(1)</ref> ) ? h G (x (1) ) 2 and h G (x <ref type="bibr" target="#b1">(2)</ref> ) ? h G (x (2) ) 2 .</p><p>? Why freezing h G (?) during the SaGe training procedure? This is mainly to avoid degeneration, i.e., h G () tending to extract very similar features for any x and hence both h G (x <ref type="bibr" target="#b0">(1)</ref> ) ? h G (x (1) ) 2 and h G (x <ref type="bibr" target="#b1">(2)</ref> ) ? h G (x (2) ) 2 become small regardless of the real similarity between the original and recovered images. In practice, switching off this option brings consistent accuracy drop (&gt; 20%) on the ImageNet-1K linear classification test.</p><p>? Why recovering the original image is required? An equivalent version: why not minimizing h G (x (?) )?z (?) 2 , but minimizing h G (x (?) ) ? h G (g(z (?) )) 2 ? First, note that z (?) = f (x (?) ) and, based on the previous question, h G (?) is frozen. So, minimizing h G (x (?) ) ? z (?) 2 implies forcing f (?) to mimic the behavior of h G (?), which limits the potential of f (?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>We evaluate SaGe by pre-training it on the unlabeled ImageNet-1K <ref type="bibr" target="#b43">[43]</ref> training set and transferring the encoder to other downstream visual recognition tasks. This protocol is widely used in the community. The downstream datasets and tasks include linear classification test, K-nearest-neighbor test, and semi-supervised test on ImageNet-1K, object detection and instance segmentation on MS-COCO <ref type="bibr" target="#b35">[35]</ref>, instance segmentation and semantic segmentation on Cityscapes <ref type="bibr" target="#b8">[9]</ref>.</p><p>We employ a 50-layer ResNet <ref type="bibr" target="#b20">[20]</ref> as the encoder. Standard data augmentations as in <ref type="bibr" target="#b17">[17]</ref> are used to generate x <ref type="bibr" target="#b0">(1)</ref> and x <ref type="bibr" target="#b1">(2)</ref> with 224 ? 224 pixels (in RGB, i.e., having 3 channels). The encoder output z (?) with a dimensionality of 7?7?2048. The feature is sent to two branches for discrimination and generation proxies, respectively. For the discrimination branch, we borrow the setting from BYOL <ref type="bibr" target="#b17">[17]</ref>, which adds a 2-layer MLP on top of the average-pooled feature (2,048D) and obtains a 256D embedding vector. For the generative branch, we first apply a 7 ? 7 convolution to reduce the dimensionality of z (?) to 1 ? 1 ? 512, and then decode it using a series of deconvolution layers with a upsampling rate of 2 (the first with rate of 7) and the output dimensionality being 7?7?512, 14?14?256, 28?28?128, 56?56?64, 112?112?32, and 224?224?16, respectively. The final one is further propagated through a 1 ? 1 convolution to outputx (?) with a dimensionality of 224 ? 224 ? 3.</p><p>The optimization mainly follows the convention. A moving-average copy of the encoder is updated with a momentum which increases from 0.9 to 1.0 during the training procedure. To avoid collapse, the stop-gradient operation is used so that the copy is not updated by gradients. The encoder is optimized using LARS <ref type="bibr" target="#b25">[25]</ref> optimizer with a momentum of 0.9, a learning rate of 4.8, and a weight decay of 10 ?6 . The decoder is trained using Adam <ref type="bibr" target="#b27">[27]</ref> with a learning rate of 0.048. The encoder's learning rate is gradually decayed to 0 in a cosine annealing schedule while the decoder's learning rate remains unchanged. Unless otherwise specified, the evaluator is another ResNet-50 pre-trained using BYOL on ImageNet-1K for 300 epochs, which get a linear evaluation result of 73.1% (72.5 is reported in the original paper). All the experiments are conducted on 32 NVIDIA Tesla-V100 GPUs. It takes about 4 days to train a 300 epochs model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance on Downstream Tasks</head><p>? Linear evaluation on ImageNet-1K. Following the standard setting, we inherit the pre-trained encoder, f (?), average-pool the output features into a 2,048-dimensional vector, and build two fully-connected layers upon it. We only fine-tune the FC layers on the entire training set of ImageNet-1K with the standard settings. As the results summarized in <ref type="table">Table 2</ref>, SaGe with either 300 or 800 pretraining epochs outperforms all competitors without multicrop augmentation, and is on par with the results upon multi-crop augmentation. In particular, with 300 pretraining epochs, SaGe outperforms the BYOL baseline with</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method epochs</head><p>Top-1 Top-5  <ref type="table">Table 3</ref>. Classification accuracy (%) under the semi-supervised learning protocol on ImageNet-1K. The entry with ? indicates that RandAugment has been used and ? indicates the method is specially designed for semi-supervised task.</p><p>800 pre-training epochs, implying its efficiency in visual representation learning.</p><p>? K-nearest-neighbor test on ImageNet-1K. Still, we make use of the pre-trained f (?) to extract a 2,048dimensional vector, and then directly use it to retrieve K training samples with the smallest distances. We report the 20-NN scores and the results are shown in the last column of <ref type="table">Table 2</ref>. In this scenario that fine-tuning is absent, the advantage of SaGe becomes more significant -it even outperforms the competitors with multi-crop augmentation, validating the effectiveness of view-specific information.</p><p>? Semi-supervised evaluation on ImageNet-1K. The ar- chitecture is same as linear evaluation, but only 1% or 10% of training set are labeled and the entire network including the pre-trained encoder can be fine-tuned. Results in <ref type="table">Table 3</ref> show the competitive performance of SaGe -among all approaches with standard data augmentations (random cropping and horizontal flipping), it is the best and secondbest on 1% and 10% labels, respectively, yet its accuracy is on par with the approaches applying RandAugment <ref type="bibr" target="#b9">[10]</ref> or specially designed for semi-supervised task.</p><p>? Object detection and instance segmentation on MS-COCO. We then transfer the pre-trained models for object detection and instance segmentation on the MS-COCO dataset <ref type="bibr" target="#b35">[35]</ref>. Following <ref type="bibr" target="#b18">[18]</ref>, we use Mask R-CNN <ref type="bibr" target="#b19">[19]</ref> with the FPN head <ref type="bibr" target="#b34">[34]</ref> and fine-tune both the backbone and head under the 1? schedule. The train2017 and val2017 sets are used for training and testing, respectively. As shown in <ref type="table">Table 5</ref>, SaGe consistently outperforms MoCo-v2 and the BYOL baseline. Interestingly, SaGe outperforms DenseCL <ref type="bibr" target="#b47">[47]</ref> and self-EMD <ref type="bibr" target="#b36">[36]</ref>, two self-supervised learning approaches that were particularly designed for object-level prediction, showing that the generation task is an alternative way of improving object-level description.</p><p>? Semantic segmentation on Cityscapes. Lastly, we transfer the pre-trained models to the Cityscapes dataset.</p><p>For instance segmentation, we use Mask R-CNN with the FPN head and perform a 2? training schedule; for semantic segmentation, we use the FCN head <ref type="bibr" target="#b38">[38]</ref>. <ref type="table">Table 4</ref> shows a similar trend as of MS-COCO experiments, showing the advantage of SaGe. This is easily interpreted. Discriminationbased learning suppresses view-specific information, but segmentation is sensitive to views. Generation-based learning complements the information loss and thus enhances the pre-trained models in such downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The Sources of Knowledge</head><p>This subsection answers an essential question: what are key proxies that support the learning procedure of ResNet-50 300 71.9 SaGe? For this purpose, we provide an ablation study that involves different combinations of discrimination and generation proxies in <ref type="table" target="#tab_3">Table 6</ref>. Throughout the remaining part of this paper, we use a shorter schedule that pre-trains SaGe for 100 epochs on ImageNet-1K. On the one hand, the effect of L D , the discrimination proxy, reflects in the direct comparisons between (#1 vs. #4), (#2 vs. #5), and (#3 vs. #9) of <ref type="table" target="#tab_3">Table 6</ref>. When there is no L G , L G is very weak, and L G is strong, introducing L D brings 65.9%, 45.6%, and 1.6% accuracy gains, respectively, showing a marginal effect.</p><p>On the other hand, the effect of L G , the generation proxy, is revealed by the direct comparisons between (#1-#3) and (#4-#9) of <ref type="table" target="#tab_3">Table 6</ref>. When L D is absent, using L G itself can force the network to learn semantics. On the other hand, L G still contributes even when a strong L D (i.e., BYOL) is present. Interestingly, when we compare #4 against #5 and #6, a random evaluator and an evaluator pre-trained for merely 1 epoch bring accuracy gains of 0.5% and 0.9%, respectively, though larger gains are obtained with stronger evaluators.</p><p>Integrating the above comparisons, we learn the lesson ID that discrimination and generation are indeed complementary in self-supervised visual representation learning -the former is good at learning image-level semantics, and the latter complements it by enforcing view-specific recovery. SaGe goes one step further by amending the goal of generation -instead of recovering every single pixel, it should focus on capturing the semantics of the original image. This is more reasonable, since the encoder often produces a lowdimensional feature vector (e.g., in SaGe, a 512-D vector that is 294? smaller than the input image) -it is expected to lose details, but deemed acceptable if most of semantics have been preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Perceptual Test and Visualization</head><p>In the last part, we investigate the quality of generated images. Besides using different options to encode and decode different views, as we show in <ref type="figure" target="#fig_2">Figure 3</ref>, we also perform a perceptual test. The setting is simple. We sample an image (and also, a view) and feed it through the encoderdecoder architecture. Then, we use a ResNet-50 model that was trained on ImageNet-1K with labels, and check if the   recovered image is correctly classified <ref type="bibr" target="#b2">3</ref> . This is an alternative way to test if the recovered image retains the original semantics. We name this task as the perceptual test -note that it is not possible for purely discrimination-based learning approaches to perform this task. Results are summarized in <ref type="table" target="#tab_4">Table 7</ref>. We first recall the readers that h G (?) contains both an MSE and higher-level loss terms -the MSE loss, though with a small coefficient, is crucial for generating a visually meaningful image. From the comparison between #1 and #5 of <ref type="table" target="#tab_4">Table 7</ref>, we learn that an MSE-absent system is unable to pass the perceptual test with an accuracy barely above the random baseline, though the absence of MSE only causes a slight accuracy drop of 0.3% in the linear test. In contrast, by comparing #2 and #5, we discover another phenomenon that missing the constraints of higher-level similarity results in a moderate drop in both tests. Interestingly, the MSE of #2 is lower than that of #5, indicating that pursuing pixel-level accuracy does not even necessarily benefit the perceptual test. The comparison between #3-#5 reveal an expected conclusion -stronger evaluators produce stronger pre-trained models in both the linear and perceptual tests, while the MSE is mostly unaffected.</p><p>We hope the above studies deliver a new message to the community that the abilities of discrimination and generation are not bound. Although discrimination seems more important for a wide range of visual recognition tasks, we advocate for more attentions to generation which may benefit low-level vision tasks such as image denoising and/or high-quality image synthesis. We enhance the understanding using the examples shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we present SaGe, a self-supervised visual representation learning approach which considers both discrimination and generation. Different from prior work, the generation branch is guided by a semantic-aware evaluator, which facilitates semantically meaningful information to be retained. Both quantitative and qualitative studies, including a perceptual test, validate the favorable ability of SaGe in transfer learning as well as preserving view-specific information. Limitations of this work. SaGe suffers two issues. First, the requirement of a pre-trained evaluator increases the de-sign and computational complexities. Second, the conflict between data augmentation and image-level consistency still exists, yet we are unready to remove the discrimination part. In the future, we will investigate the potential of SaGe in more challenging self-supervised learning scenarios, e.g., masked image modeling <ref type="bibr" target="#b1">[2]</ref> and/or reducing the dimensionality of internal representations, z, towards a higher data compression ratio. original generated <ref type="figure">Figure 4</ref>. The details of the architecture of hG(?). The goal is to reduce the distance of multi-level features between the original and recovered images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details of the Evaluator</head><p>This part complements Sections 3.3, 3.4 and 4.1 in elaborating the implementation of the proposed evaluator.</p><p>Based on a pre-trained evaluator (e.g., ResNet-50 as used in most experiments), we extract multi-level features from both the original and recovered images and compute the 2 loss between them. Specifically, four levels of features are extracted with the spatial resolutions being 28?28, 14?14, 7 ? 7, and 1 ? 1, and the numbers of channels being 512, 1024, 2048, and 2048, respectively. Note that the pixellevel 2 distance is also computed as an additional term, which we refer to as the MSE loss. We denote these losses as L MSE , L 28?28 , L 14?14 , L 7?7 , and L 1?1 , and hence the overall generative loss, L G , is written as</p><formula xml:id="formula_9">L G = ? MSE ? L MSE + ? 28?28 ? L 28?28 + ? 14?14 ? L 14?14 +? 7?7 ? L 7?7 + ? 1?1 ? L 1?1 ,<label>(4)</label></formula><p>where all the balancing coefficients are set to be 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of Data Augmentation</head><p>This part complements the details described in Section 4.1 of the main article.</p><p>During the pre-training stage, we use the same data augmentations with <ref type="bibr" target="#b17">[17]</ref>, including random cropping, leftright flip, color jitting, Gaussian bluring, and solarization. The normalized mean and variance of the image are [0.5, 0.5, 0.5] and [0.5, 0.5, 0.5], which were shown in <ref type="bibr" target="#b42">[42]</ref> to improve the quality of generated (recovered) images.</p><p>During the linear evaluation, we only preserve the center-cropping data augmentation which follows the prac-  <ref type="table">Table 10</ref>. Effect of different modules. The training (or frozen) for decoder represents we update (or freeze) the weights of decoder, and (or ) for MSE, evaluator, and multi-level represent we use them (or not). If multi-level is not used, we only compute the distance between the 1?1 features as part of the loss function. The first and last row correspond to the complete framework (SaGe) and the baseline approach (BYOL, in which there is actually no decoder at all). The accuracy comes from linear evaluation.</p><p>tice of <ref type="bibr" target="#b4">[5]</ref>. Before updating weights, we accumulate the gradient until the number of batches reaches 4,096. The warmup mechanism is also adopted for 10 epochs during training. We set the balancing coefficient, ?, between L D and L G to be 0.1 as default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation on Different Modules</head><p>This part complements the contents described in Section 4.3 of the main article. We follow the setting of Section 4.3 to pre-train all models for 100 epochs and report the linear test accuracy.</p><p>First, we investigate the dimensionality of the 1 ? 1 fea- <ref type="figure">Figure 5</ref>. Visualization of generated images under different settings. The very first column shows four examples from ImageNet-1K, each of which corresponds to a block (bounded by the same color) in the right-hand side. From each original image, we extract three views and they are shown in the 1 st column of each block. The 2nd-4 th columns correspond to three settings of bottleneck dimensionality, namely, 1,024, 512, and 256, respectively. The 5 th column shows the result when multi-level features are not used in the evaluator, and the last (6 th ) column shows that an average pooing, rather than a 7 ? 7 convolution, is applied to obtain the bottleneck vector.</p><p>ture vector that bridges the encoder and decoder. We name it the bottleneck representation as it is the most compact vector that propagates the input information to the decoder for image recovery. We test three settings, with the dimensionality of bottleneck representation being 1,024, 512, and 256, respectively. From the viewpoint of data compression, we can approximately compute the compression ratio of these settings being 224 ? 224 ? 3 (the image pixels) divided by the bottleneck dimensionality <ref type="bibr" target="#b3">4</ref> in <ref type="table">Table 8</ref>, we show the effect of different settings. SaGe is generally robust to the change of settings, yet a proper bottleneck dimensionality (e.g., 512) produces slightly better accuracy. We show the generated images under different compression ratios in <ref type="figure">Figure 5</ref>.</p><p>Next, we investigate the architecture of the decoder by changing the numbers of channels in each layer. Results are shown in <ref type="table" target="#tab_8">Table 9</ref>. Though the decoder serves as an auxiliary network (i.e., it is discarded when the encoder is transferred to the downstream tasks), increasing its size and thus the number of parameters still benefits the test accuracy. The default setting used in the main article (# 2) achieves a good tradeoff between the training complexity and performance. In comparison, a larger decoder with almost doubled parameters achieves slightly better accuracy, while a smaller <ref type="bibr" target="#b3">4</ref> we can quantify each entry of the bottleneck vector into a 8-bit floating point number which merely impacts the quality of image recovery. decoder with 30% fewer parameters works worse.</p><p>Lastly, we analyze the effects of different modules in our framework and validate the effectiveness of our design. The ablation options include freezing the decoder (i.e., not optimizing it during the entire pre-training stage), not using the MSE loss term (L MSE ), not using the higher-level features of the evaluator (i.e., only using L MSE ), and not using multi-level features for evaluator (i.e., only using L MSE and L 1?1 ). As shown in <ref type="table">Table 10</ref>, each ablation contributes accuracy drop of different extents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Results on COCO</head><p>This part complements the MS-COCO experiments shown in Section 4.2 of the main article.</p><p>We perform object detection and instance segmentation results on the MS-COCO dataset with the 2? schedule, and report the results together with that of 1? schedule in <ref type="table" target="#tab_9">Table 11</ref>. As in other downstream transfer tasks, SaGe still shows favorable performance in the 2? schedule. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The framework of SaGe.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>An example showing that three candidates which are comparable in terms of pixel-level similarity but preserve significantly different extents of semantics. The pixel-level distance is measured by MSE, while the semantic-level distance is MSE calculated upon features extracted from the penultimate layer (2,048D) of a pre-trained model by BYOL<ref type="bibr" target="#b17">[17]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Examples of generated images. The very first column shows four examples from ImageNet-1K, each of which corresponds to a block (bounded by the same color) in the right-hand side. In each block, the first column shows three views sampled from the corresponding image, and the remaining five columns correspond to the five options in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Method Mask R-CNN, R50-FPN, detection Mask R-CNN, R50-FPN, segmentation AP bb AP bb 50 AP bb 75 APS APM APL AP mk AP mk 50</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AP mk 75</cell><cell cols="2">APS APM APL</cell></row><row><cell cols="2">Supervised</cell><cell></cell><cell>38.9</cell><cell>59.6</cell><cell>42.0</cell><cell cols="3">23.0 42.9 49.9</cell><cell>35.4</cell><cell>56.5</cell><cell>38.1</cell><cell cols="2">17.5 38.2 51.3</cell></row><row><cell cols="2">MoCo v2 [7]</cell><cell></cell><cell>39.2</cell><cell>59.9</cell><cell>42.7</cell><cell cols="3">23.8 42.7 50.0</cell><cell>35.7</cell><cell>56.8</cell><cell>38.1</cell><cell cols="2">17.8 38.1 50.5</cell></row><row><cell cols="2">BYOL [17]</cell><cell></cell><cell>39.9</cell><cell>60.2</cell><cell>43.2</cell><cell cols="3">23.3 43.2 52.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">HCL [24]</cell><cell></cell><cell>40.0</cell><cell>60.6</cell><cell>43.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.4</cell><cell>57.6</cell><cell>39.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">DenseCL [47]</cell><cell></cell><cell>40.3</cell><cell>59.9</cell><cell>44.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.4</cell><cell>57.0</cell><cell>39.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">self-EMD [36]</cell><cell></cell><cell>40.0</cell><cell>60.4</cell><cell>44.0</cell><cell cols="3">23.5 43.8 52.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ORL [51]</cell><cell></cell><cell>40.3</cell><cell>60.2</cell><cell>44.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.3</cell><cell>57.3</cell><cell>38.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">SaGe (300 epochs)</cell><cell>40.2</cell><cell>61.7</cell><cell>43.8</cell><cell cols="3">23.8 44.1 51.9</cell><cell>36.7</cell><cell>58.3</cell><cell>39.2</cell><cell cols="2">17.6 39.5 52.5</cell></row><row><cell cols="3">SaGe (800 epochs)</cell><cell>40.8</cell><cell>62.4</cell><cell>44.8</cell><cell cols="3">25.1 44.5 52.5</cell><cell>37.2</cell><cell>59.0</cell><cell>40.1</cell><cell cols="2">18.5 40.0 53.2</cell></row><row><cell></cell><cell></cell><cell cols="11">Table 5. Object detection and instance segmentation APs (%) on the MS-COCO dataset.</cell></row><row><cell>ID</cell><cell>L D</cell><cell>h G (?)</cell><cell>L G</cell><cell>epochs</cell><cell cols="2">Top-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell cols="3">ResNet-18</cell><cell>random</cell><cell></cell><cell>21.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell cols="3">ResNet-50</cell><cell>300</cell><cell></cell><cell>70.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>66.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5</cell><cell cols="3">ResNet-18</cell><cell>random</cell><cell></cell><cell>67.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6</cell><cell cols="3">ResNet-18</cell><cell>1</cell><cell></cell><cell>67.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7</cell><cell cols="3">ResNet-18</cell><cell>100</cell><cell></cell><cell>68.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8</cell><cell cols="3">ResNet-18</cell><cell>300</cell><cell></cell><cell>70.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Classification accuracy (%) of linear evaluation under different configurations, where #4 is the BYOL baseline (100 epochs), and #9 is the complete version of SaGe.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>The accuracy (%) of linear classification and perceptual test as well as the MSE for different models. All the entries are built upon the BYOL baseline, i.e., LD is present. BYOL, producing a 66.8% linear classification accuracy, is not displayed since it does not generate images for the perceptual test.</figDesc><table><row><cell></cell><cell>L G</cell><cell></cell><cell>Top-1</cell><cell>Top-5</cell><cell></cell></row><row><cell>MSE</cell><cell cols="2">higher levels</cell><cell>linear</cell><cell>precept.</cell><cell>MSE</cell></row><row><cell>loss</cell><cell cols="2">h G (?) epochs</cell><cell>acc.</cell><cell>acc.</cell><cell></cell></row><row><cell>1</cell><cell>R-50</cell><cell>300</cell><cell>71.6</cell><cell>0.5</cell><cell>0.3435</cell></row><row><cell>2</cell><cell></cell><cell>-</cell><cell>67.2</cell><cell>40.5</cell><cell>0.0297</cell></row><row><cell>3</cell><cell cols="3">R-18 random 67.3</cell><cell>42.6</cell><cell>0.0290</cell></row><row><cell>4</cell><cell>R-18</cell><cell>300</cell><cell>70.1</cell><cell>50.4</cell><cell>0.0287</cell></row><row><cell>5</cell><cell>R-50</cell><cell>300</cell><cell>71.9</cell><cell>52.3</cell><cell>0.0285</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>, namely, without MSE, MSE only, MSE with random R-18, MSE with pre-trained R-18, and MSE with pre-trained R-50, respectively. Green and red dots indicate that the corresponding case passes the perceptual test or not, and the MSE is offered below each case. This figure is best viewed by zooming into details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Results of using different decoder networks. The channel item in the table denotes the initial channel number, which decreases by a factor of 2 in each layer. The corresponding spatial resolution at these layers are 7 ? 7, 14 ? 14, 28 ? 28, 56 ? 56, 112?112, and 224?224, respectively. The number of parameters is only for the decoder. The accuracy comes from linear evaluation.</figDesc><table><row><cell></cell><cell></cell><cell>dim.</cell><cell>comp. ratio</cell><cell>Acc. (%)</cell></row><row><cell></cell><cell></cell><cell>1024</cell><cell>147</cell><cell>71.8</cell></row><row><cell></cell><cell></cell><cell>512</cell><cell>294</cell><cell>71.9</cell></row><row><cell></cell><cell></cell><cell>256</cell><cell>588</cell><cell>71.7</cell></row><row><cell cols="5">Table 8. Results of different settings of the bottleneck vector,</cell></row><row><cell cols="5">which also corresponds to different compression ratios. The ac-</cell></row><row><cell cols="4">curacy comes from linear evaluation.</cell></row><row><cell>ID</cell><cell></cell><cell cols="2">channel</cell><cell>param. (M) Acc. (%)</cell></row><row><cell>1</cell><cell cols="3">[1024-512-256-128-64-3]</cell><cell>21.8</cell><cell>72.0</cell></row><row><cell>2</cell><cell cols="3">[512-256-128-64-32-3]</cell><cell>11.3</cell><cell>71.9</cell></row><row><cell>3</cell><cell cols="3">[256-128-64-32-16-3]</cell><cell>8.2</cell><cell>71.7</cell></row><row><cell cols="2">decoder</cell><cell>MSE</cell><cell cols="2">evaluator multi-level</cell><cell>Acc. (%)</cell></row><row><cell cols="2">training</cell><cell></cell><cell></cell><cell>71.9</cell></row><row><cell cols="2">frozen</cell><cell></cell><cell></cell><cell>70.5</cell></row><row><cell cols="2">training</cell><cell></cell><cell></cell><cell>71.6</cell></row><row><cell cols="2">training</cell><cell></cell><cell></cell><cell>-</cell><cell>67.2</cell></row><row><cell cols="2">training</cell><cell></cell><cell></cell><cell>71.6</cell></row><row><cell cols="2">frozen</cell><cell></cell><cell></cell><cell>66.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 .</head><label>11</label><figDesc>Object detection and instance segmentation APs (%) on the MS-COCO dataset. The results of 1? training schedule have been reported in the main article and we supplement the results of 2? training schedule for comparison.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Mask R-CNN, R50-FPN, detection</cell><cell></cell><cell></cell><cell cols="4">Mask R-CNN, R50-FPN, segmentation</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">1? schedule</cell><cell></cell><cell cols="2">2? schedule</cell><cell></cell><cell cols="2">1? schedule</cell><cell></cell><cell cols="2">2? schedule</cell></row><row><cell></cell><cell cols="2">AP bb AP bb 50</cell><cell>AP bb 75</cell><cell cols="2">AP bb AP bb 50</cell><cell>AP bb 75</cell><cell cols="2">AP mk AP mk 50</cell><cell>AP mk 75</cell><cell cols="2">AP mk AP mk 50</cell><cell>AP mk 75</cell></row><row><cell>Supervised</cell><cell>38.9</cell><cell>59.6</cell><cell>42.7</cell><cell>38.9</cell><cell>59.6</cell><cell>42.0</cell><cell>35.4</cell><cell>56.5</cell><cell>38.1</cell><cell>35.4</cell><cell>56.5</cell><cell>38.1</cell></row><row><cell>MoCo v1 [18]</cell><cell>38.5</cell><cell>58.9</cell><cell>42.0</cell><cell>40.8</cell><cell>61.6</cell><cell>44.7</cell><cell>35.1</cell><cell>55.9</cell><cell>37.7</cell><cell>36.9</cell><cell>58.4</cell><cell>39.7</cell></row><row><cell>MoCo v2 [7]</cell><cell>39.2</cell><cell>59.9</cell><cell>42.7</cell><cell>41.6</cell><cell>62.1</cell><cell>45.6</cell><cell>35.7</cell><cell>56.8</cell><cell>38.1</cell><cell>37.7</cell><cell>59.3</cell><cell>40.6</cell></row><row><cell>HCL [24]</cell><cell>40.0</cell><cell>60.6</cell><cell>43.8</cell><cell>41.8</cell><cell>62.4</cell><cell>45.7</cell><cell>36.4</cell><cell>57.6</cell><cell>39.1</cell><cell>37.8</cell><cell>59.5</cell><cell>40.8</cell></row><row><cell>DenseCL [47]</cell><cell>40.3</cell><cell>59.9</cell><cell>44.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.4</cell><cell>57.0</cell><cell>39.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>self-EMD [36]</cell><cell>40.0</cell><cell>60.4</cell><cell>44.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ORL [51]</cell><cell>40.3</cell><cell>60.2</cell><cell>44.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.3</cell><cell>57.3</cell><cell>38.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">SaGe (300 epochs) 40.2</cell><cell>61.7</cell><cell>43.8</cell><cell>41.9</cell><cell>63.0</cell><cell>45.7</cell><cell>36.7</cell><cell>58.3</cell><cell>39.2</cell><cell>38.0</cell><cell>59.9</cell><cell>40.8</cell></row><row><cell cols="2">SaGe (800 epochs) 40.8</cell><cell>62.4</cell><cell>44.8</cell><cell>42.3</cell><cell>63.6</cell><cell>46.1</cell><cell>37.2</cell><cell>59.0</cell><cell>40.1</cell><cell>38.3</cell><cell>60.4</cell><cell>41.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">It is also possible that the decoder does not work well to recover x from z, but provided that in our design, the decoder is relatively lightweight (see Section 4.1), we mainly locate the issue in the encoder.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use top-5 accuracy because the recovered images often suffer information loss in details, which causes accuracy drop especially for finegrained recognition.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15535" to="15545" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<editor>Jill Burstein, Christy Doran, and Thamar Solorio</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belghazi</surname></persName>
		</author>
		<imprint>
			<pubPlace>Ben Poole, Alex Lamb, Mart?n Arjovsky, Olivier Mastropietro, and Aaron C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo?vila</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>H?naff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida D</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Reducing the dimensionality of data with neural networks. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Heterogeneous contrastive learning: Encoding spatial information for compact visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning using new complete layer-wise adaptive rate scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouyuan</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7883" to="7890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Generative models as a data source for multiview representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05258</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio and Yann LeCun</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mean shift for self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koohpayegani</forename><surname>Soroush Abbasi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07269</idno>
	</analytic>
	<monogr>
		<title level="m">Ajinkya Tejankar, and Hamed Pirsiavash</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Efficient self-supervised vision transformers for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09785</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09962</idno>
		<title level="m">Rogerio Feris, Piotr Indyk, and Dina Katabi. Making contrastive learning robust to shortcuts</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Self-emd: Selfsupervised object detection without imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13677</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin Raffel ; Chun-Liang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alexey Kurakin</title>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Isd: Selfsupervised learning by iterative similarity distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajinkya</forename><surname>Tejankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Soroush Abbasi Koohpayegani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9609" to="9618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3024" to="3033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Iterative reorganization with weak spatial constraints: Solving arbitrary jigsaw puzzles for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingda</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1910" to="1919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Detco: Unsupervised contrastive learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8392" to="8401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Yew Soon Ong, and Chen Change Loy. Unsupervised object-level representation learning from scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11952</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16684" to="16693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Deny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12310" to="12320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Aet vs. aed: Unsupervised representation learning by autoencoding transformations rather than data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2547" to="2555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1058" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Preservational learning improves selfsupervised medical image models by reconstructing diverse contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chixiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3499" to="3509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
