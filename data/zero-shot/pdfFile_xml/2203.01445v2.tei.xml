<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LILE: Look In-Depth before Looking Elsewhere -A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022">2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danial</forename><surname>Maleki</surname></persName>
							<email>dmaleki@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Kimia Lab</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Tizhoosh</surname></persName>
							<email>tizhoosh.hamid@mayo.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Kimia Lab</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Artificial Intelligence and Informatics</orgName>
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<settlement>Rochester</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LILE: Look In-Depth before Looking Elsewhere -A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Accepted for Publication in MIDL</title>
						<imprint>
							<biblScope unit="volume">2022</biblScope>
							<biblScope unit="page" from="1" to="16"/>
							<date type="published" when="2022">2022</date>
						</imprint>
					</monogr>
					<note>? Corresponding author</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cross-Modal Retrieval</term>
					<term>Histopathology</term>
					<term>Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The volume of available data has grown dramatically in recent years in many applications. Furthermore, the age of networks that used multiple modalities separately has practically ended. Therefore, enabling bidirectional cross-modality data retrieval capable of processing has become a requirement for many domains and disciplines of research. This is especially true in the medical field, as data comes in a multitude of types, including various types of images and reports as well as molecular data. Most contemporary works apply cross attention to highlight the essential elements of an image or text in relation to the other modalities and try to match them together. However, regardless of their importance in their own modality, these approaches usually consider features of each modality equally. In this study, self-attention as an additional loss term will be proposed to enrich the internal representation provided into the cross attention module. This work suggests a novel architecture with a new loss term to help represent images and texts in the joint latent space. Experiment results on two benchmark datasets, i.e. MS-COCO and ARCH, show the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A plethora of real-world applications is emerging due to the rapid expansion in the volume and variety of data on the one hand and the recent success of deep learning on the other hand. However, the majority of existing models can only work with a single modality of data, e.g., only analyzing images or text. This restriction on the data source prevents the models from having a more generalizable and robust problem representation for downstream tasks. The absence of a model that can be used across several modalities and that can use information from multiple sources of data is in great demand and has become increasingly common in both academia and industry. One task that has gained popularity over the years is retrieving the relation between one of the data modalities; a task called "cross-modality retrieval". This task seeks to bring a pair of data from two distinct sources together by recognizing their intrinsic relationship. Because such cross-modal topologies need to learn the inter-modal correspondence and modality representation separately, their design and training can be rather challenging. Image and text are the most extensively utilized and commonly observed modalities in real-world data. However, The diversity in information sources can be observed more commonly in the medical field. Images, reports and molecular data in patients' medical records are different and indispensable sources of information. A model able to retrieve a source of data when a different one is available can help provide complete information about the patient.</p><p>The common solution to approach to explore the relationship between image and text is to map the visual semantic embeddings <ref type="bibr" target="#b26">(Karpathy and Fei-Fei, 2015;</ref><ref type="bibr" target="#b17">Frome et al.)</ref> of an image and the corresponding words/phrases/sentences into a common latent embedding space <ref type="bibr" target="#b7">(Barnard et al., 2003;</ref><ref type="bibr" target="#b9">Berg et al., 2004;</ref><ref type="bibr" target="#b46">Socher and Fei-Fei, 2010;</ref><ref type="bibr" target="#b12">Chong et al., 2009)</ref>. In these methods, the goal is generally to find a common space where the corresponding representations of images and text are as close as possible, hence making recognizing their relationship easier. Recent studies investigate the use of the attention mechanism to understand the similarity between the two modalities. The majority of studies in this category have employed cross-attention mechanisms, which allow the model to selectively attend to the parts of an instance that are relevant to the context from the other modal <ref type="bibr" target="#b31">(Lee et al., 2018;</ref><ref type="bibr" target="#b21">Huang et al., 2017)</ref>. Other methods attempt to refine their representation regarding the information of the other modal <ref type="bibr" target="#b11">(Chen et al., 2020)</ref>. Nonetheless, due to the gap between the representation of images and texts, these methods may not find the optimal representation. Moreover, ambiguity in text documents is a common challenge posing a learning obstacle to the model if it uses an injective embedding. Aside from that, humans use a hierarchical structure to organize and store diverse semantic concepts. However, the majority of the currently available approaches group semantics together in a consistent manner <ref type="bibr" target="#b31">(Lee et al., 2018;</ref><ref type="bibr" target="#b11">Chen et al., 2020;</ref><ref type="bibr" target="#b47">Song and Soleymani, 2019)</ref>.</p><p>In this study, the mentioned issues are addressed by providing an iterative regime to capture related information in the other modality and extract the most significant features by looking into its context. It can help the model capture the information related to the other modality and itself simultaneously. Accordingly, such an approach can help extract richer latent embeddings for each instance. Moreover, training the model in an iterative regime can help the model to capture higher-level features gradually based on the features acquired in the previous steps. Furthermore, refining the extracted features based on the intersection between modalities can help the model to enrich its representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Literature Review</head><p>Many works on cross-modality retrieval have been looking for a similar pattern in the feature representation of both modalities <ref type="bibr" target="#b26">(Karpathy and Fei-Fei, 2015;</ref><ref type="bibr" target="#b31">Lee et al., 2018;</ref><ref type="bibr" target="#b22">Huang et al., 2018;</ref><ref type="bibr" target="#b38">Lu et al., 2020;</ref><ref type="bibr" target="#b43">Radford et al., 2021)</ref>. These studies can generally be categorized into global and local feature matching approaches.</p><p>Global feature matching methods were among first attempts to solve the image-text retrieval task. <ref type="bibr" target="#b30">Kiros et al. (Kiros et al., 2014)</ref> proposed to use a CNN and a RNN to encode given images and text data. For the loss function, they proposed an idea to implement a pairwise ranking loss objective to rank images and correspondence descriptions. Faghri et al. <ref type="bibr" target="#b16">(Faghri et al., 2017)</ref> modified the loss function with hard negatives in the triplet loss function and applied on a similar architecture to <ref type="bibr" target="#b30">(Kiros et al., 2014)</ref>. Furthermore, by improving the quality of generative models <ref type="bibr" target="#b56">(Zhu et al., 2017;</ref><ref type="bibr" target="#b28">Karras et al., 2019)</ref>, some studies have attempted to exploit the capability of generative models to enhance the performance of proposed methods <ref type="bibr" target="#b40">(Patrick et al., 2020;</ref><ref type="bibr" target="#b41">Peng and Qi, 2019;</ref><ref type="bibr" target="#b19">Gu et al., 2018)</ref>.</p><p>Local feature matching methods consider the correspondence between image and text feature representations at the level of image regions and words. <ref type="bibr" target="#b26">Karpathy et al. (Karpathy and Fei-Fei, 2015)</ref> proposed a method to extract features for image regions at object level using Faster R- <ref type="bibr">CNN (Ren et al., 2015)</ref> and text words, and then align them into a common space. However, as the application of the attention mechanism in a variety of tasks has become more common, several approaches in image-text retrieval seek to leverage the attention mechanism <ref type="bibr" target="#b11">(Chen et al., 2020;</ref><ref type="bibr" target="#b31">Lee et al., 2018;</ref><ref type="bibr" target="#b47">Song and Soleymani, 2019;</ref><ref type="bibr">Thomas and Kovashka, 2020;</ref><ref type="bibr" target="#b31">Lee et al., 2018)</ref>. Yale et al. <ref type="bibr" target="#b47">(Song and Soleymani, 2019)</ref> proposed a method to combine global features with locally-guided features via a multi-head self-attention module. Christopher et al. <ref type="bibr">(Thomas and Kovashka, 2020)</ref> in a similar network proposed a novel within-modality loss function that drives feature representation toward more coherence. The majority of these methods rely on the premise that vision and text are mutually exclusive and equally important. However, grounding (or base) representation of each modality derived from another modality to finer details is critical for bridging the gap between vision and text modality. Kuang-Huei <ref type="bibr" target="#b31">(Lee et al., 2018)</ref> was one of the first studies that investigated the use of cross-attention to explore full latent matching using image regions and words in the text as context.</p><p>Furthermore, Hui et al. in IMRAM paper <ref type="bibr" target="#b11">(Chen et al., 2020)</ref> presented an iterative alignment method that captures the fine-grained correspondence between image and text progressively using a cross-attention module. By the development of Transformer models, serious approaches were proposed for cross-modality retrieval task using Transformers <ref type="bibr" target="#b37">(Lu et al., 2019;</ref><ref type="bibr" target="#b48">Tan and Bansal, 2019)</ref>. Authors in <ref type="bibr" target="#b37">(Lu et al., 2019)</ref> proposed a network architecture that consists of two single-modal Transformer networks applied on input images and texts data respectively.</p><p>Recent studies have shown that training on large-scale datasets could help models to achieve significant improvement in image-text retrieval task <ref type="bibr" target="#b53">(Li et al., 2020;</ref><ref type="bibr" target="#b43">Radford et al., 2021;</ref><ref type="bibr" target="#b24">Jia et al., 2021)</ref>. Jia et al. in ALIGN paper <ref type="bibr" target="#b24">(Jia et al., 2021</ref>) leveraged a dataset of over one billion paired image and alt-text data and claimed that a simple dual-encoder network could learn to align image and text representation using a contrastive loss. Similar to ALIGN paper, Xiujun et al. in Oscar paper <ref type="bibr" target="#b53">(Li et al., 2020)</ref> collected a dataset over 6.5 million paired of image and text to train their network. Another example for boosting the performance using a large amount of data can be CLIP <ref type="bibr" target="#b43">(Radford et al., 2021)</ref> paper. Authors in CLIP paper also demonstrated that the task of image-text retrieval could be outperformed using a massive dataset, 400 million in this case. However, a bottleneck of these large-dataset-dependent approaches emerges in those domains which are unlikely to be included in their collected dataset, such as histopathology domain. Moreover, training on that amount of data requires huge computational resources.</p><p>Cross-Modality Retrieval is new to the field of histopathology. There are not many studies that explore retrieving a description correspondence to an image or vice versa. As a result, a few works were trying to retrieve or utilize image-text modalities in their suggested methods. Zhang et al. <ref type="bibr" target="#b55">(Zhang et al., 2017)</ref> was among the first authors who proposed a model and dataset for image-text in pathology. Authors proposed a new framework, namely MDNet, to establish a direct multi-modal mapping between images and diagnostic reports. Moreover, they developed a private patch-based dataset based on pathology bladder cancer images. For descriptions, they asked pathologists to write 5 short sentences for each patch.</p><p>The proposed method is capable of generating short diagnostic reports and retrieving images based on symptom description. Yet the developed dataset was too small, private and limited to only H&amp;E images. Recently, <ref type="bibr" target="#b18">Gamper et al. (Gamper and Rajpoot, 2021</ref>) developed a new publicly available dataset, ARCH, that contains more than 7,000 patch-based images with the corresponded captions. The authors of the paper used PubMed medical articles (Pub) and academic pathology textbooks to collect the ARCH dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The proposed model, as demonstrated in <ref type="figure" target="#fig_0">Figure 1</ref> includes different components. LILE, a dual attention network using Transformers, takes images and texts as inputs and extracts feature representations for each of those using Transformers. Then, a self-attention module <ref type="bibr" target="#b52">(Vaswani et al., 2017)</ref>, which is applied for extracting the most significant parts of each modality with respect to itself, is applied. For the next step, a cross-attention module and gated memory block are applied to help the model refine the representation of each instance with respect to the outputs of attention modules. Additionally, an iterative matching scheme using a gated memory block is applied to refine the extracted features for each modality. Data representation, or representation learning <ref type="bibr" target="#b8">(Bengio et al., 2013)</ref>, is a collection of approaches in ML that allows a model to automatically discover the representations required from the given data. Recently, Transforms become more prevalent for representation learning. the Transformers' modular architecture enables the processing of different modalities (e.g., images, videos, text, and voice) leveraging similar processing blocks. It scales efficiently to huge capacity networks for complex tasks and performs well with massive datasets. Transformer architecture is chosen for text and image feature extractors in this study due to these advantages.</p><p>Image Representation: In this study, a pre-trained ViT <ref type="bibr" target="#b15">(Dosovitskiy et al., 2020</ref>) is implemented to encode input images . Each image is split into a sequence of fixed-size, nonoverlapping image patches before being fed to the ViT. Utilizing object detection models is another method for extracting visual information from an image. These models are trained on the input image in order to extract the objects. As a result, the input image can be represented as a set of extracted feature maps for all of the objects in the image <ref type="bibr" target="#b31">(Lee et al., 2018;</ref><ref type="bibr" target="#b11">Chen et al., 2020)</ref>. Depending on the dataset and the availability of annotations for the object detection task, one of these approaches is used in this study.</p><p>Text Representation: Text representation can be defined as a method for encoding sentences into vectors. Transformers have recently emerged as the top-performing solution for the majority of NLP tasks and outperform many other approaches <ref type="bibr" target="#b36">(Liu et al., 2019;</ref><ref type="bibr" target="#b42">Radford et al., 2019;</ref><ref type="bibr">Usi)</ref>. In this study, a pre-trained "Roberta" architecture <ref type="bibr" target="#b36">(Liu et al., 2019)</ref> which is a Transform-based model is deployed to encode the input text. It receives a text description as an input and returns a feature map that represents the given data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention and Gated Memory Blocks</head><p>After the representation for each modality instance has been extracted, a multi-head selfattention module is applied to obtain m enhanced feature maps for extracted features from the previous stage similar to the <ref type="bibr" target="#b53">(Wei et al., 2020)</ref>. These m representations highlight the most significant features of each modality in relation to itself.</p><p>A cross-attention mechanism is utilized to attend to distinct parts of one modality given the context of another modality as proposed by authors in <ref type="bibr" target="#b11">(Chen et al., 2020;</ref><ref type="bibr" target="#b53">Wei et al., 2020)</ref>. The application of cross-attention in the proposed approach is to attend to image regions regarding the text input tokens and vice versa. The gated memory block seeks to refine the extracted features from each modality considering the cross-attention feature maps. The input modalities are denoted as X = {x i |i ? <ref type="bibr">[1, m]</ref></p><formula xml:id="formula_0">, x i ? R d } and Y = {y i |i ? [1, m], y i ? R d } which X</formula><p>and Y can be either image or text features, where m is the number of attention heads in the multi-head self-attention module in the previous step. The cross-attention module helps to have the highlighted information for modality X related to modality Y . To achieve this goal, a suitable measurement is needed to quantify the similarity between each feature map and feature maps in another modality.</p><p>In the suggested solution, the cosine similarity is applied <ref type="bibr" target="#b23">(Ji et al., 2019;</ref><ref type="bibr" target="#b24">Jia et al., 2021;</ref><ref type="bibr" target="#b31">Lee et al., 2018)</ref>. The similarity between each instance in modality X and Y is determined as Eq.1, where s ij denotes the similarity between the i-th feature map for modality X and the j-th from modality Y. Furthermore, it is beneficial to threshold the similarity at 0 and normalize it <ref type="bibr" target="#b27">(Karpathy et al., 2014;</ref><ref type="bibr" target="#b31">Lee et al., 2018)</ref>.</p><formula xml:id="formula_1">s ij = x T i y j ||x i || ? ||y j || , ?i ? [1, m], ?j ? [1, m]s ij = max(0, s ij ) m i=1 max(s ij ) 2 .<label>(1)</label></formula><p>To attend on set Y with respect to a given feature x i in X, a weighted combination of y j is defined. The definition of attention function in Eq.1 is a variant of the "dot product attention" that is commonly used in other studies <ref type="bibr" target="#b39">(Luong et al., 2015)</ref>:</p><formula xml:id="formula_2">a x i = m j=1 ? ij y j , ? ij = exp(?s ij ) m j=1 exp(?s ij )</formula><p>.</p><p>(2)</p><p>In Eq.2, the parameter ? is the inverse temperature of the softmax function <ref type="bibr" target="#b13">(Chorowski et al., 2015)</ref>. A more smooth attention function can be achieved by adjusting ?. A x is defined as {a x i ? <ref type="bibr">[1, m]</ref>, a x i ? R d } where each element of A x captures significant parts of each x i given the whole Y set as context. To refine the extracted features of X regarding the important parts of each x i given Y , a memory unit has been used. It would dynamically update and refine the feature maps of X by looking to both A x and X as Eq.3, where f (?) can be defined differently <ref type="bibr" target="#b25">(Kalra et al., 2020;</ref><ref type="bibr" target="#b54">Weston et al., 2014;</ref><ref type="bibr" target="#b10">Burtsev et al., 2020)</ref>. In this study, a gated mechanism has been adopted for f (?) as follows:</p><formula xml:id="formula_3">x * = f (x i , a x i ).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Iterative Matching</head><p>As stated in section 3.2, gated memory can assist the model in refining the feature representation regarding the shared information between two modalities as proposed by authors in <ref type="bibr" target="#b11">(Chen et al., 2020)</ref>. Having the gated memory in an iterative scheme can be beneficial as each iteration step, with the help of A x feature representation of X can be re-calibrated. The iterative scheme can be summarized as Eq.4, where k is the iteration step that will be performed to refine the alignment for the next iteration:</p><formula xml:id="formula_4">X * k = Memory(X k?1 , A x ).<label>(4)</label></formula><p>At iteration step k, the similarity score between image I and text T is calculated as follows:</p><formula xml:id="formula_5">S k (I, T ) = ? 1 m m i=1 S (v,v? ?T ) k (v i , T ) + 1 m m i=1 S (w,w? ?I) k (I, w i ) + (1 ? ?) 1 m m i=1 S (v,T ) k (v i , T ) + 1 m m i=1 S (w,I) k (I, w i )<label>(5)</label></formula><p>where ? is a learnable scalar weight parameter that balances the influence of the similarity score terms. S (v,v? ?T ) (v i , T ) and S (w,w? ?I) (I, w i ) are defined as similarity score between image regions and text T and text tokens and image I respectively proposed by authors in <ref type="bibr" target="#b11">(Chen et al., 2020)</ref> . These similarity scores are derived as</p><formula xml:id="formula_6">S (v,v? ?T ) k (v i , T ) = sim(v i , A v k ), S (w,w? ?I) k (I, w i ) = sim(A t k , w i )<label>(6)</label></formula><p>The similarity score can be boosted by including directly the similarity between image and text as S (v,T ) (v i , T ) = sim(v i , T ) and S (w,I) (I, w i ) = sim(I, w i ).</p><p>This can assist the model in preserving the semantic meaning of each instance while it attempts to bring paired instances closer together. To put all k steps together, the similarity score between image I and text T will be derived as Eq.7, where k is the number of matching steps that will be set as a hyper-parameter. See appendix D. S k (I, T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S(I, T ) =</head><p>(7)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>In this study, N-pairs bi-directional triplet-loss is implemented.</p><formula xml:id="formula_7">L = [? ? S(I i , T i ) + S(I i , T j )] + + [? ? S(I i , T i ) + S(I j , T i )] + .<label>(8)</label></formula><p>In Eq.8, ? is a margin value and [x] + = max(0, x). The term S(I, T ) is defined in Eq.7 and measures the similarity between image I and text T . This similarity score forms a similarity score matrix S of size n ? n, where S is symmetric, See appendix A. In the training phase, n is the size of mini-batch. Images and text with a same subscript are paired instances which means the diagonal of the matrix should have the largest value compared to the other indices. This regime will be trained in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We used two benchmark datasets to evaluate the effectiveness of the proposed method:</p><p>? MS-COCO <ref type="bibr" target="#b35">(Lin et al., 2014)</ref>, a large-scale object detection, segmentation, key-point detection, and captioning dataset, and</p><p>? ARCH <ref type="bibr" target="#b18">(Gamper and Rajpoot, 2021)</ref>, a computational pathology multiple-instance captioning dataset, see appendix B.</p><p>To compare the proposed method with other approaches, Recall at K (R@K) is used, which measures the fraction of queries retrieved correctly among top K search results <ref type="bibr" target="#b11">(Chen et al., 2020;</ref><ref type="bibr" target="#b31">Lee et al., 2018)</ref>. To show the efficiency of the proposed method, the reported results also includes "R@sum" which is the summation of evaluation at different K, i.e., R@1+R@5+R@10 as in <ref type="bibr" target="#b21">(Huang et al., 2017)</ref>. The results are shown in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> for MS-COCO and ARCH datasets respectively. The proposed method, called LILE, outperformed the previous best model, i.e., IMRAM <ref type="bibr" target="#b11">(Chen et al., 2020)</ref>, by a large margin of 6.3 and 8.3 in terms of overall performance R@sum in MS-COCO(1K) and MS-COCO(5K) datasets, respectively. Because there is no previously reported results on the ARCH dataset, publicly available methods like IMRAM <ref type="bibr" target="#b11">(Chen et al., 2020)</ref>, PVSE <ref type="bibr" target="#b47">(Song and Soleymani, 2019)</ref> and CLIP <ref type="bibr" target="#b43">(Radford et al., 2021)</ref> were selected and modified to evaluate them on the ARCH dataset. LILE significantly outperformed the best previous best model, i.e., IMRAM, with a large margin of 28.1 in terms of overall performance R@sum. These results demonstrate the effectiveness of the proposed approach for the cross-modality retrieval task. The implementation details described in appendix C.</p><p>The cross-modal retrieval task can benefit greatly from the use of large amounts of collected data and powerful computational resources. Consequently, methods trained on large-scale datasets (i.e., CLIP <ref type="bibr" target="#b42">(Radford et al., 2019)</ref>, ALIGN <ref type="bibr" target="#b24">(Jia et al., 2021)</ref>, and OSCAR <ref type="bibr" target="#b53">(Li et al., 2020)</ref>) can not be directly compared to methods other methods and LILE which trained exclusively on smaller datasets such as MS-COCO. The values for those methods trained on massive datasets are reported to highlight latter point.</p><p>Additionally, the results on ARCH dataset show even more improvement compared to MS-COCO results. One of the reasons that may have influenced this enhancement could be the use of Transformer for encoding the input data as IMRAM <ref type="bibr" target="#b47">(Song and Soleymani, 2019)</ref> and PVSE <ref type="bibr" target="#b47">(Song and Soleymani, 2019)</ref> applied GRU for their text encoding. Moreover, analyzing the results on ARCH dataset can prove that methods like CLIP <ref type="bibr" target="#b43">(Radford et al., 2021)</ref> which have been trained on massively large datasets cannot perform very well on tasks like image-text retrieval in pathology. This type of tasks are specialized and apparently need more sophisticated methods like the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>The proposed cross-modality approach can simultaneously capture the most salient features of each modality in relation to itself and other modalities. A multi-head self-attention module was used to aid the network in gaining a better understanding of each modality. Meanwhile, to assist the model in identifying all potential alignments between two modalities, the multi-head self-attention module output is fed into a cross-attention module. This approach can aid the model in adjusting retrieved features from one modality considering the effect of the other one. Additionally, the suggested novel loss objective can assist the model in determining the optimal weight to balance both implicit and explicit sources of information used to match paired instances in different modalities. Experiments on the MS-COCO and ARCH datasets demonstrated the efficiency of LILE in terms of recall, a typical metric for retrieval tasks in both general-purpose and histopathology contexts. shows the similarity score between image I i and text T j . As the paired data has a same subscript, similarity matrix should be diagonal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. ARCH dataset</head><p>ARCH dataset <ref type="bibr" target="#b18">(Gamper and Rajpoot, 2021</ref>) is a computational pathology multiple-instance captioning dataset that includes morphological descriptions and diagnoses for a wide variety of tissue types and staining. Some samples from the dataset are shown in <ref type="figure">Figure 3</ref>. The dataset images and corresponding descriptions were mined from PubMed medical articles and pathology text books. It contains 7,579 images and a description for each image. For experiments conducted using this dataset, the 5-fold cross validation was applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Implementation Details</head><p>For those experiments using ARCH dataset, DeiT architecture <ref type="bibr" target="#b51">(Touvron et al., 2021)</ref> is used as the image encoder. This architecture includes 12 layers and 768 hidden state and split the input image into 16 ? 16 patches. As the result, each input image can be seen as 196 patches with the size of 16 ? 16. DeiT model individually is trained for a classification task on the dataset that had been used in "KimiaNet" <ref type="bibr" target="#b45">(Riasatian et al., 2021)</ref>. In the next step, the weights for the image encoder are initialized from the trained model in the previous step. As the size of the dataset was not large enough to properly train the network, only the Papillary intralymphatic angioendothelioma. This is a dermal or subcutaneous lesion of children or adults, with intravascular growth of cells that have a lymphatic endothelial immunophenotype. Some have adjacent lymphangiomas or clusters of lymphatic vessels.The earlier terminology was malignant endovascular papillary angioendothelioma of childhood , but in a more recent series, reported cases did not recur or metastasize</p><p>High PR expression (3+) in a leiomyoma,(PR x400)</p><p>Cortex in a renal biopsy specimen from a man of 38 known to have membranous nephropathy, shown by a biopsy nearly 2 years before this one. The nephrotic syndrome had persisted since the first biopsy, and acute renal failure had developed 1 month before this biopsy. There is now widespread uniform tubular atrophy, which suggests longstanding renal vein thrombosis. This was confirmed by radiologic investigation. Renal function did not recover <ref type="figure">Figure 3</ref>: Three samples from the ARCH dataset. As it can be seen, images are from different staining and and normalizations with varied description structure.</p><p>the last two blocks of image encoder were trainable and the rest of the network were frozen.</p><p>For the text encoder, the network weights are initialized from the BioMed-RoBERTa-base (Gururangan et al., 2020) which had been trained on 2.68 million scientific papers from Semantic scholar corpus (Sem). This model is a language model based on RoBERTa-base architecture <ref type="bibr" target="#b36">(Liu et al., 2019)</ref> which includes 12, 768 hidden dimensions with 110M learnable parameters. Same as image encoder, only 2 last layers of text encoder were trainable and the rest of the network were frozen. The challenging part for ARCH dataset as it shown in <ref type="figure" target="#fig_3">Figure 4</ref> is related to the descriptions length. The descriptions are mostly non-uniform, i.e., ranging from 2 tokens to 484 tokens and much longer compared to MS-COCO dataset. To address this problem, each description is split into sentences. Then, concatenation of the first sentence with every other sentence are considered as new individual descriptions. By doing this, each image can have one or more than one description which help to augment the data and uniform the descriptions. For experiments using MS-COCO dataset, the same architectures are implemented for both image encoder and text encoder. The only differences are the weight initialization applied for these networks and the number of layers that set to be trainable. The image encoder weights are initialized from a pre-trained model that trained on a classification task on ImageNet-1k dataset <ref type="bibr" target="#b14">(Deng et al., 2009</ref>), which has 1 million images with 1,000 classes. Moreover, the input data for image encoder is the feature maps that had been extracted using Faster R-CNN as explained in section method section. The last 6 layers of the image encoder were trainable and the rest were frozen. The text encoder weights are initialized from a pre-trained Roberta-base network that had been trained on the reunion of five general datasets 1 . The last 6 layers of the text encoder were trainable and the rest were frozen.</p><p>All the experiments are implemented in Pytorch V1.3 and run with two NVIDIA V100 GPUs. For the conducted experiments, images were resized to 224 ? 224 pixels and normal-  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of LILE for cross-modality retrieval 3.1. Feature Representation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 Figure 2 :</head><label>22</label><figDesc>illustrates the similarity matrix. Images and text with a same subscript are paired instances which means the diagonal of the matrix should have the largest value compared to the other indices A small dog is under a white sheet.LILE Similarity matrix for a mini-batch with size of n. Element I i .T. j in the matrix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>(a): Frequency of the length of the descriptions tokens in the ARCH dataset. (b): Frequency of the length of the descriptions in the MS-COCO dataset. The MS-COCO dataset has an average of 13.81 and a maximum of 65 tokens per caption, whereas The ARCH dataset has an average of 51.49 and a maximum of 484 for each description.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with state-of-the-art methods on MS-COCO</figDesc><table><row><cell>Method</cell><cell cols="4">Text Retrieval R@1 R@5 R@10 R@1 R@5 R@10 Image Retrieval</cell><cell>R@sum</cell></row><row><cell></cell><cell>1K</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SCO(Huang et al., 2018)</cell><cell>69.9 92.9</cell><cell>97.5</cell><cell>56.7 87.5</cell><cell>94.8</cell><cell>499.3</cell></row><row><cell>SCAN(Lee et al., 2018)</cell><cell>72.7 94.8</cell><cell>98.4</cell><cell>58.8 88.4</cell><cell>94.8</cell><cell>507.9</cell></row><row><cell>PVSE(Song and Soleymani, 2019)</cell><cell>69.2 91.6</cell><cell>96.6</cell><cell>55.2 86.5</cell><cell>93.7</cell><cell>492.8</cell></row><row><cell>VSRN(Li et al., 2019)</cell><cell>76.2 94.8</cell><cell>98.2</cell><cell>62.8 89.7</cell><cell>95.1</cell><cell>516.8</cell></row><row><cell>IMRAM(Chen et al., 2020)</cell><cell cols="3">76.7 95.6 98.5 61.7 89.1</cell><cell>95.0</cell><cell>516.6</cell></row><row><cell>OSCAR (Fine-tuned)(Li et al., 2020)</cell><cell>89.8 98.8</cell><cell>99.7</cell><cell>78.2 95.8</cell><cell>98.3</cell><cell>563.6</cell></row><row><cell>LILE</cell><cell>77.7 95.4</cell><cell cols="3">98.3 64.1 91.0 96.4</cell><cell>522.9</cell></row><row><cell></cell><cell>5K</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SCO(Huang et al., 2018)</cell><cell>42.8 72.3</cell><cell>83.0</cell><cell>33.1 62.9</cell><cell>75.5</cell><cell>369.6</cell></row><row><cell>SCAN(Lee et al., 2018)</cell><cell>50.4 82.2</cell><cell>90.0</cell><cell>38.6 69.3</cell><cell>80.4</cell><cell>410.9</cell></row><row><cell>PVSE(Song and Soleymani, 2019)</cell><cell>45.2 74.3</cell><cell>84.5</cell><cell>32.4 63.0</cell><cell>75.0</cell><cell>374.4</cell></row><row><cell>VSRN(Li et al., 2019)</cell><cell>53.0 81.1</cell><cell>89.4</cell><cell>40.5 70.6</cell><cell>81.1</cell><cell>415.7</cell></row><row><cell>IMRAM(Chen et al., 2020)</cell><cell cols="2">53.7 83.2 91.0</cell><cell>39.7 69.1</cell><cell>79.8</cell><cell>416.5</cell></row><row><cell cols="2">ALIGN (Fine-tuned) (Jia et al., 2021) 77.0 93.5</cell><cell>96.9</cell><cell>59.9 83.3</cell><cell>89.8</cell><cell>500.4</cell></row><row><cell>OSCAR (Fine-tuned)(Li et al., 2020)</cell><cell>73.5 92.2</cell><cell>96.0</cell><cell>57.5 82.8</cell><cell>89.8</cell><cell>491.8</cell></row><row><cell>CLIP(Radford et al., 2021)</cell><cell>58.4 81.5</cell><cell>88.1</cell><cell>37.8 62.4</cell><cell>72.2</cell><cell>400.4</cell></row><row><cell>LILE</cell><cell cols="4">55.6 82.4 91.0 41.5 72.1 82.2</cell><cell>424.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with previous methods using the ARCH dataset</figDesc><table><row><cell>Method</cell><cell cols="4">Text Retrieval R@1 R@5 R@10 R@1 R@5 R@10 Image Retrieval</cell><cell>R@sum</cell></row><row><cell>PVSE</cell><cell>36.2 48.8</cell><cell>53.3</cell><cell>29.3 43.8</cell><cell>52.6</cell><cell>264.0</cell></row><row><cell>IMRAM</cell><cell>40.3 52.5</cell><cell>59.5</cell><cell>32.4 48.2</cell><cell>56.1</cell><cell>289.0</cell></row><row><cell cols="2">CLIP (Zero-shot) 38.2 51.1</cell><cell>57.0</cell><cell>30.8 46.1</cell><cell>53.2</cell><cell>276.4</cell></row><row><cell>LILE</cell><cell cols="4">44.8 57.4 64.4 36.7 52.2 61.7</cell><cell>317.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The effect of the number of matching steps, K, in MS-COCO dataset.</figDesc><table><row><cell>K</cell><cell>Text Retrieval R@1 R@5 R@10 R@1 R@5 R@10 Image Retrieval</cell><cell>R@sum</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2022 D. Maleki &amp; H. Tizhoosh.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Similarity Matrix</head><p>Appendix D. Effect of the iterative Matching scheme, K To better understand the effect of K in the proposed approach, K gradually is increased from 1 to 4 to train and test on the MS-COCO (5K) benchmark dataset. <ref type="table">Table 3</ref> shows the results for R@1, R@5, R@10 and R@sum metrics on image-to-text retrieval and back. It can be observed for K = 2 that the model consistently achieved better performance than other values for K. The gap between the performance for K = 1, which is a baseline without the iterative scheme, and other values for K is significant. That gap can prove the importance of the iterative scheme in the proposed approach.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>English Wikipedia -Wikipedia</surname></persName>
		</author>
		<idno>11/04/2021</idno>
		<ptr target="https://en.wikipedia.org/wiki/English_Wikipedia" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Open clone of openai&apos;s unreleased webtext dataset scraper. this version uses pushshift.io files instead of the api for speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Github -Jcpeterson/Openwebtext</surname></persName>
		</author>
		<ptr target="https://github.com/jcpeterson/openwebtext" />
		<imprint/>
	</monogr>
	<note>Accessed on 11/04/2021</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Home -Mbweb</surname></persName>
		</author>
		<idno>11/04/2021</idno>
		<ptr target="https://yknzhu.wixsite.com/mbweb" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">News dataset available -common crawl</title>
		<idno>11/04/2021</idno>
		<ptr target="https://commoncrawl.org/2016/10/news-dataset-available/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">News dataset available -common crawl</title>
		<idno>11/04/2021</idno>
		<ptr target="https://commoncrawl.org/2016/10/news-dataset-available/,b" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semantic scholar -ai-powered research tool</title>
		<idno>11/02/2021</idno>
		<ptr target="https://www.semanticscholar.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Megatron-Nvidia Developer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blog</surname></persName>
		</author>
		<idno>11/05/2021</idno>
		<ptr target="https://rb.gy/a3s7xi" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Matching words and pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kobus</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Names and faces in the news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaety</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David A</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mikhail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burtsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Kuratov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigory V</forename><surname>Peganov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sapunov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11527</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Memory transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imram: Iterative matching with recurrent attention memory for cross-modal image-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12655" to="12663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simultaneous image classification and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1903" to="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07503</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiple instance captioning: Learning representations from histopathology textbooks and articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jevgenij</forename><surname>Gamper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16549" to="16559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Look, imagine and match: Improving textual-visual cross-modal retrieval with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7181" to="7189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Instance-aware image and sentence matching with selective multimodal lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2310" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning semantic concepts and order for image and sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Saliency-guided attention network for image-sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning permutation invariant representations using memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivam</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Adnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Tizhoosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="677" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5679</idno>
		<title level="m">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
	<note>Gang Hua, Houdong Hu, and Xiaodong He</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual semantic reasoning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4654" to="4662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<title level="m">Vilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10437" to="10446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02824</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cm-gans: Cross-modal generative adversarial networks for common representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fine-tuning and training of densenet for histopathology image representation using tcga diagnostic slides</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abtin</forename><surname>Riasatian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morteza</forename><surname>Babaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danial</forename><surname>Maleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivam</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Valipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sobhan</forename><surname>Hemati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manit</forename><surname>Zaveri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Safarpoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sobhan</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Afshari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">102032</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="966" to="973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Polysemous visual-semantic embedding for crossmodal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Preserving semantic neighborhoods for robust cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Kovashka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="317" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-modality cross attention network for image and sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10941" to="10950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Mdnet: A semantically and visually interpretable medical image diagnosis network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanpu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyong</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Mcgough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6428" to="6436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">CC-News (New, b), which contains 63M English news articles, OpenWebText (Git), which is a WebText dataset used to train GPT-2 and Stories which is a subset of CommonCrawl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bookcorpus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">English Wikipedia (Eng)</title>
		<imprint/>
	</monogr>
	<note>which contains 11,038 unpublished books. New, a) data</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">The hyper-parameter m, which specifies the number of self-attention heads, is set as 16 for optimal performance across all experiments. The Adam optimizer (Kingma and Ba, 2014) is used with an initial learning rate of 1e?4. The learning rate is decreased when the metric has stopped improving. The margine for contrastive loss function</title>
		<imprint/>
	</monogr>
	<note>ized before feeding into the model. set to be 0.2.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
