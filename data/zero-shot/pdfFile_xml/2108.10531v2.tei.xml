<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Depth Completion with Calibrated Backprojection Layers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wong</surname></persName>
							<email>alexw@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UCLA Vision Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
							<email>soatto@cs.ucla.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">UCLA Vision Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Depth Completion with Calibrated Backprojection Layers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a deep neural network architecture to infer dense depth from an image and a sparse point cloud. It is trained using a video stream and corresponding synchronized sparse point cloud, as obtained from a LIDAR or other range sensor, along with the intrinsic calibration parameters of the camera. At inference time, the calibration of the camera, which can be different than the one used for training, is fed as an input to the network along with the sparse point cloud and a single image. A Calibrated Backprojection Layer backprojects each pixel in the image to threedimensional space using the calibration matrix and a depth feature descriptor. The resulting 3D positional encoding is concatenated with the image descriptor and the previous layer output to yield the input to the next layer of the encoder. A decoder, exploiting skip-connections, produces a dense depth map. The resulting Calibrated Backprojection Network, or KBNet, is trained without supervision by minimizing the photometric reprojection error. KBNet imputes missing depth value based on the training set, rather than on generic regularization. We test KBNet on public depth completion benchmarks, where it outperforms the state of the art by 30.5% indoor and 8.8% outdoor when the same camera is used for training and testing. When the test camera is different, the improvement reaches 62%. Code available at: https://github.com/alexklwong/ calibrated-backprojection-network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sensor platforms designed to enable interaction with physical space often include optical as well as range sensors. From cars to phones, cameras are paired with active sensors such as LIDARs, Sonars or Radars. We address the case of a single camera and a single sensor that returns the three-dimensional (3D) coordinates of a number of points far fewer than the number of pixels in the RGB image. The range sensor alone provides a sparse estimate of the Euclidean geometry of the surrounding environment, but often insufficient for planning in applications such as autonomous navigation or manipulation. We wish to leverage the complementarity of the optical and range modalities to provide a dense depth map, whereby a range value 1 is associated to every pixel in the image (in the millions) as opposed to just the LIDAR or radar returns (in the thousands).</p><p>Depth completion consists of mapping a single RGB image and a sparse 3D point cloud onto a dense depth map, which requires inferring a depth value where missing. This can be done by means of regularization, or inductively using previously observed data for scenes other than the present. We assume we have available a training set consisting of monocular videos, corresponding sparse 3D point cloud, and intrinsic calibration matrix of the camera used for capture, 2 but without any manual annotation or ground-truth dense depth i.e. unsupervised.</p><p>Our goal is to use the training set to learn a function that, for a scene and camera not used for training, can map a sparse point cloud registered to an image, along with the matrix of intrinsic calibration parameters of the camera, and produce a dense depth map associated with the test image.</p><p>We propose a novel deep neural network architecture that leverages a sparse-to-dense (S2D) module and calibrated backprojection (KB) layers. S2D is comprised of various pooling and convolutional layers to yield a dense representation of the sparse points. A KB layer then maps camera intrinsics, input image, and current depth estimate onto the 3D scene. This can be thought of as a form of spatial (Euclidean) positional encoding of the image. Unlike previous architectures, camera intrinsics are an input to our model, as opposed to a fixed set of parameters in the training loss. This allows us more flexibility to transfer the trained model to sensor platforms other than that used for training.</p><p>Our network is trained unsupervised with the standard Photometric Euclidean Reprojection Loss (PERL) i.e. the absolute difference between a reconstructed image and the <ref type="bibr" target="#b0">1</ref> The depth associated with the pixel is the Euclidean distance of the closest point in the scene along the projection ray through that pixel and the optical center. We assume the sensors to be calibrated and synchronized, and in particular the intrinsic calibration matrix of the camera is known so that pixel coordinates can be converted to Euclidean 3D coordinates. <ref type="bibr" target="#b1">2</ref> Typically, range and optical sensors are calibrated mechanically and pre-registered, so extrinsic calibration is not needed. actual image measured at a time instant. We also penalize the reconstruction error of the input sparse points and Total Variation of the estimated depth map, a standard sparsityinducing prior to reduce the penalty for large depth changes at adjacent pixels that straddle occluding boundaries. At test time, no video is necessary and inference is performed on each image and sparse point cloud independently.</p><p>These innovations allow us to improve the baseline <ref type="bibr" target="#b46">[47]</ref> and state of the art <ref type="bibr" target="#b44">[45]</ref> by an average of 13.7% and 8.8%, respectively, on outdoors (KITTI <ref type="bibr" target="#b40">[41]</ref>), and 51.7% and 30.5% indoors (VOID <ref type="bibr" target="#b46">[47]</ref>), when calibration is the same for training and testing. When different calibrations are used, our method generalizes better than the baseline and state of the art by 83% and 62%, respectively, in relative error. All of this is achieved with a smaller computational footprint thanks to the inductive bias induced by KB layers, which allows us to use a smaller network than current methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work and Contributions</head><p>Depth completion is a form of imputation, which requires regularization that hinges the assumption that "nearby points" should be assigned "similar" (depth) values. Methods differ in the choice of topology i.e. what points should be considered "nearby," and how to combine the values of such points to impute the missing depth value.</p><p>Generic Image-Based Regularization. In image topology, nearby points correspond to adjacent pixels. This is not a good choice, for their depths can be arbitrarily different at occluding boundaries. In image segmentation, the RGB values are used to define a topology to partition the image domain into connected regions of nearby points, putatively corresponding to "objects." The topology induced by (color) values can be exploited by minimizing Total Variation (TV <ref type="bibr" target="#b35">[36]</ref> and "Color TV" <ref type="bibr" target="#b0">[1]</ref>) while trying to reproduce the image itself. We adopt TV as a generic regularizer since the statistics of natural range images are very similar to that of natural (intensity) images <ref type="bibr" target="#b28">[29]</ref>, whereby the gradient distribution is highly kurtotic, corresponding to homogeneous smooth regions separated by sharp boundaries.</p><p>Data-driven Regularization. "Closeness" among pixels can be defined not just within the same image, but across different images in the training set. In this case, the regularity criterion is not explicit, but implicit in the inductive bias used for training. Before training starts, the bias is encoded in the training loss (L 1 prediction error), the generic regularizers (TV), the training set, and the choice of architecture and optimization. After training is completed, all these biases are burnished in the parameters (weights) of the trained model, which inform the prediction of our depth map and therefore act as a regularizing mechanism.</p><p>Among data-driven methods for depth completion, many are supervised. Early works cast depth completion as com-pressive sensing <ref type="bibr" target="#b5">[6]</ref> and as morphological operators <ref type="bibr" target="#b6">[7]</ref>. Recent works focused on network operations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19]</ref> and architectures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">50]</ref> to effectively deal with the sparse inputs. <ref type="bibr" target="#b25">[26]</ref> proposed an early fusion architecture while <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b49">50]</ref> used late fusion to process each data modality separately. <ref type="bibr" target="#b18">[19]</ref> performed joint concatenation and convolution to upsample the sparse depth. <ref type="bibr" target="#b2">[3]</ref> proposed a 2D-3D fusion network while <ref type="bibr" target="#b23">[24]</ref> used a cascade hourglass network. <ref type="bibr" target="#b3">[4]</ref> used a convolutional spatial propagation network and <ref type="bibr" target="#b29">[30]</ref> leveraged non-local spatial propagation. Whereas, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> learned uncertainty of estimates, <ref type="bibr" target="#b41">[42]</ref> leveraged confidence maps, and <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51]</ref> used surface normals for guidance. Like us, <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b52">53]</ref> proposed lightweight networks suitable for use with SLAM/VIO systems.</p><p>All of these methods require ground truth for training, which is often unavailable and, when available, prohibitively expensive <ref type="bibr" target="#b40">[41]</ref>. Hence, these methods are limited to offline training. But even if ground truth were available online, most of these methods employ complex architectures with many layers and parameters, e.g. 25.84M for <ref type="bibr" target="#b29">[30]</ref>, 53.4M <ref type="bibr" target="#b31">[32]</ref>, and 28.99M <ref type="bibr" target="#b48">[49]</ref>, and thus are not suitable for learning online. Instead, we propose to learn dense depth from the virtually limitless amount of un-annotated images and sparse point clouds via a predictive cross-modal validation criterion. Our proposed architecture only uses 6.9M parameters and our choice of supervision allows us to continuously learn even after the system is deployed.</p><p>Unsupervised/Self-supervised depth completion assumes stereo images or monocular videos to be available during training. Both stereo <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b49">50]</ref> and monocular <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> training paradigms leverage sparse depth reconstruction and photometric reprojection error as a training signal by minimizing photometric discrepancies between the input image and its reconstruction from other views. <ref type="bibr" target="#b25">[26]</ref> used Perspective-n-Point <ref type="bibr" target="#b22">[23]</ref> and RANSAC <ref type="bibr" target="#b11">[12]</ref> to align consecutive video frames. However, <ref type="bibr" target="#b25">[26]</ref> does not generalize well to indoor scenes with many textureless surfaces. <ref type="bibr" target="#b49">[50]</ref> learned a depth prior conditioned on the image by pretraining a separate network on ground truth from an additional dataset. As mentioned earlier, this is not scalable; also, using a network trained on a specific domain (e.g. outdoors) as supervision will not generalize (e.g. indoors). Unlike <ref type="bibr" target="#b49">[50]</ref>, our method does not require ground truth and is not limited to a specific domain. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b44">45]</ref> leverage additional synthetic datasets, which require dealing with simto-real; our method is able to achieve the state-of-the-art without needing access to additional data.</p><p>The challenge of depth completion is precisely the sparsity, which renders convolutions ineffective as the activations of early layers tend to be zeros as well. To obtain a denser representation, early layers must propagate (or densify) the signal. As a result, <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50]</ref> employed very deep networks with many layers and parameters in order to learn the map from sparse depth and image to dense depth. To handle this problem, <ref type="bibr" target="#b46">[47]</ref> approximated the scene with a hand-crafted mesh, but it is not differentiable and prone to errors in regions with very few points or complex structures. <ref type="bibr" target="#b44">[45]</ref> proposed spatial pyramid pooling (SPP), but their max pooling layers decimated details on closer objects. Instead, we propose a fully differentiable sparse-to-dense module that learns the trade-off between density and detail to retain both near and far structures.</p><p>Our work goes counter to the trend of forgoing inductive bias, i.e. learning everything with generic architectures like Transformers <ref type="bibr" target="#b42">[43]</ref>, including what we already know such as basic Euclidean geometry. Our model has a strong inductive bias in our calibrated backprojection layer, which incorporates the calibration matrix directly into the architecture to yield an RGB representation lifted into scene topology via 3D positional encoding. This may seem futile as we could just add intrinsics to the long list of parameters to be learned <ref type="bibr" target="#b14">[15]</ref>. However, unlike semantic retrieval, spatial inference requires identifiability: There is one true scene in front of us, and unless information about calibration is available and properly exploited, inference yields one of infinitely many depth maps that are equally good at predicting the next frame in the training set. Since there is no supervision, calibration mediates the relation between the prediction error and the true depth. Because existing methods use calibration in the computation of the loss, which the intrinsics are encoded in the weights, hampering transferability. In our architecture, calibration is an input, which can be changed at inference time. While one could pre-process the images to a canonical calibration, this introduces latency, cost and artifacts that can affect the reconstruction quality. We note that <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref> proposed backprojection as a layer and <ref type="bibr" target="#b9">[10]</ref> used calibration as input, but we are the first to consider an RGB 3D representation for depth completion. Our contributions include (a) a sparse-to-dense module that learns a dense representation of the sparse point cloud, (b) an unsupervised depth completion method that takes calibration information as input to the model, and (c) incorporates it directly in the architecture through a novel calibrated backprojection module, which represents spatial positional encoding that is transferred laterally across different branches of the encoder. The resulting inductive bias helps select, among all depth, maps compatible with the prediction loss, those that result in a Euclidean (calibrated) reconstruction. The strong inductive bias allows us to (d) reduce the computational footprint, increase generalization and achieve performance beyond the state of the art despite having fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method Formulation</head><p>Our goal is to recover a 3D scene from an RGB image I : ? ? R 2 ? R 3 + and the associated sparse point cloud <ref type="figure">Figure 1</ref>: KBNet architecture. Our architecture takes, as input, an RGB image, the corresponding sparse depth map and camera calibration matrix. We first learn a dense representation of the sparse points with our sparse-to-dense module. The result of which and the calibration matrix are used for calibrated lifting, which allows us to backproject image features onto 3D space (akin to spatial positional encodings) to yield a RGB 3D representation. Our network is very light-weight and fast, yet achieves the state of the art.</p><p>projected onto the image plane z : ? z ? ? ? R + , without access to ground-truth depth annotations.</p><p>We propose a sparse-to-dense module ( <ref type="figure">Fig. 2)</ref> f ? , parameterized by ?, that captures local and global structure of the sparse inputs by combining min and max pooling at different scales. The result is a dense or quasi-dense depth representation f ? (z), depending on the sparsity of the input, which frees the rest of network to utilize its early convolutional layers to learn scene structure rather than to densify the input -making the overall architecture more efficient.</p><p>The sparse-to-dense module (Sec. 2.1) is part of an overall encoder-decoder architecture f ? , parameterized by ?, called KBNet (Sec. 2.2), that includes a Calibrated Backprojection layer which explicitly backprojects pixels onto 3D space using intrinsic camera calibration and depth encoding from f ? . Unlike previous works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref> that encode depth and image in two separate branches, we leverage camera calibration and our depth encoding to lift the image representation to 3D and passed it to the decoder via skip connections. KBNet ( <ref type="figure">Fig. 1</ref>) produces dense dept? d := f ? (f ? (z), I, K), where K ? R 3?3 is the uppertriangular matrix of intrinsic calibration parameters. To train our model, we use monocular videos to compose a loss function from temporally adjacent frames (Sec. 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Sparse-to-Dense Module (S2D)</head><p>Our S2D module f ? <ref type="figure">(Fig. 2</ref>) performs multi-scale densification on the input sparse depth map z using a series of min and max pooling layers with various kernel sizes, which are chosen based on the sparsity of the point cloud e.g. from LIDAR returns or sparse points tracked by VIO <ref type="bibr" target="#b10">[11]</ref> (see Supp. Mat. for kernel sizes). The outputs of the pooling layers are concatenated and fed into three 1?1 convolutions to learn the trade-offs between pooling types and <ref type="figure">Figure 2</ref>: Sparse-to-dense module. We perform min and max pooling with various kernel sizes to produce a dense representation. There exists trade-offs between density and detail (large vs. small kernel sizes) and preservation of near and far structures (min vs. max pooling, as highlighted in green). We balance these trade-offs with 1 ? 1 convolutions and fuse the result with the input via a 3 ? 3 convolution. kernel sizes. The result of which is fused with the z via a 3 ? 3 convolutional layer, yielding a dense or quasi-dense depth representation that is fed to the rest of the network f ? .</p><p>Because the depth inputs are sparse, we design our min pooling layers to avoid pooling zeros or invalid (negative) depth values. We set all values z(x) less than zero to be infinity for x ? ?:</p><formula xml:id="formula_0">z (x) = z(x) if z(x) &gt; 0 ? otherwise.<label>(1)</label></formula><p>z is fed to a min pooling layer with k ? k kernel size, p = minpool(z , k).</p><p>Finally, for all x, any infinity values pooled due to large empty regions are set to zero:</p><formula xml:id="formula_2">p min (x) = p(x) if p(x) = ? 0 otherwise.<label>(3)</label></formula><p>Our approach involves two main trade-offs: (i) density versus detail and (ii) preservation of near versus far structures. Density versus details. For the purpose of densification, one may perform pooling with large kernel sizes, but it comes at the expense of details of local structures. In contrast, pooling with small kernel sizes in an attempt to retain detail will result in very few neuron activations, which hinders learning. Hence, to retain local details while obtaining a dense representation, we propose to perform pooling with both small and large kernel sizes.</p><p>Near versus far. When pooled solely with max pooling, farther structures are preserved, but details of the closer ones are decimated as the kernel size grows larger. For instance in <ref type="figure">Fig. 2</ref>, thin structures close to the camera i.e. the highlighted pole "disappears" due to large max pooling kernel size. On the other hand, when only using min pooling, the closer structures become more prominent, but in turn, the farther regions are corrupted. Moreover, in cluttered scenes, min pooling causes adjacent structures to "bleed" into each other. Hence, to preserve close and far structures, we employ both min and max pooling layers.</p><p>To optimize for both trade-offs, we concatenate the outputs of min and max pooling together and feed them into 1 ? 1 convolutional layers. Finally, we use a 3 ? 3 convolution to fuse the multi-scale depth features back into the original sparse depth via a residual connection, yielding a dense representation f ? (z) to be fed to f ? .</p><p>We note that our S2D bares some resemblance to spatial pyramid pooling (SPP) <ref type="bibr" target="#b17">[18]</ref>; however, SPP was designed to ensure the same size feature maps are maintained when different size of inputs. It is also intended to operate on dense inputs. While <ref type="bibr" target="#b44">[45]</ref> also proposed an SPP for sparse inputs, its use of max pooling decimated details for nearby structures. Neither are substitutes for our S2D module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">KBNet Architecture</head><p>Motivation. Unsupervised methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> use the photometric reprojection error perl as a training signal. The input image I t is reconstructed from temporally adjacent frames I ? for ? ? T . = {t ? 1, t + 1} to yield? ? ,</p><formula xml:id="formula_3">I ? (x,d, g ? t ) = I ? ?g ? t K ?1xd (x) ,<label>(4)</label></formula><p>and the per pixel photometric reprojection error is measured</p><formula xml:id="formula_4">by perl = |? ? (x,d, g ? t ) ? I t (x)|. Herex = [x , 1]</formula><p>are the homogeneous coordinates of x ? ?. Using the notation in <ref type="bibr" target="#b26">[27]</ref>, g ? t ? SE(3) is the relative pose (rotation and translation) of the camera from time t to time ? , K denotes the intrinsic calibration matrix, and ? is a canonical perspective projection. For simplicity, we will refer to the reconstruction from time ? at a coordinate x as? ? (x).</p><p>Inferring Euclidean structure and motion in the absence of calibration information is notoriously difficult and dependent on conditions rarely satisfied in ordinary training videos, such as rotation around three independent axes <ref type="bibr" target="#b26">[27]</ref>. Minimizing any form of perl forces the network to implicitly learn the calibration matrix K, as all prior work does. As pretrained models are commonly deployed on sensor platforms different than those used during training, this hinders generalization as the network becomes overfitted to the camera used to collect training data. In contrast, our network, KBNet, takes it as input; this allows us to use different calibrations in training and test, which significantly improves generalization <ref type="table">(Table 5)</ref>.</p><p>Calibrated Backprojection Layers take, as input, the depth and RGB image encodings, and the camera calibra-  <ref type="bibr" target="#b46">[47]</ref> are combined using the calibration matrix as additional input. Calibration is used to lift pixel coordinates to three dimensions, which are backprojected by a compressed depth descriptor into a 3D positional encoding. The result is concatenated with the image encoding and the output of the previous KB layer, and fused with a 1 ? 1 convolution. This yields an RGB 3D representation, which is used as a skip connection to the decoder and input to subsequent layers. tion matrix K and output not only the corresponding encodings of the depth map and of the RGB image, but also an encoding of the RGB image backprojected onto 3D space. Once we have formed this RGB 3D representation, it is fed as input to subsequent Calibrated Backprojection (KB) layers and as skip connection to the decoder and once we have form this representation ( <ref type="figure" target="#fig_0">Fig. 3</ref>).</p><p>To realize a KB layer, first, we use the calibration matrix to lift the coordinates of each pixel x ? ? to three dimensional space x ? K ?1x . Then, the feature map of the depth encoder ?(x) ? R M , with M ranging from 16 in the first layer to 128 in the last one, is collapsed to a scalar by a trainable projection or "compression" module q, d(x) = q ?(x). The imputed depth d(x) is used to backproject the lifted coordinatex to yield a 3D positional encoding for each pixel</p><formula xml:id="formula_5">x 3D = K ?1x d(x).</formula><p>Here ? ? R 2 is discretized into a lattice of H ? W pixels in the first layer, corresponding to the resolution of the original image, that decreases by a factor of 2 in each subsequent layer until the 5-th or last layer at H/32?W/32. Hence, the intrinsics parameters, focal lengths and principal point, must also be scaled by the same factor according to the resolution reduction in each layer.</p><p>The 3D positional encoding is concatenated with the image encoding ?(x) ? R N , and, if available, the output of the previous KB layer ? 3D (x) ? R N where N ranges from 48 in the first layer to 386 in the last. This is fused together by a 1 ? 1 convolution to yield the output RGB 3D encoding. This encoding is fed to the next layer and also replaces the typical RGB skip connection to the decoder. Finally, the output depth and image encodings of the KB layer are produced by convolving separate 3 ? 3 kernels. After which, both are also passed to the next layer as input.</p><p>In addition to benefits of generalization <ref type="table">(Table 5)</ref>, KB layers also produce depth estimates that better respect object boundaries. Because each layer encodes "closeness" based on the scene topology via 3D positional encoding rather than the 2D image topology (as in previous works), adjacent pixels in the image that are often confused to be close are now well separated ( <ref type="figure" target="#fig_1">Fig. 4</ref>) and hence distinct adjacent objects are better delineated and points belonging to the same surface are better regularized. This reduces the common bleed effect observed when a depth map is backprojected to a point cloud in 3D. Moreover, by instilling 3D structure as an architectural inductive bias, we enable a faster and slimmer network with fewer layers and parameters to achieve better performance (see <ref type="table" target="#tab_2">Table 2</ref> <ref type="bibr">, 4)</ref>.</p><p>We note that our S2D module complements our KB layers as it provides us with dense or quasi-dense depth representation. Without it, we are left with sparse geometry, which limits the potential performance gain. Yet, as demonstrated in <ref type="table">Table 3</ref>, there are still benefits to using calibrated backprojection with a sparse representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Loss Function</head><p>Similar to previous works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>, our loss function is the linear combination of three terms:</p><formula xml:id="formula_6">L = w ph ph + w sz sz + w sm sm<label>(5)</label></formula><p>where ph denotes photometric consistency, sz sparse depth consistency, and sm local smoothness. Each term is weighted by their associated w (see Sec. 3.1). Photometric Consistency. As mentioned in Sec. 2.2, unsupervised methods leverage photometric reprojection error as a supervisory signal by reconstructing I t from I ? for ? ? T . = {t ? 1, t + 1} via Eqn. 4. To accomplish this, one can obtain pose from a VIO <ref type="bibr" target="#b10">[11]</ref> or employ a pose network to estimate the relative pose between I t and I ? (see full system diagram in Supp. Mat.). We note that pose is only needed for training and is not used at test time.</p><p>From the reconstructions, the photometric consistency loss measures the average photometric reprojection error using a combination of L 1 penalty and SSIM <ref type="bibr" target="#b43">[44]</ref>:</p><formula xml:id="formula_7">ph = 1 |?| ? ?T x?? w co |? ? (x) ? I t (x)|+ w st 1 ? SSIM(? ? (x), I t (x)) ,<label>(6)</label></formula><p>w co and w st are weights for each term and are discussed in Sec. 3.1. We note that if g ? t is estimated via a pose network, instead of a VIO, it can be jointly learned with KBNet ( <ref type="figure">Fig. 1)</ref> as a by product from minimizing Eqn. 6 and 7, and hence does not require any extra supervision. </p><formula xml:id="formula_8">Metric Definition MAE 1 |?| x?? |d(x) ? dgt(x)| RMSE 1 |?| x?? |d(x) ? dgt(x)| 2 1/2 iMAE 1 |?| x?? |1/d(x) ? 1/dgt(x)| iRMSE 1 |?| x?? |1/d(x) ? 1/dgt(x)| 2 1/2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse Depth Consistency.</head><p>Minimizing the reprojection error will reconstruct the scene structure up to an unknown scale. To ground the predictions to metric scale, we minimize the L 1 difference between our predictionsd and the sparse depth inputs over its domain (? z ):</p><formula xml:id="formula_9">sz = 1 |? z | x??z |d(x) ? z(x)|.<label>(7)</label></formula><p>Local Smoothness. We enforce local smoothness and connectivity overd by minimizing the L 1 penalty on its gradients in the x? (? X ) and y? (? X ) directions. We also weight each term using its respective image gradients, ? X = e ?|? X It(x)| and ? Y = e ?|? Y It(x)| , to allow discontinuities along object boundaries:</p><formula xml:id="formula_10">sm = 1 |?| x?? ? X (x)|? Xd (x)| + ? Y (x)|? Yd (x)|. (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and Results</head><p>We evaluate our method on benchmark datasets, KITTI <ref type="bibr" target="#b40">[41]</ref> for outdoors settings, and VOID <ref type="bibr" target="#b46">[47]</ref> for indoors, using metrics describes in <ref type="table" target="#tab_0">Table 1</ref>. We also demonstrate that our approach generalizes well to scenes captures by camera setup different than that used to collect the training set by training our model on VOID and testing it on NYUv2 <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation Details</head><p>We implemented our method in PyTorch <ref type="bibr" target="#b30">[31]</ref>. End-toend inference takes 16ms per frame. We used Adam <ref type="bibr" target="#b20">[21]</ref> with ? 1 = 0.9 and ? 2 = 0.999 to optimize our network. Training on KITTI <ref type="bibr" target="#b40">[41]</ref> takes 70 hours for 60 epochs, VOID <ref type="bibr" target="#b46">[47]</ref> 16 hours for 15 epochs, and NYUv2 <ref type="bibr" target="#b39">[40]</ref> 13 hours for 15 epochs on an Nvidia GTX 1080Ti GPU. We use a batch size of 8 with 768 ? 320 crops for KITTI, 640 ? 480 for VOID and 576 ? 416 for NYUv2. For KITTI, we choose w ph = 1, w co = 0.15, w st = 0.95, w sz = 0.6, and w sm = 0.04; for VOID and NYUv2, we set w sz = 2 and w sm = 2. For detailed learning rate schedule, augmentations and S2D kernel sizes used for each dataset, please see Supp. Mat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Datasets</head><p>KITTI <ref type="bibr" target="#b40">[41]</ref> provides ?80,000 raw image frames and associated sparse depth maps. The sparse depth maps are the  raw output from the Velodyne lidar sensor, each with a density of ?5%. Ground-truth depth is obtained by accumulating 11 neighbouring raw lidar scans. Semi-dense depth is available for the lower 30% of the image space. We use the official 1,000 samples for validation and test on 1,000 designated samples (evaluated on their online test server). VOID <ref type="bibr" target="#b46">[47]</ref> contains synchronized 640 ? 480 RGB images and sparse depth maps of indoor (laboratories, classrooms) and outdoor (gardens) scenes. ? 1500 sparse depth points (covering ? 0.5% of the image) are the set of features tracked by XIVO <ref type="bibr" target="#b10">[11]</ref>, a VIO system. The ground-truth depth maps are dense and are acquired by active stereo. The entire dataset contains 56 sequences with challenging motion. Of the 56 sequences, 48 sequences (? 40, 000) are designated for training and 8 for testing. The testing set contains 800 frames. We follow the evaluation protocol of <ref type="bibr" target="#b46">[47]</ref> and cap the depths between 0.2 and 5 meters.</p><p>NYUv2 <ref type="bibr" target="#b39">[40]</ref> consists of 372K synchronized 640 ? 480 RGB images and depth maps for 464 indoors scenes (household, offices, commercial), captured with a Microsoft Kinect. The official split consisting in 249 training and 215 test scenes. For training, we evenly sample a subset of the training split to yield 46K frames. We use the official validation set of 795 images and test set of 654 images. Because there are no sparse depth maps provided, we sampled ? 1500 points from the depth map via Harris corner detector <ref type="bibr" target="#b16">[17]</ref> to mimic the sparse depth produced by SLAM/VIO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">KITTI Depth Completion Benchmark</head><p>We compare our method against recent unsupervised depth completion methods on the KITTI test set in <ref type="table" target="#tab_2">Table 2</ref> (results taken from online leaderboard). Compared to the baseline <ref type="bibr" target="#b46">[47]</ref>, we improve by an average of 13.7% across metrics and by as much as 17.1% in iRMSE while reducing   <ref type="table">Table 3</ref>: Ablation study on KITTI validation set. Without S2D (row 3), our performance degrade because our 3D positional features will only encode sparse geometry, but we still beat <ref type="bibr" target="#b46">[47]</ref> in rows 1, 2 ("w/o Scaffolding" is <ref type="bibr" target="#b46">[47]</ref> with sparse representation). We observe similar degradation without KB layers (row 6, replaced with VGG block used by <ref type="bibr" target="#b46">[47]</ref>). Substituting our S2D with Scaffolding <ref type="bibr" target="#b46">[47]</ref> or SPP <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b44">45]</ref> also hurts performance (rows 7, 8).</p><p>model size by 29%. Overall, we beat the best performing method <ref type="bibr" target="#b44">[45]</ref> by an average of 8.8% and up to 10.6% on the iMAE metric with a 11.5% reduction in model size. We note that top methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b44">45]</ref> use additional synthetic data for training; whereas, we do not. Also, for inference, our method takes 16ms per image (62 FPS), which is 2.75? faster than <ref type="bibr" target="#b46">[47]</ref> 3 and 2? faster than the state of the art <ref type="bibr" target="#b44">[45]</ref>. We note that our method significantly improves the iMAE and iRMSE metrics, to the point where we are comparable to some of the supervised methods for close range performance. For example, our iMAE score is ranked 5th across all methods (see <ref type="table">Table 9</ref>, 10 in Supp. Mat.). To the best of our knowledge, we are the first work in unsupervised depth completion to demonstrate comparable performance to supervised methods. <ref type="bibr" target="#b2">3</ref> The reported run time of <ref type="bibr" target="#b46">[47]</ref> on the KITTI leaderboard did not include their scaffolding step; whereas, the number in <ref type="table" target="#tab_2">Table 2</ref> accounts for it. <ref type="figure">Figure 5</ref>: Sensitivity to changes in calibration on KITTI. Focal length and principal point are altered to test sensitivity to changes in intrinsics parameters. Our method is robust to change up to ? 10%. After which, performance degrades.</p><p>To show the improvements from our contributions, we show head-to-head qualitative comparisons against the baseline <ref type="bibr" target="#b46">[47]</ref> in <ref type="figure" target="#fig_1">Fig. 4</ref>. Our method performs better in regions where depth discontinuities occur in image topology i.e. street sign and wall (left panel, highlighted in green) and far regions of the road (right panel, in orange). This is in part thanks to our calibrated backprojection (KB) layer which goes counter to the current trend of learning everything with generic architectures, including what we already know about basic Euclidean geometry. Our KB layers imposes strong inductive bias by incorporating the camera intrinsic calibration matrix to yield 3D positional encoding that lifts the image representation into scene topology -this delineates points where in 2D image topology are "close", but can be far in 3D scene topology. <ref type="table">Table 3</ref> shows an ablation study on the KITTI validation set. As mentioned in Sec. 2.2, our sparse-to-dense module (S2D) provides dense depth representation which in turn enables dense 3D topology in our calibrated backprojection <ref type="figure">Figure 6</ref>: Qualitative results on VOID test set. Comparison against <ref type="bibr" target="#b46">[47]</ref>. Our method performs better overall.   <ref type="bibr" target="#b44">[45]</ref>, we improve by an average of 30.5%.</p><p>(KB) layers. Hence, removing it ("w/o S2D) will hurt performance because it results in a sparse 3D positional encoding. Nonetheless, sparse geometry is still helpful as we outperform <ref type="bibr" target="#b46">[47]</ref> in rows 1, 2. Similarly, replacing our KB ("w/o KB layers") with VGG blocks used by <ref type="bibr" target="#b46">[47]</ref> also hurts performance as the model now lacks 3D spatial position. We show in rows 5 and 6 that one cannot simply substitute S2D with scaffolding <ref type="bibr" target="#b46">[47]</ref> or SPP <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b44">45]</ref>. In <ref type="figure" target="#fig_4">Fig. 10</ref>, we perform a sensitivity study of our model to calibration on the KITTI validation set. To this end, we altered the calibration by increasing or decreasing focal length (f ) and/or principal point (c x , c y ) and feed it as input. Our model is robust to changes up to ? 10%; after which, performance degrades. While changes in c x , c y have minor effects (which is scene-dependent), we observe a sharp decrease in performance when we decrease f by 20 to 25%. This is because, geometrically, decreasing f backprojects points to a larger field of view, distorting surfaces and sending points of the same surface far from each other. Increasing f conversely "packs" them tighter; this is okay for small increases, but for larger values, points will get "squashed together" -thus hurting performance. Also, to quantify the effect of sparsity, we provide a sensitivity study on various density levels in Supp. Mat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">VOID Depth Completion Benchmark</head><p>In the indoor scenario, the point clouds are on orders of hundreds to several thousand points (if we are being generous); hence, because of the sparsity, perturbations to the point cloud can yield vastly different sparse geometry.  <ref type="table">Table 5</ref>: Quantitative results on the NYUv2 test set. Column titled "Trained on" denotes the dataset each method is trained on. <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47]</ref> degrade much more than our method when tested on a dataset captured by a different sensor platform than the one used for gathering its training data.</p><p>This increases a model's sensitivity to the distribution to the sparse points. As there exists many complex scene layouts for the indoor setting, learning a dense representation and understanding the 3D topology of the scene become even more important. This is shown in <ref type="table" target="#tab_5">Table 4</ref> where we outperform <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref> across all metrics to achieve the state of the art on VOID. A key comparison is between our method and <ref type="bibr" target="#b46">[47]</ref>. Even though <ref type="bibr" target="#b46">[47]</ref> creates a hand-crafted scaffolding of the scene to obtain a dense representation, because there are very few points, it is prone to error i.e. forming surfaces between discontinuous objects and sensitive to changes in the points sampled. This is where our method shines. By optimizing for the trade-off between density and detail, our S2D module learns to exploit the natural statistics of the dataset to obtain a dense representation more compatible with the scene. Also, our KB layers introduces 3D topology as an inductive bias, allowing the network to delineate points that are close in image topology, but are far in scene topology -culminating in 51.7% and 30.5% improvement over <ref type="bibr" target="#b46">[47]</ref> and the state of the art <ref type="bibr" target="#b44">[45]</ref>, respectively. In <ref type="table">Table 5</ref>, we show that our method generalizes well to sensor platforms not used in the training set by training our method on VOID (captured on Intel RealSense) and testing it on NYUv2 (Microsoft Kinect). Similarly, we test models pretrained on VOID released by <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47]</ref> on NYUv2. We also train our method and <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47]</ref> from scratch on NYUv2 to show the paragon performance (rows 1, 3, 5). Rows 1 shows that <ref type="bibr" target="#b46">[47]</ref> does not generalize well to NYUv2 where error increases by 56% (as much as 94% in iRMSE). While <ref type="bibr" target="#b44">[45]</ref> does better, there is still a sharp decrease of 25.1% in performance. This is in part due to the change in sensor platform as well scene distribution in NYUv2. While we do not achieve paragon performance, our method generalizes better with a reasonable 10% increase in error -improving over <ref type="bibr" target="#b46">[47]</ref> by 83% and <ref type="bibr" target="#b44">[45]</ref> by 62% in relative error. We note that while training on the full set for NYUv2 should yield better results for paragon performance, our model trained on VOID performs better than VOICED <ref type="bibr" target="#b46">[47]</ref> and comparable to ScaffNet <ref type="bibr" target="#b46">[47]</ref> trained on the subset of NYUv2. For qualitative comparisons, please see <ref type="figure" target="#fig_1">Fig. 14</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>We present an approach to unsupervised depth completion that imposes strong inductive biases on Euclidean reconstruction in the architecture, rather than learning from data with a generic model such as a Transformer. This presents some advantages. First, it allows feeding calibration as an input, which means that we can easily use a model trained with a certain sensor platform with a different one at inference time. Second, the calibrated backprojection layer explicitly incorporates a basic geometric image formation model based on Euclidean transformations in 3D and central perspective projection onto 2D. This allows us to reduce the model size while still achieving the state of the art.</p><p>However, imposing strong inductive biases also presents some risks and limitations. First, if the camera is miscalibrated, inputing the wrong calibration can backfire, yielding distorted depth maps. Second, only a very rudimentary calibration model is used, so if a sensor platform has fancy optics such as omnidirectional lenses, one cannot use one of our pre-trained models but rather has to modify the core backprojection module. Third, even with these ad-hoc architectural choices, our model suffers the limitations of all imputations, which is that where there is insufficient evidence to constrain the solution, the regularizer dominates, which is a form of hallucination and can yield wildly wrong inferences. This would be mitigated by having an accurate measure of uncertainty associated to the depth map, this is an open problem well beyond our focus here. <ref type="figure">Figure 7</ref>: System diagram during training. We assume we are given monocular video sequences, synchronized sparse point clouds projected onto the image plane as 2.5D depth maps, and camera calibration. A training sample is therefore (I t , I ? , z, K). Sparse depth inputs (z) are fed to our sparse-to-dense module (f ? ) to yield a dense or quasi-dense representation. Along with image (I t ) and camera calibration matrix (K), it is then fed into our depth completion network (f ? ) comprised of calibrated backprojection layers to produce dense depth predictiond. Relative pose (g ? t ) between images I t and I ? can be estimated from a VIO or a network. In the case of the latter, pose can be jointly learned with depth. We note that pose is only needed to give the reconstruction? ? for constructing the loss function and is not needed during inference.</p><p>Code available at: https://github.com/alexklwong/calibrated-backprojection-network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>Summary of contents. In Sec. A, we provide an overview of our full system and more details on our loss function. We also provide the kernel sizes used in our sparse-to-dense module, augmentations used during training and our learning rate schedule to reproduce our results on KITTI <ref type="bibr" target="#b40">[41]</ref>, VOID <ref type="bibr" target="#b46">[47]</ref>, and NYUv2 <ref type="bibr" target="#b39">[40]</ref>. In Sec. B, we visualize and compare features learned by our proposed sparse-to-dense module to those from typical convolutional block, and show that our spare-to-dense module yields a much denser representation for the the depth completion network to ingest. In Sec. C.1, we consider the possibility of miscalibration and examine the sensitivity of our model to changes in intrinsics parameters i.e. incorrect calibration. We show that our model is robust to reasonable ranges of calibration error. In Sec. C.2, we study the sensitivity of our model to changes in sparse depth input density levels and demonstrate that we are robust even when sparse point cover only 0.15% of the image space. In Sec. D, we discuss our method's ability to generalize to test time sensor platforms with a different camera than the one used in training. Finally, in Sec. E, we show that we can beat several supervised methods on KITTI online leaderboard and that we rank 5th amongst all methods for the iMAE metric. <ref type="figure">Fig. 7</ref> shows a diagram of our full system. Our model takes an RGB image I, a sparse depth map z, and the camera intrinsics matrix K as input. First, the sparse depth map z is fed into our sparse-to-dense module f ? to obtain a dense or qusai-dense representation (Sec. <ref type="figure">2.1, main text)</ref>. Then, the depth representation f ? (z), RGB image I, and intrinsics K are fed into the depth completion network f ? , which is comprised of an encoder with calibrated backpro-jection layer followed by a decoder (Sec. 2.2, main text). Each calibrated backprojection realizes the backprojection process into 3D camera space by performing calibrated lifting of pixel coordinates using K, and projecting the depth representation to 1 dimension and multiplying it with the lifted coordinates -result of which is a 3D positional encoding of the scene structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. System Overview</head><p>To yield a unified depth and RGB representation, the 3D positional encoding from the depth branch is passed laterally to the RGB branch to enable association between each RGB feature and its 3D position. By doing so, we introduce 3D structure as an architectural inductive bias, which allows the network to reason about "close" points in the 2D image topology that are actually far in 3D scene topology. The RGB 3D representation is finally fed through the decoder to produce the final depth predictiond.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Loss Function</head><p>To train our model, we assume the availability of previous and next RGB frames I ? of the given image I or I t (to denote the current time frame) where ? ? T . = {t?1, t+1}. During training, we estimate the relative pose g ? t between images at time t and ? . Using I ? , K and g ? t , we can create the reconstruction? t of I t via reprojection (Eqn. 4, main text) to enable an unsupervised loss (Eqn. 6-9, main text), which include a photometric reconstruction term, a sparse depth reconstruction term and a local smoothness term.</p><p>We note that the photometric term can be replaced with more sophisticated measures of reprojection error <ref type="bibr" target="#b13">[14]</ref> and additional regularizers such as pose consistency <ref type="bibr" target="#b46">[47]</ref> or adaptive regularization weighting schemes <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b45">46]</ref> -which would likely boost performance even more. However, we choose a simple loss to demonstrate the efficacy of our novel architecture. We note that g ? t can be obtained by the means of a visual inertial odometry (VIO) system or a pose network if the VIO is not available. In the case where pose is obtained from network, the pose network can be trained jointly with our depth completion network (KBNet). Relative pose is learned as a byproduct of minimizing Eqn. 6 in main text. Also, since g ? t is only need for reprojection during training; hence, the VIO system and the pose network are not necessary for inference. Because our network is fast and light-weight (16ms run time per image, 6.9M parameters and 2.6GB memory as benchmarked on 1216 ? 352 images from KITTI <ref type="bibr" target="#b40">[41]</ref>), it can be deployed with a VIO system to learn online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Implementation and Training Details</head><p>We optimized our networks using Adam <ref type="bibr" target="#b20">[21]</ref> with ? 1 = 0.9 and ? 2 = 0.999. We trained for a total of 60 epochs on KITTI <ref type="bibr" target="#b40">[41]</ref>, 15 epochs on VOID <ref type="bibr" target="#b46">[47]</ref>, and 15 epochs on NYUv2 <ref type="bibr" target="#b39">[40]</ref>. We use a batch size of 8 with 768 ? 320 crops for KITTI, 640 ? 480 for VOID and 576 ? 416 for NYUv2.   <ref type="table">Table 7</ref>: Min pool and max pool kernel sizes for our sparseto-dense module Kernel sizes for VOID <ref type="bibr" target="#b46">[47]</ref> and NYUv2 <ref type="bibr" target="#b39">[40]</ref> are larger because the point cloud generated from VIO <ref type="bibr" target="#b10">[11]</ref> is much sparser than that of LIDAR in KITTI <ref type="bibr" target="#b40">[41]</ref>.</p><p>For KITTI, we choose w ph = 1, w co = 0.15, w st = 0.95, w sz = 0.6, and w sm = 0.04; for VOID and NYUv2, we set w sz = 2 and w sm = 2. Kernel sizes for our sparse-to-dense (S2D) module are shown in <ref type="table">Table 7</ref> for each dataset. We detail our learning rate schedule for each dataset in <ref type="table" target="#tab_8">Table 6</ref>. For data augmentations on KITTI, we performed random horizontal shifts to the image and depth map and randomly removed between 60% to 70% of the sparse points. For VOID and NYUv2, we randomly removed 30% to 60% of the sparse points. Augmentations are enabled 100% of the time up for VOID and NYUv2. For KITTI it is applied 100% of the time up to the 50th epoch and decreased by half every 5 epoch up to 60 epochs. Each augmentation has a 50% probability of being applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Features Learned by Sparse-to-Dense</head><p>In Sec 2.1 of the main text, we proposed a sparse-todense module (S2D) to learn a dense or quasi-dense representation of the sparse depth inputs. S2D utilizes a series of min and max pooling layers of various kernel sizes to densify the sparse depth inputs (for a list of kernel sizes used  <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref> without any form of densification. Row 4: "After Sparse-to-Dense" denotes the depth features learned by the proposed sparse-to-dense (S2D) module. Those learned without our module are still sparse; whereas S2D produces a dense or quasi-dense representation before it reaches the depth completion network. This alleviates the network from having to densify or propagate the sparse signal, making the overall architecture more efficient.</p><p>for each dataset, please see <ref type="table">Table 7</ref>). To balance the tradeoff between density and detail (large vs. small kernel sizes), and near and far structures (min vs. max pooling), we concatenate the pooled results and learn three 1 ? 1 convolutions. The output of which is fused with the input sparse depth using a 3 ? 3 convolution to "fill in the gaps". <ref type="figure" target="#fig_3">Fig. 8</ref> shows visualizations of features learned by S2D and a comparison to the features learned by typical convolutional e.g. ResNet or VGG blocks used by <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref>. Row 2 of <ref type="figure" target="#fig_3">Fig. 8</ref> shows that despite passing through several convolutional layers (? 10K to 20K parameters), the representation obtained by a typical convolution block is still sparse; so the later layers will still have many zeroactivations and must continue to densify the features. In contrast, using our proposed S2D (? 900 parameters), the depth representation learned is dense or quasi-dense before reaching the depth completion network (row 3). This enables non-zero activations in the later layers, which allows the network to use its early convolutions for learning scene geometry rather than densification.</p><p>We note that our sparse-to-dense module may bare some resemblance to Spatial Pyramid Pooling (SPP) employed in classification <ref type="bibr" target="#b17">[18]</ref> or stereo matching <ref type="bibr" target="#b1">[2]</ref>. However, we note that <ref type="bibr" target="#b17">[18]</ref> used SPP with max pooling to ensure that feature map sizes are consistent for different input sizes. <ref type="bibr" target="#b1">[2]</ref> used average pooling to increase receptive field. Both use cases are intended for dense input. We discussed the drawbacks of max pooling <ref type="bibr" target="#b17">[18]</ref> in Sec. 2.1 of the main text and showed in <ref type="table">Table 3</ref> of main text that SPP underperforms compare to our S2DM. Also we note that using average pooling <ref type="bibr" target="#b1">[2]</ref> will destroy the signal because the kernel will convolve and average over mostly zeros. The work that is most similar to our S2D module is the SPP for depth completion proposed by <ref type="bibr" target="#b44">[45]</ref>. However, <ref type="bibr" target="#b44">[45]</ref> only uses max pooling which decimates the detail of nearby structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sensitivity Studies</head><p>In this section, we provide additional studies to quantify the sensitivity of our model to incorrect calibration and various sparse depth density levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. To Incorrect Calibration</head><p>We showed in <ref type="table">Table 5</ref> of the main text that our method generalizes well when given the correct calibration at test time. To consider the scenario of a miscalibrated camera, we studied the sensitivity of our model to incorrect calibration on the KITTI dataset (outdoor scenarios) in <ref type="figure">Fig. 5</ref> in the main text (also here in <ref type="figure" target="#fig_4">Fig. 10</ref>). Now, we further extend the sensitivity study to the indoor setting by conducting a similar sensitivity study on the VOID dataset ( <ref type="figure" target="#fig_5">Fig. 11</ref>). To this end, we consider changes to the focal length (f ) and principal point (c x , c y ) parameters to create erroneous intrinsic calibration matrices for input to a pretrained model on VOID.</p><p>The overall trend for indoor setting, (VOID, <ref type="figure" target="#fig_5">Fig. 11</ref>) is similar to that of outdoor setting (KITTI, <ref type="figure" target="#fig_4">Fig. 10</ref>). For both indoors and outdoors, our model is robust to changes in principal point parameters (c x , c y ) -increasing or decreasing them by up to 25% has little effect on performance. <ref type="figure">Figure 9</ref>: Visualization of predicted depth for incorrect calibration. -25% K denotes 25% decrease to intrinsics parameters and +25% K denotes 25% increase. Overall error in -25% is increased (slight brigher shade of red). Larger errors caused by incorrect intrinsics is generally located at the edge of the depth map. +25% have little effect on our predictions. This is because decreasing focal length causes surfaces to be distorted, which in turn affect depth predictions. On the other hand, increasing focal length packs points closer together, which is less detrimental in comparison. This is because these parameters shifts the optical center so they do not affect the overall structure of the scene. We note that for large values outside of reasonable perturbation range will cause the performance to decrease.</p><p>Unlike its behavior with changes in the principal point, the model degrades when focal length (f ) is decreased. For both indoors and outdoors, we are robust up to 10% decrease in focal length, after which error will increase. We note that the performance drop is asymmetric, our model is robust to increases in focal length up to 20%. The reason for this phenomeon is as follows: Geometrically, decreases in focal length will cause points to backproject to a wider field of view, which distorts surfaces by pushing points that belong to the same surface far from each other. On the other hand, increases in focal length will cause points to pack tighter together. This does not disrupt the scene struc- <ref type="figure">Figure 12</ref>: Visualization of predicted depth for various density levels on VOID. Columns 1, 2: Our method works well for density levels of 0.5% and 0.15%. Column 3: The quality of predicted depth begins to degrade in far homogeneous regions where there are no sparse points e.g. wall when density level drops to 0.05%. <ref type="figure" target="#fig_0">Figure 13</ref>: VOID test set across various density levels. We compare our method against VOICED <ref type="bibr" target="#b46">[47]</ref> and ScaffNet <ref type="bibr" target="#b44">[45]</ref> on the VOID test set for various density levels (0.5%, 0.15%, 0.05%). In terms of MAE, our method performs better than other methods across all density levels. ture for small values, but for large values, points will get squashed together; this is demonstrated by the small uptick in error when increasing focal length by 20 to 25%.</p><p>We note that these values are well out of the typical range of calibration error and should not be of concern. For example when using off-the-shelf calibration packages that implements <ref type="bibr" target="#b51">[52]</ref> to calibrate our camera, we obtained a standard error of ? 0.6%, which yields ? ? 1.1% margin of   <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47]</ref>; however, at 0.05%, <ref type="bibr" target="#b44">[45]</ref> performs better on the RMSE metric.</p><p>error for a 95% confidence interval. Nonetheless, there exists the risk of using the wrong calibration; however, we <ref type="figure" target="#fig_1">Figure 14</ref>: Qualitative results on generalization to novel scenes captured by a different sensor platform. We trained our model on VOID <ref type="bibr" target="#b46">[47]</ref> (captured by Intel RealSense) and tested the model on NYUv2 <ref type="bibr" target="#b39">[40]</ref> (captured by Microsoft Kinect). We also used a pretrained model (on VOID) of <ref type="bibr" target="#b46">[47]</ref> as the baseline and tested it on NYUv2. Here, we show the predicted depth as point clouds, backprojected to 3D and colored. <ref type="bibr" target="#b46">[47]</ref> predicted a distorted scene where the points are bowed towards the camera; whereas, while our predictions are not perfect, they are reasonable.</p><p>believe this trade-off is well worth the performance boost provided by the proposed architecture. <ref type="figure">Fig. 9</ref> shows a visualization of depth predicted by our model when using erroneous calibration. -25% K denotes a 25% decrease to focal length and principal point and +25% K denotes a 25% increase to both. As we can see, the larger errors are typically located along the border of the predicted depth map; there is also a slight increase in error (brighter shade of red) for the entire scene. Increasing intrinsics by 25% affects the output less significantly, but nonetheless we observe an increase in errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. To Various Density Levels</head><p>In <ref type="table" target="#tab_10">Table 8</ref>, we consider three different levels of density for the sparse depth inputs, 0.50%, 0.15%, 0.05% of the image space, that are provided by the VOID dataset <ref type="bibr" target="#b46">[47]</ref>. To this end, we train a single model on VOID using sparse depth maps of 0.50% density and evaluate it on 0.50%, 0.15%, 0.05% density test sets. We also compare our method against <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47]</ref> under these density levels.</p><p>As expected, as density decreases, our performance also degrades. However, we still outperform both <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47]</ref> under all three levels, see <ref type="figure" target="#fig_0">Fig. 13</ref>. We note that at the sparsest setting of 0.05%, <ref type="bibr" target="#b44">[45]</ref> does beat us on the RMSE metric. The reason for this is that we selected the kernel sizes for our model based on the sparsity level of 0.5%; therefore, when testing it on 10? sparser point cloud, our depth representa-tion will be more sparse as well, which limits the potential of our calibrated backprojection layers. In contrast, <ref type="bibr" target="#b44">[45]</ref> proposed a network to first estimate the dense coarse topology. This phenomenon is also observed in KITTI, shown in <ref type="table">Table 3</ref> of the main text, where we removed our sparse-todense module and we observed a significant drop in performance. <ref type="figure">Fig. 12</ref> shows qualitative evaluations on the three density levels. For 0.50%, error is low overall and the shape of the recovered scene resembles that of the ground truth. When we decrease density to 0.15%, we observe slight blurring in object shapes and increased errors in homogeneous regions. At 0.05%, we begin to observe artifact such as the green "blob" corresponding to the wall with more exaggerated errors in homogeneous regions. This is because locally the textureless surfaces give little to no information on object shape. Without sparse depth to anchor their values, they can be arbitrary. In this case the "empty" region is predicted as far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Generalization to Other Sensor Platforms</head><p>In Sec. 3.4 of the main text, we discussed our ability to generalize to other sensor platforms that may use a different test time camera than one used to collect training data. In <ref type="table">Table 5</ref> of the main text, we showed quantitatively that we generalize better than the baseline. Here, we demonstrate this qualitatively in    <ref type="table">Table 9</ref>: KITTI supervised depth completion benchmark.</p><p>Results are directly taken from online leaderboard. Note: SS-S2D <ref type="bibr" target="#b25">[26]</ref> and DDP <ref type="bibr" target="#b49">[50]</ref> compete in both supervised and unsupervised benchmarks. Our results are italicized. Despite being an unsupervised method, our method beats some supervised methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and our iMAE score (1.02) is ranked 5th amongst supervised methods.</p><p>To this end, we trained our model on VOID <ref type="bibr" target="#b46">[47]</ref> (captured by Intel RealSense) and tested the model on NYUv2 <ref type="table" target="#tab_0">Table 10</ref>: KITTI unsupervised depth completion benchmark. Results are directly taken from online leaderboard. Note: SS-S2D <ref type="bibr" target="#b25">[26]</ref> and DDP <ref type="bibr" target="#b49">[50]</ref> compete in both supervised and unsupervised benchmarks. Our method outperforms is trained only on KITTI, but still the state of the art <ref type="bibr" target="#b44">[45]</ref> (trained on KITTI and Virtual KITTI <ref type="bibr" target="#b12">[13]</ref>) by an average of 8.8% across all metrics. * denotes methods that use additional synthetic data for training.</p><p>[40] (captured by Microsoft Kinect). We similarly trained the baseline <ref type="bibr" target="#b46">[47]</ref> on VOID and tested it on NYUv2. <ref type="figure" target="#fig_1">Fig. 14</ref> shows the predicted depth, backprojected to the point clouds in 3D and colored. As we can see, <ref type="bibr" target="#b46">[47]</ref> predicted a distorted scene; in contrast, ours is not perfect, but reasonable. This demonstrates the benefit of taking calibration as input. It allows the model to generalize well when it is deployed to a sensor platform where the camera that is used is different than the one used for training. We also note that neither models have been trained on NYUv2 which features a different scene distribution than that of VOID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. KITTI Depth Completion Benchmark</head><p>In Sec. 3.3 of the main text, we compare our method against unsupervised methods on the KITTI online leaderboard. Here, we show quantitative comparisons against both supervised <ref type="table">(Table 9</ref>) and unsupervised <ref type="table" target="#tab_0">(Table 10)</ref> methods. Results and method names are directly taken from the KITTI online leaderboard. Here we refer to our method as KNBet, as listed on the leaderboard. We note that SS-S2D <ref type="bibr" target="#b25">[26]</ref> and DDP <ref type="bibr" target="#b49">[50]</ref> compete in both supervised and unsupervised benchmarks. Additionally, we provide high resolution examples of our output in <ref type="figure" target="#fig_7">Fig. 15</ref>.</p><p>Despite being trained without ground-truth annotations, <ref type="table">Table 9</ref> shows that our method is competitive even amongst supervised method. We outperform some supervised methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> across most metrics. We note that our method significantly improves the iMAE and iRMSE metrics, to the point where we are comparable to some of the supervised methods for close range performance. Our iMAE score, which penalizes mean error in close range regions, is ranked 5th overall amongst both supervised and unsupervised methods. To the best of our knowledge, we are the first work in unsupervised depth completion to demonstrate comparable performance to supervised methods. We note that supervised methods are generally more computationally expensive with high model complexity e.g. in terms of number of parameters, <ref type="bibr" target="#b29">[30]</ref> uses 25.84M, <ref type="bibr" target="#b31">[32]</ref> 53.4M, and <ref type="bibr" target="#b48">[49]</ref> 28.99M; whereas we only use 6.9M.</p><p>Compared to unsupervised methods <ref type="table" target="#tab_0">(Table 10)</ref>, we rank first amongst all methods with the best scores across all metrics. Our model even beat methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b44">45]</ref> that use additional synthetic data (Virtual KITTI <ref type="bibr" target="#b12">[13]</ref>) for training, amongst which is the state of the art <ref type="bibr" target="#b44">[45]</ref>. Despite this, we beat <ref type="bibr" target="#b44">[45]</ref> by an average of 8.8% across all metrics while using 11.5% fewer parameters. These results demonstrates the potential of our method to bridge the gap between supervised and unsupervised method. Moreover, our network is light-weight and can be deployed on VIO system <ref type="bibr" target="#b10">[11]</ref>. While there is still a long road ahead, these results show a lot of promise in enabling unsupervised methods to learn online and to be used for real-time application for low-cost hardware systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Calibrated Backprojection (KB) Layer. The standard depth and color image encoding layers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results on KITTI test set. Head-to-head comparison against<ref type="bibr" target="#b46">[47]</ref>. Thanks to our 3D positional encoding, our method performs well on regions where adjacent structures in 2D image space are far apart in the 3D scene e.g. street sign and wall (left panel, highlighted in green) and far region of the road (right panel, in orange).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of depth features. Row 3: "After Convolution Block" denotes the depth features produced by a typical first convolutional block used by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>Sensitivity to changes in calibration on KITTI. Focal length and principal point are altered to test sensitivity to changes in intrinsics parameters. Our method is robust to change up to ? 10% change. After which, performance degrades. We note that changes in principal point (c x , c y ) have little effect; whereas decreasing focal length (f ) causes large drop in performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 :</head><label>11</label><figDesc>Sensitivity to changes in calibration on VOID. Focal length and principal point are altered to test sensitivity to changes in intrinsics parameters. Our method is robust to change up to ? 10% change. After which, performance degrades. We note that changes in principal point (c x , c y ) have little effect; whereas decreasing focal length (f ) causes large drop in performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig. 14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 15 :</head><label>15</label><figDesc>Qualitative results on KITTI depth completion benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Error metrics. d gt denotes the ground-truth depth.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results on the KITTI test set. Our method outperforms all unsupervised methods across all metrics on the KITTI leaderboard. Compared to the the baseline<ref type="bibr" target="#b46">[47]</ref>, we improve by an average of 13.7% across all metrics while using 29% fewer parameters.</figDesc><table /><note>* denotes methods that use additional synthetic data for training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Quantitative results on VOID test set. We outper- form all competing methods across all metrics. Compared to</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>in Supp. Mat.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Learning schedule for KITTI, VOID, and NYUv2.</figDesc><table><row><cell>Dataset</cell><cell>Min Pool</cell><cell>Max Pool</cell></row><row><cell>KITTI [41]</cell><cell>5, 7, 9, 11, 13</cell><cell>15, 17</cell></row><row><cell>VOID [47]</cell><cell>15, 17</cell><cell>23, 27, 29</cell></row><row><cell>NYUv2 [40]</cell><cell>15, 17</cell><cell>23, 27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Sensitivity study on various sparse depth density levels on VOID. We train a single model on VOID using sparse depth maps of 0.50% density and evaluate it on 0.50%, 0.15%, 0.05% density test sets. As expected, performance degrade as the input become more sparse. Overall, we perform better than</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by ARL W911NF-20-1-0158 and ONR N00014-17-1-2072.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Color tv: total variation methods for restoration of vector-valued images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Blomgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="304" to="309" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning joint 2d-3d representations for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cspn++: Learning context and resource aware convolutional spatial propagation networks for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep convolutional compressed sensing for lidar depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Chodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning morphological operators for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Dimitrievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Veelaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Philips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Concepts for Intelligent Vision Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uncertainty-aware cnns for depth completion: Uncertainty from beginning to end</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Holmquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Persson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12014" to="12023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Propagating confidences through cnns for sparse data regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad Shahbaz</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Camconvs: Camera-aware multi-scale convolutions for singleview depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Facil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Civera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geosupervised visual depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Yohann Cabon, and Eleonora Vig. Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8977" to="8986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">gvnn: Neural network library for geometric computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viorica</forename><surname>P?tr?ucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Alvey vision conference</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1988" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="10" to="5244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hms-net: Hierarchical multi-scale sparsity-invariant network for sparse depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenggan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3429" to="3441" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse and dense data with cnns: Depth completion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoul</forename><forename type="middle">De</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fawzi</forename><surname>Nashashibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR: International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">In defense of classical image processing: Fast depth completion on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 15th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Epnp: An accurate o (n) solution to the pnp problem. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">155</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A multi-scale guided cascade hourglass network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggen</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanchao</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="32" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Project to adapt: Domain adaptation for depth completion from noisy and sparse sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Lopez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilherme</forename><surname>Venturelli Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An invitation to 3-d vision: from images to geometric models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Shankar</forename><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust monocular visual-inertial depth completion for embedded systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Geneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoquan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Optimal approximations by piecewise smooth functions and associated variational problems. Communications on pure and applied mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bryant Mumford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Non-local spatial propagation network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungdon</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Kuei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In-So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV 2020. European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Bayesian deep basis fitting for depth completion with uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camillo J</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15254</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Depth completion via deep basis fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ty</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camillo</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kornia: an open source differentiable computer vision library for pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3674" to="3683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Leonid I Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emad</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: nonlinear phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Khiem Vuong, and Stergios I Roumeliotis. Deep depth estimation from visualinertial slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kourosh</forename><surname>Sartipi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Ke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Uwe Franke, Marc Pollefeys, and Christoph Stiller. Semantically guided depth upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pinggera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German conference on pattern recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dfusenet: Deep fusion of rgb and sparse depth information for image guided dense depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shreyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ty</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camillo J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sparse and noisy lidar completion with rgb guidance and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 16th International Conference on Machine Vision Applications (MVA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning topology from synthetic data for unsupervised depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safa</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An adaptive framework for learning unsupervised depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byung-Woo</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised depth completion from visual inertial odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Tsuei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5644" to="5653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Depth completion from sparse lidar data with depth-normal constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dense depth posterior (ddp) from single image and sparse range</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A flexible new technique for camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Codevio: Visual-inertial odometry with learned optimizable dense depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoquan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
