<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Nested Named Entity Recognition with Partially-Observed TreeCRFs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Fu</surname></persName>
							<email>yao.fu@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<email>f.huang@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Nested Named Entity Recognition with Partially-Observed TreeCRFs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Named entity recognition (NER) is a well-studied task in natural language processing. However, the widely-used sequence labeling framework is difficult to detect entities with nested structures. In this work, we view nested NER as constituency parsing with partially-observed trees and model it with partially-observed TreeCRFs. Specifically, we view all labeled entity spans as observed nodes in a constituency tree, and other spans as latent nodes. With the TreeCRF we achieve a uniform way to jointly model the observed and the latent nodes. To compute the probability of partial trees with partial marginalization, we propose a variant of the Inside algorithm, the MASKED INSIDE algorithm, that supports different inference operations for different nodes (evaluation for the observed, marginalization for the latent, and rejection for nodes incompatible with the observed) with efficient parallelized implementation, thus significantly speeding up training and inference. Experiments show that our approach achieves the state-of-the-art (SOTA) F1 scores on the ACE2004, ACE2005 dataset, and shows comparable performance to SOTA models on the GENIA dataset. Our approach is implemented at: https://github.com</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Named entity recognition (NER) is a fundamental task in natural language processing <ref type="bibr" target="#b24">(McCallum and Li 2003)</ref>. Although recent work shows huge success in flat NER with modern neural architectures and pretrained encoders <ref type="bibr" target="#b9">(Huang, Xu, and Yu 2015;</ref><ref type="bibr" target="#b3">Devlin et al. 2019)</ref>, NER with nested structures is still difficult since simple sequence labeling techniques cannot model these structures <ref type="bibr" target="#b7">(Finkel and Manning 2009)</ref>. Nested NER is also important because entities of nested structures are observed in many domains due to their compositionality <ref type="bibr" target="#b1">(Alex, Haddow, and Grover 2007)</ref> and consequently involved in many real-world applications <ref type="bibr" target="#b14">(Kim et al. 2003)</ref>. <ref type="figure">Figure 1</ref> gives an example sentence with nested entities. We observe that an inner entity can be part of an outer entity, which is quite similar to the constituency structure. Additionally, the boundaries of nested entities cannot cross. <ref type="figure">Figure 1</ref>: An example sentence in the ACE dataset with its nested entities. Viewing the nested entity structure as a partially observed tree, a key observation is that other spans without annotation can be modeled as latent nodes (dashed lines) in a full tree. This observation motivates us to model nested NER with partially-observed constituency trees.</p><p>These observations motivate us to formulate nested NER as parsing with partially observed constituency trees: we can view entities with annotations as observed constituents, and assume a distribution of latent constituents over spans without annotation. For example, State Department and Richard Boucher can be two possible latent entities in <ref type="figure">Figure 1</ref>.</p><p>In this work, we propose to model observed and latent entities jointly with a TreeCRF <ref type="bibr" target="#b40">(Zhang, Zhou, and Li 2020;</ref><ref type="bibr" target="#b28">Rush 2020)</ref>. Specifically, we use a pretrained encoder to obtain word representations, a biaffine scoring mechanism (Dozat and Manning 2017) to obtain log potentials, and a TreeCRF to decode full constituency trees. Using TreeCRFs gives the advantage of modeling different types of entities in a probabilistically principled way and properly handling the ambiguities of the latent constituents. For optimization, we marginalize all latent constituents out, and maximize the resulting probability of observed partial trees.</p><p>Previously, the application of TreeCRFs for parsing is limited by the cubic time complexity of the Inside algorithm <ref type="bibr" target="#b6">(Eisner 2016)</ref>. Recent works show that it is possible to parallelize the Inside algorithm on modern hardware <ref type="bibr" target="#b28">(Rush 2020)</ref>. While a vanilla Inside algorithm sums over all possible trees, in our setting, we require an Inside-styled partial marginalization which only sums over latent nodes. To adapt the Inside algorithm to partial summation, we propose a masking method that differentiates different nodes during marginalization. Furthermore, to efficiently compute partial marginalization, we propose a MASKED INSIDE algorithm that performs different inference operations for different types of nodes in a unified masked summation framework. We highlight the advantages of the MASKED INSIDE compared with a naive partial marginalization algorithm from two perspectives: (a) it is highly batchifiable and parallelizable, allowing us to fully exploit the computational power of modern hardware (like GPUs) and tensor libraries (like Pytorch); (b) it is conceptually simple and can be easily implemented by reusing existing implementation of the original Inside algorithm in highly optimized structured prediction libraries (like Torch-Struct <ref type="bibr" target="#b28">Rush 2020)</ref>.</p><p>We further propose two regularization techniques for TreeCRFs. Specifically, we propose potential normalization, inspired by batch normalization <ref type="bibr" target="#b10">(Ioffe and Szegedy 2015)</ref>, and structure smoothing, inspired by label smoothing <ref type="bibr" target="#b26">(M?ller, Kornblith, and Hinton 2019)</ref>. These two regularizations can be seamlessly integrated with MASKED INSIDE, making their implementation simple and efficient.</p><p>We conduct experiments on three standard benchmark datasets. Experimental results show that our approach achieves 86.6, 85.4, and 78.2 scores in terms of F1 on the ACE2004, ACE2005, and GENIA datasets, respectively, which achieves SOTA F1 scores on the ACE2004, ACE2005 dataset, and shows comparable performance to SOTA models on the GENIA dataset. We will release the codes for further research. Our contributions are:</p><p>? We propose to formulate nested NER as constituency parsing with partial trees and use partially-observed TreeCRFs to jointly model observed and latent nodes.</p><p>? We propose the MASKED INSIDE algorithm for efficient partial marginalization and its regularization techniques.</p><p>? We demonstrate the effectiveness of our proposed methods with extensive experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Nested NER It has been a long history of research involving named entity recognition <ref type="bibr" target="#b42">(Zhou and Su 2002;</ref><ref type="bibr" target="#b24">McCallum and Li 2003)</ref>.</p><p>In the era of deep learning, the LSTM-CRF model achieves very good results in recognizing named entities <ref type="bibr" target="#b9">(Huang, Xu, and Yu 2015;</ref><ref type="bibr" target="#b17">Lample et al. 2016)</ref>, especially when equipped with pretrained encoders <ref type="bibr" target="#b27">(Peters et al. 2018;</ref><ref type="bibr" target="#b3">Devlin et al. 2019</ref>). <ref type="bibr" target="#b7">Finkel and Manning (2009)</ref> point out that named entities are often nested while traditional sequential labeling models cannot handle the nested structure because they can only assign one label to each token. Earlier research on nested NER is rule-based <ref type="bibr" target="#b37">(Zhang et al. 2004)</ref>. Recent works in nested NER are in various paradigms as follows:</p><p>Hypergraph-based <ref type="bibr" target="#b22">Lu and Roth (2015)</ref>; <ref type="bibr" target="#b13">Katiyar and Cardie (2018)</ref>;  propose the hypergraph-based method to solve this problem. They design a hypergraph to represent all possible nested structures, which guarantees that nested entities can be recovered from the hypergraph tags. However, the hypergraph needs to be carefully designed to avoid spurious structures and structural ambiguities which inevitably leads to higher modeling and compu-tational complexity. Compared with hypergraph-based approaches, our method tackles ambiguities in a probabilistically principled way by marginalizing all possible latent spans out, and can be implemented easily and efficiently. Transition-based Transition-based models are generally similar to shift-reduce parsers with tailored actions for different formalisms.  propose a method to construct nested mentions via a sequence of shift/ reduce/ unary actions. <ref type="bibr" target="#b8">Fisher and Vlachos (2019)</ref> propose to form nested structures by merging tokens and/or entities into entities for entity representation. Compare with these methods, our approach does not involve the manual labor for designing specialized transition systems, which largely requires domain expertise thus being not generalizable. Our partially observed TreeCRFs is more general-purpose and can be flexibly applied to partial trees of different formalisms. Span-based Another strategy for nested NER is the spanbased methods <ref type="bibr" target="#b35">(Xu, Jiang, and Watcharawittayakul 2017;</ref><ref type="bibr" target="#b29">Sohrab and Miwa 2018;</ref><ref type="bibr" target="#b34">Xia et al. 2019;</ref><ref type="bibr" target="#b23">Luan et al. 2019;</ref><ref type="bibr" target="#b41">Zheng et al. 2019;</ref><ref type="bibr" target="#b31">Tan et al. 2020;</ref><ref type="bibr" target="#b12">Jue et al. 2020)</ref>. These models first compute the representations for all subsequences in a sentence with tailored neural architectures, then classify these spans with locally-normalized scores. Compared with these models, our TreeCRF can model the dependency for all subsequences with a globally-normalized structured distribution, which consequently leads to clear performance improvements.</p><p>Others There are many other attempts for Nested NER. <ref type="bibr" target="#b25">Muis and Lu (2017)</ref> develop a gap-based tagging schema to capture nested structures. <ref type="bibr" target="#b21">Lin et al. (2019)</ref> propose a sequence-to-nuggets architecture for nested mention detection. <ref type="bibr" target="#b30">Strakov?, Straka, and Hajic (2019)</ref> propose to use a sequence-to-sequence framework to predict the entity label one by one.  treat NER as a machine reading comprehension task. Generally, it is hard to study nested NER in a unified framework and we aim to model it in a probabilistically principled way with TreeCRFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constituency Parsing with TreeCRFs</head><p>TreeCRFs <ref type="bibr" target="#b5">(Eisner 2000</ref><ref type="bibr" target="#b6">(Eisner , 2016</ref><ref type="bibr" target="#b40">Zhang, Zhou, and Li 2020)</ref> are well-studied in the parsing literature. Before the resurgence of deep learning, their application is limited due to its cubic time complexity. Recent work shows that with parallel computation <ref type="bibr" target="#b40">(Zhang, Zhou, and Li 2020;</ref><ref type="bibr" target="#b28">Rush 2020)</ref>, they can be efficiently implemented on modern hardware with high-optimized tensor operation libraries, reducing the complexity to at least quadratic time. Traditional literature focus on parsing with full annotations. Although some works study partial annotation, many of them are limited in simulated datasets, e.g., by dropping out certain nodes from fully-annotated trees <ref type="bibr" target="#b38">(Zhang et al. 2017;</ref>. Rather than being simulated, we emphasize that our application of TreeCRFs on Nested NER is a real-world example. Moreover, we note that our approach is not limited to nested NER, and an interesting future direction would be applying it for parsing with other types of partial trees.</p><formula xml:id="formula_0">A B C D E F G A B C D E F G Observed Rejected Latent Latent -Realized</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node type Inference</head><p>Observed Evaluation</p><p>Rejected Rejection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Marginalization</head><p>Latent -Realized - <ref type="figure">Figure 2</ref>: An example symbol tree. Left: observed partial tree (lower) and its corresponding symbol tree (upper) with different types of nodes induced from the observed tree. The symbol tree notation further enables us to build different types of masks for the MASKED INSIDE algorithm. Right: a full tree compatible with the left partial tree (lower) by realizing the latent nodes from to <ref type="bibr">(upper)</ref>. An entity with dashed lines corresponds to a realized latent node . A partial tree may correspond to different realized full trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Our model consists of a pretrained encoder, a biaffine scoring module, and a TreeCRF model. Given a sentence, we obtain the contextualized representations from the encoder, feed the representations to the biaffine layer to get the log potentials for the TreeCRF model, then use the TreeCRF to decode a constituency tree. To calculate the probability of partial trees, we propose the MASKED INSIDE algorithm as a simple, efficient algorithm for partial marginalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Base Biaffine Scoring Architecture</head><p>Given a sentence x as a sequence of words: x = [x 1 , x 2 , ..., x n ], x i ? V, V is the vocabulary and n is the sentence length. We use a base biaffine encoder similar to the biaffine dependency parser in Dozat and Manning (2017) to predict span scores: e 1 , ..., e n = FF(Enc(x)) (1)</p><formula xml:id="formula_1">s ijk = e i U (1) k e j + (e i + e j ) U (2) k + b k<label>(2)</label></formula><p>Where Enc(?) denotes a pretrained encoder, FF(?) denotes a feed-forward network, e i denotes the contextualized embedding for word</p><formula xml:id="formula_2">x i , U (1) k , U (2) k</formula><p>and b k are the parameters for the biaffine scoring mechanism, and s ijk means the score for a constituent spanning from word x i to x j (inclusive) with label k where i, j ? [1, 2, ..., n], k ? [1, 2, ..., |L|], L is the set of labels for the constituents. We further note L is a union of observed labels L o and latent labels L l , as we will explain later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Partially-Observed TreeCRFs</head><p>A constituency TreeCRF is a probabilistic discriminative model that defines a distribution over constituency trees T given sentence x. We represent a labeled constituency tree as a rank-3 binary tensor T where T ijk = 1 means that there is a span from word x i to x j with label k. We use the biaffine scores s ijk as the log potentials, and the probability of a tree is given by the Gibbs distribution:</p><p>Algorithm 1 SYMBOL TREE AND MASK CONSTRUCTION 1: Input: partial tree T , Lo observed labels, L l latent labels. 2: Initialize all nodes as latent :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>For all i, j ? {1, 2, ..., n}:</p><formula xml:id="formula_3">T [i, j] = ?k1 ? Lo, M [i, j, k1] = 0; ?k2 ? L l , M [i, j, k2] = 1 4: for i, j ? 1 to n do 5: if ?k, T ijk = 1 then Observe entity (i, j) with label k 6:T [i, j] = ? 7: M [i, j, k] = 1, ?m ? L, m = k, M [i, j, m] = 0 8:</formula><p>For all spans with crossed boundaries with (i, j):</p><formula xml:id="formula_4">9: ?(i , j ), i &lt; i &amp; i ? j &lt; j: T [i , j ] = ?, ?m ? L, M [i , j , m] = 0 10: ?(i , j ), i &lt; i ? j &amp; j &lt; j : T [i , j ] = ?, ?m ? L, M [i , j , m] = 0 11: Return: symbol treeT , mask M s(T ) = ijk T ijk s ijk (3) p(T |x) = exp(s(T )) Z (4) Z = T exp(s(T )) = INSIDE(s)<label>(5)</label></formula><p>Where Z is the partition function that sums over all possible tree structure T , which can be computed exactly with the Inside algorithm <ref type="bibr" target="#b6">(Eisner 2016)</ref>. For nested NER, the annotations of trees are incomplete so we only get partial trees. With a slight abuse of notation, we still use T to denote partial trees, and there are locations in T that might be 1 but filled in with 0 because it is not observed (latent). To better understand the nature of different nodes in a partial tree, we introduce a convenient symbol tree notationT given a partial tree T .T is a n?n matrix with difference types of nodes. Algorithm 1 gives details for building symbol treesT (we delay the discussion about the mask M returned from Algorithm 1 to the next section). Nodes in T can only be one of ?, or ?. <ref type="figure">Figure 2</ref> left gives an example ofT . A ? inT means an observed node (a labeled entity) in T , a ? means a node that is incompatible with observed nodes because it overlaps with an observed entity (e.g., there cannot be a latent entity [BC] because B is already in the observed entity [AB] and the boundaries of entities cannot cross) and a means a latent node that is possible to be realized in a full tree (e.g., there is a possible entity [ABC] with some latent label). We note leaf nodes and the root can only be observed ? or latent .</p><p>Given the partial tree T , a full treeT compatible with T can be constructed by realizing to inT (Figure 2 right. A in the upper part denotes a realized latent span corresponding to an entity with a dashed line in the lower part). We further denote the set of labels for latent spans L l as opposed to the labels for observed entities L o . We restrict the labels for the latent spans to be within L l . For example, the labels for the constituents [ABC] and <ref type="bibr">[DEF]</ref> are only allowed to be in L l because they are latent. This separation would allow us to decode partial trees during inference by dropping out entities with latent labels. Since there are mul-Algorithm 2 INSIDE FOR PARTIAL MARGINALIZATION 1: Input: Scores s, partial tree T and its correspondingT 2: for i ? 1 to n do 3:</p><p>ifT</p><formula xml:id="formula_5">[i, i] = ? then Observed leaf 4: ?k ? Lo, T iik = 1, ?[i, i, k] = exp(s iik ) 5: ?m = k, ?[i, i, m] = 0 6: else ifT [i, i] = then Latent leaf 7: ?k ? Lo, ?[i, i, k] = 0 8: ?k ? L l , ?[i, i, k] = exp(s iik ) 9: for d ? 1 to n ? 1 do 10: for i ? 1 to n ? d do 11: j = i + d 12: ifT [i, j] = ? then Observed 13: ?k ? Lo, T ijk = 1 14: ?[i, j, k] = exp(s ijk )? j?1 l=i k 1 ,k 2 ?L ?[i, l, k1]?[l + 1, j, k2] 15: ?m = k, ?[i, j, m] = 0 16: else ifT [i, j] = then Latent 17: ?k ? L l , ?[i, j, k] = exp(s ijk )? j?1 l=i k 1 ,k 2 ?L ?[i, l, k1]?[l + 1, j, k2] 18: ?k ? Lo, ?[i, j, k] = 0 19: else ifT [i, j] = ? then Rejected 20: ?k ? L, ?[i, j, k] = 0 21: ifT [1, n] = ? then Observed root 22: ?k ? Lo, T 1nk = 1. Return s(T ) = ?[1, n, k] 23: else ifT [1, n] = then</formula><p>Latent root 24:</p><p>Return</p><formula xml:id="formula_6">s(T ) = log( k?L l ?[1, n, k])</formula><p>tiple ways to complete a partial tree, we useT to denote the set of full trees completed from T . To train the TreeCRF with partial trees, we maximize the conditional probability of p(T |x) computed by marginalizing all latent nodes out:</p><formula xml:id="formula_7">log p(T |x) = s(T ) ? log Z (6) s(T ) = log T ?T exp(s(T ))<label>(7)</label></formula><p>This objective could equivalently be viewed as the average probability of the observed partial tree T over its all possible compatible full trees inT .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked Inside for Efficient Partial Marginalization</head><p>To compute the partial marginalization in equation <ref type="formula" target="#formula_7">(7)</ref>, we give a tailored Inside algorithm that supports different inference operations for different nodes. As is shown in Algorithm 2, during the summation process, if the current node is: (a) an observed ?, then we evaluate (add) its corresponding score (line 4 and 14); (b) a latent whose label can only be in L l . So we reject (do not add) all the scores corresponding to observed labels L o (line 7 and 18), and sum over all scores corresponding to latent labels L l for this node (line 8 and 17); (c) a rejected ?, we reject all scores corresponding to this node (line 20). However, a naive implementation of Algorithm 2 can be inefficient with O(n 3 ) complexity. Such inefficiency has previously restricted the application of TreeCRFs in parsing literature. More severely, Algorithm 2 does not support Algorithm 3 MASKED INSIDE 1: Input: Scores s, mask M 2: for i ? 1 to n do 3:</p><formula xml:id="formula_8">?[i, i, k] = M [i, i, k] ? exp(s iik )</formula><p>Masked leaf scores 4: for d ? 1 to n ? 1 do 5:</p><p>Parallelization on i, tensor operation on l, k, k1, k2</p><formula xml:id="formula_9">1 ? i ? n ? d; j = i + d; k, k1, k2 ? {1, ..., |L|} 6: ?[i, j, k] = (M [i, j, k] exp(s ijk )) ? Masked scores j?1 l=i k 1 ,k 2 ?L ?[i, l, k1]?[l + 1, j, k2] 7: Return: s(T ) = log( k?L ?[1, n, k])</formula><p>batch computation over sentences, making it more impractical. Recent works show that, for the original Inside algorithm, it is possible to parallelize it on modern hardware architectures with efficient batch computation, reducing the complexity from O(n 3 ) to at least O(n 2 ), and could further be O(n log n) 1 <ref type="bibr" target="#b28">(Rush 2020;</ref><ref type="bibr" target="#b40">Zhang, Zhou, and Li 2020)</ref>. It would be ideal if we could use similar batchification techniques for Algorithm 2, which motivates our proposed MASKED INSIDE algorithm for efficient partial marginalization.</p><p>As is shown in Algorithm 3, the key insight of MASKED INSIDE is that all if-else statements for partial summation in Algorithm 2 can be re-written in a unified masked summation (line 3 and 6 in Algorithm 3) with a pre-computed mask M from Algorithm 1. To be specific, in Algorithm 1: (a) for an observed node ?, we mask out all scores except the score of its observed label (line 7), which corresponds to lines 4, 14 and 21 in Algorithm 2; (b) for a rejected node ?, we mask out all its scores (lines 9-10), which corresponds to line 19 in Algorithm 2; (c) for a latent node , we mask out the scores for all observed labels L o , and retain scores for all latent labels L l (line 3), which corresponds to lines 6, 16, and 23 in Algorithm 2. Applying different masks to Algorithm 3, we can recover all if-else statements in Algorithm 2. As two special cases, if all masks are 1 (i.e., not masked), we recover the original Inside algorithm; if the masks are constructed from a full tree, we recover the original bottom-up evaluation.</p><p>As an efficient alternative for Algorithm 2, the advantages of Algorithm 3 is that it is (a) conceptually much simpler and (b) highly parallelizable. The later allows us to fully exploit the computational power of modern hardware architectures (like GPUs) and highly optimized tensor operation libraries (like Pytorch). We note that it the parallelization on i in Algorithm 3 that reduces the complexity to at least O(n 2 ), Furthermore, as an equivalent implementation to Algorithm 3, we can apply masks to scores in the logarithm scale 2 then feed the masked scores to a normal Inside algorithm (rather ACE2004 ACE2005</p><p>GENIA <ref type="table" target="#tab_2">Train  Dev  Test  Train  Dev  Test  Train  Dev  Test  # sentences  7,078  859  922  7,194  969  1,047 14,836 1,855 1,855  with nested entities  2,691  290  377  2,691  338  330  3,199  362  448  # entities  22,172 2,510 3,024 24,441 3,200 2,993 46,473</ref>  </p><formula xml:id="formula_10">s(T ) = MASKEDINSIDE(s, M ) = INSIDE(log M + s) (8) p(T |x) = exp(s(T )) Z<label>(9)</label></formula><p>In practice, we compute the masks M in the data-processing stage. For training, by reusing existing implementations of the Inside algorithm in highly-optimized structured prediction libraries like Torch-Struct (Rush 2020), we can implement the MASKED INSIDE with one single line of code, as is shown in equation <ref type="formula">(8)</ref>, thus significantly reducing the implementation complexity required by Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularization</head><p>We propose two regularization techniques for TreeCRFs: (a) potential normalization, which is inspired by batch normalization <ref type="bibr" target="#b10">(Ioffe and Szegedy 2015)</ref>, and (b) structure smoothing, which is inspired by label smoothing <ref type="bibr" target="#b26">(M?ller, Kornblith, and Hinton 2019)</ref>. Potential normalization (PN) is simple: we normalize the scores s to an empirical distribution of zero mean and one variance. The difference with batch-norm (BN) is that we apply PN at an instance-level, rather than a batch-level. In our experiments, we observe that PN gives a slightly better convergence. Structure smoothing regularizes TreeCRFs by putting a small portion of weights to nodes that are marginalized out. Specifically, during the partial marginalization, instead of using a zero mask that does not include the weights of rejected nodes, we change the mask to a small value</p><formula xml:id="formula_11">M [i, j, k] = 0 ? M [i, j, k] = for rejected ?<label>(10)</label></formula><p>This would effectively add the weights of all rejected nodes to s(T ) with a multiplier . This is similar to label smoothing which adds a small portion of weights to all labels other than the target label. The reason that we call it structure smoothing is that it not only smooths over the labels, but also smooths over different tree structures 3 . We further observe that structure smoothing should be based on potential normalization for numerical stability, otherwise it does not converge in our experiments. The implementation of structure smoothing is still easy and aligns with previous discussions about equation <ref type="formula">(8)</ref> as one only needs to change the zeros in M to .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and Inference</head><p>During training, we maximize the log conditional probability log p(T |x) efficiently computed by equation <ref type="formula">(8)</ref> and <ref type="formula" target="#formula_10">(9)</ref>. During inference, we use CKY decoding to decode a full tree with the maximum probability. We only include nodes whose labels are in the observed label set L o , and dismiss nodes whose labels are in the latent set L l . This would allow us to decode nested entities (partial trees) for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We conduct experiments on three standard benchmark datasets. We show that our proposed approach achieves SOTA performance. We further conduct detailed error analysis, case study, and time complexity analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We conduct experiments on the ACE2004, ACE2005 (Doddington et al. 2004), and GENIA <ref type="bibr" target="#b14">(Kim et al. 2003)</ref> datasets.</p><p>There are seven types of entities as 'FAC', 'LOC', 'ORG', 'PER', 'WEA', 'GPE', 'VEH' in the ACE datasets and five types of entities as 'G#DNA', 'G#RNA', 'G#protein', 'G#cell line', 'G#cell type' in the GENIA dataset. The statistics of these datasets are shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We use variants of BERT <ref type="bibr" target="#b3">(Devlin et al. 2019)</ref> to encode sentences. For the ACE2004 and ACE2005 datasets, we use the bert-large-cased checkpoint. For GENIA, we use BioBERT v1.1 <ref type="bibr" target="#b18">(Lee et al. 2020)</ref>. As words in the sentence are divided into word pieces, we use the representation of the first piece to represent each word after BERT encoding. The parameter in BERT is also trainable. We use AdamW optimizer with the learning rate 2e-5 on ACE2004 dataset and 3e-5 on ACE2005 and GENIA dataset. The used for structure smoothing is 0.01 on ACE2004 dataset and 0.02 on ACE2005 and GENIA dataset. We apply 0.2 dropout after BERT encoding. Denote the hidden size of the encoder as h (h = 1024 for BERT Large, and 768 for BioBERT). We apply two feed-forward layers before the biaffine scoring mechanism, with h and h/2 hidden size, respectively. Consequently, the size of biaffine matrix is h/2 ? h/2. We set the size of latent labels L l to 1 as in our preliminary experiments we find out the performance does not differ significantly with more latent labels.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>Below we list our baseline models with comparable settings. We also include the results of models that use additional supervision, which are not directly comparable to ours.</p><p>LSTM-CRF is a classical baseline for NER. This model cannot solve the problem of nested entities <ref type="bibr" target="#b17">(Lample et al. 2016)</ref>.</p><p>FOFE is a span-based method that classifies over all subsequences of a sentence with a fixed-size forgetting encoding <ref type="bibr" target="#b35">(Xu, Jiang, and Watcharawittayakul 2017)</ref>.</p><p>Transition is a shift-reduce based system that learns to construct the nested structure in a bottom-up manner through an action sequence .</p><p>Cascaded-CRF applies several stacked CRF layers to recognize nested entities at different levels in an inside-out manner <ref type="bibr" target="#b11">(Ju, Miwa, and Ananiadou 2018)</ref>.</p><p>SH improves LH <ref type="bibr" target="#b13">(Katiyar and Cardie 2018)</ref> by considering the transition between labels to alleviate labeling ambiguity of hypergraphs .</p><p>MGNER first applies the Detector to generate possible spans as candidates and then applies a Classifier for the entity type <ref type="bibr" target="#b34">(Xia et al. 2019)</ref>.</p><p>Merge and Label (ML) first merges tokens and/or entities into entities forming nested structures and then labels entities to corresponding types <ref type="bibr" target="#b8">(Fisher and Vlachos 2019)</ref>. Seq2seq is under a encoder-decoder framework to predict the entity one by one <ref type="bibr" target="#b30">(Strakov?, Straka, and Hajic 2019)</ref>. BENSC is a span-based method that incorporates a boundary detection task for multitask learning <ref type="bibr" target="#b31">(Tan et al. 2020)</ref>. Pyramid is the state-of-the-art method without external supervision. It recursively inputs tokens and regions into flat NER layers for span representations <ref type="bibr" target="#b12">(Jue et al. 2020)</ref>. <ref type="table" target="#tab_2">Table 2</ref> shows the overall results on ACE2004, ACE2005, and GENIA. We primarily compare our model with the Pyramid(BERT) model <ref type="bibr" target="#b12">(Jue et al. 2020)</ref>, as it achieves SOTA scores without additional supervision signals. As we believe there are still rooms for further performance improvements, e.g., to use a more powerful, larger encoder (like <ref type="bibr">GPT3 Brown et al. 2020)</ref> or to use more ensemble methods, e.g., to ensemble FLAIR <ref type="bibr" target="#b0">(Akbik, Blythe, and Vollgraf 2018)</ref> and other pretrained encoders, to standardize the comparison and validate the effectiveness of TreeCRFs, we restrict the encoder to be BERT. We denote our partially-observed TreeCRF as PO-TreeCRF. Our PO-TreeCRF achieves 86.6, 85.4, and 78.2 scores in terms of F 1 ? human rights group amnesty international ? ? the center for strategic and international studies in Jakarta ? <ref type="figure">Figure 3</ref>: Inferred latent tree structure examples. Solid lines: predicted entities; dashed lines: realized latent constituents. Although there are spans that are not that meaningful like rights group, we still observe meaningful inferred spans like for strategic and international studies and in Jakarta.  on the ACE2004, ACE2005, and GENIA datasets, respectively, which achieves the state of the art F1 scores on the ACE2004, ACE2005 dataset, and shows comparable performance to Pyramid(BERT) on the GENIA dataset. We further emphasize the evaluation of nested NER is not strictly standardized and there are more or less differences across different works. As is in <ref type="table" target="#tab_2">Table 2</ref>, there are models that show improvements with additional information. Specifically, DYGIE <ref type="bibr" target="#b23">(Luan et al. 2019</ref>) uses the OntoNotes annotations for better coreference resolution. <ref type="bibr" target="#b36">Yu, Bohnet, and Poesio (2020)</ref> train and evaluate their model at the paragraph level which gives a better coreference resolution performance. Their work is under a different setting as the training and evaluation of most works are at the sentence level. BERT-MRC  takes annotation guideline notes as references to construct queries, which is strong supervision and the corresponding corpus are not always easy to obtain. As we try to align our evaluation with the majority of literature, the models mentioned above are not directly comparable to our method. <ref type="table" target="#tab_2">Table 2</ref> lowest rows show the results of ablation study. We note that structure smoothing should be based on potential normalization otherwise the model does not converge. By without TreeCRFs we mean to use the biaffine scorer and normalize the scores locally, similar to <ref type="bibr" target="#b36">Yu, Bohnet, and Poesio (2020)</ref>. Not using TreeCRFs would lead to the largest performance drop which demonstrates the effectiveness of global normalization with TreeCRFs. We also observe performance drop when we do not use potential normalization  and structure smoothing which validates their effectiveness for regularizing TreeCRFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error Analysis</head><p>In our experiments, we find out the recall for unlabeled spans is 96.3 on ACE2005, which means that the spans for most entities are correctly covered, and it is their labels that are more difficult to predict. To see which labels are more prone to errors, we report the error distribution in <ref type="table" target="#tab_4">Table 3</ref>. We see that the VEH, FAC, and LOC are the top three classes prone to errors as they are extremely imbalanced (0.03, 0.05, and 0.02 respectively), and many of them are predicted as latent. This indicates that a future direction is to adapt the TreeCRF to imbalanced labels. We leave it to future work. <ref type="figure">Figure 3</ref> gives examples of inferred latent tree structures compatible with predicted entities. As the learning of latent structures is completely unsupervised, we may not expect that the inferred subtrees should align with human intuition, and we do observe some spans that are not that interpretable like rights group. However, we still observe some meaningful constituents like for strategic and international studies and in Jakarta, which indicates that our approach is indeed learning meaningful tree structures to a certain extent. We note there are also related unsupervised grammar induction works with TreeCRFs , and we leave the application of our model to grammar induction to future work. <ref type="table">Table 8</ref> shows the speed for training different models. We primarily focus on GPU time, but also report CPU time. The base Biaffine model is similar to the model in <ref type="bibr" target="#b36">Yu, Bohnet, and Poesio (2020)</ref> which uses locally normalized scores, instead of using a TreeCRF. This model eliminates the complexity of the Inside algorithm and can be computed in O(1) time. which can be viewed as an upper bound of time complexity. Thanks to the masking mechanism that is compati-ble with parallelization and tensor operations, our MASKED INSIDE is significantly quicker than a vanilla implementation of Inside for partial marginalization, and is close to the base Biaffine in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time Complexity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we propose to view nested entity structures as partially observed constituency trees, and model it with partially observed TreeCRFs. We use a pretrained encoder and a biaffine scoring module to predict the log potentials, then use the TreeCRF to decode the entities. We give a detailed discussion of different nodes within partial trees and their corresponding inference operations during partial marginalization. To facilitate efficient computation with modern hardware and tensor libraries, we propose the MASKED INSIDE algorithm that is conceptually simple and practically efficient. We demonstrate the effectiveness and efficiency of our approach with extensive experiments. <ref type="table" target="#tab_8">Table 5</ref> gives the full results of all 5 runs. We report the results simply to check if the model is sensitive to random seeds. The numbers show that the F1 scores do not vary much with different initialization, which means that the model is consistent under random initialization. <ref type="table">Table 6</ref> shows the performance change as we increase the number of latent states. When the number of latent states is relatively small (less than 5), the performance does not vary much. So we report the performance for 1 latent state in the main paper. We also note that when we increase the number of latent states to be larger than the number of observed states, the performance drops significantly. <ref type="table">Table 6</ref> shows an interesting trend of changing the size of latent labels: as we increase the number of latent labels, the precision is gradually decreasing while the recall is gradually increasing. To explain this observation, we make the following conjecture: (a) the sum of the prior probability of observed labels and latent labels is 1, and the prior probability of observed labels is fixed in the training set, so different latent labels would share the rest prior probability; (b) as the number of latent labels increases, each latent label would in average receive less probability; (c) during CKY decoding, since the probability of each latent label decreases on average, it is more likely that a tree involving more observed labels would be of higher probability, resulting the algorithm to decode out trees with more observed labels; (d) consequently, the model makes more radical predictions, leading to higher coverage of entities (higher recall) but lower precision. We note this reasoning chain is only a rough sketch, and we have not yet able to investigate the detailed mechanism. We leave it to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Latent Labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alignment to Syntax Trees</head><p>We further compare the trees inferred by our model with the syntactical constituency trees of the same sentence. We use unlabeled attachment score (UAS) as the similarity metric. We report the similarity on the training set. Specifically, we use Stanford CoreNLP to obtain the oracle constituency trees of the training sentences, binarize these trees to CNF using an always-left strategy. The UAS of the oracle trees means the similarity between the left-binarized oracle trees to the original oracle trees. We compare: (a) random trees (b) left-branching trees (d) right-branching trees (d) trees inferred from our NER model to the oracle trees. All baseline trees align poorly to the oracle constituency trees, while ours have the highest UAS. We emphasize that our trees are trained under a NER objective, and do not necessarily need to be similar to syntax trees. However, there is still a fraction of the tree structures inferred from our model that is similar to the syntactical constituency structures. We leave further investigation about the relationship between our latent trees and syntactical constituency trees to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batchification Speed with MASKED INSIDE</head><p>We note that the vanilla partial marginalization algorithm (Algorithm 2) in the main paper does not support sentencelevel bachification because different sentences have different tree structures. This inefficiency is addressed by the MASKED INSIDE algorithm. To see how sentence-level batchification leads to further speed improvement, we report the speed comparison in <ref type="table">Table 8</ref>   <ref type="table">Table 6</ref>: Results of varying the number of latent labels |L l | on the ACE2004, ACE2005, and GENIA dataset. The observed label size is 7 in ACE2004 and ACE2005 datasets, and 5 in GENIA dataset. As the size of the latent labels increases, we observe an interesting trend of increasing recall and decreasing precision. This means that the model is making more radical predictions, targeting on coverage (recall) rather than confidence (precision  <ref type="table">Table 7</ref>: Alignment to syntactical constituency trees. Oracle trees are obtained by using CoreNLP to parse the sentences, then left-binarized to CNF. All the reported methods align poorly to the syntactical constituency trees, however, our method still gives higher UUAS than all baseline, meaning that a small fraction of tree structures predicted by our model are aligned with the syntactical structures.  <ref type="table">Table 8</ref>: Time for training one epoch on ACE2004. GPU Nvidia P100, CPU Intel 2.6Hz quad-core i7. The vanilla partial marginalization algorithm does not support sentence batchification since different sentences have different tree structures, thus being significantly slower than the masked inside.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>20.69 20.96 19.21 18.93 17.19 30.13 29.17 30.48    Table 1: Statistics of ACE2004, ACE2005, and GENIA datasets.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">5,014 5,600</cell></row><row><cell># nested entities</cell><cell>10,080 1,086 1,410 9,389 1,112 1,118 8,337</cell><cell>903</cell><cell>1,217</cell></row><row><cell>avg length</cell><cell>20.38</cell><cell></cell><cell></cell></row></table><note>than multiplying the masks inside Algorithm 3):</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Main results and ablation studies on three datasets. We report the average scores of 5 runs for main results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Error distribution on ACE2005. Rows: entities predicted by our model. None denotes the entities that are not predicted. Columns: labeled entities. Numbers are normalized by columns. ? denotes the prior distribution of labels.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Time for training one epoch on ACE2004. GPU</cell></row><row><cell>Nvidia P100, CPU Intel 2.6Hz quad-core i7.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>. Generally, MASKED IN-SIDE is significantly faster than a vanilla partial marginalization algorithm. 86.94 86.20 86.57 84.82 86.33 85.57 77.31 79.31 78.29 Run 2 86.92 86.63 86.77 84.80 86.42 85.61 78.92 77.58 78.24 Run 3 86.83 85.97 86.40 83.92 86.76 85.31 77.96 78.39 78.17 Run 4 86.14 86.63 86.38 84.16 86.52 85.33 77.85 78.62 78.24 Run 5 87.03 87.09 87.06 84.62 86.09 85.35 79.00 77.29 78.14</figDesc><table><row><cell></cell><cell>ACE2004</cell><cell></cell><cell></cell><cell>ACE2005</cell><cell></cell><cell></cell><cell>GENIA</cell><cell></cell></row><row><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Run 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Detailed Results of 5 Runs. The numbers does not vary much, which means that our model performance is not sensitive to random initialization.</figDesc><table><row><cell></cell><cell></cell><cell>ACE2004</cell><cell></cell><cell></cell><cell>ACE2005</cell><cell></cell><cell></cell><cell>GENIA</cell><cell></cell></row><row><cell># Latent Labels</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>1</cell><cell cols="9">86.7 86.5 86.6 84.5 86.4 85.4 78.2 78.2 78.2</cell></row><row><cell>2</cell><cell cols="9">86.5 86.0 86.3 84.1 86.9 85.1 76.6 79.9 78.2</cell></row><row><cell>3</cell><cell cols="9">86.2 86.9 86.6 83.0 87.4 85.1 77.7 78.1 77.9</cell></row><row><cell>4</cell><cell cols="9">85.6 86.6 86.1 83.1 86.8 84.9 75.2 80.2 77.6</cell></row><row><cell>5</cell><cell cols="6">84.8 88.1 86.4 83.7 86.9 85.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>6</cell><cell cols="6">84.9 87.4 86.1 82.6 87.8 85.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>label size</cell><cell cols="9">84.3 87.4 85.8 82.6 87.5 85.0 76.7 77.6 77.2</cell></row><row><cell>2 * label size</cell><cell cols="9">84.2 87.5 85.8 82.6 87.0 84.7 77.0 76.7 76.9</cell></row><row><cell>3 * label size</cell><cell cols="9">83.5 87.4 85.4 82.3 86.5 84.3 76.6 77.3 77.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>). See the corresponding paragraph for detailed discussions.</figDesc><table><row><cell>Method</cell><cell>UAS</cell></row><row><cell>Random</cell><cell>10.83</cell></row><row><cell>Left-branching</cell><cell>5.61</cell></row><row><cell>Right-branching</cell><cell>0.15</cell></row><row><cell cols="2">PO-TreeCRF (ours) 16.95</cell></row><row><cell>Oracle</cell><cell>78.51</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The complexity discussed here does not include the summation over k1, k2 in line 6. If the summation over l is implemented with a binary tree shaped summation then the complexity is O(n log n), if it is sequential then the complexity is O(n 2 ). The optimization of this summation is usually implemented inside tensor computation libraries.2 In our implementation, we use ?10 6 to substitute the undefined log 0 for numerical stability.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Additionally, if we randomize and make them i.i.d. Gumbel samples, we recover the Stochastic Softmax Trick (?), which generalize Gumbel-Softmax to combinatorial structures.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We greatly thank all anonymous reviewers for their helpful comments. We also thank Yijia Liu, Yu Zhang, Yue Zhang, Rui Wang, and Ningyu Zhang for helpful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Detail Results of 5 Runs</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual String Embeddings for Sequence Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/C18-1139" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognising nested named entities in biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</title>
		<meeting>the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kr?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Language Models are Few-Shot Learners. ArXiv abs/2005.14165</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Automatic Content Extraction (ACE) Program-Tasks, Data, and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Biaffine Attention for Neural Dependency Parsing. ICLR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Lrec</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bilexical Grammars and their Cubic-Time Parsing Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inside-Outside and Forward-Backward Algorithms Are Just Backprop (tutorial paper)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPNLP@EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Merge and Label: A Novel Neural Network Architecture for Nested NER</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5840" to="5850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">Bidirectional LSTM-CRF models for sequence tagging</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A neural layered model for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1446" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pyramid: A Layered Model for Nested Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5918" to="5928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nested named entity recognition revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="861" to="871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">GENIA corpus-a semantically annotated corpus for biotextmining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="180" to="182" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Compound Probabilistic Context-Free Grammars for Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2369" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised Recurrent Neural Network Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Melis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural Architectures for Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Unified MRC Framework for Named Entity Recognition</title>
		<idno type="DOI">10.18653/v1/2020.acl-main.519</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.519" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<biblScope unit="page" from="5849" to="5859" />
		</imprint>
	</monogr>
	<note>Online: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequenceto-Nuggets: Nested Entity Mention Detection via Anchor-Region Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5182" to="5192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint mention extraction and classification with mention hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="857" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3036" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="188" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Labeling Gaps Between Words: Recognizing Overlapping Mentions with Mention Separators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Muis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2608" to="2618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4694" to="4703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-1202" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Torch-Struct: Deep Structured Prediction Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-demos.38</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-demos.38" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="335" to="342" />
		</imprint>
	</monogr>
	<note>Online: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep exhaustive model for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Sohrab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2843" to="2849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural Architectures for Nested NER through Linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Strakov?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hajic</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1527</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1527" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5326" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Boundary Enhanced Neural Span Classification for Nested Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9016" to="9023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural Segmental Hypergraphs for Overlapping Mention Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="204" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Neural Transition-based Model for Nested Mention Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1011" to="1017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-grained Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1430" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A local detection approach for named entity recognition and mention detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watcharawittayakul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1237" to="1247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Named Entity Recognition as Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.577</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.577" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6470" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Enhancing HMM-based biomedical named entity recognition by studying special phenomena</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="411" to="422" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dependency Parsing with Partial Annotations: An Empirical Comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/I17-1006" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient Second-Order TreeCRF for Neural Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.302</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.302" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3295" to="3305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Fast and Accurate Neural CRF Constituency Parsing. IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Boundary-aware Neural Model for Nested Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Named entity recognition using an HMM-based chunk tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
