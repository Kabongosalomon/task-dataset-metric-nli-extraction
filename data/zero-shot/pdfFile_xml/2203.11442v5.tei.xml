<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Multi-object Identification for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongxin</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxu</forename><surname>Miao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">Scalable Multi-object Identification for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video Object Segmentation</term>
					<term>Vision Transformer</term>
					<term>Instance Segmentation</term>
					<term>Metric Learning !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new semi-supervised video object segmentation framework that can process multiple objects in a single network pass and has a dynamically scalable architecture for speed-accuracy trade-offs. State-of-the-art methods prefer to match and segment a single positive object and have to process objects one by one under multi-object scenarios, consuming multiple times of computation resources. Besides, previous methods always have static network architectures, which are not flexible enough to adapt to different speed-accuracy requirements. To solve the above problems, we proposed an Associating Objects with Scalable Transformers (AOST) approach to match and segment multiple objects collaboratively with online network scalability. To match and segment multiple objects as efficiently as processing a single one, AOST employs an IDentification (ID) mechanism to assign objects with unique identities and associate them in a shared high-dimensional embedding space. In addition, a Scalable Long Short-Term Transformer (S-LSTT) is designed to construct hierarchical multi-object associations and enable online adaptation of accuracy-efficiency trade-offs. By further introducing scalable supervision and layer-wise ID-based attention, AOST is not only more flexible but more robust than previous methods. We conduct extensive experiments on multi-object and single-object benchmarks to evaluate AOST variants. Compared to state-of-the-art competitors, our methods can maintain superior run-time efficiency with better performance. Notably, we achieve new state-of-the-art performance on popular VOS benchmarks, i.e., YouTube-VOS (86.5%), DAVIS 2017 Val/Test (87.0%/84.7%), and DAVIS 2016 (93.0%). Project page: https://github.com/z-x-yang/AOT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity of VOS Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Light Heavy</head><p>Low High</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Requirement of VOS Accuracy</head><p>Encoder Decoder VOS Module Encoder Decoder VOS Module Encoder Decoder VOS Module Model A Model B Model C Complexity of Sub-model Light Heavy Low High Requirement of VOS Accuracy Sub-model A Sub-model B Sub-model C Encoder Decoder VOS Block VOS Block VOS Block (a) No scalability LN Self-Attention + + LN Feed Forward</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Video Object Segmentation (VOS) is a fundamental task in video understanding with many potential applications, including augmented reality <ref type="bibr" target="#b0">[1]</ref> and self-driving cars <ref type="bibr" target="#b1">[2]</ref>. The goal of semi-supervised VOS, the main task in this paper, is to track and segment object(s) across an entire video sequence based on the object mask(s) given in the first frame.</p><p>Thanks to the recent advance of deep neural networks, many deep learning based VOS algorithms have been proposed and achieved promising performance. STM <ref type="bibr" target="#b2">[3]</ref> and its following works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> leverage a memory network to store and read the target features of predicted past frames and apply a non-local attention mechanism to match the target in the current frame. FEELVOS <ref type="bibr" target="#b5">[6]</ref> and CFBI <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> utilize global and local matching mechanisms to match target pixels or patches from both the first and the previous frames to the current frame.</p><p>Even though the above methods have achieved significant progress, they learn to decode scene features that contain a single positive object. Thus under a multi-object scenario, they have to match each object independently and ensemble all the single-object predictions into a multiobject segmentation, as shown in <ref type="figure">Fig. 2a</ref>. Such a postensemble manner eases network architectures' design since the networks are not required to adapt the parameters or structures for different object numbers. However, modeling multiple objects independently, instead of jointly, is inefficient in exploring multi-object contextual information to Z. <ref type="bibr">Yang</ref> learn a more robust feature representation for VOS. In addition, processing multiple objects separately yet in parallel requires multiple times the amount of GPU memory and computation for processing a single object. This problem restricts the training and application of VOS under multiobject scenarios, especially when computing resources are limited.</p><p>Apart from the multi-object problem, previous methods are usually only designed for specific application goals, such as improving accuracy <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> or pursuing realtime efficiency <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Therefore, previous methods pay little attention to the flexibility and scalability of proposed VOS modules. Thus, in real-world applications, such methods are difficult to adapt their architectures for deployments with different speed-accuracy requirements or computational limitations.</p><p>To solve the above two problems, a feasible approach should be able to associate and decode multiple objects collaboratively in an end-to-end framework ( <ref type="figure">Fig. 2b)</ref> and can dynamically adapt the architecture for different speedaccuracy requirements <ref type="figure">(Fig. 1b</ref>). Therefore, we propose an Associating Objects with Scalable Transformers (AOST) framework to simultaneously match and decode multiple targets with online network scalability. First, an identification mechanism is proposed to assign each target a unique identity and embed multiple targets into the same feature space. Hence, the network can learn the association or correlation among all the targets. Moreover, multi-object segmentation can be directly decoded by utilizing assigned identity information. Second, a Scalable Long Short-Term Transformer (S-LSTT) is devised for constructing hierarchical object matching and propagation. Each S-LSTT block utilizes a long-term attention for matching with the embed-  <ref type="figure">Fig. 1</ref>: (a) Previous VOS methods (e.g., <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>) are not flexible for different deployment requirements. (b) Our Associating Objects with Scalable Transformers (AOST) approach supports run-time adjustment for speed-accuracy trade-offs. (c) Under multi-object scenarios, AOST is dynamically scalable and significantly outperforms the state-of-the-art competitors <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref> on both speed and accuracy. All the results are fairly recorded (using 1 Tesla V100 GPU) on YouTube-VOS 2018 <ref type="bibr" target="#b10">[11]</ref>.</p><p>ding of the reference frame(s) and a short-term attention for matching with nearby frames' embeddings. By further introducing scalable supervision and layer-wise identification &amp; gating weights, S-LSTT supports dynamic and effective adjustment of the transformer's depth and thus enables a test-time balance of segmentation accuracy and efficiency.</p><p>We conduct extensive experiments on commonly-used multi-object benchmarks for VOS, i.e., large-scale YouTube-VOS <ref type="bibr" target="#b10">[11]</ref> and small-scale DAVIS 2017 <ref type="bibr" target="#b13">[14]</ref>, to validate the effectiveness and efficiency of AOST variants. Even using a light-weight encoder, Mobilenet-V2 <ref type="bibr" target="#b14">[15]</ref>, AOST achieves flexible and superior performance on YouTube-VOS 2018 (J &amp;F, 80.6?84.4%) while keeping 10? ? 1.8? faster multi-object speed (34.4?15.9FPS), when fairly compared to the state-of-the-art competitors (e.g., CFBI+ <ref type="bibr" target="#b7">[8]</ref>, 82.8%, 4FPS) as shown in <ref type="figure">Fig. 1c</ref>. After applying stronger encoder network <ref type="bibr" target="#b16">[16]</ref> and test-time augmentations <ref type="bibr" target="#b7">[8]</ref>, AOST series networks achieve new state-of-the-art single-model results on three popular benchmarks, i.e., YouTube-VOS (86.5%), DAVIS 2017 Val/Test (87.0%/84.7%), and singleobject DAVIS 2016 <ref type="bibr" target="#b17">[17]</ref> (93.0%). Particularly, we ranked 1 st in Track 1 (Video Object Segmentation) of the 3rd Largescale Video Object Segmentation Challenge <ref type="bibr" target="#b18">[18]</ref>. This paper is an extension of our previous conference version <ref type="bibr" target="#b19">[19]</ref>. The current work adds to the initial version in some significant aspects. First, we propose a more flexible and robust VOS framework, AOST, the first VOS framework that supports a run-time network adjustment between stateof-the-art accuracy and real-time efficiency to the best of our knowledge. Second, we design a more effective layerwise ID-based attention to couple identification and vision embeddings. By introducing such a module into S-LSTT, we improve AOST's performance with negligible parameters and computation increment. Third, we introduce numerous new experimental results, including AOST series variants, qualitative results, and ablation studies. AOST achieves new state-of-the-art results on popular VOS benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Semi-supervised Video Object Segmentation. Given one or more annotated frames (the first frame in general), semisupervised VOS methods propagate the manual labeling to the entire video sequence. Traditional methods often solve an optimization problem with an energy defined over a graph structure <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>. In recent years, VOS methods have been mainly developed based on deep neural networks (DNN), leading to better results.</p><p>Early DNN methods rely on fine-tuning the networks at test time to make segmentation networks focus on a specific object. Among them, OSVOS <ref type="bibr" target="#b23">[23]</ref> and MoNet <ref type="bibr" target="#b24">[24]</ref> finetune pre-trained networks on the first-frame ground-truth at test time. OnAVOS <ref type="bibr" target="#b25">[25]</ref> extends the first-frame fine-tuning by introducing an online adaptation mechanism. Following these approaches, MaskTrack <ref type="bibr" target="#b26">[26]</ref> and PReM <ref type="bibr" target="#b27">[27]</ref> utilize optical flow to help propagate the segmentation mask from one frame to the next. Despite achieving promising results, the test-time fine-tuning restricts networks' efficiency.</p><p>Recent works aim to achieve a better run-time and avoid using online fine-tuning. OSMN <ref type="bibr" target="#b28">[28]</ref> employs one convolutional network to extract object embedding and another one to guide segmentation predictions. PML <ref type="bibr" target="#b29">[29]</ref> learns pixel-wise embedding with the nearest neighbor classifier, and VideoMatch <ref type="bibr" target="#b30">[30]</ref> uses a soft matching layer that maps the pixels of the current frame to the first frame in a learned embedding space. Following the above methods, FEELVOS <ref type="bibr" target="#b5">[6]</ref> and CFBI(+) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b31">[31]</ref> extend the pixellevel matching mechanism by additionally matching between the current and previous frames. RGMP <ref type="bibr" target="#b32">[32]</ref> also gathers guidance information from both the first frame and the previous frame but uses a siamese encoder with two shared streams. Moreover, RPCM <ref type="bibr" target="#b33">[33]</ref> proposes a correction module to improve the reliability of pixel-level matching. Instead of using matching mechanisms, LWL <ref type="bibr" target="#b34">[34]</ref> proposes to use an online few-shot learner to learn to decode object segmentation.</p><p>Apart from the above methods, attention mechanisms have recently attracted more attention. STM <ref type="bibr" target="#b2">[3]</ref> and its following works (e.g., <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr">[</ref> Computational complexity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AOST (ours)</head><p>Post-ensemble (c) Comparison <ref type="figure">Fig. 2</ref>: Common VOS methods, e.g., <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>, process multi-object scenarios in a post-ensemble manner (a). In contrast, our AOST associates all the objects uniformly (b), leading to better efficiency (c).</p><p>network to embed past-frame predictions into memory and apply non-local attention mechanisms to propagate object information from the memory to the current frame. SST <ref type="bibr" target="#b36">[36]</ref> utilizes attention mechanisms differently, i.e., transformer blocks <ref type="bibr" target="#b37">[37]</ref> are used to extract pixel-level affinity maps and spatial-temporal features. Such features are target-agnostic instead of target-aware like our S-LSTT since the mask information in past frames is not propagated and aggregated in transformer blocks. Most VOS methods learn to decode features with a single positive object and thus have to match and segment each target separately under multi-object scenarios, consuming multiple times the computing resources of singleobject cases. Some methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref> relieve such a problem by sharing backbone features and pixel-matching maps among objects. However, the mask propagation and segmentation are still computed individually for different objects. Moreover, previous methods are usually only designed for specific application goals, such as improving accuracy <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b38">[38]</ref> or pursuing real-time efficiency <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Such methods are difficult to adapt their architectures for real-world deployments with different performance requirements or computational limitations. The above problems restrict the application and development of the VOS with multiple targets. Hence, we propose our AOST to associate and decode multiple targets simultaneously, as efficiently as processing a single object, with runtime speed-accuracy scalability. Vision Transformers. Transformers <ref type="bibr" target="#b37">[37]</ref> was proposed to build hierarchical attention-based networks for machine translation. Similar to Non-local Neural Networks <ref type="bibr" target="#b39">[39]</ref>, transformer blocks compute correlation with all the input elements and aggregate their information by using attention mechanisms <ref type="bibr" target="#b40">[40]</ref>. Compared to RNNs, transformer networks model global correlation or attention in parallel, leading to better memory efficiency, and thus have been widely used in natural language processing (NLP) tasks <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b43">[43]</ref>. Recently, transformer blocks were introduced to many computer vision tasks, such as image classification <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b45">[45]</ref>, object detection <ref type="bibr" target="#b46">[46]</ref>/segmentation <ref type="bibr" target="#b47">[47]</ref>, and image generation <ref type="bibr" target="#b48">[48]</ref>, and have shown promising performance compared to CNN-based networks.</p><p>Many VOS methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b49">[49]</ref> have utilized attention mechanisms to match the object features and propagate the segmentation mask from past frames to the current frames. Nevertheless, these methods consider only one positive target in the attention processes, and how to build hierarchical attention-based propagation has rarely been studied. This paper carefully designs the scalable long short-term transformer, which can effectively construct multi-object matching and propagation within hierarchical structures for VOS and enables a test-time balance of segmentation accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">REVISIT PREVIOUS VOS SOLUTIONS</head><p>In VOS, many common video scenarios have multiple targets or objects required for tracking and segmenting. Benefiting from deep networks, current state-of-the-art VOS methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref> have achieved promising performance. Nevertheless, these methods focus on matching and decoding a single object. Under a multi-object scenario, they thus have to match each object independently and ensemble all the single-object predictions into a multi-object prediction, as demonstrated in <ref type="figure">Fig. 2a</ref>. Let F N denote a VOS network for predicting single-object segmentation, and A is an ensemble function such as sof tmax or the soft aggregation <ref type="bibr" target="#b2">[3]</ref>, the formula of such a post-ensemble manner for processing N objects is like,</p><formula xml:id="formula_0">Y = A(F N (I t , I m , Y m 1 ), ..., F N (I t , I m , Y m N ))</formula><p>, where I t and I m denote the image of the current frame and memory frames respectively, and {Y m 1 , ..., Y m N } are the memory masks (containing the given reference mask and past predicted masks) of all the N objects. This manner extends networks designed for single-object VOS into multiobject applications, so there is no need to adapt the network for different object numbers.</p><p>Although the above post-ensemble manner is prevalent and straightforward in the VOS field, processing multiple objects separately yet in parallel requires multiple times the amount of GPU memory and computation for matching a single object and decoding the segmentation. This problem restricts the training and application of VOS under multiobject scenarios when computing resources are limited. To make the multi-object training and inference as efficient as single-object ones, an expected solution should be capable of associating and decoding multiple objects uniformly instead of individually. To achieve such an objective, we propose an identification mechanism to embed the masks of any number (required to be smaller than a pre-defined large number) of targets into the same high-dimensional space. Based on   the identification mechanism, a novel and efficient framework, i.e., Associating Objects with Scalable Transformers (AOST), is designed to propagate all object embeddings uniformly and hierarchically from memory frames to the current frame. As shown in <ref type="figure">Fig. 2b</ref>, our AOST associates and segments multiple objects within an end-to-end framework. For the first time, processing multiple objects can be as efficient as processing a single object ( <ref type="figure">Fig. 2c</ref>). Compared to previous methods, our training under multi-object scenarios is also more efficient since AOST can associate multiple object regions and learn contrastive feature embeddings among them uniformly. Furthermore, AOST supports runtime speed-accuracy trade-offs ( <ref type="figure">Fig. 1b)</ref> by introducing scalable long short-term transformers with scalable supervision. Under the AOST framework, a single network can be dynamically balanced between real-time speed and state-ofthe-art accuracy during inference. Such a capability is not available in previous methods with no network scalability design, as shown in <ref type="figure">Fig. 1a</ref>.</p><formula xml:id="formula_1">! = 1 ! = 2 ! = 3 ID Scalable Supervision ? " ? # ? $ Shared Decoder (b) Training</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY OF AOST</head><p>In this section, we first introduce our identification mechanism proposed for efficient multi-object VOS. Then, a Scalable Long Short-Term Transformer (S-LSTT) is introduced for constructing hierarchical multi-object associations. S-LSTT has dynamically scalable depth under scalable supervision and supports adaptive accuracy-efficiency trade-offs at run-time. An overview of our Associating Objects with Scalable Transformers (AOST) approach is shown in <ref type="figure" target="#fig_2">Fig. 3a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Identification Mechanism</head><p>Many recent VOS methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> utilized attention mechanisms and achieved promising results. To formulate, we define Q ? R HW ?C , K ? R T HW ?C , and V ? R T HW ?C as the query embedding of the current frame, the key embedding of the memory frames, and the value embedding of the memory frames respectively, where T , H, W , C denote the temporal, height, width, and channel dimensions. The formula of a common attention-based matching and propagation is,</p><formula xml:id="formula_2">Att(Q, K, V ) = Corr(Q, K)V = sof tmax( QK tr ? C )V, (1)</formula><p>where a matching map is calculated by the correlation function Corr, and then the value embedding, V , will be propagated into each location of the current frame.</p><p>In the common single-object propagation <ref type="bibr" target="#b2">[3]</ref>, the binary mask information in memory frames is embedded into V with an additional memory encoder network. The information thus can be propagated to the current frame by using Eq. 1. Following the propagated feature, a convolutional decoder network will decode the aggregated feature and predict the single-object probability logit of the current frame.</p><p>The main problem of propagating and decoding multiobject mask information in an end-to-end network is how to adapt the network to different target numbers. To overcome this problem, we propose an identification mechanism consisting of identification embedding and decoding based on attention mechanisms.</p><p>First, an Identification Embedding mechanism is proposed to embed the masks of multiple different targets into the same feature space for propagation. As seen in <ref type="figure" target="#fig_2">Fig. 3c</ref>, we initialize an identity bank, D ? R M ?C , where M identification vectors with C dimensions are stored. For embedding multiple different target masks, each target will be randomly assigned a different identification vector. Assuming N (N &lt; M ) targets are in the video scenery, the formula of embedding the targets' one-hot mask, Y ? {0, 1} T HW ?N , into a identification embedding, E ? R T HW ?C , by randomly assigning identification vector from the bank D is,</p><formula xml:id="formula_3">E = ID(Y, D) = Y P D,<label>(2)</label></formula><p>where P ? {0, 1} N ?M is a random permutation matrix, satisfying that P tr P is equal to a M ? M unit matrix, for randomly selecting N identification embeddings. After the ID assignment, different targets have different identification embeddings, and thus we can propagate all the target identification information from memory frames to the current frame by attaching the identification embedding E with the attention value V . A simple method to attach E is adding it to V directly, i.e.,</p><formula xml:id="formula_4">V = Att(Q, K, V + ID(Y, D)) = Att(Q, K, V + E),<label>(3)</label></formula><p>where V ? R HW ?C aggregates all the multiple targets' embeddings from the propagation. For Identification Decoding, i.e., predicting all the targets' probabilities from the aggregated feature V , we firstly predict the probability logit for every identity in the bank D by employing a convolutional decoding network F D , and then select the assigned ones and calculate the probabilities, i.e.,</p><formula xml:id="formula_5">Y = sof tmax(P F D (V )) = sof tmax(P L D ),</formula><p>where L D ? R HW ?M is all the M identities' probability logits, P is the same as the selecting matrix used in the identity assignment (Eq. 2), and Y ? [0, 1] HW ?N is the probability prediction of all the N targets.</p><p>For training, common multi-class segmentation losses, such as cross-entropy loss, can be used to optimize the multi-object Y regarding the ground-truth labels. The identity bank D is trainable and randomly initialized at the training beginning. To ensure that all the identification vectors have the same opportunity to compete with each other, we randomly reinitialize the identification selecting matrix P in each video sample and each optimization iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scalable Long Short-Term Transformer</head><p>Previous methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> always utilize only one layer of attention (Eq. 1) to aggregate single-object information and are not dynamically scalable for different deployment requirements. In our identification-based multi-object pipeline, we found that a single attention layer cannot fully model multiobject association, which naturally should be more complicated than single-object processes. Thus, we consider constructing hierarchical matching and propagation by using a series of attention layers. The hierarchical architecture is also more convenient for realizing dynamic network structure adjustment. Recently, transformer blocks <ref type="bibr" target="#b37">[37]</ref> have been demonstrated to be stable and promising in constructing hierarchical attention structures in visual tasks <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b46">[46]</ref>. We carefully design a Scalable Long Short-Term Transformer (S-LSTT) to enable run-time speed-accuracy trade-offs. S-LSTT stacks a series of S-LSTT blocks, but the depth is adaptive. In an S-LSTT with L layers at most, the subtransformer of S-LSTT has a variable depth L , which can be changed from 1 to L, and the sub-transformer shares the parameters of S-LSTT's first L layers. AOST's encoder and decoder are shared for all the sub-transformers, and AOST's decoder processes each sub-transformer's output individually during training. At run-time, the accuracyefficiency trade-offs of AOST can be easily controlled by choosing different S-LSTT depths. Following the common transformer blocks <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b41">[41]</ref>, the block of S-LSTT firstly employs a self-attention layer, which is responsible for learning the association or correlation among the targets within the current frame. Then, S-LSTT additionally introduces a long-term attention, for aggregating targets' information from long-term memory frames and a short-term attention, for learning temporal smoothness from nearby short-term frames. The final module is based on a common 2-layer feed-forward MLP with GELU <ref type="bibr" target="#b51">[51]</ref> non-linearity in between. <ref type="figure" target="#fig_2">Fig. 3d</ref> shows the structure of an S-LSTT block. Notably, all these attention modules are implemented in the form of the multi-head attention <ref type="bibr" target="#b37">[37]</ref>, i.e., multiple attention modules followed by concatenation and a linear projection. Nevertheless, we only introduce their single-head formulas below for simplicity. Layer-wise ID-based Attention. Before introducing the long-term and short-term attentions, we first introduce their basic attention formula based on the ID embedding described in Sec. 4.1. Although Eq. 3 gives a straightforward to match and propagate ID embeddings, we argue that the representation ability of value embeddings will be restricted when stacking multiple layers of Eq. 3, since all the layers share the same identification embedding, and all the value embeddings are placed into the same embedding space of identification embedding. Besides, identification embeddings are only attached to value embeddings. In other words, the key embeddings (K), which are used to calculate attention maps and match object patches, cannot utilize identification information (e.g., where is the background regions) directly.</p><formula xml:id="formula_6">Mul Mul + ID ! "# Mul Mul ID + ?C ? ? ? ?C ? ? ? ? ? ? ? ?C ?C ? ? ? ? ? ? ! $ (a) Vanilla Attention Mul Mul + ID ! "# Mul Mul ID + ?C ? ? ? ?C ? ? ? ? ? ? ? ?C ?C ? ? ? ? ? ? ! $ (b) Layer-wise Attention</formula><p>Hence, we further propose to couple identification and vision embeddings in different embedding spaces for different layers. Let W ID l ? R C?C and W G l ? R C?1 denote trainable layer-wise Identification Weight and Gating Weight for l-th attention layer, we modify Eq. 3 to be</p><formula xml:id="formula_7">V = AttID(Q, K, V, Y |D, W ID l , W G l ) = Att(Q, K ?(EW G l ), V + EW ID l ),<label>(4)</label></formula><p>where E = ID(Y, D) as Eq. 4.1, denotes Hadamard product, and ? is a gating activation function <ref type="bibr" target="#b52">[52]</ref>. We set ?( * ) = 1 + tanh( * ), which is capable of learning a gate of identify mapping and improves training stability <ref type="bibr" target="#b52">[52]</ref>. An illustration is shown in <ref type="figure" target="#fig_3">Fig. 4b</ref>. By introducing the identification weight W ID l , the identification embedding, E, will be projected into different spaces for different attention layers before the addition with V . Such a technique improves the representation ability of feature space, especially when sharing the other parameters among different transformer blocks. In addition, the key embedding, K, is adjusted by a gating function ?(W G l E) conditioned on the identification information. The gating weight W G l reduces the channel dimensions of E to a single channel, and the gating function adjusts K in a lightweight position-wise manner. By doing this, we couple the identification and visual information into key embeddings in a layer-wise lightweight manner, and the key embeddings can utilize identification information directly in matching objects. Long-Term Attention is responsible for aggregating targets' information from memory frames, which contains the reference frame and stored predicted frames, to the current frame. Since the time intervals between the current and past frames are variable and can be long-term, temporal smoothness is difficult to guarantee. Thus, long-term attention employs non-local attention like Eq. 1. Let X t l ? R HW ?C denotes the input feature embedding at time t and in block l, where l ? {1, ..., L} is the block index of S-LSTT, the formula of the long-term attention is,</p><formula xml:id="formula_8">AttLT (X t l , X m l , Y m ) = AttID(X t l W K l ,X m l W K l , X m l W V l , Y m |D), where X m l = Concat(X m1 l , ..., X m T l ) and Y m = Concat(Y m1 , .</formula><p>.., Y m T ) are the input feature embeddings and target masks of memory frames with indices m = {m 1 , ..., m T }. Besides, W K l ? R C?C k and W V l ? R C?Cv are trainable parameters of the space projections for matching and propagation, respectively. Instead of using different projections for X t l and X m l , we found the training of S-LSTT is more stable with a siamese-like matching, i.e., matching between the features within the same embedding space (l-th features with the same projection of W K l ). Short-Term Attention is employed for aggregating information in a spatial-temporal neighborhood for each currentframe location. Intuitively, the image changes across several contiguous video frames are always smooth and con-tinuous. Thus, the target matching and propagation in contiguous frames can be restricted in a small spatialtemporal neighborhood, leading to better efficiency than non-local processes. Considering n neighbouring frames with indices n = {t ? 1, ..., t ? n} are in the spatialtemporal neighbourhood, the features and masks of these frames are X n l = Concat(X t?1 l , ..., X t?n l ) and Y n = Concat(Y t?1 , ..., Y t?n ), and then the formula of the shortterm attention at each spatial location p is,</p><formula xml:id="formula_9">AttST (X t l , X n l , Y n |p) = AttLT (X t l,p , X n l,N (p) , Y n l,N (p) ), where X t l,p ? R 1?C</formula><p>is the feature of X t l at location p, N (p) is a ? ? ? spatial neighbourhood centered at location p, and thus X n l,N (p) and Y n l,N (p) are the features and masks of the spatial-temporal neighbourhood, respectively, with a shape of n? 2 ? C or n? 2 ? N .</p><p>When extracting features of the first frame t = 1, there is no memory frames or previous frames, and hence we use X 1 l to replace X m l and X n l . In other words, long-term attention and short-term attention are changed into selfattentions without adjusting the network structures and parameters. The illustrations are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. Scalable Supervision. In order to supervise the training of AOST efficiently, we apply segmentation loss to all the AOST sub-networks simultaneously. Let L AOT (Y , Y ) and Y l denote AOT's loss function and the prediction of the sub-network with L = l S-LSTT layers, the loss formula of AOST is</p><formula xml:id="formula_10">L AOST = L l=1 ? l L AOT (Y l , Y ) L l=1 ? l ,<label>(5)</label></formula><p>where ? is a balance weight to re-weight the loss ratio of different sub-networks. When ? &gt; 1, L AOST will focus more on deeper sub-transformers' losses. Generally, shallower sub-transformers have lower accuracy, higher losses, and larger training gradients. To balance the gradient contribution of sub-networks, we have to increase the weight of deeper sub-networks. In our default setting, ? = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPLEMENTATION DETAILS</head><p>Architecture Variants: Apart from AOST variants, we further build Associating Objects with Transformers (AOT) <ref type="bibr" target="#b19">[19]</ref> variants with different transformer layer number L or longterm memory size m for comparisons. For simplicity, AOT variants do not utilize scalable supervision and use only vanilla ID-based attention (Eq. 3). The hyper-parameters of these variants are: In AOT-T/S/B, only the first frame is considered in longterm memory, which is similar to <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, leading to stable run-time speeds. In AOT-L/AOST, the predicted frames are stored in long-term memory per ? frames, following the memory reading strategy <ref type="bibr" target="#b2">[3]</ref>. We set ? to 2/5 for training/testing. To verify the effectiveness of the layer-wise attention, we further build a AOST-L variant, which does not use scalable supervision and has only the deepest structure of AOST (L = 3). Compared to AOT-L, AOST-L additionally uses layer-wise ID-based attention (Eq. 4). Network Details: For sufficiently validating the effectiveness of our method, we use light-weight backbone encoder, MobileNet-V2 <ref type="bibr" target="#b14">[15]</ref>, and decoder, FPN <ref type="bibr" target="#b53">[53]</ref> with Group-Norm <ref type="bibr" target="#b54">[54]</ref> in default. To verify scalability, we also use stronger ResNet-50 (R50) <ref type="bibr" target="#b55">[55]</ref> and Swin-B <ref type="bibr" target="#b16">[16]</ref> as the encoder. The spatial neighborhood size ? is set to 15, and the size of the identity bank, M , is set to 10, which is consistent with the maximum object number in the benchmarks <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>For MobileNet-V2 encoder, we increase the final resolution of the encoder to 1/16 by adding a dilation to the last stage and removing a stride from the first convolution of this stage. For ResNet-50 and SwinB encoders, we remove the last stage directly as <ref type="bibr" target="#b2">[3]</ref>. The encoder features are flattened into sequences before S-LSTT. In S-LSTT, the feature dimension is 256, and the head number is 8 for all the attention modules. To increase the receptive field of S-LSTT, we insert a depth-wise convolution layer with a kernel size of 5 in the middle of each feed-forward module. The short-term memory n only considers the previous (t?1) frame, which is similar to the local matching strategy <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. After S-LSTT, all the output features of S-LSTT blocks are reshaped into 2D shapes and will serve as the decoder input. Then, the FPN decoder progressively increases the feature resolution from 1/16 to 1/4 and decreases the channel dimension from 256 to 128 before the final output layer, which is used for identification decoding. Patch-wise Identity Bank: Since the spatial size of S-LSTT features is only 1/16 of the input video, we can not directly assign identities to the pixels of high-resolution input mask and construct a low-resolution identification embedding. To overcome this problem, our implementation uses a strategy named patch-wise identity bank. In detail, we first separate the input mask into non-overlapping patches of 16?16 pixels. The original identity bank with M identities is also expanded to a patch-wise identity bank, in which each identity has 16?16 sub-identity vectors corresponding to 16?16 positions in a patch. Hence, the pixels of an object region with different patch positions will be given different sub-identity vectors under the same assigned identity. By summing all the assigned sub-identities in each patch, we can directly construct a low-resolution identification embedding while keeping identities' shape information inside each patch. Training Details: Following <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b32">[32]</ref>, the training stage is divided into two phases: (1) pre-training on synthetic video sequence generated from static image datasets <ref type="bibr" target="#b56">[56]</ref>, <ref type="bibr" target="#b57">[57]</ref>, <ref type="bibr" target="#b58">[58]</ref>, <ref type="bibr" target="#b59">[59]</ref>, <ref type="bibr" target="#b60">[60]</ref> by randomly applying multiple image augmentations <ref type="bibr" target="#b32">[32]</ref>. (2) main training on the VOS benchmarks <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref> by randomly applying video augmentations <ref type="bibr" target="#b6">[7]</ref>.</p><p>All the videos are firstly down-sampled to 480p resolution, and the cropped window size is 465? 465. For optimization, we adopt the AdamW <ref type="bibr" target="#b61">[61]</ref> optimizer and the sequential training strategy <ref type="bibr" target="#b6">[7]</ref>, whose sequence length is set to 5. The loss function is a 0.5:0.5 combination of bootstrapped cross-entropy loss and soft Jaccard loss <ref type="bibr" target="#b62">[62]</ref>. For stabilizing the training, the statistics of BN <ref type="bibr" target="#b63">[63]</ref> modules and the first two stages in the encoder are frozen, and Exponential Moving Average (EMA) <ref type="bibr" target="#b64">[64]</ref> is used. Besides, we apply stochastic depth <ref type="bibr" target="#b65">[65]</ref> to the self-attention and the feed-forward modules in S-LSTT.</p><p>The batch size is 16 and distributed on 4 Tesla V100 1001_3iEIq5HBY1s time 1007_YCTBBdbKSSg <ref type="figure">Fig. 7</ref>: Panoptic propagation. Our method performs well when propagating the panoptic annotations of two complex scenarios (with 44/43 objects) from VIPSeg <ref type="bibr" target="#b66">[66]</ref>.</p><p>GPUs. For pre-training, we use an initial learning rate of 4 ? 10 ?4 and a weight decay of 0.03 for 100,000 steps. For main training, the initial learning rate is set to 2 ? 10 ?4 , and the weight decay is 0.07. In addition, the training steps are 100,000 for YouTube-VOS and 50,000 for DAVIS.</p><p>To relieve over-fitting, the initial learning rate of encoders is reduced to a 0.1 scale of other network parts. All the learning rates gradually decay to 2 ? 10 ?5 in a polynomial manner <ref type="bibr" target="#b6">[7]</ref>. For evaluation, only 1 Tesla V100 GPU is used, and the scales used in test-time multi-scale augmentation are {1.2, 1.3, 1.4} ? 480p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL RESULTS</head><p>We evaluate AOT/AOST variants on popular multi-object benchmarks, YouTube-VOS <ref type="bibr" target="#b10">[11]</ref> and DAVIS 2017 <ref type="bibr" target="#b13">[14]</ref>, and single-object benchmark, DAVIS 2016 <ref type="bibr" target="#b17">[17]</ref>. The variants are trained on the YouTube-VOS 2019 training split and the DAVIS-2017 training split in the default setting. To validate our generalization ability, we choose only the last checkpoint of each variant, and all the benchmarks share the same model parameters. When evaluating YouTube-VOS, we use the default 6FPS videos, and all the videos are restricted to be smaller than 1.3 ? 480p resolution. As to DAVIS, the default 480p 24FPS videos are used. The evaluation metrics include the J score (calculated as the average Intersect over Union (IoU) score between the prediction and the ground truth mask), the F score (calculated as an average boundary similarity measure between the boundary of the prediction and the ground truth), and their mean value (denoted as J &amp;F ). We evaluate all the results on official evaluation servers or with official tools. <ref type="bibr" target="#b10">[11]</ref> is the latest large-scale benchmark for multi-object video segmentation and is about 37 times larger than DAVIS 2017 (120 videos). Specifically, YouTube-VOS contains 3471 videos in the training split with 65 categories and 474/507 videos in the Validation 2018/2019 split with additional 26 unseen categories. The unseen categories do not exist in the training split to evaluate the algorithms' generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Compare with the State-of-the-art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YouTube-VOS</head><p>As shown in <ref type="table">Table 1a</ref>, AOT/AOST variants achieve superior performance on YouTube-VOS compared to the previous state-of-the-art methods. With our identification mechanism, AOT-S (82.6% J &amp;F) is comparable with CFBI+ <ref type="bibr" target="#b7">[8]</ref> (82.8%) while running about 7? faster (27.1 vs. 4.0FPS). By using more proposed transformer blocks, AOT-B improves the performance to 83.5%. Moreover, AOT-L (83.8%) further improves both the seen and unseen scores by utilizing the memory reading strategy.</p><p>After introducing the layer-wise ID-based attention, AOST-L significantly outperforms AOT-L (84.5% vs. 83.7%) while maintaining nearly equal efficiency (15.9 vs. 16.0FPS). Furthermore, equipped with the scalable LSTT, our AOST can be dynamically adapted from the state-of-the-art performance (84.4%, 15.9FPS) to real-time speed (80.6%, 34.4FPS) at run-time. Besides, the amount of AOST's parameters is no more than 20% parameters of any other competitors (e.g., <ref type="bibr" target="#b7">8</ref>.8M/AOST vs. 42.8M/HMMN).</p><p>We can improve the performance of AOST variants by using stronger backbones (e.g., 84.8%/R50-AOST-L and 85.1%/SwinB-AOST-L). Replacing the encoder from MobileNet-V2 to ResNet-50 only loses a litter efficiency (15.9 vs. 14.9FPS). Finally, we can further boost the accuracy by using 30FPS videos and test-time augmentations in inference, and our SwinB-AOST-L achieves new state-of-theart performance (86.5%/86.5%) on the Validation 2018/2019 split of YouTube-VOS. DAVIS 2017 <ref type="bibr" target="#b13">[14]</ref> is a multi-object extension of DAVIS 2016. The validation split of DAVIS 2017 consists of 30 videos with 59 objects, and the training split contains 60 videos with 138 objects. Moreover, the testing split contains 30 more challenging videos with 89 objects. <ref type="table">Table 1b</ref> shows that our R50-AOST surpasses all the competitors on both the DAVIS-2017 validation (85.6%) and testing (79.9%) splits and maintains an efficient speed (17.5FPS). Notably, such a multi-object speed is the same as our single-object speed on DAVIS 2016. For the first time, processing multiple objects can be as efficient as processing a single object over AOT series frameworks. Apart from this, R50-AOST can also be adapted to real-time by reducing S-LSTT's layer number L . In detail, R50-AOST with L = 2/1 achieves 85.3%/83.7% at a speed of 24.3/37.4FPS on DAVIS-2017 Validation.</p><p>We also evaluate our method without training with YouTube-VOS, and AOT-S (79.2%) performs much better than KMN <ref type="bibr" target="#b3">[4]</ref> (76.0%) by +3.2%. Moreover, we achieve new state-of-the-art performance on both the DAVIS-2017 validation (87.0%/SwinB-AOST-L) and testing (84.7%/SwinB-AOST) splits after using test-time augmentations. DAVIS 2016 <ref type="bibr" target="#b17">[17]</ref> is a single-object benchmark containing 20 videos in the validation split. Although our AOT series frameworks aim at improving multi-object video segmentation, we also achieve a new state-of-the-art performance on DAVIS 2016 (SwinB-AOST with/without test-time augmentations, 93.0%/92.4%). Under single-object scenarios, the multi-object superiority of AOT is limited, but R50-AOST (L = 2) still maintains a comparable efficiency compared to STCN (24.3 vs. 27.2FPS) and performs better (92.0% vs. 91.6%). Furthermore, our AOST with L = 1 achieves comparable performance with HMMN (90.5% vs. 90.8%) while running about 4? faster (38.6 vs. 10.0FPS). Qualitative results: <ref type="figure" target="#fig_6">Fig. 6</ref> visualizes some qualitative results in comparison with CFBI <ref type="bibr" target="#b6">[7]</ref>, which only associates each object with its relative background. As demonstrated, CFBI is TABLE 1: The quantitative evaluation on multi-object benchmarks, YouTube-VOS <ref type="bibr" target="#b10">[11]</ref> and DAVIS 2017 <ref type="bibr" target="#b13">[14]</ref>. AOST-L shares the structure of AOST (L = 3) but does not use scalable supervision and has no scalability. AF: using All-Frames (30FPS) videos instead of default (6FPS) videos in inference. P: parameter number (M). : recorded on our device.  <ref type="bibr" target="#b9">[10]</ref> 83.0 81.9 86.5 77.9 85.7 8.4 54.5 RPCM[AAAI22] <ref type="bibr" target="#b33">[33]</ref> 84    easier to confuse multiple highly similar objects. In contrast, our AOT/AOST (L = 3) tracks and segments all the targets accurately by associating all the objects uniformly. However, AOT series frameworks fail to segment some tiny objects (ski poles and watch) since we do not make special designs for tiny objects.</p><p>To further demonstrate AOT's effectiveness in associating multiple objects, we select two complex scenarios (containing more than 40 objects) from VIPSeg <ref type="bibr" target="#b66">[66]</ref>, <ref type="bibr" target="#b69">[69]</ref>, a video dataset with panoptic segmentation annotations, and use our method to propagate their first-frame annotations to the entire videos. As shown in <ref type="figure">Fig. 7</ref>, our method accurately tracks most of the objects and handles occlusions well under such complicated multi-object scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Study</head><p>In this section, we analyze the main components and hyperparameters of AOST and evaluate their impact on the VOS performance in <ref type="table">Table 3</ref> and 4. Long-term Attention Short-term Attention <ref type="figure">Fig. 9</ref>: Visualization of long-term and short-term attention maps of AOT-B on DAVIS 2017 <ref type="bibr" target="#b13">[14]</ref>. The video has three similar people, marked in different colors, and the red person is occluded in frame t ? 1. For visualization, we propagate the colored multi-object masks in long-term or short-term memory to the current frame regarding the corresponding attention maps. The brighter the color, the stronger the attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">ID Mechanism and Long Short-term Attention</head><p>Identity number: The number of the identification vectors, M , has to be larger than the object number in videos. Thus, we set M to 10 in default to be consistent with the maximum object number in the benchmarks <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>. As seen in <ref type="table">Table 3a</ref>, M larger than 10 leads to worse performance. To analyze the reason, we visualize the learned identity banks in <ref type="figure">Fig. 8</ref>. Intuitively, all the identification vectors should be equidistant away from each other in the feature space because their roles are equivalent, which is consistent with the phenomenon in our default setting, M = 10 <ref type="figure">(Fig. 8a)</ref>. However, <ref type="figure">Fig. 8b, 8c</ref>, and 8d demonstrate that maintaining equidistant between every two vectors becomes more difficult when the identity bank becomes larger, especially when M = 30. There are two possible reasons for this phenomenon: <ref type="bibr" target="#b0">(1)</ref> No training video contains enough objects to be assigned so many identities, and thus the network cannot learn to associate all the identities simultaneously;</p><p>(2) the used space with only 256 dimensions is difficult for keeping more than 10 objects to be equidistant. To further prove that AOT benefits from multi-object association, we also try to set M = 1 and use the postensemble manner in inference. By doing this, <ref type="table">Table 3a</ref> shows the result drops from 80.3% to 78.7%. Local window size: <ref type="table">Table 3b</ref> shows that larger local window size, ?, results in better performance. Without the local attention, ? = 0, the performance of AOT significantly drops from 80.3% to 74.3%, demonstrating the necessity of local attention. Local frame number: In <ref type="table">Table 3c</ref>, we also try to employ more previous frames in the local attention, but using only the t ? 1 frame (80.3%) performs better than using 2/3 frames (80.0%/79.1%). A possible reason is that the longer the temporal interval, the more intense the motion between frames. Hence, it is easier to introduce more errors in the local matching when using an earlier previous frame.  <ref type="bibr" target="#b2">3</ref>: Ablation study of identification mechanism and long short-term attention. The experiments are conducted on YouTube-VOS <ref type="bibr" target="#b10">[11]</ref> using AOT-S without pre-training on synthetic videos for simplicity. J S /J U : J on seen/unseen classes. Self: the position embedding type used in the self-attention. Rel: relative positional embedding <ref type="bibr" target="#b68">[68]</ref>   Block number: As shown in <ref type="table">Table 3d</ref>, the AOT performance increases by using more transformer blocks. Notably, the AOT with only one block (77.9%) reaches a fast realtime speed (41.0FPS) on YouTube-VOS, although the performance is -2.4% worse than AOT-S (80.3%). By adjusting the block number, we can flexibly balance the accuracy and speed of AOT.</p><p>To further validate the hierarchical association's effectiveness and analyze each transformer layer's behavior, we visualize long-term and short-term attention maps in each layer during inference, as shown in <ref type="figure">Fig. 9</ref>. As the layer index increases, the mask information of all the objects is gradually aggregated so that the long-term attention becomes more and more accurate, as shown at the bottom of <ref type="figure">Fig. 9</ref>. Similarly, short-term attention's quality, especially the boundary of objects, is improved as the layer index increases. Notably, short-term attention performs well with the yellow person even in the first layer, l = 1, which is different from long-term attention. The reason is that the neighborhood matching of short-term attention is easier than the long-term matching of long-term attention. However, long-term attention is still necessary because shortterm attention will fail with occlusions, such as the red person in <ref type="figure">Fig. 9</ref>.</p><p>In short, the visual analysis further proves the necessity and effectiveness of our hierarchical association, which is not simply a combination of multiple matching processes. Critically, the multi-object information will be gradually aggregated and associated as the transformer goes deeper, leading to more accurate attention-based matching.</p><p>Position embedding: In our default setting, we apply fixed sine spatial positional embedding to the self-attention following <ref type="bibr" target="#b46">[46]</ref>, and our local attention is equipped with learned relative positional embedding <ref type="bibr" target="#b68">[68]</ref>. The ablation study is shown in <ref type="table">Table 3e</ref>, where removing the sine embedding decreases the performance to 80.1% slightly. In contrast, relative embedding is more critical than sine embedding.</p><p>Without the relative embedding, the performance drops to 79.7%, which means the motion relationship between adjacent frames is helpful for local attention. We also tried to apply learned positional embedding to self-attention modules, but no positive effect was observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">S-LSTT and Layer-wise ID-based Attention</head><p>Balance weight: To select a suitable balance weight ?, we train AOST with different ? from 1 to 8. When ? ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>, <ref type="table" target="#tab_8">Table 4a</ref> shows that using larger ? makes the performance of AOST with L = 2/3 better but decreases the performance of L = 1, because larger ? gives the sub-AOST with deeper S-LSTT depth more weight in the loss function (Eq. 5). However, it will decrease AOST's overall performance by increasing ? to 8. Finally, we select ? = 2, which makes a better performance balance among different L . Scalable LSTT: <ref type="table" target="#tab_8">Table 4b</ref> shows the performances of AOST frameworks with different S-LSTT types. When we share the parameters (except for the layer-wise identification and gating weights) among different S-LSTT blocks, we can reduce the total parameters of AOST from 8.8M to 5.8M, and the model still performs well with only a little accuracy drop (80.3/82.8/83.6% vs. 80.6/83.7/84.4%). If we replace S-LSTT with LSTT <ref type="bibr" target="#b19">[19]</ref>, AOST becomes not scalable, and the performance of L = 3 will drop from 84.4 to 83.7. Notably, we further tried to share parameters among LSTT blocks, and the performance significantly drops from 83.7 to 82.8. The above comparisons indicate that S-LSTT is more robust and flexible than LSTT, and the use of layer-wise weights improves the performance of hierarchical matching and propagation, especially when sharing parameters among blocks. Layer-wise ID-based Attention: We further evaluate the effectiveness of identification weight W ID l and gating weight W G l in Eq. 4. As shown in <ref type="table" target="#tab_8">Table 4c</ref>, introducing W ID l or W G l into S-LSTT will bring performance improvements and only marginally increase the network parameters. Particularly, the improvements from identifica-tion weight W ID l (80.3/83.5/84.1% vs. 79.8/82.6/83.4%) are more significant than gating weight W G l (80.0/83.3/83.7% vs. 79.8/82.6/83.4%). This phenomenon further proves that using different identification embeddings in different blocks can effectively improve the representative ability of S-LSTT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>This paper proposes a novel and efficient approach for video object segmentation by associating objects with scalable transformers. The proposed AOST series frameworks achieve superior performance on three popular benchmarks. A simple yet effective identification mechanism is proposed in the frameworks to associate, match, and decode all the objects uniformly under multi-object scenarios. For the first time, processing multiple objects in VOS can be as efficient as processing a single object using the identification mechanism. In addition, a scalable long shortterm transformer is designed for constructing hierarchical object matching and propagation for VOS. It enables runtime adaptation between real-time speed and state-of-theart performance. We hope the identification mechanism will help ease the future study of multi-object VOS and related tasks (e.g., video instance segmentation, interactive VOS, and multi-object tracking), and AOST series frameworks will serve as solid baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The overview of Associating Objects with Scalable Transformers (AOST). (a)(b) Illustrations of inference and training of AOST. The multi-object masks are embedded by using our identification mechanism. Moreover, a Scalable Long Short-Term Transformer (S-LSTT) with dynamic depth (L = 1/2/3) is responsible for matching multiple objects collaboratively and hierarchically. (c) An illustration of the IDentity assignment (ID) designed for transferring a N -object mask into an identification embedding. (d) The structure of an S-LSTT block. LN: layer normalization [50].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Compared to the vanilla attention (Eq. 3), S-LSTT introduces trainable layer-wise identification weight W ID l and gating weight W G l for l-th attention layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Illustrations of the long-term attention and the shortterm attention. (a) Long-term attention employs a non-local manner to match all the locations in the long-term memory. (b) In contrast, the short-term attention only focuses on a nearby spatial-temporal region with a shape of n? 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>? AOST: L = 3 ,</head><label>3</label><figDesc>L = 1/2/3, m = {1, 1 + ?, 1 + 2?, ...}. ? AOT-Large: L = 3, m = {1, 1 + ?, 1 + 2?, ...}; ? AOT-Base: L = 3, m = {1}; ? AOT-Small: L = 2, m = {1}; ? AOT-Tiny: L = 1, m = {1};</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative results. (top) Compared with CFBI<ref type="bibr" target="#b6">[7]</ref>, AOT performs better when segmenting multiple highly similar objects (carousels and zebras). (middle) The results of deeper AOST (L = 3) are finer than AOT, but using shallower AOST (L = 1) degrades the accuracy. (bottom) AOT/AOST fails to segment some tiny objects (ski poles and watch) since we have no specific design for processing rare tiny objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>AOST (L = 1) 80.6 80.0 84.8 74.7 83.1 34.4 5.8 R50-AOST (L = 1) 81.6 81.4 86.1 75.5 83.5 30.9 12.5 SwinB-AOST (L = 1) 82.6 82.7 87.6 76.2 84.0 13.5 62.9 AOT-S 82.6 82.0 86.7 76.6 85.0 27.1 7.0 AOST (L = 2) 83.7 82.8 87.7 77.7 86.4 21.9 7.3 R50-AOST (L = 2) 84.5 83.5 88.5 78.8 87.2 20.2 13.9 SwinB-AOST (L = 2) 84.7 84.2 89.2 78.5 86.9 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>30 Fig. 8 :</head><label>308</label><figDesc>(a) M = 10 (b) M = 15 (c) M = 20 (d) M = Visualization of the cosine similarity between every two of M identification vectors in the identity bank. We use a M ? M symmetric matrix to visualize all the similarities, and diagonal values equal 1. The darker the color, the higher the similarity. When M = 10, the similarities are stable and balanced. As M increases, the matrix becomes less smooth, making the similarities unstable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, X. Wang, J. Miao, and Y. Yang are with the CCAI, College of Computer Science and Technology, Zhejiang University, Hangzhou, China (Email: {yangzongxin, jiaxumiao, xiaohan.wang, yangyics}@zju.edu.cn). Y. Wei is with the Institute of Information Science, Beijing Jiaotong University, Beijing, China (Email: yunchao.wei@bjtu.edu.cn). Y. Yang is the corresponding author.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>memory VOS Network VOS Network VOS Network Reference Current Separation Ensemble Prediction</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Reference</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>4x</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3x</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Multi-object</cell></row><row><cell>Multi-object</cell><cell>Single-object</cell><cell>Multi-object</cell><cell>2x</cell></row><row><cell></cell><cell></cell><cell>Current</cell><cell>Prediction</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1x</cell></row><row><cell></cell><cell></cell><cell>AOST</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Single-object</cell><cell>Multi-object</cell><cell>Object number 1 2 3</cell><cell>4</cell></row><row><cell></cell><cell>(a) Post-ensemble</cell><cell cols="2">(b) Associating objects (ours)</cell></row></table><note>10], [35]) leverage a</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>-Y: without using YouTube-VOS for training. : using 600p instead of 480p videos in inference. ? : timing extrapolated from single-object speed assuming linear scaling in the number of objects. (MS): using test-time multi-scale and flipping augmentations.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">(a) YouTube-VOS</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Seen</cell><cell cols="2">Unseen</cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">AF J &amp;F</cell><cell>J</cell><cell>F</cell><cell>J</cell><cell>F</cell><cell cols="2">FPS Param</cell></row><row><cell></cell><cell></cell><cell cols="3">Validation 2018 Split</cell><cell></cell><cell></cell><cell></cell></row><row><cell>STM[ICCV19] [3]</cell><cell></cell><cell cols="5">79.4 79.7 84.2 72.8 80.9</cell><cell>-</cell><cell>34.0</cell></row><row><cell>KMN[ECCV20] [4]</cell><cell></cell><cell cols="5">81.4 81.4 85.6 75.3 83.3</cell><cell>-</cell><cell>-</cell></row><row><cell>CFBI[ECCV20] [7]</cell><cell></cell><cell cols="6">81.4 81.1 85.8 75.3 83.4 3.4</cell><cell>66.1</cell></row><row><cell>LWL[ECCV20] [34]</cell><cell></cell><cell cols="5">81.5 80.4 84.9 76.4 84.4</cell><cell>-</cell><cell>36.7</cell></row><row><cell>SST[CVPR21] [36]</cell><cell>-</cell><cell cols="2">81.7 81.2</cell><cell>-</cell><cell>76.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HMMN[ICCV21] [67]</cell><cell></cell><cell cols="5">82.6 82.1 87.0 76.8 84.6</cell><cell>-</cell><cell>42.8</cell></row><row><cell>CFBI+[TPAMI22] [8]</cell><cell></cell><cell cols="6">82.8 81.8 86.6 77.1 85.6 4.0</cell><cell>74.3</cell></row><row><cell>STCN[NeurIPS21]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 :</head><label>2</label><figDesc>The quantitative evaluation on the single-object DAVIS 2016<ref type="bibr" target="#b17">[17]</ref>.</figDesc><table><row><cell>Methods</cell><cell>J &amp;F</cell><cell>J</cell><cell>F</cell><cell>FPS</cell></row><row><cell>CFBI[ECCV20] [7]</cell><cell>89.4</cell><cell>88.3</cell><cell>90.5</cell><cell>6.3</cell></row><row><cell>CFBI+[TPAMI22] [8]</cell><cell>89.9</cell><cell>88.7</cell><cell>91.1</cell><cell>5.9</cell></row><row><cell>RPCM[AAAI22] [33]</cell><cell>90.6</cell><cell>87.1</cell><cell>94.0</cell><cell>5.8</cell></row><row><cell>HMMN[ICCV21] [67]</cell><cell>90.8</cell><cell>89.6</cell><cell>92.0</cell><cell>10.0</cell></row><row><cell>STCN[NeurIPS21] [10]</cell><cell>91.6</cell><cell>90.8</cell><cell>92.5</cell><cell>27.2</cell></row><row><cell>AOT-T</cell><cell>86.8</cell><cell>86.1</cell><cell>87.4</cell><cell>51.4</cell></row><row><cell>AOST (L = 1)</cell><cell>90.5</cell><cell>89.3</cell><cell>91.7</cell><cell>38.6</cell></row><row><cell>R50-AOST (L = 1)</cell><cell>90.3</cell><cell>89.6</cell><cell>90.9</cell><cell>37.4</cell></row><row><cell>SwinB-AOST (L = 1)</cell><cell>92.1</cell><cell>90.4</cell><cell>93.7</cell><cell>17.9</cell></row><row><cell>AOT-S</cell><cell>89.4</cell><cell>88.6</cell><cell>90.2</cell><cell>40.0</cell></row><row><cell>AOST (L = 2)</cell><cell>90.9</cell><cell>89.5</cell><cell>92.2</cell><cell>24.8</cell></row><row><cell>R50-AOST (L = 2)</cell><cell>92.0</cell><cell>90.5</cell><cell>93.4</cell><cell>24.3</cell></row><row><cell>SwinB-AOST (L = 2)</cell><cell>92.2</cell><cell>90.5</cell><cell>93.8</cell><cell>14.4</cell></row><row><cell>AOT-B</cell><cell>89.9</cell><cell>88.7</cell><cell>91.1</cell><cell>29.6</cell></row><row><cell>AOT-L</cell><cell>90.4</cell><cell>89.6</cell><cell>91.1</cell><cell>18.7</cell></row><row><cell>AOST (L = 3)</cell><cell>91.6</cell><cell>90.1</cell><cell>93.0</cell><cell>18.2</cell></row><row><cell>R50-AOT-L</cell><cell>91.1</cell><cell>90.1</cell><cell>92.1</cell><cell>18.0</cell></row><row><cell>R50-AOST (L = 3)</cell><cell>92.1</cell><cell>90.6</cell><cell>93.6</cell><cell>17.5</cell></row><row><cell>SwinB-AOT-L</cell><cell>92.0</cell><cell>90.7</cell><cell>93.3</cell><cell>12.1</cell></row><row><cell>SwinB-AOST (L = 3)</cell><cell>92.4</cell><cell>90.5</cell><cell>94.2</cell><cell>12.0</cell></row><row><cell>R50-AOST-L</cell><cell>91.4</cell><cell>90.3</cell><cell>92.5</cell><cell>17.5</cell></row><row><cell>SwinB-AOST-L</cell><cell>92.4</cell><cell>90.6</cell><cell>94.1</cell><cell>12.0</cell></row><row><cell>SwinB-AOST (L = 3, MS)</cell><cell>93.0</cell><cell>91.5</cell><cell>94.5</cell><cell>1.3</cell></row><row><cell>SwinB-AOST-L (MS)</cell><cell>93.0</cell><cell>91.6</cell><cell>94.4</cell><cell>1.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>in short-term attention. (a) Identity number M J &amp;F J S J U</figDesc><table><row><cell></cell><cell>(b) Local window size</cell><cell cols="4">(c) Local frame number</cell><cell>(d) Block number</cell><cell cols="2">(e) Positional embedding</cell></row><row><cell></cell><cell>? J &amp;F J S J U</cell><cell cols="3">n J &amp;F J S</cell><cell>J U</cell><cell>L J &amp;F J S J U FPS</cell><cell cols="2">Self Rel J &amp;F J S J U</cell></row><row><cell>10 80.3 80.6 73.7 20 78.3 79.4 70.8 30 77.2 78.5 70.2 1 78.7 78.0 73.0</cell><cell>15 80.3 80.6 73.7 11 78.8 79.5 71.9 7 78.3 79.3 70.9 0 74.3 74.9 67.6</cell><cell>1 2 3 0</cell><cell>80.3 80.0 79.1 74.3</cell><cell cols="2">80.6 73.7 79.8 73.7 80.0 72.2 74.9 67.6</cell><cell>2 80.3 80.6 73.7 27.1 3 80.9 81.1 74.0 20.5 1 77.9 78.8 71.0 41.0</cell><cell>sine none sine</cell><cell>80.3 80.6 73.7 80.1 80.4 73.5 -79.7 80.1 72.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 4 :</head><label>4</label><figDesc>Ablation study of S-LSTT and layer-wise ID-based attention. The experiments are based on AOST, and the evaluation metric is J &amp;F on YouTube-VOS<ref type="bibr" target="#b10">[11]</ref>.</figDesc><table><row><cell cols="4">(a) Balance weight</cell><cell></cell><cell cols="3">(b) Scalable LSTT</cell><cell></cell><cell cols="6">(c) Layer-wise ID-based attention</cell></row><row><cell>?</cell><cell>1</cell><cell>L 2</cell><cell>3</cell><cell>LSTT Type</cell><cell>1</cell><cell>L 2</cell><cell>3</cell><cell>Param</cell><cell>W ID l</cell><cell>W G l</cell><cell>1</cell><cell>L 2</cell><cell>3</cell><cell>Param</cell></row><row><cell cols="4">2 80.6 83.7 84.4</cell><cell>S-LSTT</cell><cell cols="4">80.6 83.7 84.4 5.8/7.3/8.8</cell><cell></cell><cell></cell><cell cols="4">80.6 83.7 84.4 5.8/7.3/8.8</cell></row><row><cell cols="4">1 81.3 83.9 84.2</cell><cell cols="4">Shared S-LSTT 80.3 82.8 83.6</cell><cell>5.8</cell><cell></cell><cell></cell><cell cols="4">80.0 83.2 83.7 5.7/7.2/8.7</cell></row><row><cell cols="4">4 79.8 83.5 84.5</cell><cell>LSTT [19]</cell><cell>-</cell><cell>-</cell><cell>83.7</cell><cell>8.3</cell><cell></cell><cell></cell><cell cols="4">80.3 83.5 84.1 5.8/7.3/8.8</cell></row><row><cell cols="4">8 79.5 83.3 84.1</cell><cell>Shared LSTT</cell><cell>-</cell><cell>-</cell><cell>82.8</cell><cell>5.7</cell><cell></cell><cell></cell><cell cols="4">79.8 82.6 83.4 5.7/7.2/8.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Video segmentation and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="669" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kernelized memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video object segmentation with episodic graph memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collaborative video object segmentation by foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Collaborative video object segmentation by multi-scale foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video object segmentation with adaptive feature bank and uncertain-region refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rethinking space-time networks with improved memory coverage for efficient video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast video object segmentation using the global context module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">State-aware tracker for real-time video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards multi-object association from foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Associating objects with transformers for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Label propagation in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3265" to="3272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Active frame selection for label propagation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="496" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Seamseg: Video object segmentation using patch seams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ramakanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Monet: Deep motion exploitation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1140" to="1148" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2663" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Premvos: Proposalgeneration, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ACCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="565" to="580" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="6499" to="6507" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1189" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="54" to="70" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cfbi+: Collaborative video object segmentation by multi-scale foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reliable propagation-correction modulation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2022</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning what to learn for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video object segmentation using kernelized memory network with multiple kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sstvos: Sparse spatiotemporal transformers for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aarabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint inductive and transductive learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9670" to="9679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="7794" to="7803" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">End-to-end asr: from supervised to semi-supervised learning with modern architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in ICML Workshops, 2020. 3</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in ICLR, 2021. 3, 5</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">904</biblScope>
			<biblScope unit="page" from="12" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8741" to="8750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. PMLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Agss-vos: Attention guided single-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3949" to="3957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Gated channel transformation for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno>2020. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Hierarchical image saliency detection on extended cssd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="717" to="729" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Optimal decisions from probabilistic models: the intersection-over-union case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="548" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Large-scale video panoptic segmentation in the wild: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Hierarchical memory matching network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in NAACL</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="464" to="468" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Vspw: A large-scale dataset for video scene parsing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="4133" to="4143" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
