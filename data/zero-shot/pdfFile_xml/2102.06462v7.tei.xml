<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
						</author>
						<title level="a" type="main">Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In node classification tasks, graph convolutional neural networks (GCNs) have demonstrated competitive performance over traditional methods on diverse graph data. However, it is known that the performance of GCNs degrades with increasing number of layers (oversmoothing problem) and recent studies have also shown that GCNs may perform worse in heterophilous graphs, where neighboring nodes tend to belong to different classes (heterophily problem). These two problems are usually viewed as unrelated, and thus are studied independently, often at the graph filter level from a spectral perspective.</p><p>In this work, we take a new unified perspective to understand the performance degradation of GCNs at the node level. Specifically, we profile the nodes via two quantitative metrics: the relative degree of a node (compared to its neighbors) and the node-level heterophily. Our theory shows that the interplay of these two profiling metrics defines three cases of node behaviors, which explain the oversmoothing and heterophily problems jointly and can predict the performance of GCNs. Based on insights from our theory, we show theoretically and empirically the effectiveness of two strategies: degree correction, which learns to adjust degree coefficients, and signed messages, which may be useful (under conditions) by learning to optionally negate the messages. Compared to other approaches, which tend to handle well either heterophily or oversmoothing, we show that our modified GCN model that incorporates the two strategies performs well in both problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>applications ranging from social science <ref type="bibr">(Li &amp; Goldwasser, 2019)</ref> and biology <ref type="bibr">(Yan et al., 2019)</ref> to program understanding <ref type="bibr" target="#b0">(Allamanis et al., 2018;</ref><ref type="bibr">Shi et al., 2019)</ref>. A typical GCN architecture <ref type="bibr">(Gilmer et al., 2017)</ref> for the node classification task can be decomposed into two main components: propagation/aggregation, and combination. Messages are first exchanged between neighboring nodes, then aggregated. Afterwards, they are combined with the self-representations (a.k.a., the current node representations) to update the node representations. Though GCNs are generally effective, their performance may degrade in some cases. <ref type="bibr">Li et al. (2018)</ref> found that GCNs perform worse with increasing number of layers, which is termed as the "oversmoothing problem". Recent works claim that oversmoothing could be caused by GCNs exponentially losing expressive power in the node classification task <ref type="bibr">(Oono &amp; Suzuki, 2019)</ref> and that the node representations converge to a stationary state which is decided by the degree of the nodes and the input features <ref type="bibr">(Chen et al., 2020b;</ref><ref type="bibr">Wang et al., 2019;</ref><ref type="bibr">Rong et al., 2019;</ref><ref type="bibr">Rossi et al., 2020)</ref>. These works analyze the asymptotic node representations in the limit of infinite layers, but they do not characterize how the node representations change over the layers (node behaviors) and how different node behaviors contribute to the oversmoothing problem. <ref type="bibr">Chen et al. (2020a)</ref> empirically define metrics to measure the oversmoothing problem, but it remains unclear what causes it and how the metrics are related in theory. Going beyond empirical definitions, we propose theoreticallygrounded node-level metrics that characterize different node behaviors across layers of GCNs, and show theoretically and empirically how they can explain the oversmoothing problem and identify the nodes that trigger it.</p><p>GCNs may also perform poorly on heterophilous graphs <ref type="bibr">(Pei et al., 2019;</ref><ref type="bibr">Lim et al., 2021)</ref>, which-unlike homophilous graphs-comprise many neighboring nodes that belong to different classes <ref type="bibr">(Newman, 2002)</ref>. This is termed as the "heterophily problem". For instance, in protein networks, amino acids of different types tend to form links <ref type="bibr">(Zhu et al., 2020)</ref>, and in transaction networks, fraudsters are more likely to connect to accomplices than to other fraudsters <ref type="bibr">(Pandit et al., 2007)</ref>. Most <ref type="bibr">GCNs (Kipf &amp; Welling, 2016;</ref><ref type="bibr">Veli?kovi? et al., 2017)</ref> fail to effectively capture heterophily, so various de-signs have been proposed to handle it <ref type="bibr">(Pei et al., 2019;</ref><ref type="bibr">Zhu et al., 2020;</ref><ref type="bibr">Chien et al., 2021;</ref><ref type="bibr">Bo et al., 2021)</ref>. These works take the spectral perspective and design various highfrequency graph filters to address heterophily. However, they neglect the fact that different node behaviors impact GCNs' performance under heterophily differently. In this work, we show that GCNs can perform differently on graphs that have similar heterophily levels but are dominated by different node behaviors.</p><p>These two problems, which cause performance degradation, have mostly been studied independently. Recent work on oversmoothing <ref type="bibr">(Chen et al., 2020b)</ref> was shown only empirically to address heterophily, and vice versa <ref type="bibr">(Chien et al., 2021)</ref>. Motivated by this empirical observation, we set out to find a joint explanation for the two problems. Specifically, we aim to identify meaningful node-level metrics that are theoretically grounded and their interplay can be used to characterize different node behaviors (profiles), which in turn can explain both problems. We found that the relative degree of a node (compared to its neighbors) and its nodelevel heterophily define three types of node behaviors, two of which are related to performance degeneration. Based on our theoretical insights, we show theoretically and empirically the effectiveness of two strategies: degree correction, which learns to adjust the degree coefficients, and signed messages, which may be useful by learning to optionally negate the messages. Though the concept of signed messages has been used in prior work <ref type="bibr">(Chien et al., 2021;</ref><ref type="bibr">Bo et al., 2021)</ref>, we introduce a new, more expressive design and focus on analyzing its effectiveness via node profiling instead of the typically-used perspective of spectral analysis. Unlike other works, we also determine the condition under which signed messages can be useful.</p><p>In sum, we make the following contributions:</p><p>? Theoretically-grounded Node Metrics: We introduce two theoretically-grounded metrics, relative degree and node-level heterophily, to profile the nodes across layers in GCNs. The profiling provides a joint explanation for what triggers the heterophily and oversmoothing problems. ? Insights: Our theory states that for strongly homophilous graphs, low-degree nodes trigger the oversmoothing problem, while for weakly homophilous (i.e., heterophilous) graphs, high-degree nodes tend to be misclassified. Under certain conditions, we show that using signed messages is helpful in alleviating both problems. ? Improved Model &amp; Empirical Analysis: Based on our insights, we show theoretically and empirically the effectiveness of two strategies: degree correction, which learns to adjust degree coefficients, and signed messages, which learn to optionally negate the messages. Our empirical results show that our model, GGCN, which leverages the two strategies is robust to oversmoothing, achieves state-of-the-art performance on datasets with high levels of heterophily, and achieves competitive performance on homophilous datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>We first provide the notations &amp; definitions that we use in the paper, and a brief background on GCNs.</p><p>Notation. We denote an unweighted and self-loop-free graph as G (V, E) and its adjacency matrix as A. We represent the degree of node v i ? V by d i , and the degree matrix-which is a diagonal matrix whose elements are node degrees-by D. Let N i be the set of nodes directly connected to v i , i.e., its neighbors. I is the identity matrix. We denote the node representations at l-th layer as F (l) , and the i-th row of F is f</p><formula xml:id="formula_0">(l)</formula><p>i , which is the representation of node v i . The input node features are given by F (0) . The weight matrix and bias vector at the l-th layer are denoted as W (l) and b (l) , respectively.</p><p>Supervised Node Classification Task. We focus on node classification: Given a random sample of node representations {f</p><formula xml:id="formula_1">(0) 1 . . . f (0)</formula><p>n } ? R m and their labels {y 1 . . . y n } ? R n for training, we aim to learn a function F :</p><formula xml:id="formula_2">R m ? R n , such that the loss E(L (y i ,? i )) is minimized, where? i = F (f (0) i ) is the predicted label of v i .</formula><p>The misclassification rate is defined as the probability P(y i =? i ) to misclassify an arbitrary node in the node representation space.</p><p>GCNs. In node classification tasks, an L-layer GCN contains two components <ref type="bibr">(Gilmer et al., 2017)</ref>: <ref type="formula" target="#formula_8">(1)</ref> neighborhood propagation and aggregation: f</p><formula xml:id="formula_3">(l) i = AGGREGATE(f (l) j , v j ? N i ), and (2) combination: f (l+1) i = COMBINE( f (l) i , f (l)</formula><p>i ), where AGGREGATE and COMBINE are learnable functions. The loss is given by L CE =CrossEntropy(Softmax(f (L) i W (L) + b (L) ), y i ). The vanilla GCN suggests a renormalization trick on the adjacency A to prevent gradient explosion <ref type="bibr">(Kipf &amp; Welling, 2016)</ref>. The (l + 1)-th output is given by:</p><formula xml:id="formula_4">F (l+1) = ?(?F (l) W (l) ), where? =D ?1/2 (I + A)D ?1/2 ,D</formula><p>is the degree matrix of I + A, and ? is ReLU. When the non-linearities in the vanilla GCN are removed, it reduces to a linear model called SGC <ref type="bibr">(Wu et al., 2019)</ref>, which has competitive performance and is widely used in theoretical analyses <ref type="bibr">(Oono &amp; Suzuki, 2019;</ref><ref type="bibr">Chen et al., 2020b)</ref>. For SGC, the l-th layer representations are given by: F (l) =? l F (0) and the last layer is a logistic-regression layer:? i = Softmax(F (L) W (L) + b (L) ). We note that only one weight matrix W (L) is learned, which is equivalent to the products of all weight matrices in a linear GCN. More related works can be found in ? 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Theoretical Analysis</head><p>In this section, we formally introduce two metrics: nodelevel homophily h i and relative degree r i . We show theoreti- cally (1) how the two metrics and their extensions (effective homophily h l i and effective relative degreer i l ) characterize different node behaviors across GCN layers, and (2) how this node profiling can be used to explain the oversmoothing and heterophily problems.</p><p>To begin with, we first introduce the theoretical setup. Throughout the section, we analyze binary node classification using the typically-studied SGC model ( ? 2). The nodes in class 1 are denoted as set V 1 and nodes in class 2 are denoted as set V 2 . Later, in ? 5, we show empirically that the insights obtained in this section are effective for other non-linear models in multi-class classification. 3.1. Assumptions Notations. We use "i.d." to represent random variables / vectors that follow the same marginal distribution and their joint probability density function (PDF) is a permutationinvariant function p(x 1 , . . . , x n ) = p(P(x 1 , . . . , x n )), where P(?) means permutation. We use E A|B (?) to denote the expectation taken over the randomness of A given B.</p><p>We make the following assumptions:</p><p>(1) Random Graph: Node degrees {d i } are i.d. random variables, where {(?) i } represents a set with i = 1, . . . , |V|.</p><p>(2) Inputs: (2.1) Node labels {y i } are i.d. Bernoulli random variables given by the ratio ?: ? ? P(yi=1) P(yi=2) , ?i. The event {y i = y j |v j ? N i } is independent of y i , ?i, j.</p><p>(2.2) Initial input node features {f (0) i } are random vectors given by (PDF) f (x), which is expressed as:</p><formula xml:id="formula_5">f (x) = f 1 (x), when y i = 1. f 2 (x), when y i = 2. E(f (0) i |y i ) = ?, when y i = 1 ???, when y i = 2 , so E(f (0) i )=0. (3) Independence: A is independent of {y i } and {f (0) i }. ?i, j given y i , f (0) i</formula><p>and y j are conditional independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Node-level Metrics: Definitions</head><p>Node-level Homophily and Heterophily. Given a set of node labels/classes, homophily captures the tendency of a node to have the same class as its neighbors. Specifically, the homophily of node v i is defined as:</p><p>h i ? P(y i = y j |v j ? N i ). High homophily corresponds to low heterophily, and vice versa, so we use these terms interchangeably.</p><p>Relative Degree r i . The relative degree of node v i is:</p><formula xml:id="formula_6">r i ? E A|di ( 1 d i j?Ni r ij |d i ), where r ij ? d i + 1 d j + 1 .</formula><p>It evaluates the node degree compared to its neighbors' degrees. When all the nodes have the same degree, r i = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Node Profiling</head><p>In this section, we theoretically show how the two metrics can characterize different node behaviors across layers.</p><p>Movements of Node Representations. We monitor the node behaviors by tracking the changes of node representations across the layers. Each node representation is mapped to a point in the feature space whose coordinates are decided by the representation vector. In this way, the changes of a node's representations across the layers can be viewed as the movements of the mapped point. For example, f</p><formula xml:id="formula_7">(l+1) i ? f (l) i</formula><p>is referred to as the movement of node v i 's representation at the l-th layer. Next, we show that the interplay of the two metrics relates to different types of movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">MOVEMENTS AT THE INITIAL LAYER</head><p>We examine how the node representations change in expectation. Without loss of generality, we assume v i ? V 1 , the other case can be derived similarly.</p><p>Theorem 3.1. Given v i ? V 1 and d i , the conditional expectation of representation f</p><p>(1) i is given by:</p><formula xml:id="formula_8">E A,{yi},{f (0) i }|di,vi?V1 (f (1) i |v i ? V 1 , d i ) = ((1 + ?)h i ? ?)d i r i + 1 d i + 1 E(f (0) i |v i ? V 1 ) ? ? 1 i E(f (0) i |v i ? V 1 ),<label>(1)</label></formula><p>where the multiplicative factor ? 1 i is:</p><formula xml:id="formula_9">? 1 i ? ? ? ? ? ? (??, 1 2 ], if h i ? ? 1+? (0, 1], if h i &gt; ? 1+? &amp; r i ? 1 (1+?)hi?? (1, ?),</formula><p>otherwise.</p><p>(2)</p><p>In case 1, ? 1 i decreases as d i increases; In case 3, ? 1 i increases as d i increases.</p><p>Proof. We provide the proof in App. A.1. From Thm. 3.1, we identify three types of movements of node representations, which are characterized by relative degree r i and homophily level h i . For illustration purposes, in <ref type="figure" target="#fig_0">Fig. 1</ref>, we illustrate the three cases when we apply our theorem to 1D node representations. The bars reflect the value change of v i 's node representation. Intuitively, under heterophily (case 1), node representations tend to move closer to the representations of the other class. The higher the degree, the more the representation moves. Under high homophily but low degrees (case 2), node representations still tend to move towards the other class, but not as much as in case 1. Only when both the homophily and the degree is high, the node representations may move away from the other class. Thus, case 3 is the only favorable case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">MOVEMENTS AT DEEPER LAYERS</head><p>The scenarios at deeper layers are more complex. However, by extending the definitions of the two metrics, we can obtain a similar equation, and the extended metrics can characterize the nodes into 3 cases similar to Thm. 3.1.</p><formula xml:id="formula_10">(? l i (1 + ? l i ) ? ? l i )d iri l + 1 d i + 1 ? l i E(f (0) i |y i ) (4) ? ? (l+1) i E(f<label>(0)</label></formula><p>i |y i ). Proof. We provide the proof in App. A.2.</p><p>Movements &amp; Misclassification Rate. In App. A.4, we prove that under certain conditions, moving towards the other class by a non-zero step increases the misclassification rate, causing performance degradation.</p><p>Explanation for Heterophily and Oversmoothing:</p><p>? In heterophilous graphs, node representations move heavily towards the opposite class. Performance degradation is severe and causes the heterophily problem (case 1).</p><p>? In homophilous graphs, high-degree nodes initially benefit (case 3), leading to performance gain in shallow layers. However, the representations of low-degree nodes tend to move towards the opposing class (case 2) and eventually flip the labels. The misclassification of low-degree nodes causes a pseudo-heterophily situation (case 1) later. For the oversmoothing problem, based on our node profiling, we can decompose the process into two stages. Next, we use homophilous graphs as an example: Initial Stage. Nodes of case 2 and case 3 are the major components. If case 3 dominates, GCNs benefit from graph convolution. In this stage, nodes with higher degree have higher classification accuracy. Developing Stage. Nodes of case 2 eventually flip their labels, which reduces their neighbors' effective homophily. Then, nodes of case 2 and case 3 turn to case 1 due to reduced effective homophily, and so case 1 dominates. In this stage, the accuracy of nodes with higher degree drops more sharply than that of low-degree nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Node Profiling With Signed Messages</head><p>In this section, we discuss how the interplay of the two metrics changes when using signed messages. We provide theory to show when signed messages can help enhance the performance in heterophilous graphs and alleviate oversmoothing. Due to limited space, we only show the effect of signed messages at initial layer; similar results can be derived in deeper layers.</p><p>Setup. Signed messages consist of the negated messages sent by neighbors of the opposing class (i.e., after multiplying initial messages by -1), and the unchanged messages ("positive messages") sent by neighbors of the same class. In reality, ground-truth labels are not accessible for every node, so we use approximations, which introduces errors. For node v i , we define m l i as the ratio of neighbors that send incorrect messages at the l-th layer (i.e., different-class neighbors that send non-negated messages and same-class neighbors that send negated messages). We define the lth layer error rate as e l i = E(m l i ), where the expectation is over the randomness of the neighbors that send incorrect messages. We make the following assumption: m l i is independent of {d i }, {y i } and {f 0 i }. Theorem 3.3. [Signed Messages] By allowing the messages to be optionally multiplied by a negative sign, the movements of representations will be less affected by the initial homophily level h i , and will be dependent on the error rate e l i . Specifically, the multiplicative factor ? 1 i at the first layer is:</p><formula xml:id="formula_11">E A,{yi},{f (0) i }|di,vi?V1 (f (1) i |d i , v i ? V 1 ) (5) = (1 ? 2e 0 i )(? + (1 ? ?)h i )d i r i + 1 d i + 1 E(f (0) i |v i ? V 1 ), where ? 1 i ? ? ? ? ? ? (??, 1 2 ], if e 0 i ? 0.5 (0, 1], if e 0 i &lt; 0.5 &amp; r i ? 1 (1?2e 0 i )(?+(1??)hi)</formula><p>(1, ?), otherwise.</p><p>(6) Proof. The proof is provided in App. A.3. From Eq. 6, we see that when using signed messages, to benefit from case 3 (? 1 i &gt; 1), the minimum relative degree satisfies:</p><formula xml:id="formula_12">r i &gt; 1 (1?2e 0 i )(?+(1??)hi) . Given h i ? 1, if the error rate is low (e 0 i 0.5), we get: 1 (1?2e 0 i )(?+(1??)hi) ? 1<label>(</label></formula><p>1+?)hi?? , and 1 (1+?)hi?? is the minimum relative degree required when not using signed messages. This implies that more nodes can benefit from using signed messages. We note that if low error rate cannot be guaranteed, signed messages may hurt the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model Design</head><p>Based on our theoretical analysis, we propose two new, simple mechanisms to address both the heterophily and oversmoothing problems: degree corrections and signed messages. We integrate these mechanisms, along with a decaying combination of the current and previous node representations (Chen et al., 2020b), into a generalized GCN model, GGCN. In ? 5, we empirically show its effectiveness in addressing the two closely-related problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Degree Corrections</head><p>Our analysis in ? 3.3 and ? 3.4 highlights that, when the error rate and homophily level are high, oversmoothing is initially triggered by low-degree nodes. Thus, we aim to compensate for low degrees via degree corrections.</p><p>Based on Eq. (6), and given that most well-trained graph models have a relatively low error rate at the shallow layers (i.e, e l i 0.5 for small l), we require that the node degrees satisfy r i &gt; 1 (1?2e 0 i )(?+(1??)hi) to prevent oversmoothing. Since the node degrees cannot be modified, our strategy is to rescale them. Specifically, we correct the degrees by multiplying with scalars ? l ij and change the original formulation as follows:</p><formula xml:id="formula_13">(?F (l) )[i, :] = F (l) [i, :] di + 1 + v j ?N i F (l) [j, :] ? di + 1 dj + 1 =? v j ?N i ? l i,j F (l) [j, :] ? di + 1 dj + 1 .<label>(7)</label></formula><p>This multiplication is equivalent to changing the ratio r ij in Thm. 3.1 to</p><formula xml:id="formula_14">(? l ij ) 2 (di+1) dj +1</formula><p>. That is, a larger ? l ij increases the effective r i at layer l. Training independent ? l i,j is not practical because it would require O(|V| 2 ) additional parameters per layer, which can lead to overfitting. Moreover, low-rank parameterizations suffer from unstable training dynamics. Intuitively, when r ij is small, we would like to compensate for it via a larger ? l i,j . Thus, we set ? l i,j to be a function of r ij as follows:</p><formula xml:id="formula_15">? l ij = softplus ? l 0 1 r ij ? 1 + ? l 1 ,<label>(8)</label></formula><p>where ? l 0 and ? l 1 are learnable parameters. We subtract 1 so that when r ij = 1 (i.e., d i = d j ), then ? l ij = softplus(? l 1 ) is a constant bias.</p><p>Let T T T (l) be a matrix with elements ? l ij . Our model GGCN learns a corrected adjacency matrix at l-th layer:</p><p>?l =? T T T (l)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Signed Messages</head><p>Thm 3.3 points out the importance of signed messages in tackling the heterophily and oversmoothing problems. Inspired by this, we use cosine similarity to approximate signs.</p><p>For expressiveness, as in GCN (Kipf &amp; Welling, 2016), we first perform a learnable linear transformation of each node's representation at the l-th layer:</p><formula xml:id="formula_16">F (l) = F (l) W (l) + b (l)</formula><p>Then, we define a sign function to be multiplied with the messages exchanged between neighbors. To allow for backpropagation of the gradient information, we approximate the sign function with cosine similarity. Denote S l as the matrix which stores the sign information about the edges, defined as:</p><formula xml:id="formula_17">S (l) [i, j] =Cosine(f (l) i , f (l) j ) if (i = j) &amp; (v j ? N i ); 0 otherwise.</formula><p>In order to separate the contribution of similar neighbors (likely in the same class) from that of dissimilar neighbors (unlikely to be in the same class), we split S (l) into a positive matrix S (l) pos and a negative matrix S (l) neg . Thus, our proposed GGCN model learns a weighted combination of the selfrepresentations, the positive messages, and the negative messages:</p><formula xml:id="formula_18">F (l+1) = ? ?l (? l 0 F (l) +? l 1 (S (l) pos ?l ) F (l) +? l 2 (S (l) neg ?l ) F (l) ) ,</formula><p>where? l 0 ,? l 1 and? l 2 are scalars obtained by applying softmax to the learned scalars ? l 0 , ? l 1 and ? l 2 ; the non-negative scaling factor? l = softplus(? l ) is derived from the learned scalar ? l ; is element-wise multiplication; and ? is the nonlinear function Elu. We note that we learn different ? and ? parameters per layer for flexibility. We also require the combined weights,? l?l x , to be non-negative so that they do not negate the intended effect of the signed information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Decaying Aggregation</head><p>In addition to our two proposed mechanisms that are theoretically grounded in our analysis ( ? 3), we also incorporate into GGCN an existing design-decaying aggregation of messages-that empirically increases performance. However, we note that, even without this design, our GCN architecture still performs well under heterophily and is robust to oversmoothing (App. ?B.1).</p><p>Decaying aggregation was introduced in (Chen et al., 2020b) as a way to slow down the convergence rate of node repre-sentations. Inspired by this work, we modify the decaying function,?, and integrate it to our GGCN model:</p><formula xml:id="formula_19">F (l+1) = F (l) +? ? ?l (? l 0 F (l) +? l 1 (S (l) pos ? T T T (l) ) F (l) +? l 2 (S (l) neg ? T T T (l) ) F (l) ) .<label>(9)</label></formula><p>In practice, we found that the following decaying function works well:? ? ln( ? l k + 1), iff l ? l 0 ;? = 1, otherwise. The hyperparameters k, l 0 , ? are tuned on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In the main paper, we focus on the following four questions: (Q1) Compared to the baselines, how does GGCN perform on homophilous and heterophilous graphs? (Q2) How robust is it against oversmoothing under homophily and heterophily? (Q3) Is our node profiling effective in predicting the performance degradation of GCNs in heterophilous graphs? (Q4) How can we verify the correctness of our theorems about oversmoothing on real datasets?</p><p>We provide the ablation study of signed messages and degree correction in App. ? B.1 and the study of different normalization strategies in App. ? B.2. 5.1. Experimental Setup Datasets. We evaluate the performance of our GGCN model and existing GNNs in node classification on various realworld datasets <ref type="bibr">(Tang et al., 2009;</ref><ref type="bibr">Rozemberczki et al., 2019;</ref><ref type="bibr">Sen et al., 2008;</ref><ref type="bibr">Namata et al., 2012;</ref><ref type="bibr">Bojchevski &amp; G?nnemann, 2018;</ref><ref type="bibr">Shchur et al., 2018)</ref>. We provide their summary statistics in <ref type="table" target="#tab_0">Table 1</ref>, where we compute the homophily level h of a graph as the average of h i of all nodes v i ? V. For all benchmarks, we use the feature vectors, class labels, and 10 random splits (48%/32%/20% of nodes per class for train/validation/test 1 ) from <ref type="bibr">(Pei et al., 2019)</ref>. Baselines. For baselines we use (1) classic GNN models for node classification: vanilla <ref type="bibr">GCN (Kipf &amp; Welling, 2016)</ref>, <ref type="bibr">GAT (Veli?kovi? et al., 2017)</ref> and GraphSage <ref type="bibr">(Hamilton et al., 2017)</ref>; (2) recent models tackling heterophily: Geom-GCN (Pei et al., 2019), <ref type="bibr">H2GCN (Zhu et al., 2020)</ref> and <ref type="bibr">GPRGNN (Chien et al., 2021)</ref>; (3) models tackling oversmoothing: PairNorm (Zhao &amp; Akoglu, 2019) and GCNII (Chen et al., 2020b) (state-of-the-art); and (4) 2layer MLP (with dropout and Elu non-linearity). For GCN, PairNorm, Geom-GCN, GCNII, H2GCN and GPRGNN, we use the original codes provided by the authors. For GAT, we use the code from a well-accepted Github repository 2 . For GraphSage, we report the results from <ref type="bibr">(Zhu et al., 2020)</ref>, which uses the same data and splits. For the baselines that have multiple variants (Geom-GCN, GCNII, H2GCN), we choose the best variant for each dataset and denote them as [model]*. For each dataset and baseline, if applicable, we use the best hyperparameters provided by the authors. Otherwise, we perform parameter search to set the best hyperparameters for each baseline. The setting and analysis of hyperparameters are in App. ? C. Machine. We ran our experiments on Nvidia V100 GPU. <ref type="table" target="#tab_0">Table 1</ref> provides the test accuracy of different GNNs on the supervised node classification task over datasets with varying homophily levels (arranged from low homophily to high homophily). A graph's homophily level is the average of its nodes' homophily levels. We report the best performance of each model across different layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Q1. Performance Under Homophily &amp; Heterophily</head><p>GGCN performs the best in terms of average rank (1.78) across all datasets, which suggests its strong adaptability to graphs of various homophily levels. In particular, GGCN achieves the highest accuracy in 5 out of 6 heterophilous graphs (h is low). For datasets like Chameleon and Cornell, GGCN enhances accuracy by around 6% and 3% compared to the second-best model. On homophily datasets (Citeseer, Pubmed, Cora), the accuracy of GGCN is within a 1% difference of the best model.</p><p>Our experiments highlight that MLP is a good baseline in heterophilous datasets. In heterophilous graphs, the models that are not specifically designed for heterophily usually perform worse than an MLP. Though H2GCN* is the second best model in heterophilous datasets, we can still see that in the Actor dataset, MLP performs better. GPRGNN and Geom-GCN*, which are specifically designed for heterophily, achieve better performance than classic GNNs (GCN and GAT) in heterophilous datasets, but do not show clear advantage over MLP. Our GGCN model is the only model that performs better than MLP across all the datasets.</p><p>In general, GNN models perform well in homophilous datasets. GCNII* performs the best, and GGCN, H2GCN*, GPRGNN and Geom-GCN* also achieve high performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Q2. Oversmoothing</head><p>We also test how robust the models are to oversmoothing. To this end, we measure the supervised node classification accuracy for 2 to 64 layers. <ref type="table" target="#tab_1">Table 2</ref> presents the results for two homophilous datasets (top) and two heterophilous datasets (bottom). Per model, we also report the layer at which the best performance is achieved (column 'Best'). We provides <ref type="table" target="#tab_1">Table 2</ref> in larger font in App. ? D.</p><p>According to <ref type="table" target="#tab_1">Table 2</ref>, GGCN and GCNII* achieve increase in accuracy when stacking more layers in four datasets, while GPRGNN and PairNorm exhibit robustness against oversmoothing. Models that are not designed for oversmoothing have various issues. The performance of GCN and Geom-GCN* drops rapidly as the number of layers grows; H2GCN* requires concatenating all the intermediate outputs and quickly reaches memory capacity; GAT's atten-  tion mechanism also has high memory requirements. We also find that GAT needs careful initialization when stacking many layers as it may suffer from numerical instability in sparse tensor operations.</p><p>In general, models like GGCN, GCNII*, and GPRGNN that perform well under heterophily usually exhibit higher resilience against oversmoothing. One exception is Geom-GCN*, which suffers more than GCN. This model incorporates structurally similar nodes into each node's neighborhood; this design may benefit Geom-GCN* in the shallow layers as the node degrees increase. However, as we point out in Thm. 3.2, when the effective homophily is low, higher degrees are harmful. If the structurally similar nodes introduce lower homophily levels, their performance will rapidly degrade once the effective homophily is lower than</p><formula xml:id="formula_20">? l i 1+? l i .</formula><p>On the other hand, GGCN virtually changes the degrees thanks to the degree correction mechanism ( ? 4.1), and, in practice, this design has positive impact on its robustness to oversmoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Q3. Effectiveness of Node Profiling</head><p>In <ref type="table" target="#tab_2">Table 3</ref>, we provide results both for the three cases that our theory determines and for the frequently-used graphlevel homophily h, which is defined as the average of the nodes' homophily levels. We observe that our node profiling better explains the performance of GCN shown in <ref type="table" target="#tab_0">Table 1</ref>. In the Texas, Wisconsin and Actor datasets, only case 1 dominates; this case hurts the performance most, which explains why GCN performs worse than most methods. In the Squirrel and Chameleon datasets, both case 1 and case 2 dominate, so GCN yields acceptable results though the graph-level homophily is still low. Using the three cases to analyze the datasets is a better metric, because they align better with GCN's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Q4. Empirical Verification of the Initial &amp; Developing Stages</head><p>Figure 2: Accuracy of nodes grouped by degree di on Citeseer. Initial stage: when mean effective homophily h l i (ratio of a node's neighbors in the same class- ? 3.3.2) is high, the accuracy increases as the degree increases. Developing stage: when h l i is low, the accuracy of high-degree nodes drops more sharply. Using the vanilla GCN model <ref type="bibr">(Kipf &amp; Welling, 2016)</ref>, we validate our theorems by measuring the test accuracy and effective homophily for different node degrees (binned logarithmically) on real datasets. We estimate the effective homophily as the portion of the same-class neighbors that are correctly classified before the last propagation. <ref type="figure">Figure 2</ref> shows the results for Citeseer. In the initial stage ( h l i is high), the accuracy increases with the degree, but in the developing stage, the trend changes, with high-degree nodes being impacted the most, as predicted by our theorems. In App. ? B.3, we provide more details for this analysis and we further verify our conclusion on Cora. . Though recent work (Chen et al., 2020b) focused on solving the oversmoothing problem, it also empirically showed improvement on heterophilous datasets; these empirical observations formed the basis of our work. Finally, <ref type="bibr">(Chien et al., 2021)</ref> recently proposed a PageRank-based model that performs well under heterophily and alleviates the oversmoothing problem. However, they view the two problems independently and analyze their model via an asymptotic spectral perspective. Our work studies the representation dynamics and unveils the connections between the oversmoothing and heterophily problems theoretically and empirically. As we demonstrate with GGCN, addressing both issues in a principled manner provides superior performance across a variety of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Our work provides the first theoretical and empirical analysis that unveils the connections between the oversmoothing and heterophily problems. By analyzing the statistical change of the node representations after the graph convolution, we identified two causes, i.e., the relative degree of a node compared to its neighbors and the level of heterophily in its neighborhood, which influence the movements of node representations and lead to a higher misclassification rate. Based on our new, unified theoretical perspective, we obtained three important insights: (1) Nodes with high heterophily tend to be misclassified after graph convolution;</p><p>(2) Even with low heterophily, low-degree nodes can trigger a pseudo-heterophily situation that explains oversmoothing.</p><p>(3) Allowing signed messages (instead of only positive messages) helps alleviate the heterophily and oversmoothing problems. Based on these insights, we designed a generalized model, GGCN, that addresses the identified causes using signed messages and degree corrections. Though other designs may also address these two problems, our work points out two effective directions that are theoretically grounded ( ? 3). In summary, our research suggests it is beneficial to study the oversmoothing and heterophily problems jointly; this leads to architectural insights that can improve the learned representations of graph neural network Bo, D., Wang, X., Shi, C., and Shen, H. Beyond lowfrequency information in graph convolutional networks. arXiv preprint arXiv:2101.00797, 2021. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detailed Proofs of Theorems in ? 3</head><p>A.1. Proof of Theorem 3.1</p><p>Proof. The node representations of first layer are given by:</p><formula xml:id="formula_21">f (1) i = f (0) i d i + 1 + j?Ni f (0) j ? d i + 1 ? d j + 1 .<label>(10)</label></formula><p>Without loss of generality, we assume node v i is in the first class V 1 . Then, we can express the conditional expectation of f</p><p>(1) i as:</p><formula xml:id="formula_22">E A,{yi},{f (0) i }|di,vi?V1 (f (1) i |v i ? V 1 , d i ) = E A|di,vi?V1 (E {yi},{f (0) i }|A,vi?V1 (f (1) i |v i ? V 1 , A))<label>(11)</label></formula><p>Recall that A is the graph adjacency matrix; the first expectation is taken over the randomness of ground truth labels and initial input features, and the second expectation is taken over the randomness of graph structure (A) given the degree of node v i and its label.</p><formula xml:id="formula_23">E {yi},{f (0) i }|A,vi?V1 (f (1) i |v i ? V 1 , A) =E {yi},{f (0) i }|A,vi?V1 f (0) i d i + 1 |v i ? V 1 , A + j?Ni E {yi},{f (0) i }|A,vi?V1 f (0) j ? d i + 1 ? d j + 1 |v i ? V 1 , A = ? d i + 1 + j?Ni E {yi},{f (0) i }|A,vi,vj ?V1 f (0) j ? d i + 1 ? d j + 1 |A, v i , v j ? V 1 ? P(v j ? V 1 |A, v i ? V 1 ) +E {yi},{f (0) i }|A,vi?V1,vj ?V2 f (0) j ? d i + 1 ? d j + 1 |A, v i ? V 1 , v j ? V 2 ? P(v j ? V 2 |A, v i ? V 1 )<label>(12)</label></formula><p>Due to our assumptions in ? 3, A is independent of {y i }. Thus, for ?v j ? N i , we have:</p><formula xml:id="formula_24">P(v j ? V 1 |A, v i ? V 1 ) = P(v j ? V 1 |v i ? V 1 ) = P(y j = y i |y i = 1)<label>(13)</label></formula><p>Given that {y i = y j |v j ? N i } is independent of y i ( ? 3) and h i = P(y i = y j |v j ? N i ), therefore, when v j ? N i ,</p><formula xml:id="formula_25">P(v j ? V 1 |A, v i ? V 1 ) = P(y j = y i |y i = 1) = P(y j = y i ) = h i . Similarly, we have: P(v j ? V 2 |A, v i ? V 1 ) = 1 ? h i , given v j ? N i .</formula><p>Given that A and {f and y i are conditional independent given y j ,</p><formula xml:id="formula_26">E {yi},{f (0) i }|A,vi,vj ?V1 (f (0) j ) = E {yi},{f (0) i }|vj ?V1 (f (0) j ) = ?</formula><p>Similarly, we have:</p><formula xml:id="formula_27">E {yi},{f (0) i }|A,vi?V1,vj ?V2 (f (0) j ) = ??</formula><p>Thus, we have:</p><formula xml:id="formula_28">E {yi},{f (0) i }|vi?V1,A (f (1) i |v i ? V 1 , A) = ? d i + 1 + j?Ni h i ? d i + 1 ? d j + 1 ? ? (1 ? h i ) ? d i + 1 ? d j + 1 ?? = ? d i + 1 + ((1 + ?)h i ? ?)? ? d i + 1 j?Ni 1 d j + 1 = ? ? 1 d i + 1 + j?Ni 1 ? dj +1 ? d i + 1 ((1 + ?)h i ? ?) ? ? ? = ? ? ? 1 d i + 1 + j?Ni ? di+1 ? dj +1 d i + 1 ((1 + ?)h i ? ?) ? ? ? ? = 1 d i + 1 + d i d i + 1 j?Ni r ij d i ((1 + ?)h i ? ?) ?.<label>(14)</label></formula><p>Given that {y i } is independent of A, we can obtain the following equation by combining Equation <ref type="formula" target="#formula_8">(11)</ref> and Equation <ref type="formula" target="#formula_8">(14)</ref>:</p><formula xml:id="formula_29">E A,{yi},{f (0) i }|di,vi?V1 (f (1) i |v i ? V 1 , d i ) = 1 d i + 1 + d i d i + 1 E A|di,vi?V1 j?Ni r ij d i ((1 + ?)h i ? ?) ? = 1 + ((1 + ?)h i ? ?)d i r i d i + 1 ? ? ? 1 i ?.<label>(15)</label></formula><p>Define ? (1 + ?)h i ? ?, and we consider three cases:</p><formula xml:id="formula_30">(1) h i ? ? 1+? , (2) h i &gt; ? 1+? &amp; r i ? 1 , and (3) h i &gt; ? 1+? &amp; r i &gt; 1 . ? CASE 1: h i ? ? 1+? h i ? ? 1+? h i ? ? 1+?</formula><p>? Upper Bound</p><p>We have:</p><formula xml:id="formula_31">? 1 i ? 1 d i + 1 ? 1 2 .<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Lower Bound</head><p>When h i ? ? 1+? , we have:</p><formula xml:id="formula_32">1. ((1 + ?)h i ? ?) ? 0 2. di di+1 is an increasing non-negative function of d i 3. 1 di+1 is a decreasing function of d i When r i is an increasing non-negative function of d i , 1+((1+?)hi??)diri di+1</formula><p>is a decreasing function of d i .</p><p>When</p><formula xml:id="formula_33">h i = ? 1+? , 1+((1+?)hi??)diri di+1 = 1 di+1 and 0 &lt; 1 di+1 ? 1 2 . When h i &lt; ? 1+? , 1 + ((1 + ?)h i ? ?)d i r i d i + 1 ? ((1 + ?)h i ? ?)d i r i d i + 1 + 1 2 ? ((1 + ?)h i ? ?)r i 2 + 1 2 .<label>(17)</label></formula><p>And we know that:</p><formula xml:id="formula_34">lim ri?? ((1 + ?)h i ? ?)r i 2 + 1 2 = ??.<label>(18)</label></formula><p>Thus,</p><formula xml:id="formula_35">lim ri?? 1 + ((1 + ?)h i ? ?)d i r i d i + 1 = ??. (19) ? CASE 2: h i &gt; ? 1+? &amp; r i ? 1 h i &gt; ? 1+? &amp; r i ? 1 h i &gt; ? 1+? &amp; r i ? 1 If h i &gt; ? 1+? , 0 &lt; ? 1; if r i ? 1 , 0 &lt; r i ? 1.</formula><p>Given</p><formula xml:id="formula_36">E(f 1 i ) = 1 + d i r i d i + 1 ?.<label>(20)</label></formula><formula xml:id="formula_37">we have 0 &lt; 1 di+1 &lt; ? 1 i ? 1. ? CASE 3: h i &gt; ? 1+? &amp; r i &gt; 1 h i &gt; ? 1+? &amp; r i &gt; 1 h i &gt; ? 1+? &amp; r i &gt; 1 In this case, Equation (20) still holds, since h i &gt; ? 1+? . ? Lower Bound</formula><p>If r i &gt; 1 , then r i &gt; 1 and therefore ? 1 i &gt; 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Upper Bound</head><p>When &gt; 0,</p><formula xml:id="formula_38">1 + d i r i d i + 1 &gt; d i r i d i + 1 ? r i 2 .<label>(21)</label></formula><p>Because lim</p><formula xml:id="formula_39">ri?? r i 2 = ?,<label>(22)</label></formula><p>we have: lim</p><formula xml:id="formula_40">ri?? 1 + d i r i d i + 1 = ?.<label>(23)</label></formula><p>Given that di di+1 is an increasing non-negative function of d i and &gt; 0, when r i is an increasing non-negative function of d i , 1+ diri di+1 is an increasing function of d i . To sum it up,</p><formula xml:id="formula_41">? 1 i ? ? ? ? ? ? (??, 1 2 ], if h i ? ? 1+? (0, 1], if h i &gt; ? 1+? &amp; r i ? 1 (1+?)hi?? (1, ?),</formula><p>otherwise.</p><p>(24)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remarks: Special cases</head><p>For different types of graphs and different nodes in the graph, r i might be different. For a d-regular graph whose nodes have constant degree or a graph whose adjacency matrix is row-normalized, we will have:</p><formula xml:id="formula_42">? 1 i = 1 + ((1 + ?)h i ? ?)d i r i d i + 1 ?( 1 (d i + 1) + d i (d i + 1) ) = 1<label>(25)</label></formula><p>Equality is achieved if and only if h i = 1. To note that, h i = 1 is not achievable for every node as long as there are more than one class. Boundary nodes will suffer most.</p><p>Proof. After propagating over multiple layers, we use ? l i to account for the accumulated scaling effects (? l i ). That is,</p><formula xml:id="formula_43">E A,{yi},{f (0) i }|di,yi f (l) i |d i , y i = ? l i E(f (0)</formula><p>i |y i ). Furthermore, conditioned on d i , A and y i , the set of nodes contributing positively to node v i is defined as:</p><formula xml:id="formula_44">N s i (A, y i , d i ) ? {v j |v j ? N i and f (l) i ? f (l) j T &gt; 0}. E A,{yi},{f (0) i }|di,yi,vj ?Ni f (l) j r ij |d i , y i , v j ? N i = ? l i E(f (0) i |y i ) v j ?N s i (A, y i , d i ) ?? l i ? l i E(f (0) i |y i ) v j ?N s i (A, y i , d i )</formula><p>In the following proof, we useN s i to refer toN s i (A, y i , d i ) for conciseness. The representations at (l + 1)-th layer:</p><p>Combining Equation 29 and Equation 34, we can obtain:</p><formula xml:id="formula_45">E A,{yi},{f (0) i }|di,yi f (l+1) i |d i , y i (35) = ? l i E(f (0) i |y i ) d i + 1 + d i d i + 1 E A,{yi},{f (0) i }|di,yi,vj ?Ni f (l) j ? d i + 1 d j + 1 |d i , y i , v j ? N i (36) = ? l i E(f (0) i |y i ) d i + 1 + d i d i + 1 E A,{yi},{f (0) i }|di,yi,vj ?N s i ,vj ?Ni f (l) j ? d i + 1 d j + 1 |d i , y i , v j ?N s i , v j ? N i (37) ? P v j ?N s i |v j ? N i , d i , y i + E A,{yi},{f (0) i }|di,yi,vj ?N s i ,vi?Ni f (l) j ? d i + 1 d j + 1 |d i , y i , v j ?N s i , v j ? N i (38) ? P v j ?N s i , |d i , y i , v j ? N i (39) = ? l i E(f (0) i |y i ) d i + 1 + d i P(v j ?N s i |d i , y i , v j ? N i ) d i + 1 ? l i E(f (0) i |y i ) ? d i (1 ? P(v j ?N s i |d i , y i , v j ? N i )) d i + 1 ? l i ? l i E(f (0) i |y i ) (40) = ? l i E(f (0) i |y i ) d i + 1 + d i (P(v j ?N s i |d i , y i , v j ? N i )(1 + ? l i ) ? ? l i )? l i E(f (0) i |y i ) d i + 1 (41) Definer i l ? ? l i ? l i</formula><p>. If we know the degree d i , y i , and that a neighbor v j is in the groupN s i ,r i l actually represents the ratio of expected f l j r i,j to expected f l i . We regard it as the effective related degree of node v i at l-th layer. For the initial layer, if y j = y i , the ratio of f 0 j r i,j to f 0 i is r ij . Recall that the related degree r i at the initial layer is the expected average of r ij in the neighborhood. Thusr i l is an extension of r i . Moreover, let? l i = P(v j ?N s i |v j ? N i , d i , y i ) represent the probability of a neighbor whose representation has a positive contribution in expectation. This naturally extends the meaning of homophily in deeper layers. Thus, we regard it as the effective homophily of node v i at l-th layer. Given the degree d i and y i , ? l i represents the ratio of the probability that a neighbor will have a positive rather than a negative contribution to v i . This naturally extends the meaning of ?.</p><p>We further write Equation 41 as:</p><formula xml:id="formula_46">? l i d i + 1 1 + (? l i (1 + ? l i ) ? ? l i )d iri l ) ?</formula><p>and it will have three cases similar to Thm. 3.1.</p><formula xml:id="formula_47">d i + 1 + E m 0 i |vi?V1,di E A|di,vi?V1,m 0 i ? ? j?Ni ( (1 ? 2m 0 i )k i ? ? d i + 1 ? d j + 1 + (1 ? 2m 0 i )(1 ? k i )?? ? d i + 1 ? d j + 1 ) ? ? = ? d i + 1 + E m 0 i |vi?V1,di E A|di,vi?V1,m 0 i ? ? j?Ni (1 ? 2m 0 i )(? + (1 ? ?)k i )? ? d i + 1 ? d j + 1 ? ? = ? d i + 1 + E m 0 i |vi?V1,di (1 ? 2m 0 i )(? + (1 ? ?)h i )d i r i d i + 1 ? = 1 + (1 ? 2e 0 i )(? + (1 ? ?)h i )d i r i d i + 1 ? ? ? 1 i E(f (0) i ).<label>(48)</label></formula><p>When</p><formula xml:id="formula_48">h i ? 1, ? + (1 ? ?)h i = (1 ? h i )? + h i &gt; 0. Define ? (1 ? 2e 0 i )(? + (1 ? ?)h i ).</formula><p>To obtain the ranges of ? 1 i , when e 0 i ? 0.5, ? 0, it resembles the derivations of CASE 1 in Proof A.1; when e 0 i &lt; 0.5, 0 &lt; r i ? 1, it resembles the derivations of CASE 2; when e 0 i &lt; 0.5, r i &gt; 1, it resembles the derivations of CASE 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Movements &amp; Misclassification Rate</head><p>In this section, we illustrate why the movements of node representations are a good indicator of SGC's performance. The misclassification rate of an L-layer SGC can be studied through its last logistic-regression layer, the input to which is {f</p><formula xml:id="formula_49">(L) i }.</formula><p>To study the misclassification rate of an (L + 1)-layer SGC, we can view the input to the last layer {f i }. The misclassification rate is closely related to the decision boundary and will be our tool for studying the change of the misclassification rate. Our goal is to study how the movements change the decision boundary, which in turn affect the misclassification rate. Recall that we are studying the SGC model for a binary classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.1. PRELIMINARIES</head><p>Lemma In SGC, the decision boundaries w.r.t. {f <ref type="bibr">(L)</ref> i } in multi-class classification are linear hyperplanes. In particular, the decision boundary is a single hyperplane for binary classification.</p><p>Proof. The loss function for SGC is: CrossEntropy(Softmax(F (L) W (L) + b (L) ), y i ). Rewrite W (L) into an array of column vectors:</p><formula xml:id="formula_50">W (L) = [w (L) 0 , w (L) 1 , . . . w (L) |L| ], b (L) into an array of scalars: b (L) = [b (L) 0 , b (L) 1 , . . . b (L)</formula><p>|L| ], and F (L) into an array of row vectors:</p><formula xml:id="formula_51">F (L) = [f (L) 0 ; f (L) 1 ; . . . f (L) |V| ] ([?; ?] means stacking vertically). Let p i,c ? e f (L) i w (L) c +b (L) c j e f (L) i w (L) j +b (L) j</formula><p>and let y i,c ? 1, iff y i = c 0, otherwise then we rewrite the loss function as:</p><formula xml:id="formula_52">? 1 n n i=1 |L| c=1 y i,c ? log(p i,c ),<label>(49)</label></formula><p>n is the number of nodes used for training. To predict that the node v i belongs to class c, we require: ?c = c, p i,c &gt; p i,c . Thus, we have: e f (L)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Experiments</head><p>B.1. Ablation study We now study the impact of two proposed mechanisms (signed messages and degree correction, ? 4). To better show their effects, we add each design choice to a base model and track the changes in the node classification performance. As the base model, we choose a GCN variant that uses the message passing mechanism (Kipf &amp; Welling, 2016) with weight bias, a residual connection (but not decaying aggregation) for robustness to oversmoothing, and Elu non-linearity ?. We denote the model that replaces the message passing with our signed messages mechanism as +sign, and the model that incorporates the degree correction as +deg. The model that uses both designs is denoted as +sign,deg. <ref type="table" target="#tab_5">Table B</ref>.1 gives the accuracy of the models in the supervised node classification task for different layers.</p><p>We observe that both mechanisms alleviate the oversmoothing problem. Specifically, the base model has a sharp performance decrease after 32 layers, while the other models have significantly higher performance. In general, the +deg model is better than +sign in alleviating oversmoothing, and has a consistent performance gain across different data. For Chameleon, we observe increase in accuracy as we stack more layers; the large performance gain of GGCN results from the degree correction. In Cora and Cornell, +sign,deg consistently performs better across different layers than the model with only one design, demonstrating the constructive effect from our two proposed mechanisms.</p><p>The signed message design has an advantage in heterophilous datasets. In the Cornell dataset, using signed information rather than plain message passing results in over 20% gain, which explains the strong performance of GGCN. However, in homophilous datasets, the benefit from signed messages is limited, as these datasets have few different-class neighbors. This is because when the effective homophily h l i is high and error rate e 0 i is low, lim h l i ?1,e 0 i ?0</p><formula xml:id="formula_53">(1+?) h l i ?? (1?2e 0 i )(?+(1??) h l i )</formula><p>= 1, so using signed messages is less beneficial in homophilous datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Batch norm &amp; Layer norm</head><p>In ? 4, we use decaying aggregation instead of other normalizing mechanisms. Other mechanisms, such as batch or layer norm, may be seen as solutions to the heterophily and oversmoothing problems. However, batch norm cannot compensate for the dispersion of the mean vectors ( ? 3) due to different degrees and homophily levels of the nodes. Although, to some extent, it reduces the speed by which the representations of the susceptible nodes (case 1 &amp; 2) move towards the other class (good for oversmoothing), it also prevents the representations of the nodes that could benefit from the propagation (case 3) from increasing the distances (drop in accuracy). Layer norm is better at overcoming the dispersion effect but may lead to a significant accuracy drop in some datsets when a subset of features are more important than the others. Thus, we do not use any of these normalizations.</p><p>Next, we provide experiments to show the effects of batch norm and layer norm. We use the following base model (the same model used in ?B.1): a GCN (Kipf &amp; Welling, 2016) with weight bias, Elu non-linearity and residual connection. We do not include any of our designs so as to exclude any other factors that can affect the performance. The models we compare against are +BN and +LN, which represent the models that add batch norm and layer norm right before the non-linear activation, respectively. <ref type="table" target="#tab_5">Table B</ref>.2 shows that both batch norm and layer norm can help with oversmoothing. Moreover, adding layer norm is in general better than adding batch norm. This is expected because the scaling effect caused by the propagation can be alleviated by normalizing across the node representations. Thus, the dispersion of the expected representations can be mitigated. On the other hand, batch norm normalizes across all the nodes, so it requires sacrificing the nodes that benefit Measurement of effective homophily h l i h l i h l i . To estimate the effective homophily h l i , we measure the portion of neighbors that have the same ground truth label as v i and are correctly classified. Following our theory, we estimate h l i before the last propagation takes place and analyze its impact on the accuracy of the final layer. In more detail, we obtain the node representations before the last propagation from a trained vanilla <ref type="bibr">GCN (Kipf &amp; Welling, 2016)</ref> and then perform a linear transformation using the weight matrix and bias vector from the last layer. Then, we use the transformed representations to classify the neighbors of node v i and compute h l i . We note that we leverage the intermediate representations only for the estimation of h l i ; the accuracy of the final layer is still measured using the outputs from the final layer. Degree intervals. To investigate the change in GCN accuracy with different layers for nodes with different degrees, we categorize the nodes in n degree intervals. For the degree intervals, we use logarithmic binning (base 2). In detail, we denote the highest and lowest degree by d max and d min , respectively, and let ? ? log 2 dmax?log 2 dmin n . Then, we divide the nodes into n intervals, where the j-th interval is defined as: [d min ? 2 (j?1)? , d min ? 2 j? ).</p><p>Dataset: Citeseer. <ref type="figure">Figure 2 and</ref>  <ref type="table" target="#tab_5">Table B</ref>.3 show how the accuracy changes with the number of layers for different node degree intervals. We observe that in the initial stage, the accuracy increases as the degree and? h l i increase. However, in the developing stage, the accuracy of high-degree nodes drops more sharply than that of low-degree nodes.</p><p>Dataset: Cora. The results for Cora are shown in <ref type="figure">Figure B</ref>.1 and <ref type="table" target="#tab_5">Table B</ref>.4. In the initial stage, the nodes with lower <ref type="figure">Figure B</ref>.1: Cora: Accuracy per (logarithmic) degree bin. <ref type="table" target="#tab_5">Table B</ref>.4: Cora: Accuracy and average effective homophily (? h l i ) for nodes with different degrees across different layers. Last layer of initial stage marked in gray. degrees usually have lower accuracy. One exception is the nodes with degrees in the range <ref type="bibr">[3,</ref><ref type="bibr">7]</ref>. These nodes have higher accuracy because the average effective homophily? h l i of that degree group is the second highest. In the developing stage, the accuracy of the high-degree nodes drops more than the accuracy of the remaining node groups.</p><p>GCN's behavior on both Citesser and Cora datasets verifies our conjecture based on our theorems in ? 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Complexity analysis</head><p>We first analyze the time complexity of the forward path GGCN. For degree correction, we need to compute the r ij . However, we do not need to compute all of them, and we only need to compute r ij for node pairs who are linked. The time complexity is O(|E|), but it is a one-time computation, the results of which can be saved. We compute ? l ij based on the learned weights ? l 0 , ? l 1 . The time complexity is O(|E| ? L), where L is the number of layers. The time complexity to compute the signed function is O(|E| ? H ? L), where H is the hidden dimension of representations, and we only compute the cosine similarity between the nodes that are linked. We also need to compute the multiplication of the propagation matrix and representation matrices. The time complexity is O(|V| 2 ? H ? L), . Similarly, the time complexity of the multiplication of representation matrices and the weight matrices is O(|V| ? H 2 ? L). The total time complexity would be O(max(|V| 2 , |E|) ? H ? L). The complexity of GGCN resembles the complexity of attention-based model. Thus we compare with <ref type="bibr">GAT (Veli?kovi? et al., 2017)</ref> the total time to train and test on 4 larger homophilous and heterophilous datasets. We run the training and testing for 10 times. In general, GGCN runs faster than GAT because degree correction and signed messages in GGCN learn fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hyperparameters and Parameters</head><p>C.1. Hyperparameter settings Experiments for <ref type="table" target="#tab_0">Table 1 &amp; Table 2</ref> For the baselines, we set the same hyperparameters that are provided by the original papers or the authors' github repositories, and we match the results they reported in their respective papers. In our experiments, we find that the original hyperparameters set by the authors are already well-tuned.</p><p>All the models use Adam as the optimizer. GAT sets the initial learning rate as 0.005 and Geom-GCN uses a custom learning scheduler. All the other models (include GGCN) use the initial learning rate 0.01.</p><p>For GGCN, we use the following hyperparameters:</p><p>? k in the decaying aggregation: 3</p><p>We tune the parameters in the following ranges:</p><p>? The hyperparameters that are used in all the models (Base, +deg, +sign, +deg,sign, +BN, +LN) are set to be the same and they are tuned for every dataset. Those common hyperparameters are:</p><p>? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization</head><p>For GGCN, we adopt the following parameter initialization in the experiments for <ref type="table" target="#tab_0">Table 1 &amp; Table 2</ref> ? Initialization of ? l 0 and ? l 1 : 0.5 and 0, respectively ? Initialization of ? l , ? l 0 , ? l 1 and ? l 2 : 2, 0, 0, 0, respectively.</p><p>We initialize ? l {0,1,2} = 0 in Eq. (6) because, after applying softmax,? l {0,1,2} = 1/3; this ensures equal contributions from positive and negative neighbors and themselves (and sum=1). We initialize ? l 1 = 0 in Eq. (9) following the common practice for initializing the bias. We set ? l = 2 and ? l 0 = 0.5 in Eq. (6) and Eq. (9), because when r ij ? +?, the degree correction (including global scaling) is:? l ? l ij = softplus(2) ? softplus(0.5 ? (0 ? 1) + 0) ? 1. As we mention in Thm. 1, when the homophily level is high, nodes with large r i may benefit (case 3), thus we do not want to compensate for these nodes and would like to keep? l ? l ij close to 1.  Corrected r ij are given by? l ? l ij rij, which are a combination of global scaling and local degree correction. Because ? l {0,1,2} sum to 1 and do not change global scaling, they are not considered in the corrected r ij . As can be seen in <ref type="figure" target="#fig_0">Fig. C.1</ref>, after the training, GGCN learns to increase r ij , which satisfies our theorems. D. <ref type="table" target="#tab_1">Table 2 with larger fonts  Table D</ref>.1: Model performance for different layers: mean accuracy ? stdev over different data splits. Per dataset and GNN model, we also report the layer at which the best performance (given in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Parameters after training</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Node representation dynamics during neighborhood aggregation (in 1D for illustration purposes; 'MR': misclassification rate). The expectation of node representations from class 1 &amp; 2 are denoted by ? and ??, respectively. The bars show the expected node representations of node vi before and after the aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Graph Convolutional Neural Networks. Early on,(Defferrard et al., 2016)  proposed a GCN model that combines spectral filtering of graph signals and non-linearity for supervised node classification. The scalability and numerical stability of GCNs was later improved with a localized first-order approximation of spectral graph convolutions proposed in(Kipf &amp; Welling, 2016).(Veli?kovi? et al.,  2017)  proposes the first graph attention network to improve neighborhood aggregation. Many more GCN variants have been proposed for different applications such as: computer vision (Satorras &amp; Estrach, 2018), social science (Li &amp; Goldwasser, 2019), biology (Yan et al., 2019), algorithmic tasks(Veli?kovi? et al., 2020; Yan et al., 2020), and inductive classification(Hamilton et al., 2017). Concurrent work<ref type="bibr" target="#b1">(Baranwal et al., 2021)</ref> provides theoretical analysis on the linear separability of graph convolution but does not provide effective strategies to increase the separability.Oversmoothing. The oversmoothing problem was first discussed in(Li et al., 2018), which proved that by repeatedly applying Laplacian smoothing, the representations of nodes within each connected component of the graph converge to the same value. Since then, various empirical solutions have been proposed: residual connections and dilated convolutions(Li et al., 2019); skip links(Xu et al., 2018); new normalization strategies (Zhao &amp; Akoglu, 2019); edge dropout(Rong et al., 2019); and a new model that even increases performance as more layers are stacked(Chen et al.,  2020b). Some recent works provide theoretical analyses:(Oono &amp; Suzuki, 2019)  showed that a k-layer renormalized graph convolution with a residual link simulates a lazy random walk and(Chen et al., 2020b)  proved that the convergence rate is related to the spectral gap of the graph.Heterophily &amp; GCNs. Heterophily has recently been recognized as an important issue for GCNs. It is first outlined in the context of GCNs in(Pei et al., 2019).(Zhu et al.,  2020)  identified a set of effective designs that allow GCNs to generalize to challenging heterophilous settings, and(Zhu  et al., 2021)  introduced a new GCN model that leverages ideas from belief propagation(Gatterbauer et al., 2015)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure C. 1 :</head><label>1</label><figDesc>Original rij (avg. rij) and corrected (pre-and posttraining).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure C. 1</head><label>1</label><figDesc>shows the original and corrected r ij (avg r ij ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Real data: mean accuracy ? stdev over different data splits. Per GNN model, we report the best performance across different layers. Best model per benchmark highlighted in gray. The " ? " results (GraphSAGE) are obtained from(Zhu et al., 2020).84.86?4.55 86.86?3.29 37.54?1.56 55.17?1.58 71.14?1.84 85.68?6.63 77.14?1.45 89.15?0.37 87.95?1.05 1.78 GPRGNN 78.38?4.36 82.94?4.21 34.63?1.22 31.61?1.24 46.58?1.71 80.27?8.11 77.13?1.67 87.54?0.38 87.95?1.18 5.56 H2GCN* 84.86?7.23 87.65?4.98 35.70?1.00 36.48?1.86 60.11?2.15 82.70?5.28 77.11?1.57 89.49?0.38 87.87?1.20 3.89 GCNII* 77.57?3.83 80.39?3.4 37.44?1.30 38.47?1.58 63.86?3.04 77.86?3.79 77.33?1.48 90.15?0.43 88.37?1.25 3.56 Geom-GCN* 66.76?2.72 64.51?3.66 31.59?1.15 38.15?0.92 60.00?2.81 60.54?3.67 78.02?1.15 89.95?0.47 85.35?1.57 6.11 PairNorm 60.27?4.34 48.43?6.14 27.40?1.24 50.44?2.04 62.74?2.82 58.92?3.15 73.59?1.47 87.53?0.44 85.79?1.01 7.78 GraphSAGE ? 82.43?6.14 81.18?5.56 34.23?0.99 41.61?0.74 58.73?1.68 75.95?5.01 76.04?1.30 88.45?0.50 86.90?1.04 5.78 GCN 55.14?5.16 51.76?3.06 27.32?1.10 53.43?2.01 64.82?2.24 60.54?5.3 76.50?1.36 88.42?0.5 86.98?1.27 6.56 GAT 52.16?6.63 49.41?4.09 27.44?0.89 40.72?1.55 60.26?2.5 61.89?5.05 76.55?1.23 86.33?0.48 87.30?1.10 7.22 MLP 80.81?4.75 85.29?3.31 36.53?0.70 28.77?1.56 46.21?2.99 81.89?6.40 74.02?1.90 87.16?0.37 75.69?2.00 6.78</figDesc><table><row><cell></cell><cell>Texas</cell><cell>Wisconsin</cell><cell>Actor</cell><cell cols="3">Squirrel Chameleon Cornell</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Cora</cell><cell></cell></row><row><cell>Hom. level h #Nodes #Edges #Classes</cell><cell>0.11 183 295 5</cell><cell>0.21 251 466 5</cell><cell>0.22 7,600 26,752 5</cell><cell>0.22 5,201 198,493 5</cell><cell>0.23 2,277 31,421 5</cell><cell>0.3 183 280 5</cell><cell>0.74 3,327 4,676 7</cell><cell>0.8 19,717 44,327 3</cell><cell>0.81 2,708 5,278 6</cell><cell>Avg Rank</cell></row><row><cell>GGCN (ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Model performance for different layers: mean accuracy ? stdev over different data splits. Per dataset and GNN model, we also report the layer at which the best performance (given inTable 1) is achieved. 'OOM': out of memory; 'INS': numerical instability. For larger font, refer to Table D.1 in the Appendix. 00?1.15 87.48?1.32 87.63?1.33 87.51?1.19 87.95?1.05 87.28?1.41 32 76.83?1.82 76.77?1.48 76.91?1.56 76.88?1.56 76.97?1.52 76.65?1.38 10 GPRGNN 87.93?1.11 87.95?1.18 87.87?1.41 87.26?1.51 87.18?1.29 87.32?1.21 4 77.13?1.67 77.05?1.43 77.09?1.62 76.00?1.64 74.97?1.47 74.41?1.35?1.56 85.35?1.48 86.38?0.98 87.12?1.11 87.95?1.23 88.37?1.25 64 75.42?1.78 75.29?1.90 76.00?1.66 76.96?1.38 77.33?1.48 77.18?1.47 32 PairNorm 85.79?1.01 85.07?0.91 84.65?1.09 82.21?2.84 60.32?8.28 44.39?5.60 2 73.59?1.47 72.62?1.97 72.32?1.58 59.71?15.97 27.21?10.95 23.82?6.64 2 Geom-GCN* 85.35?1.57 21.01?2.61 13.98?1.48 13.98?1.48 13.98?1.48 13.98?1.48 2 78.02?1.15 23.01?1.95 7.23?0.87 7.23?0.87 7.23?0.87 7.23?0.87 2 GCN 86.98?1.27 83.24?1.56 31.03?3.08 31.05?2.36 30.76?3.43 31.89?2.08 2 76.50?1.36 64.33?8.27 24.18?1.71 23.07?2.95 25.3?1.77 24.73?1.66 2 GAT 87.30?1.10 86.50?1.20 84.97?1.24 78?6.73 83.78?6.16 84.86?5.69 83.78?6.73 83.78?6.51 84.32?5.90 6 70.77?1.42 69.58?2.68 70.33?1.70 70.44?1.82 70.29?1.62 70.20?1.95 5 GPRGNN 76.76?8.22 77.57?7.46 80.27?8.11 78.38?6.04 74.59?7.66 70.00?5.73 8 46.58?1.771 45.72?3.45 41.16?5.79 39.58?7.85 35.42?8.52 36.38?2.40 2 H2GCN* 81.89?5.98 82.70?6.27 80.27?6.63 57?11.34 64.59?9.63 73.24?5.91 77.84?3.97 75.41?5.47 73.78?4.37 16 61.07?4.10 63.86?3.04 62.89?1.18 60.20?2.10 56.97?1.81 55.99?2.27 4 PairNorm 50.27?7.17 53.51?8.00 58.38?5.01 58.38?3.01 58.92?3.15 58.92?3.15 32 62.74?2.82 59.01?2.80 54.12?2.24 46.38?2.23 46.78?2.26 46.27?3.24 2 Geom-GCN* 60.54?3.67 23.78?11.64 12.97?2.91 12.97?2.91 12.97?2.91 12.97?2.91 2 60.00?2.81 19.17?1.66 19.58?1.73 19.58?1.73 19.58?1.73 19.58?1.73 2 GCN 60.54?5.30 59.19?3.30 58.92?3.15 58.92?3.15 58.92?3.15 58.92?3.15 2 64.82?2.24 53.11?4.44 35.15?3.14 35.39?3.23 35.20?3.25 35.50?3.08 2 GAT 61.89?5.05 58.38?4.05 58.38?3.86</figDesc><table><row><cell>Layers</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>Best</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>Best</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Cora (h=0.81)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Citeseer (h=0.74)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GGCN (ours)</cell><cell cols="14">87.65 2</cell></row><row><cell>H2GCN*</cell><cell cols="3">87.87?1.20 86.10?1.51 86.18?2.10</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>2</cell><cell cols="3">76.90?1.80 76.09?1.54 74.10?1.83</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>1</cell></row><row><cell>GCNII*</cell><cell cols="4">85.INS</cell><cell>INS</cell><cell>INS</cell><cell>2</cell><cell cols="3">76.55?1.23 75.33?1.39 66.57?5.08</cell><cell>INS</cell><cell>INS</cell><cell>INS</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Cornell (h=0.3)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Chameleon (h=0.23)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GGCN (ours)</cell><cell cols="4">83.OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>1</cell><cell cols="2">59.06?1.85 60.11?2.15</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>4</cell></row><row><cell>GCNII*</cell><cell cols="4">67.INS</cell><cell>INS</cell><cell>INS</cell><cell>2</cell><cell cols="3">60.26?2.50 48.71?2.96 35.09?3.55</cell><cell>INS</cell><cell>INS</cell><cell>INS</cell><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The percentage (%) of nodes in each case (Fig. 1and Thm. 3.1): case 1: hi ? ? 1+? , case 2: hi &gt; ? 1+? &amp; ri ? 1 (1+?)h i ?? and case 3: otherwise. The dominant case for each dataset is colored in grey.</figDesc><table><row><cell></cell><cell cols="9">Texas Wisconsin Actor Squirrel Chameleon Cornell Citeseer Pubmed Cora</cell></row><row><cell cols="2">Hom. level h 0.11</cell><cell>0.21</cell><cell>0.22</cell><cell>0.22</cell><cell>0.23</cell><cell>0.3</cell><cell>0.74</cell><cell>0.8</cell><cell>0.81</cell></row><row><cell>Case 1</cell><cell>87.43</cell><cell>78.49</cell><cell cols="2">63.49 47.74</cell><cell>48.48</cell><cell>79.23</cell><cell>18.33</cell><cell>14.80</cell><cell>6.50</cell></row><row><cell>Case 2</cell><cell>8.74</cell><cell>15.93</cell><cell>29.53</cell><cell>50.14</cell><cell>48.84</cell><cell>6.01</cell><cell>25.58</cell><cell cols="2">33.75 32.98</cell></row><row><cell>Case 3</cell><cell>3.83</cell><cell>5.58</cell><cell>6.99</cell><cell>2.11</cell><cell>2.68</cell><cell>14.75</cell><cell>56.09</cell><cell cols="2">51.45 60.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Li, C. and Goldwasser, D. Encoding social information with graph convolutional networks forpolitical perspective detection in news media.In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2594-2604, 2019. Rozemberczki, B., Allen, C., and Sarkar, R. Multiscale attributed node embedding. arXiv preprint arXiv:1909.13021, 2019. Satorras, V. G. and Estrach, J. B. Few-shot learning with graph neural networks. International Conference on Learning Representations, 2018.Yan, Y., Swersky, K., Koutra, D., Ranganathan, P., and Hashemi, M. Neural execution engines: Learning to execute subroutines. Advances in Neural Information Processing Systems, 33, 2020.</figDesc><table><row><cell></cell><cell></cell><cell>Zhao, L. and Akoglu, L. Pairnorm: Tackling oversmoothing</cell></row><row><cell></cell><cell></cell><cell>Li, G., Muller, M., Thabet, A., and Ghanem, B. Deepgcns: in gnns. International Conference on Learning Represen-</cell></row><row><cell></cell><cell></cell><cell>Can gcns go as deep as cnns? In Proceedings of the tations, 2019.</cell></row><row><cell cols="2">Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., and Eliassi-Rad, T. Collective classification in network data. AI magazine, 29(3):93-93, 2008. Shchur, O., Mumme, M., Bojchevski, A., and G?nnemann,</cell><cell>IEEE/CVF International Conference on Computer Vision, pp. 9267-9276, 2019. Zhu, J., Yan, Y., Zhao, L., Heimann, M., Akoglu, L., and Koutra, D. Beyond homophily in graph neural networks: Current limitations and effective designs. Advances in Li, Q., Han, Z., and Wu, X.-M. Deeper insights into graph convolutional networks for semi-supervised learning. In Neural Information Processing Systems, 33, 2020.</cell></row><row><cell cols="2">S. Pitfalls of graph neural network evaluation. Relational Representation Learning Workshop, NeurIPS 2018, 2018.</cell><cell>Proceedings of the AAAI Conference on Artificial Intelli-gence, volume 32, 2018. Zhu, J., Rossi, R. A., Rao, A., Mai, T., Lipka, N., Ahmed, N. K., and Koutra, D. Graph neural networks with het-</cell></row><row><cell cols="2">Bojchevski, A. and G?nnemann, S. Deep gaussian em-bedding of graphs: Unsupervised inductive learning via tations, 2019. tations (ICLR), 2018. fusion. International Conference on Learning Represen-ranking. International Conference on Learning Represen-Shi, Z., Swersky, K., Tarlow, D., Ranganathan, P., and Hashemi, M. Learning execution through neural code</cell><cell>simple methods. Advances in Neural Information Pro-non-homophilous graphs: New benchmarks and strong Lim, D., Hohne, F., Li, X., Huang, S. L., Gupta, V., Bhalerao, O., and Lim, S. N. Large scale learning on erophily. In AAAI Conference on Artificial Intelligence, 2021.</cell></row><row><cell></cell><cell></cell><cell>cessing Systems, 34, 2021.</cell></row><row><cell cols="2">Tang, J., Sun, J., Wang, C., and Yang, Z. Social influence</cell><cell></cell></row><row><cell cols="2">analysis in large-scale networks. In Proceedings of the</cell><cell>Namata, G., London, B., Getoor, L., Huang, B., and EDU, U.</cell></row><row><cell cols="2">15th ACM SIGKDD international conference on Knowl-</cell><cell>Query-driven active surveying for collective classification.</cell></row><row><cell cols="2">edge discovery and data mining, pp. 807-816, 2009.</cell><cell>In 10th International Workshop on Mining and Learning</cell></row><row><cell></cell><cell></cell><cell>with Graphs, volume 8, 2012.</cell></row><row><cell cols="2">Veli?kovi?, P., Cucurull, G., Casanova, A., Romero, A.,</cell><cell></cell></row><row><cell cols="2">Lio, P., and Bengio, Y. Graph attention networks. arXiv</cell><cell>Newman, M. E. Assortative mixing in networks. Physical</cell></row><row><cell>preprint arXiv:1710.10903, 2017.</cell><cell>1725-1735.</cell><cell>review letters, 89(20):208701, 2002.</cell></row><row><cell cols="2">PMLR, 2020b. Veli?kovi?, P., Buesing, L., Overlan, M., Pascanu, R.,</cell><cell>Oono, K. and Suzuki, T. Graph neural networks expo-</cell></row><row><cell cols="2">Chien, E., Peng, J., Li, P., and Milenkovic, O. Adaptive uni-Vinyals, O., and Blundell, C. Pointer graph networks.</cell><cell>nentially lose expressive power for node classification.</cell></row><row><cell cols="2">versal generalized pagerank graph neural network, 2021. Advances in Neural Information Processing Systems, 33,</cell><cell>International Conference on Learning Representations,</cell></row><row><cell cols="2">Defferrard, M., Bresson, X., and Vandergheynst, P. Con-2020.</cell><cell>2019.</cell></row><row><cell cols="2">volutional neural networks on graphs with fast localized Wang, G., Ying, R., Huang, J., and Leskovec, J. Improv-</cell><cell>Pandit, S., Chau, D. H., Wang, S., and Faloutsos, C. Net-</cell></row><row><cell cols="2">spectral filtering. In Proceedings of the 30th International ing graph attention networks with large margin-based</cell><cell>probe: a fast and scalable system for fraud detection in</cell></row><row><cell cols="2">Conference on Neural Information Processing Systems, constraints. arXiv preprint arXiv:1910.11945, 2019.</cell><cell>online auction networks. In Proceedings of the 16th in-</cell></row><row><cell>pp. 3844-3852, 2016.</cell><cell></cell><cell>ternational conference on World Wide Web, pp. 201-210,</cell></row><row><cell cols="2">Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Wein-Gatterbauer, W., G?nnemann, S., Koutra, D., and Faloutsos, berger, K. Simplifying graph convolutional networks. In C. Linearized and single-pass belief propagation. Proc. International conference on machine learning, pp. 6861-VLDB Endow., 8(5):581-592, January 2015. 6871. PMLR, 2019.</cell><cell>2007. Pei, H., Wei, B., Chang, K. C.-C., Lei, Y., and Yang, B. Geom-gcn: Geometric graph convolutional networks.</cell></row><row><cell cols="2">Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K.-Dahl, G. E. Neural message passing for quantum chem-i., and Jegelka, S. Representation learning on graphs istry. In International conference on machine learning, with jumping knowledge networks. In International Con-pp. 1263-1272. PMLR, 2017. ference on Machine Learning, pp. 5453-5462. PMLR,</cell><cell>International Conference on Learning Representations, 2019. Rong, Y., Huang, W., Xu, T., and Huang, J. Dropedge: Towards deep graph convolutional networks on node clas-</cell></row><row><cell cols="2">Hamilton, W. L., Ying, R., and Leskovec, J. Inductive 2018.</cell><cell>sification. International Conference on Learning Repre-</cell></row><row><cell cols="2">representation learning on large graphs. In Proceedings of Yan, Y., Zhu, J., Duda, M., Solarz, E., Sripada, C., and the 31st International Conference on Neural Information Koutra, D. Groupinn: Grouping-based interpretable neu-Processing Systems, pp. 1025-1035, 2017. ral network for classification of limited, noisy brain data.</cell><cell>sentations, 2019.</cell></row><row><cell cols="2">Kipf, T. N. and Welling, M. Semi-supervised classifica-In Proceedings of the 25th ACM SIGKDD International</cell><cell></cell></row><row><cell cols="2">tion with graph convolutional networks. arXiv preprint Conference on Knowledge Discovery &amp; Data Mining, pp.</cell><cell></cell></row><row><cell>arXiv:1609.02907, 2016. 772-782, 2019.</cell><cell></cell><cell></cell></row></table><note>Chen, D., Lin, Y., Li, W., Li, P., Zhou, J., and Sun, X. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. In Proceedings of the AAAI Conference on Artificial Intelli- gence, volume 34, pp. 3438-3445, 2020a. Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y. Sim- ple and deep graph convolutional networks. In Interna- tional Conference on Machine Learning, pp.Rossi, R. A., Jin, D., Kim, S., Ahmed, N. K., Koutra, D., and Lee, J. B. On proximity and structural role-based embeddings in networks: Misconceptions, techniques, and applications. ACM Trans. Knowl. Discov. Data, 14 (5), August 2020.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table B.1: Ablation study: degree correction has consistent benefits (robust to oversmoothing &amp; ability to handle heterophily) in different datasets while signed information has more benefits in heterophilous datasets. Best performance of each model is highlighted in gray.</figDesc><table><row><cell>Layers</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table B .</head><label>B</label><figDesc>2: Effects of using batch norm &amp; layer norm: decrease in accuracy but improvement in oversmoothing. Best performance of each model across different layers is highlighted in gray.</figDesc><table><row><cell>Layers</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table B</head><label>B</label><figDesc>) for nodes with different degrees across various layers. Last layer of initial stage marked in gray.</figDesc><table><row><cell cols="3">.3: Citeseer: Accuracy (Acc) and average effective ho-</cell></row><row><cell cols="2">mophily (? h l i Degrees</cell><cell></cell></row><row><cell>Layers</cell><cell cols="2">[1, 2] [3, 6] [7, 15] [16, 39] [40, 99]</cell></row><row><cell>2</cell><cell cols="2">Acc 74.44 78.51 84.04 96.00 100.00 h l i 0.65 0.67 0.72 0.83 0.91</cell></row><row><cell>3</cell><cell cols="2">Acc 70.92 77.59 83.03 92.75 100.00 h l i 0.63 0.66 0.71 0.84 0.92</cell></row><row><cell>4</cell><cell cols="2">Acc 60.54 68.56 77.63 94.00 100.00 h l i 0.54 0.58 0.66 0.79 0.84</cell></row><row><cell>5</cell><cell>Acc 41.66 48.70 56.97 61.33 h l i 0.36 0.38 0.45 0.48</cell><cell>11.11 0.11</cell></row><row><cell>6</cell><cell>Acc 23.61 28.87 41.17 31.83 h l i 0.18 0.19 0.27 0.22</cell><cell>0.00 0.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table B .</head><label>B</label><figDesc>5: Training and test time (m:minutes, s:seconds) for 10 runs. Shorter time is colored in grey.</figDesc><table><row><cell></cell><cell cols="4">Actor Chameleon Citeseer Cora</cell></row><row><cell cols="2">#Nodes 7,600</cell><cell>2,277</cell><cell>3,327</cell><cell>2,708</cell></row><row><cell cols="2">#Edges 26,752</cell><cell>31,421</cell><cell>4,676</cell><cell>5,278</cell></row><row><cell cols="2">GGCN 7m24s</cell><cell>2m29s</cell><cell cols="2">2m9s 5m47s</cell></row><row><cell>GAT</cell><cell>8m14s</cell><cell>7m34s</cell><cell cols="2">9m39s 4m11s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Experiments forTable B.1 and Table B.2</figDesc><table><row><cell>Dropout rate: [0.0, 0.7]</cell></row><row><cell>? Weight decay: [1e-7, 1e-2]</cell></row><row><cell>? Hidden units: {8, 16, 32, 64, 80}</cell></row><row><cell>? Decay rate ?: [0.0, 1.5]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 1</head><label>1</label><figDesc>) is achieved. 'OOM': out of memory; 'INS': numerical instability. 00?1.15 87.48?1.32 87.63?1.33 87.51?1.19 87.95?1.05 87.28?1.41 32 76.83?1.82 76.77?1.48 76.91?1.56 76.88?1.56 76.97?1.52 76.65?1.38 10 GPRGNN 87.93?1.11 87.95?1.18 87.87?1.41 87.26?1.51 87.18?1.29 87.32?1.21 4 77.13?1.67 77.05?1.43 77.09?1.62 76.00?1.64 74.97?1.47 74.41?1.65</figDesc><table><row><cell>Best</cell><cell></cell></row><row><cell>64</cell><cell></cell></row><row><cell>32</cell><cell></cell></row><row><cell>16</cell><cell>(h=0.74)</cell></row><row><cell></cell><cell>Citeseer</cell></row><row><cell>8</cell><cell></cell></row><row><cell>4</cell><cell></cell></row><row><cell>2</cell><cell></cell></row><row><cell>Best</cell><cell></cell></row><row><cell>64</cell><cell></cell></row><row><cell>32</cell><cell></cell></row><row><cell>16</cell><cell>(h=0.81)</cell></row><row><cell></cell><cell>Cora</cell></row><row><cell>8</cell><cell></cell></row><row><cell>4</cell><cell></cell></row><row><cell>2</cell><cell>87.</cell></row><row><cell>Layers</cell><cell>GGCN (ours)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1"> (Pei et al., 2019)  claims that the ratios are 60%/20%/20%, which is different from the actual data splits shared on GitHub.2 https://github.com/Diego999/pyGAT</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In ? 3, we assume that {d j } follow the same distribution and their joint distribution function is permutation-invariant, and {y j } and {f l j } also have this property. Thus, conditioned on d i , y i and N i , the distribution of</p><p>is the same for ?j ? N i (neighbors are indistinguishable) and we obtain:</p><p>Given that the joint distribution of {d j }, the joint distribution of {y j } and the joint distribution of {f l j } are permutation-</p><p>We note that v j in Equation 34 can be any node except v i due to the equivalence of those nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.2. THEOREM</head><p>Theorem Moving representations {f (L) i } from class 1 by adding ?tw * T , t &gt; 0, s.t. w * T w * &gt; 0, the new total misclassification rate is no less than the misclassification rate before the movements.</p><p>This Theorem studies the movements that bring the node representations closer to the original decision boundary; we will prove that moving towards the original decision boundary by a non-zero step is not beneficial (i.e., harmful) to the SGC's performance.</p><p>Proof. We prove it by contradiction and suppose that after the movements, the total misclassification rate is lowered. We denote the conditional distribution of {f <ref type="bibr">(L)</ref> i } as f L 1 (x) conditioned on that they are from class 1 and f L 2 (x) conditioned on that they are from class 1 or class 2, respectively. To note that, the distributions of different layers are different.</p><p>After the movements, the representations from class 1 become: {f (L) i ? tw * T }, t &gt; 0. tw * T represents moving towards the original decision boundary along the norm direction by a non-zero step. This causes a corresponding change in the conditional PDF given that they are from class 1: f L 1 (x + tw * T ). Recall that in ? 3, the parameters for the original optimal hyperplane w * and b * , and the parameters for the later optimal hyperplane w * and b * are normalized such that when xw * + b * &gt; 0 and xw * + b * &gt; 0, we predict class 1.</p><p>The new misclassification rate conditioned on class 1 is:</p><p>The new misclassification rate conditioned on class 2 is:</p><p>The new total misclassification rate is:</p><p>Next, we will prove that if the total misclassification is lowered, xw * + b * = 0 is not the optimal hyper-plane before the movements which should achieve the lowest total misclassification rate.</p><p>Consider a hyper-plane xw * +b * ? tw * T w * 2 = 0. Given that w * T w * &gt; 0 and t &gt; 0, for ?x,</p><p>Because f L 1 (x) is a PDF which is a nonnegative function,</p><p>Similarly, we can obtain</p><p>Thus,</p><p>Define</p><p>The quantity M f represents the total misclassification rate before the movements if the decision boundary is xw * + b * ? tw * T w * 2 = 0. Given Eq. 55, 57 and 59, we have:</p><p>Let M denotes the total classification rate before the movements. Based on the assumption that the total misclassification rate is lowered after the movements (M &gt; M ), we have:</p><p>Eq. 62 indicates that we find a hyper-plane xw * + b * ? tw * T w * 2 = 0, which yields smaller total misclassification rate than xw * + b * = 0 before the movements. This contradicts to the fact that xw * + b * = 0 is the optimal hyper-plane before the movements.</p><p>Note that the special case where the representations of the two classes swap positions (e.g, bipartite graphs) violates the condition w * T w * &gt; 0 and leads to different conclusions. We refer to the condition w * T w * &gt; 0 as "non-swapping condition" and throughout the paper, we analyze SGC under that. The theorem shows that, under the "non-swapping condition", if {f (l) i } move towards the original decision boundary (or the other class), SGC tends to perform worse.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to represent programs with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khademi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Graph convolution for semi-supervised classification: Improved linear separability and out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baranwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fountoulakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jagannath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06966</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cora</forename></persName>
		</author>
		<idno>h=0.81) Citeseer (h=0.74) Base 86.56?1.21 86.04?0.72 85.51?1.51 85.33?0.72 85.37?1.58 72.17?8.89 76.51?1.63 75.03?1.67 73.96?1.52 73.59?1.51 71.91?1.94 32.08?15.74 +deg 86.72?1.29 86.02?0.97 85.49?1.32 85.27?1.59 85.27?1.51 84.21?1.22 76.63?1.38 74.64?1.97 74.15?1.61 73.73?1.31 73.61?1.84 70.56?2.27 +sign 84.81?1.63 86.06?1.7 85.67?1.26 85.39?0.97 84.85?0.98 78.57?6.73 77.13?1.69 74.56?2.02 73.64?1.65 72.31?2.32 71.98?3.44 68.68?6.72</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornell</forename></persName>
		</author>
		<idno>h=0.3) Chameleon (h=0.23) Base 61.89?3.72 60.00?5.24 58.92?5.24 56.49?5.73 58.92?3.15 49.19?16.70 64.98?1.84 62.65?3.09 62.43?3.28 54.69?2.58 47.68?2.63 29.74?5.21 +deg 63.78?5.57 62.70?5.90 59.46?4.52 56.49?5.73 57.57?4.20 58.92?3.15 66.54?2.19 68.31?2.70 68.99?2.38 67.68?3.70 56.86?8.80 41.95?9.56 +sign 85.41?7.27 76.76?7.07 70.00?5.19 67.57?9.44 63.24?6.07 63.24?6.53 65.31?3.20 53.55?6.35 53.05?2.28 51.93?4.00 57.17?3.39 51.93?8.95</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cora</forename></persName>
		</author>
		<idno>h=0.81) Citeseer (h=0.74) Base 86.56?1.21 86.04?0.72 85.51?1.51 85.33?0.72 85.37?1.58 72.17?8.89 76.51?1.63 75.03?1.67 73.96?1.52 73.59?1.51 71.91?1.94 32.08?15.74 +BN 84.73?1.10 83.76?1.61 83.94?1.51 84.57?1.22 84.63?1.58 85.17?1.18 71.62?1.48 71.58?1.00 72.18?1.39 72.45?1.42 72.76?1.31 72.61?1.41 +LN 84.73?1.63 86.60?1.01 86.72?1.36 86.08?1.16 85.67?1.23 85.13?1.20 76.11?1.80 74.02?2.77 75.00?1.95 74.50?0.96 74.49?2.10 73.94?2.03</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornell</forename></persName>
		</author>
		<idno>h=0.3) Chameleon (h=0.23) Base 61.89?3.72 60.00?5.24 58.92?5.24 56.49?5.73 58.92?3.15 49.19?16.70 64.98?1.84 62.65?3.09 62.43?3.28 54.69?2.58 47.68?2.63 29.74?5.21 +BN 58.38?6.42 59.19?4.59 55.41?6.65 57.30?3.15 57.57?6.29 57.02?6.19 60.88?2.24 61.38?2.17 61.84?4.08 61.97?3.01 59.04?3.79 57.84?3.67 +LN 58.11?6.19 55.68?6.19 58.92?7.63 59.19?3.07 58.92?3.15 58.00?3.03 61.86?1.73 62.17?2.48 62.41?2.99 60.37?2.36 58.25?3.03 58.92?3.15</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">) to compensate for the nodes that are prone to moving towards the other classes (case 1 &amp; case 2). As a result, batch norm is less effective in mitigating oversmoothing and leads to a bigger decrease in accuracy</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Another finding is that both layer norm and batch norm lead to a significant accuracy decrease (2%-3%) in the heterophilous datasets. +BN has a clear accuracy drop even in the homophilous datasets. As Thm. 3.1 points out: higher heterophily level may result in sign flip. If the representations flip the sign, using batch norm or layer norm will not revert the sign</title>
		<imprint/>
	</monogr>
	<note>but they may instead encourage the representations to move towards the other class more</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Given the limitations shown above, we do not use either batch norm or layer norm in our proposed model, GGCN. B.3. More on the Initial &amp; Developing Stages Section 5.5 shows how the node classification accuracy changes for nodes of different degrees with the number of layers on Citeseer. Here, we provide more details of this experiment and give the results on another dataset</title>
		<imprint>
			<pubPlace>Cora</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">According to Thm. 3.1 and 3.2, in order to see both the initial and the developing stage, we need to use homophilous datasets. In heterophilous datasets, most nodes satisfy case 1, so the initial stage does not exist</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Datasets</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
