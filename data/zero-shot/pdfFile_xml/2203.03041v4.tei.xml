<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Highly Accurate Dichotomous Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><forename type="middle">Qin</forename><surname>Mbzuai</surname></persName>
							<email>xuebinua@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abu</forename><surname>Dhabi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uae</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><forename type="middle">Dai</forename><surname>Mbzuai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abu</forename><surname>Dhabi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uae</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Hu</surname></persName>
							<email>xiaobin.hu@tum.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Switzerland</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<email>ling.shao@ieee.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">TUM Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Terminus Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Luc Van Gool ETH</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Highly Accurate Dichotomous Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>4097x7952 2236x1677 Figure 1. Sample images (backgrounds partially removed by ground truth (GT) masks) from our DIS5K dataset. Zoom-in for best view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present a systematic study on a new task called dichotomous image segmentation (DIS), which aims to segment highly accurate objects from natural images. To this end, we collected the first large-scale dataset, called DIS5K, which contains 5,470 high-resolution (e.g., 2K, 4K or larger) images covering camouflaged, salient, or meticulous objects in various backgrounds. All images are annotated with extremely fine-grained labels. In addition, we introduce a simple intermediate supervision baseline (IS-Net) using both feature-level and mask-level guidance for DIS model training. Without tricks, IS-Net outperforms various cutting-edge baselines on the proposed DIS5K, making it a general self-learned supervision network that can help facilitate future research in DIS. Further, we design a new metric called human correction efforts (HCE) which approximates the number of mouse clicking operations required to correct the false positives and false negatives. HCE is utilized to measure the gap between models and real-world applications and thus can complement existing metrics. Finally, we conduct the largest-scale benchmark, evaluating 16 representative segmentation models, providing a more insightful discussion regarding object complexi-* Corresponding author.</p><p>ties, and showing several potential applications (e.g., background removal, art design, 3D reconstruction). Hoping these efforts can open up promising directions for both academic and industries. Our DIS5K dataset, IS-Net baseline, HCE metric, and the complete benchmarks will be made publicly available at: https://xuebinqin. github.io/dis/index.html.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Sample images (backgrounds partially removed by ground truth (GT) masks) from our DIS5K dataset. Zoom-in for best view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present a systematic study on a new task called dichotomous image segmentation (DIS), which aims to segment highly accurate objects from natural images. To this end, we collected the first large-scale dataset, called DIS5K, which contains 5,470 high-resolution (e.g., 2K, 4K or larger) images covering camouflaged, salient, or meticulous objects in various backgrounds. All images are annotated with extremely fine-grained labels. In addition, we introduce a simple intermediate supervision baseline (IS-Net) using both feature-level and mask-level guidance for DIS model training. Without tricks, IS-Net outperforms various cutting-edge baselines on the proposed DIS5K, making it a general self-learned supervision network that can help facilitate future research in DIS. Further, we design a new metric called human correction efforts (HCE) which approximates the number of mouse clicking operations required to correct the false positives and false negatives. HCE is utilized to measure the gap between models and real-world applications and thus can complement existing metrics. Finally, we conduct the largest-scale benchmark, evaluating 16 representative segmentation models, providing a more insightful discussion regarding object complexi-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In many years, the accuracy of annotations in computer vision datasets that drive a tremendous amount of Artificial Intelligence (AI) models satisfy the requirements of machine perceiving systems to some extent. However, AI has entered an era of demanding highly accurate outputs from computer vision algorithms to support delicate human-machine interaction and immersed virtual life. Image segmentation, as one of the most fundamental techniques in computer vision, plays a vital role in enabling the machines to perceive and understand the real world. Compared with image classification <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b83">84]</ref> and object detection <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b79">80]</ref>, it can provide more geometrically accurate descriptions of the targets used in a wide range of applications, such as image editing <ref type="bibr" target="#b35">[36]</ref>, 3D reconstruction <ref type="bibr" target="#b56">[57]</ref>, augmented reality (AR) <ref type="bibr" target="#b75">[76]</ref>, satellite image anal-ysis <ref type="bibr" target="#b99">[100]</ref>, medical image processing <ref type="bibr" target="#b80">[81]</ref>, robot manipulation <ref type="bibr" target="#b6">[8]</ref>, etc. We can categorize the above applications as "light" (e.g., image editing and image analysis) and "heavy" (e.g., manufacturing and surgical robots), based on their immediate affects on real-world objects. The "light" applications ( <ref type="figure" target="#fig_8">Fig.9</ref>) are relatively tolerant to the segmentation deflects and failures because these issues mainly lead to more labors and time costs, which are usually affordable. While, in the "heavy" applications, those deflects or failures are more likely to cause serious consequences, which are usually physic damages on objects or injuries, sometimes fatal for creatures, e.g., humans and animals. Hence, these applications require the models to be highly accurate and robust. Currently, most of the segmentation models are still less applicable in those "heavy" applications because of the accuracy and robustness issues, which restricts the segmentation techniques from playing more essential roles in broader applications. Here, our goal is to address the "heavy" and "light" applications in a general framework, we called this task as dichotomous image segmentation (DIS), which aims to segment highly accurate objects from the nature images.</p><p>However, existing image segmentation tasks mainly focus on segmenting objects with specific characteristics, e.g., salient <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b108">109]</ref>, camouflaged <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b84">85]</ref>, meticulous <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b104">105]</ref> or specific categories <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b82">83]</ref>. Most of them have the same input/output formats, and barely use exclusive mechanisms designed for segmenting targets in their models, which means almost all tasks are datasetdependent. Thus, we propose to formulate a categoryagnostic DIS task defined on non-conflicting annotations for accurately segmenting objects with different structure complexities, regardless of their characteristics. Compared with semantic segmentation <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b119">120</ref>], the proposed DIS task usually focuses on images with single or a few targets, from which getting richer accurate details of each target is more feasible. To this end, we provide four contributions:</p><p>1. A large-scale, extendable DIS dataset, DIS5K, contains 5,470 high-resolution images paired with highly accurate binary segmentation masks. 2. A novel baseline IS-Net built with our intermediate supervision reduces over-fitting by enforcing direct feature synchronization in high-dimensional feature spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A newly designed human correction efforts (HCE)</head><p>metric measures the barriers between model predictions and real-world applications by counting the human interventions needed to correct the faulty regions.</p><p>4. Based on the new DIS5K, we establish the complete DIS benchmark, making ours the most extensive DIS investigation. We compared our IS-Net with 16 cutting-edge segmentation models and showed promising results for background removal and 3D reconstruction applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Tasks and Datasets of image segmentation are closely related in deep learning era. Some of the segmentation tasks like <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b104">105]</ref>, are even directly built upon the datasets. Their problem formulations are exactly the same: P = F (?, I), where I and P are the input image and the binary map output, respectively. However, the relevance between most of these tasks are rarely studied, which somehow restricts the models trained for certain tasks from being generalized to wider applications. Besides, the datasets used in different tasks are not exclusive, which shows a unified task for dichotomous image segmentation (DIS) is possible. However, most of the existing datasets are built on low-resolution images with objects of simple structures. There lacks an dataset built on the accurately labeled high-resolution images which contain objects with diversified shape complexities from different categories.</p><p>Models are often struggling with the conflicts between stronger representative capabilities and higher risks of overfitting. To obtain more representative features, FCN-based models <ref type="bibr" target="#b59">[60]</ref>, Encoder-Decoder <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b80">81]</ref>, Coarse-to-Fine <ref type="bibr" target="#b95">[96]</ref>, Predict-Refine <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b89">90]</ref>, Vision Transformer <ref type="bibr" target="#b117">[118]</ref> and so on are developed. Besides, many real-time models are designed <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b113">114]</ref> to balance the performance and the time costs. Other methods, such as weights regularization <ref type="bibr" target="#b36">[37]</ref>, dropout <ref type="bibr" target="#b85">[86]</ref>, dense supervision <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b101">102]</ref>, and hybrid loss <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b115">116]</ref>, focus on alleviating the over-fitting. Dense supervision is one of the most effective ways for reducing the over-fitting. However, supervising the side outputs from the intermediate deep features may not be the best option because the supervision is weakened by the conversion from deep features (multichannel) to side outputs (one-channel).</p><p>Evaluation Metrics can be categorized as region-based (e.g., IoU or Jaccard index [1], F-measure <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b91">92]</ref> or Dice's coefficient <ref type="bibr" target="#b87">[88]</ref>, weighted F-measure <ref type="bibr" target="#b63">[64]</ref>), boundarybased (e.g., CM <ref type="bibr" target="#b68">[69]</ref>, boundary F-measure <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b112">113]</ref>, boundary IoU <ref type="bibr" target="#b9">[11]</ref>, boundary displacement error (BDE) <ref type="bibr" target="#b29">[31]</ref>, Hausdorff distances <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b38">39]</ref>), structure-based (e.g., S-measure <ref type="bibr" target="#b20">[22]</ref>, E-measure <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b23">25]</ref>), confidencebased (e.g., MAE <ref type="bibr" target="#b72">[73]</ref>), etc. They mainly measure the consistencies between the predictions and the ground truth from mathematical or cognitive perspectives. But the costs of synchronizing the predictions against the requirements in real-world applications are not well studied. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed DIS5K Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Collection and Annotation</head><p>Data Collection. To address the dataset issue (see ?2), we build a highly accurate DIS dataset named DIS5K. We first manually collected over 12,000 images from Flickr 1 based on our pre-designed keywords 2 . Then, according to the structural complexities of the objects, we obtained 5,470 images covering 225 categories ( <ref type="figure" target="#fig_0">Fig.2</ref>) in 22 groups. Note that the adopted selection strategy is similar to Zhou et al. <ref type="bibr" target="#b118">[119]</ref>. Most selected images only contain single objects to obtain rich and highly accurate structures and details. Meanwhile, the segmentation and labeling confusions caused by the co-occurrence of multiple objects from different categories are avoided to the greatest extent. Specifically, the image selection criteria can be summarized as follows:</p><p>? Cover more categories while reducing the number of "redundant" samples with simple structures, which other existing datasets have already covered.</p><p>? Enlarge the intra-category dissimilarities (See ?2.3 of the supplementary (SM)) of the selected categories by adding more diversified intra-category images ( <ref type="figure">Fig.3</ref>f).</p><p>1 Images with the license of "Commercial use &amp; mods allowed" 2 Since the long-term goal of this research is to facilitate the "safe" and "efficient" interaction between the machines and our living/working environments, these keywords are mainly related to the common targets (e.g., bicycle, chair, bag, cable, tree, etc.) in our daily lives.</p><p>? Include more categories with complicated structures, e.g., fence, stairs, cable, bonsai, tree, etc., which are common in our lives but not well-labeled ( <ref type="figure">Fig.3</ref>-a) or neglected by other datasets due to labeling difficulties ( <ref type="figure" target="#fig_2">Fig.4)</ref>.</p><p>Therefore, the labeled targets in our DIS5K are mainly the "foreground objects of the images defined by the predesigned keywords" regardless of their characteristics e.g., salient, common, camouflaged, meticulous, etc. Data Annotation. Each image of DIS5K is manually labeled with pixel-wise accuracy using GIMP <ref type="bibr" target="#b1">3</ref> . The average per-image labeling time is ?30 minutes and some images cost up to 10 hours. It is worth mentioning that some of our labeled ground truth (GT) masks are visually close to the image matting GT. The labeled targets, including transparent and translucent, are binary masks with one pixel's highest accuracy. Here, the DIS task is category-agnostic while our DIS5K is collected based on pre-designed keywords/categories, which seems contradictory. The reasons are threefold. (1) The keywords greatly facilitate the retrieval and organization of the large-scale dataset. (2) To achieve the goal of category-agnostic segmentation, diversified samples are needed. Collecting samples based on their categories is a reasonable way to guarantee the diversities' lower bound of the dataset. The diversities' upper bound of our DIS5K is determined by the diversified characteristics (e.g., textures, structures, shapes, contrasts, complexities, (3) There are no perfect datasets, so re-organizing or further extension of the existing datasets is usually necessary for different real-world applications. The category information will significantly facilitate tracing the collected and to-be-collected samples. Therefore, the category-based data collection is not contradictory but internally consistent with the goal of DIS task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Analysis</head><p>For deeper insights into DIS dataset, we compare our DIS5K against 19 other related datasets including: (1) nine salient object detection (SOD) datasets: SOD <ref type="bibr" target="#b68">[69]</ref>, PASCAL-S <ref type="bibr" target="#b51">[52]</ref>, ECSSD <ref type="bibr" target="#b103">[104]</ref>, HKU-IS <ref type="bibr" target="#b49">[50]</ref>, MSRA-B <ref type="bibr" target="#b58">[59]</ref>, DUT-OMRON <ref type="bibr" target="#b105">[106]</ref>, MSRA10K <ref type="bibr" target="#b12">[14]</ref>, DUTS <ref type="bibr" target="#b93">[94]</ref>, and SOC <ref type="bibr" target="#b19">[21]</ref>; (2) two high-resolution salient object detection (HR-SOD) datasets: HR-SOD <ref type="bibr" target="#b108">[109]</ref> and HR-DAVIS-S <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b108">109]</ref>; (3) four camouflaged object detection (COD) datasets: CAMO <ref type="bibr" target="#b47">[48]</ref>, CHAMELEON <ref type="bibr" target="#b84">[85]</ref>, COD10K <ref type="bibr" target="#b24">[26]</ref>, and NC4K <ref type="bibr" target="#b62">[63]</ref>; (4) two semantic segmentation (SMS) 4 datasets: R-PASCAL <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b18">20]</ref> and BIG <ref type="bibr" target="#b11">[13]</ref>; (5) two thin object segmentation (TOS) datasets: COIFT <ref type="bibr" target="#b53">[54]</ref> and Thi-nObject5K <ref type="bibr" target="#b53">[54]</ref>. The comparisons are conducted mainly from the following three perspectives: image number, image dimension, and object complexity as illustrated in Tab.1 Image Dimension is crucial to segmentation tasks. Because it has significant impacts on accuracy, efficiency, and computational costs. The mean (H, W , D) and their standard deviations (? H , ? W , ? D ) of the image height, width and diagonal length are provided in Tab.1. The BIG dataset has the largest average image dimensions, but it is a smallscale dataset that contains only 150 images. Although HR-SOD has slightly greater dimensions than ours, the dataset scale and complexity are less comparable. Compared with the SOD datasets, the average image dimensions of our DIS5K are almost eight times larger than theirs. The COD datasets have larger dimensions than SOD datasets, but they are still much smaller than ours. Besides, most of the targets in COD datasets are animals. Thus, it is difficult to generalize them to diversified tasks. Object Complexity is described by three metrics including the isoperimetric inequality quotient (IP Q) <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b104">105]</ref>, the number of object contours (C num ) and the number of dominant points P num . The IP Q mainly describes the overall structure complexity as IP Q = L 2 4?A , where L and A denote the object perimeter and the region area, respectively. It is designed to differentiate objects with elongated components and thin concave structures from closeto-convex objects. The C num is used to represent the topological complexity in contour level for observing the objects consisting of many (small) contours which usually have minor influences on the IP Q. To describe the object complexity at a finer level, we employ P num to count the number of the dominant points <ref type="bibr" target="#b78">[79]</ref> along the object boundaries. Therefore, the complexities of the small jagged segments along the boundaries, which usually cannot be accurately measured by IP Q and C num , can be well-evaluated with P num . Essentially, P num is the total number of the polygon corners needed for approximating the segmentation masks, which also directly reflects the human labeling costs. Thus, it is then adapted to our Human Correction Efforts (HCE) metric ( ?5) for evaluating the prediction quality. Discussion. The metrics above are all computed on the la-   beled GT masks and illustrated in Tab.1 and <ref type="figure" target="#fig_0">Fig.2</ref> (Left). It shows that DIS5K is around 20 (up to 50) times more complicated than the SOD datasets in terms of average structure complexity IP Q. Although other datasets such as CHAMELEON, COD10K, BIG, COIFT, and ThinOb-ject5K have higher average IP Q against the SOD datasets, their complexities are still much less than ours. The HR-SOD and HR-DAVIS-S datasets contain large-size images with accurately labeled boundaries. However, there are no significant differences between their IP Q and that of SOD datasets. Because IP Q is insensitive to the complexities of fine details as mentioned above. The average contourlevel complexities C num of different datasets are almost consistent with their IP Q. The average C num and its standard deviation of DIS5K are over 100 and 400, which are much higher than other datasets. This indicates the objects in DIS5K contain more detailed structures that are comprised of multiple contours. The average P num of DIS5K is over 1400, which is almost five and three times greater than those of HR-SOD and the synthetic ThinObject5K, respectively. There is an interesting observation that the P num of HR-SOD, HR-DAVIS-S, BIG, and ThinObject5K are not proportional to their IP Q and C num , but it shows positive correlations with their image dimensions. One of the reasons is that most of the objects in these datasets are close to convex and comprised of single or a few contours, which leads to low IP Q and C num . Nevertheless, their boundaries (e.g., small jagged segments) are accurately labeled in high-resolution images that significantly increase the P num . On the other hand, larger sizes of GT masks often directly lead to greater P num because the dominant points are searched by <ref type="bibr" target="#b78">[79]</ref>, which filters out redundant boundary points based on their deviation distances (epsilon) against the straight lines constructed by their neighboring dominant points. For example, given two objects with the same shape comprised of smooth boundaries but different sizes, more dominant points are generated from the larger one with the same threshold of epsilon. That means P num is determined by both the boundary complexity and the GT mask dimension. Therefore, these three complexity measurements are complementary to provide a comprehensive analysis of the object complexities.The large standard deviations in Tab.1 demonstrate the great diversities of DIS5K from different perspectives. <ref type="figure">Fig.3</ref>-a shows an observation tower from DUT-OMRON. Similar object (b) has also been included in our DIS5K, which has higher labeling accuracy and structural complexity. <ref type="figure">Fig.3</ref>-c shows a sample from COD10K where the relatively higher structure complexity than that of SOD datasets is partially caused by the labeled occlusions, which are not the structural complexity of the target itself. A sample, where a set of the barbell is floating in the sky, from the synthesized ThinObject5K dataset is shown in <ref type="figure">Fig.3-d</ref>. Synthesizing images is a common way for generating training sets in image matting <ref type="bibr" target="#b102">[103,</ref><ref type="bibr" target="#b107">108]</ref>, where training samples are difficult to be labeled. However, the synthesized images usually show different characteristics from the real ones, which leads to biases in both training and evaluation. <ref type="figure">Fig.3</ref>-e and <ref type="figure">Fig.3</ref>-f demonstrate the larger diversity of intra-categorical structure complexities of our DIS5K. In <ref type="figure" target="#fig_2">Fig.4</ref>, we provide the sample masks with their complexity scores in DIS5K. The bottom-left samples with large regional components have relatively low IP Q, and the top-right samples with more thin and complicated fine structures have much higher IP Q and P num .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dataset Splitting</head><p>We split 5,470 images in DIS5K into three subsets: DIS-TR (3,000), DIS-VD (470), and DIS-TE (2,000) for training, validation, and testing. The categories in DIS-TR and those in DIS-VD and DIS-TE are mainly consistent. Since our dataset's object shapes and structure complexities are diversified, the 2000 images of DIS-TE are further split into four subsets with ascending shape complexities for a more comprehensive evaluation. Specifically, we first rank the 2,000 testing images in ascending order according to the multiplication (IP Q ? P num ) of their structure complexities IP Q and boundary complexities P num . Then, DIS-TE is split into four subsets (DIS-TE1?DIS-TE4) with 500 images in each subset to represent four testing difficulty levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed IS-Net Baseline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overview</head><p>As shown in <ref type="figure" target="#fig_3">Fig.5</ref>, our IS-Net consists of a ground truth (GT) encoder, a image segmentation component, and a newly proposed intermediate supervision strategy. The GT encoder (27.7 MB) is designed to encode the GT masks into high-dimensional spaces and then used to enforce intermediate supervision on the segmentation component. While, the image segmentation component (176.6 MB) is expected to have the capability of capturing fine structures and handle large size e.g., 1024 ? 1024, inputs with affordable memory and time costs. In the following experiment, we choose U 2 -Net <ref type="bibr" target="#b76">[77]</ref> as the image segmentation component because of its strong capability in capturing fine structures. Note that other segmentation models, such as transformer backbone, are also compatible with our strategy. Technique Details. U 2 -Net was originally designed for small size (320 ? 320) SOD image. Because of its GPU memory costs, it cannot be used directly for handling large size (e.g., 1024 ? 1024) inputs. We adapt the architecture of U 2 -Net by adding an input convolution layer before its first encoder stage. The input convolution layer is set as a plain convolution layer with a kernel size of 3 ? 3 and stride of 2. Given an input image with a shape of I 1024?1024?3 , the input convolution layer first transforms it to a feature map f 512?512?64 and this feature map is then directly fed to the original U 2 -Net, where the input channel is changed to 64 correspondingly. Compared with directly feeding I 1024?1024?3 to U 2 -Net, the input convolution layer helps the whole network reduce three quarters of the overall GPU memory overhead while maintaining spatial information in feature channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Intermediate Supervision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DIS can be seen as a mapping in segmentation models</head><formula xml:id="formula_0">from image domain I ? R H?W ?3 to segmentation GT do- main G ? R H?W ?1 : G = F (?, I),</formula><p>where F indicates the model that uses learnable weights ? to map inputs from image to mask domain. Most of the models are easy to over-fit on the training set. Thus, the deep supervision has been proposed to supervise the intermediate outputs of a given deep network <ref type="bibr" target="#b48">[49]</ref>. In <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b101">102]</ref>, the dense supervisions are usually applied to the side outputs, which are singlechannel probability maps produced by convolving the last feature maps of particular deep layers. However, transforming high-dimensional features to single-channel probability maps is essentially a dimension reduction operation, inevitably losing critical cues.</p><p>To avoid this issue, we propose a novel intermediate supervision training strategy. Given an input image I H?W ?3 and its corresponding segmentation mask G W ?H?1 , we first train a self-supervised GT encoder to extract the highdimensional features using a lightweight deep model F gt , <ref type="figure" target="#fig_3">Fig.5</ref></p><formula xml:id="formula_1">-b, as: argmin ?gt D d=1 BCE(F gt (? gt , G) d , G),</formula><p>where ? gt indicates the model weights, BCE is the binary cross entropy loss and D denotes the number of the intermediate feature maps.</p><p>After obtaining the GT encoder F gt , its weights ? gt are frozen for generating the "ground truth" high-dimensional</p><formula xml:id="formula_2">intermediate deep features by: f G D = F ? gt (? gt , G), D = {1, 2, 3, 4, 5, 6}, where F ?</formula><p>gt represents the F gt without the last convolution layers for generating the probability maps. F ? gt is to supervise those corresponding features f I D from the segmentation model F sg . In the image segmentation component F sg ( <ref type="figure" target="#fig_3">Fig.5-a</ref>   </p><formula xml:id="formula_3">L sg = D d=1 ? sg d BCE(F sg (? sg , I), G), where ? sg d repre-</formula><p>sents the hyperparameter to weight each side output loss. <ref type="figure" target="#fig_4">Fig.6</ref> illustrates the feature maps from the stage 2 in <ref type="figure" target="#fig_3">Fig.5</ref>, EN 2, of the GT encoder. We can see the diversified characteristics of the input mask are encoded into different channels. For example, the 21 st channel encodes both the fine and large structures close to the original mask. While the 23 rd , 29 th , and 37 th channels encode the middle size structures (frame, seat, wheels), delicate structures (brake cables and spokes), large size region (the overall shape of the bicycle), respectively. These diversified features of the GT can provide stronger regularizations and more comprehensive supervisions for reducing the risks of over-fitting. <ref type="figure">Figure 7</ref>. Faulty regions to be corrected. Refer to ?5 for details.</p><formula xml:id="formula_4">(b) (e) (d) (c) (a) Error Map (b) FNN (c) FNTP (d) FPP (e) FPTN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Proposed HCE Metric</head><p>Given a predicted segmentation probability map P ? R W ?H?1 and its corresponding GT mask G ? R W ?H?1 , the existing metrics, e.g., IoU, boundary IoU <ref type="bibr" target="#b10">[12]</ref>, Fmeasure <ref type="bibr" target="#b0">[2]</ref>, boundary F-measure <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b77">78]</ref>, and MAE <ref type="bibr" target="#b72">[73]</ref>, usually evaluate the quality of the prediction P by calculating the scores based on the mathematical or cognitive consistency (or inconsistency) between P and G. In other words, these metrics describe how significant the "gap" is between P and M from different perspectives. However, measuring the magnitude of the "gap" is insufficient when applying the models in many real-world applications, where evaluating the costs of filling the "gap" is more important.</p><p>Therefore, we propose a novel evaluation metric, Human Correction Efforts (HCE), which approximates the human efforts required in correcting faulty predictions to satisfy specific accuracy requirements in real-world applications. According to our labeling experiences, there are mainly two frequently used operations: (1) points selection along target boundaries to formulate polygons and (2) region selection based on similar pixel intensities inside the region. Both operations correspond to one mouse click by the human operator. Therefore, the HCE here is quantified by the approximated number of mouse clicking numbers. Particularly, to correct a faulty predicted mask, the operators need to manually sample dominant points along the erroneously predicted targets' boundaries or regions for correcting both False Positive (FP) and False Negative (FN) regions. As shown in <ref type="figure">Fig.7</ref>, the FNs and FPs can be categorized into two classes, respectively, according to their adjacent regions: FN N (N=TN+FP), FN TP , FP P (P=TP+FN) and FP TN . To correct the FN N regions, its boundaries adjacent to the TN need to be manually labeled with dominant points ( <ref type="figure" target="#fig_7">Fig.7b)</ref>. Similarly, to correct the FP P regions, we only need to label its boundaries adjacent to the TP regions ( <ref type="figure">Fig.7-d)</ref>. The FN TP regions ( <ref type="figure">Fig.7</ref> gorithm <ref type="bibr" target="#b78">[79]</ref> based on the contours obtained by OpenCV findContours <ref type="bibr" target="#b86">[87]</ref> function and the connected regions labeling algorithm <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b100">101]</ref>, respectively, in the evaluation stage. Relax HCE. Practically, some applications may be tolerant to certain minor prediction errors. Therefore, we extend the computation of HCE by taking the error tolerance ? into consideration (HCE ? ). The key idea is to relax the FP and FN regions by excluding the small FP and FN components using erosion <ref type="bibr" target="#b37">[38]</ref> and dilation <ref type="bibr" target="#b37">[38]</ref> operations. Given a segmentation map P , its corresponding GT mask G, the error tolerance (e.g., ? = 5, which denotes the size of the to-be-ignored small faulty regions), the epsilon of DP algorithm, the computation of the HCE ? can be summarized in Alg. 1. Note that the erosion operation (Line5 of Alg. 1) can remove all thin and fine components of P or G. However, some of the thin components (e.g., thin cables, nets) are critical in representing the targets, and they need to be retained regardless of their sizes. To address this, the skeleton of the GT mask is extracted by <ref type="bibr" target="#b111">[112]</ref> and combined with the relaxed F N mask for retaining these structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DIS5K Benchmark</head><p>As discussed above, our DIS5K is built from scratch to cover highly diversified objects with very different geometrical structures and image characteristics. One of the most important reasons is to exclude the existing datasets' <ref type="table">Table 2</ref>. Quantitative evaluation on DIS5K validation and test sets. R = ResNet <ref type="bibr" target="#b40">[41]</ref>. R2 = Res2Net <ref type="bibr" target="#b30">[32]</ref>. S-813 = STDC813 <ref type="bibr" target="#b25">[27]</ref>, E-B1 = EffinetB1 <ref type="bibr" target="#b88">[89]</ref>. possible biases (to specific image or object characteristics). Therefore, its diversities (e.g., resolutions, image characteristics, object complexities, labeling accuracy) and distributions differ from the existing datasets. All models are trained, validated, and tested on DIS-TR, DIS-VD, and DIS-TE, respectively, to provide a fair comparison. Currently, cross-dataset evaluations <ref type="bibr" target="#b90">[91]</ref> are not conducted mainly because their labeling accuracy is not consistent with ours.</p><p>Metrics. To provide relatively comprehensive and unbiased evaluations, six different metrics, including maximal F-measure (F mx ? ?) <ref type="bibr" target="#b0">[2]</ref>, weighted F-measure (F w ? ?) <ref type="bibr" target="#b63">[64]</ref>, mean absolute error (M ?) <ref type="bibr" target="#b72">[73]</ref>, structural measure (S ? ?) <ref type="bibr" target="#b20">[22]</ref>, mean enhanced alignment measure (E m ? ?) <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b23">25]</ref> and our human correction efforts (HCE? ?), are used to evaluate the performance from different perspectives.</p><p>Competitors. To provide comprehensive evaluations, we compared our IS-Net with 16 popular networks designed for different segmentation tasks, including (i) popular medical image segmentation model, U-Net <ref type="bibr" target="#b80">[81]</ref>; (ii) salient object detection models such as BASNet <ref type="bibr" target="#b77">[78]</ref>, GateNet <ref type="bibr" target="#b116">[117]</ref>, F 3 Net [99], GCPA <ref type="bibr" target="#b8">[10]</ref> and U 2 -Net <ref type="bibr" target="#b76">[77]</ref>; (iii) models designed for COD like SINet-V2 <ref type="bibr" target="#b22">[24]</ref> and PFNet <ref type="bibr" target="#b65">[66]</ref>; (iv) semantic segmentation models: PSPNet <ref type="bibr" target="#b114">[115]</ref>, DeepLab-V3+ <ref type="bibr" target="#b5">[7]</ref> and HRNet <ref type="bibr" target="#b92">[93]</ref>; (v) real-time semantic segmentation models: BiSeNetV1 <ref type="bibr" target="#b106">[107]</ref>, ICNet <ref type="bibr" target="#b113">[114]</ref>, MobileNet-V3-Large <ref type="bibr" target="#b42">[43]</ref>, STDC <ref type="bibr" target="#b26">[28]</ref> and HyperSegM <ref type="bibr" target="#b69">[70]</ref>. All models are re-trained using DIS-TR set (on Tesla V100 or RTX A6000) and the time costs in Tab.2 are all tested on RTX A6000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Quantitative Evaluation</head><p>From Tab.2, compared with the 16 SOTA models, our IS-Net achieves the most competitive performance across all metrics. According to our observations, the performance of different models may be partially related to the model input size and the spatial size of their feature maps. As we know, most of the segmentation models introduce the existing image classification backbones to construct their encoderdecoder architectures for image segmentation tasks. However, some of the backbones like ResNet-50 <ref type="bibr" target="#b40">[41]</ref> starts with an input convolution layer (stride of two) followed by a pooling operation (stride of two) to reduce the spatial size of the feature maps to a quarter of the input size, which leads to the loss of much spatial information and significant performance degradation. When the shape of the to- be-segmented target is close to convex, the information lost and performance degradation is less significant. However, many objects in DIS5K are non-convex, and they have very complicated and fine structures. Therefore, DIS5K requires the models to keep the spatial information as much as possible, which is challenging to most models. <ref type="figure" target="#fig_6">Fig.8</ref> presents qualitative comparisons between our approach and four SOTA baselines. Our model achieves promising results on the diverse scenes no matter that they are salient (gate), camouflaged (centipede), thin (shopping cart) or meticulous (fence) objects, demonstrating the generalization capability of our IS-Net baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Qualitative Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation Study</head><p>To validate the effectiveness of our adaptation on recent SOTA model e.g., U 2 -Net and our newly proposed intermediate supervision strategy, we conduct comprehensive ablation studies. Input Size. As can be seen in Tab.3, a larger input size can improve the performance of U 2 -Net. However, it also increases the GPU memory costs so that we need to reduce the batch size (3 on Tesla V100, 32 GB) when the input size is 1024 ? 1024, which degrades the performance. Our simple and effective variant (i.e., Adp, 4 rd row) addresses this memory issue and improves the performance. Supervision on Different Decoder Stages. In Tab.3, Last- <ref type="table">Table 3</ref>. Ablation studies on our DIS-VD set. is better than KL divergence and L1. Besides, sharing the "outconvs", which transform the deep feature maps to the segmentation probability maps, of the GT encoders and the segmentation decoders leads to negative impacts. Random Seeds. To study the influences of random weights initialization, we trained the same GT encoder multiple times with weights initialized by different random seeds.</p><formula xml:id="formula_5">Settings F mx ? ? F w ? ? M ? S? ? E m ? ? HCE? ? U 2 -Net 320 2 (baseline</formula><p>As seen, although the performance produced by different random seeds are different, their variations are minor, and all of them are better than that of the models (U 2 -Net and Adp) trained without our intermediate supervision strategy. Since the model from seed 5289 ranks the 1 st on five out of six overall metrics, we use this model as our IS-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We have systematically studied the highly accurate dichotomous image segmentation (DIS) task from both the application and the research perspective. To prove that the task is solvable, we have built a new challenging DIS5K dataset, introduced a simple and effective intermediate supervision network, called IS-Net, to achieve highquality segmentation results in real-time, and designed a novel Human Correction Efforts (HCE) metric by considering the shape complexities for applications. With an extensive ablation study and comprehensive benchmarking, we obtained that our newly formulated DIS task is solvable. Broader impacts. This work may greatly facilitate the applications of segmentation techniques in both academia and industry, and hereby invite all the researchers in related fields to collaborate and improve the whole eco-system.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Multi-class vs. Dichotomous Segmentation</head><p>Multi-class (e.g. semantic <ref type="bibr" target="#b59">[60]</ref>, panoptic <ref type="bibr" target="#b52">[53]</ref>) segmentation aims at simultaneously labeling all the pixels in an image of complex scenario <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b119">120]</ref>, which contains many different objects, with the pre-defined multiple categories encoded in one-hot vectors. However, the one-hot representation of the categories is memory exhaustive when the number of categories is huge (e.g., 10,000 categories), especially on high-resolution images. Besides, some input images only contain objects from several categories (e.g., one or two). Outputting the full-length one-hot dense predictions (10,000 categories) is not a resource-saving option. A possible alternative could be a two-step solution: "detection + segmentation", in which a bounding box and category of the certain object can be predicted first. The segmentation process can then be conducted in a dichotomous way within the bounding box region by producing a singlechannel probability map (e.g., similar to Mask R-CNN <ref type="bibr" target="#b39">[40]</ref>. However, Mask R-CNN still uses the one-hot representation in the segmentation step).</p><p>Moreover, many practical applications, such as image editing, art design, shape from silhouette, robot manipulation, are usually category-agnostic, where the applications require highly accurate segmentation results of certain objects regardless of their categories. Different from the images of complex scenarios in semantic <ref type="bibr" target="#b55">[56]</ref> or panoptic <ref type="bibr" target="#b119">[120]</ref> segmentation, the images in these applications usually contain one or a few objects with very high resolutions, less occlusions. To this end, many related tasks have been proposed, such as salient object detection (SOD) <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b96">97,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b105">106]</ref>, salient object in clutter (SOC) <ref type="bibr" target="#b19">[21]</ref>, high-resolution salient object detection (HRS) <ref type="bibr" target="#b108">[109]</ref>, camouflaged object detection (COD) <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b84">85]</ref>, thin object segmentation (TOS) <ref type="bibr" target="#b53">[54]</ref>, meticulous object segmentation (MOS) <ref type="bibr" target="#b104">[105]</ref>, video object segmentation (VOS) <ref type="bibr" target="#b73">[74]</ref>, classagnostic very high-resolution segmentation (VHRS) <ref type="bibr" target="#b11">[13]</ref>, etc. Most of these tasks try to solve dichotomous segmentation problems on images which are sharing specific characteristics. The exclusive mechanisms for certain tasks are barely used so that their problem formulations are almost the same, which means most of these tasks are datadependent. Simply combining these tasks by merging their datasets is not a decent option because these tasks' image resolutions and labeling qualities are diversified.</p><p>Considering these facts, we re-formulate a new categoryagnostic dichotomous segmentation task, highly accurate Dichotomous Image Segmentation (DIS), where achieving highly accurate segmentation results of objects with diversified shapes and structures is the key concern.</p><p>1. <ref type="bibr" target="#b0">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Datasets</head><p>Datasets are the basis of most computer vision tasks. In the past decades, many segmentation datasets for related tasks have been created. For example, semantic (PASCAL-VOC <ref type="bibr" target="#b18">[20]</ref>, MS-COCO <ref type="bibr" target="#b55">[56]</ref>) and panoptic (Cityscapes <ref type="bibr" target="#b14">[16]</ref>, ADE20K <ref type="bibr" target="#b119">[120]</ref>) segmentation (SMS) datasets usually contain large number of images with multiple objects from different categories in each of them. But they either have low geometrical labeling accuracy or relatively small resolutions, where details of objects are hard to be included and segmented. The entity segmentation (ES) <ref type="bibr" target="#b74">[75]</ref> datasets proposed for class-agnostic segmentation has similar issues. Images in the salient object detection (SOD) <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b105">106]</ref> and camouflaged object detection (COD) <ref type="bibr" target="#b24">[26]</ref> datasets are usually low-resolution ones, which contains objects with simple structures. The high-resolution salient object detection (HRS) <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b108">109]</ref> datasets have higher resolution, but they are built upon images with objects of simple structures similar to that in SOD and COD datasets. The meticulous object segmentation (MOS) <ref type="bibr" target="#b104">[105]</ref> and thin object segmentation (TOS) <ref type="bibr" target="#b53">[54]</ref> datasets show competitive resolution and object structure complexity characteristics. However, MOS is too small to enable thorough training and comprehensive evaluation, while the TOS dataset is built with synthetic images. Therefore, there is a need for a new extendable large-scale dataset built upon the highresolution images with diversified object structure complexities and highly accurate labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Existing Models</head><p>Models are the cores of vision tasks. Currently, deep models are the most popular solutions for most of the segmentation tasks. Many different deep architectures have been proposed to achieve better performance, such as FCNbased <ref type="bibr" target="#b59">[60]</ref> feature aggregation models <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b109">110,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b116">117]</ref>, Encoder-Decoder architectures <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b80">81]</ref>, Coarse-to-Fine (or Predict-Refine) models <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b95">96]</ref>, Vision Transformers <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b117">118]</ref>, etc. Besides, many real-time models <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b113">114]</ref> are developed to balance the performance and time costs. To achieve highly accurate results in our DIS, the models are expected to capture fine details (and complicated structures) and large components of the diversified objects from largesize (e.g., 2K, 4K or even larger) images with affordable memory, computation and time costs. These requirements are very challenging to the existing segmentation models. Therefore, more effective, more efficient, and more stable models are needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Over-fitting vs. Regularization</head><p>Most deep segmentation models can fit the training sets very well (training accuracy close to 100%) while having different performances on the testing sets. To the best of our knowledge, there could be two main reasons. On one hand, the "distributions" between the training, validation, and testing sets are not guaranteed to be the same, which leads to performance degradation of almost all the models on testing sets. On the other hand, different model architectures have diversified capabilities of feature representations, which means they are more likely to fit the training sets in very different ways, namely, transforming the input images into other high-dimensional spaces. Most of the works are following this direction to develop more representative architectures. However, there lacks an effective way to measure the representation capabilities of these architectures before testing, so the model design is usually conducted by trial and error. Hence, some researchers turn to search for different ways for reducing over-fitting. Different supervision strategies, such as weights regularization <ref type="bibr" target="#b36">[37]</ref>, dropout <ref type="bibr" target="#b85">[86]</ref>, dense (deep) supervision <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b101">102]</ref>, hybrid loss <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b115">116]</ref> and so on, have been proposed. The dense (deep) supervision <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b101">102]</ref>, which imposes ground truth supervisions on the side outputs from several of the deep intermediate layers, is one of the most popular ways. However, transforming the deep intermediate features (multi-channel) into the side outputs (single-channel) in dichotomous image segmentation (DIS) is essentially a dimension reduction operation, which leads to information losses, so that weaken the supervisions. In this paper, instead of developing more complicated deep architectures, we follow the dense supervision idea but develop a simple yet more effective supervision strategy, intermediate supervision, to directly enforce the supervisions on highdimensional intermediate deep features in addition to the side outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.">Evaluation Metrics</head><p>The evaluation strategies and metrics are expected to provide comprehensive and practically meaningful evaluations to analyze the prediction qualities. Currently, many evaluation metrics, such as IoU, boundary IoU <ref type="bibr" target="#b10">[12]</ref>, Fmeasure <ref type="bibr" target="#b0">[2]</ref>, boundary F-measure <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b77">78]</ref>, boundary displacement error (BDE) <ref type="bibr" target="#b29">[31]</ref>, boundary IoU <ref type="bibr" target="#b10">[12]</ref>, structural measure (S m ) <ref type="bibr" target="#b20">[22]</ref>, Mean Absolute Error (MAE) <ref type="bibr" target="#b72">[73]</ref>, and so on, are usually defined based on consistencies (or inconsistencies) between the model predictions and the ground truth. Most of them are usually biased to certain types of structures. For example, IoU and F-measure mainly rely on the object components with large areas while neglecting the fine details with relatively small areas. To alleviate this issue, boundary F-measure, BDE, and boundary IoU are developed to focus on the boundary quality. However, these boundary-based metrics are often highly dependent on those long smooth boundary segments' qualities while failing to describe the qualities of those short jagged boundary segments. Besides, the above metrics are mostly de-fined from the mathematical or cognitive perspective; none of them are able to reflect the barriers (or costs) of applying the predictions in real-world applications, where certain accuracy requirements have to be satisfied. To address these issues, we propose a novel metric, named as human correction efforts (HCE), to measure the barriers by approximating the human efforts for correcting the faulty regions of the model predictions.  <ref type="figure">Fig. 11</ref> shows some samples from our DIS5K, which have certain characteristics similar to that of the existing dichotomous segmentation tasks, such as salient object detection (SOD) <ref type="bibr" target="#b93">[94]</ref>, salient object in clutter (SOC) <ref type="bibr" target="#b19">[21]</ref>, camouflaged object detection (COD) <ref type="bibr" target="#b24">[26]</ref>, thin object segmentation (TOS) <ref type="bibr" target="#b53">[54]</ref>, meticulous object segmentation (MOS) <ref type="bibr" target="#b104">[105]</ref>. It is worth mentioning that "salient object", "salient object in clutter" and "camouflaged object" are mainly defined based on the contrast between foreground targets and background environments. In comparison, "thin object" and "meticulous object" are based on the geometric structure complexities of the foreground targets. Therefore, the first three types of objects and the last two types of targets are not exclusive. For example, the basket in <ref type="figure">Fig. 11</ref> (a) and the shrimp in <ref type="figure">Fig. 11</ref> (c) can also be taken as meticulous because the basket has many holes and the shrimp has jagged boundaries. Besides, the boundaries among SOD, SOC, and COD and the boundaries between TOS and MOS are blurring. There are some overlaps between them in terms of data samples. Our DIS5K contains all the above types of images paired with highly-accurate ground truth masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">More Details of DIS5K Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Per-category and per-group statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Typical Samples from DIS5K</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Object Structure Analysis</head><p>In addition to the above mentioned image characteristics, there are also some interesting observations on object structures from our DIS5K, as shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. Intra-category structure similarity. As shown in <ref type="figure" target="#fig_0">Fig. 12</ref> (a) and (b), the objects in the same categories are usually showing the same or similar structures and shapes. We call this intra-category structure similarity, which is one of the main cues for categorizing. However, the intra-category structure similarity is not always guaranteed. <ref type="figure" target="#fig_0">Fig. 12</ref>    <ref type="figure">Figure 11</ref>. Sample images and ground truth masks with objects of certain characteristics.   <ref type="figure" target="#fig_0">Fig. 12 (d)</ref> show very different structures and shapes, which indicates low intra-category similarity. Because artists or designers usually prefer to design unique architectures, which leads to very diversified object appearances and structures. Besides, compared against the relatively stable shapes and structures of the natural targets (e.g., animals, plants), the structures of these human-created objects, which play vital roles in the human-environment interaction of our daily lives, are updated very fast, which further magnifies the challenges in the DIS task. These intra-category dissimilarities significantly increase the difficulty of accurate segmentation and lead to robustness risks.</p><formula xml:id="formula_6">I num H ? ? H W ? ? W D ? ? D IP Q ? ? IP Q C num ? ? C P num ? ? P DIS<label>DIS5K</label></formula><p>Inter-category structure similarity. In contrary to the low intra-category similarity, there also exist some categories that have high inter-category structure similarity. <ref type="figure" target="#fig_0">Fig. 12</ref> (e) shows some targets from different categories, such as crack, lightning, cable, rope, pipe and so on. These targets are mainly comprised of thin and elongated components. For example, the shapes of the crack and the lightning are very close to each other so that they are hard to be differentiated without showing the RGB images. The cable, rope, and pipe are also comprised of thin and elongated components with relatively smoother boundaries. Besides other targets like roads and rivers in satellite images, vessels in medical images also have similar structural characteristics to those mentioned above. The inter-category structure similarities haven't been thoroughly studied, which could be promising directions for exploring the models' explain-abilities and data augmentation strategies.</p><p>Our DIS5K dataset provides relatively richer samples for studying the intra-category and inter-category similarities and dissimilarities. More qualitative and quantitative studies will be helpful to diversified vision tasks, such as image (shape) classification, segmentation, etc. <ref type="table" target="#tab_5">Table 4</ref> illustrates the essential attributes of the subsets of our DIS5K dataset. As seen, the image dimensions of these subsets are close to each other. At the same time, the complexities of the four testing subsets are in ascending order. <ref type="figure" target="#fig_14">Fig. 13</ref> shows the qualitative comparisons of the structural complexities of our four testing subsets, DIS-T1?DIS-TE4. Their structure complexities in ascending order can be visually perceived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Attributes of Subsets in DIS5K</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">More Details of Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation details</head><p>Our models and other baseline models are trained with our DIS-TR (3,000 images) and validated on DIS-VD (470 images). The input size of our model is set to 1024 ? 1024. It is worth noting that there are many large-size images in our dataset so that the image loading operations in the training and validation are very time-consuming. To address this issue and boost the speed of training and validation, we resize all the input images and their corresponding ground truth to 1024 ? 1024 off-line and store them as Pytorch tensor files on the hard disk drive. Although this strategy requires relatively more storage space, it dramatically reduces the time costs for the data loading process in the training and validation stages. Our training process consists of two training stages: (i) the training stage of the ground truth encoder and (ii) the training stage of the image segmentation component. In both training stages, these three-channel inputs (GT masks are repeated to have three channels) are normalized to [-0.5, 0.5] and only augmented with horizontal flipping. The models weights are initialized by Xavier <ref type="bibr" target="#b34">[35]</ref> and optimized with Adam <ref type="bibr" target="#b45">[46]</ref>   (initial learning rate lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight decay=0) for both the ground truth encoder and the segmentation component. The batch size of each training step is set to eight, and the validation on DIS-VD is con-    According to our experiments, the training process of our ground truth encoder is easy to converge, and it usually takes only 1,000 iterations (stop training when the valid maxF is greater than 0.99). While the segmentation com-ponent of our model usually converges after around 100k iterations, and the whole training process takes less than 48 hours. Besides, all the models are implemented using Pytorch 1.8.0. Some experiments are conducted on a desktop that has a 2.9GHz CPU (128 cores AMD Ryzen Threadripper 3990X), 256 GB RAM and a NVIDIA RTX A6000 GPU. Some other models are trained on NVIDIA TESLA V100 GPU (32 GB). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">More Analysis of the Experimental Results</head><p>Performance comparisons among different models. As shown in <ref type="table">Table 2</ref>, our model achieves the most competitive performance against other existing models in terms of almost all the evaluation metrics on different datasets. Among the dichotomous segmentation models, U-Net <ref type="bibr" target="#b80">[81]</ref>, BAS-Net <ref type="bibr" target="#b77">[78]</ref>, U 2 -Net <ref type="bibr" target="#b76">[77]</ref> and PFNet <ref type="bibr" target="#b65">[66]</ref> performs relatively better against other SOD and COD models. Among the semantic segmentation and real-time semantic segmentation models, the results of HRNet <ref type="bibr" target="#b92">[93]</ref> and HyperSeg-M <ref type="bibr" target="#b69">[70]</ref> show more competitive performance. Among all the existing models, the performance of HyperSeg-M and U 2 -Net are close and perform better than other models in both validation and testing sets. Although HRNet and BASNet show slightly inferior performance against HyperSeg-M and U 2 -Net, they are still more competitive than others. <ref type="figure" target="#fig_2">Fig. 14</ref> provides the qualitative comparisons of our model and other four competitive baseline models. As can be seen, our model achieves the best overall performance on different objects. Surprisingly, other models like U 2 -Net, HyperSeg-M, and HRNet also obtain encouraging results on certain targets, such as the tree, the gate and the shopping cart, after training on our DIS-TR dataset, which further proves the value of DIS5K.</p><p>Performance comparisons among different test sets. performance analysis based on the targets' complexities for demonstrating the importance of our newly proposed HCE ? ? metric. As shown in <ref type="table">Table 2</ref>, our model achieves different performances on the four testing sets, obtained by ordering (ascending) and splitting the whole test set according to the structural complexities of the to-be-segmented objects. However, except for our newly proposed HCE ? ?, other metrics, such as maxF ? ?, F w ? ?, M ?, S ? ? and E m ? ?, of DIS-TE1, DIS-TE2, DIS-TE3, and DIS-TE4 show no strong (negative or positive) correlations with respect to the shape complexities. For example, M of our model on these DIS-TE1 (0.074) and DIS-TE4 (0.072) are very close. The maxF ? ?, F w ? ?, S ? ? and E m ? ? of DIS-TE4 are even greater than those of DIS-TE1, which probably provides misleading information that DIS-TE4 is less challenging than DIS-TE1. On the contrary, the HCE ? ? of our model on DIS-TE1 and DIS-TE4 are 149 and 2,888, respectively. That indicates the cost for correcting the predictions of DIS-TE4 is around 20 times more than that of correcting predictions on DIS-TE1, which is consistent with the complexities illustrated in <ref type="table" target="#tab_5">Table 4</ref>. It means our HCE ? ? can correctly describe the correlations between prediction quality and the shape complexities. Thus, it can assess the human interventions needed when applying the models to real-world applications. We can get similar observations from the evaluation scores of other models on different test sets, which further proves the importance of our HCE ? ? in evaluating highly accurate dichotomous image segmentation results. It is worth noting that the weak correlations between the conventional metrics and the shape complexities of different test sets are partial because image context complexity also plays a vital role in determining the segmentation difficulties. But this factor is hard to be quantified and has relatively less impact on the labeling workloads. Therefore, it is not considered in this work and will be studied in the future. In addition, performance comparisons of different models based on different groups are illustrated in <ref type="table" target="#tab_9">Table 5</ref> and 6, from which the per-group segmentation difficulties and performance can be found.   <ref type="figure" target="#fig_4">Figure 16</ref>. 3D models built upon the ground truth masks sampled from DIS5K by the "Extrude" operation in Blender.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Our Intermediate Supervision To fur-</head><p>ther demonstrate the effectiveness of our intermediate supervision, we show the training loss and validation mean absolute error M ? curves of our adapted U 2 -Net with and without our intermediate supervisions in <ref type="figure" target="#fig_3">Fig.15</ref>. The top part of <ref type="figure" target="#fig_3">Fig.15</ref> shows the training loss of the last side output, which is taken as the final result in the inference stage. As can be seen, the models with intermediate supervisions converge faster before around 10,000 iterations. Later, the model without intermediate supervisions gradually produces a lower loss. These curves demonstrate that our intermediate supervision plays a typical role of regularizer for reducing the probability of over-fitting. The bottom plot of <ref type="figure" target="#fig_3">Fig.15</ref> shows that our intermediate supervision significantly decreases the M ? on the validation set, which validates its effectiveness in performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Applications</head><p>Our DIS task will benefit both academia and industrious. In addition to the DIS task, we believe that our highly accurate large-scale DIS5K dataset can also be used in various related research fields, such as:</p><p>? providing pre-trained segmentation models for other specific object segmentation tasks as well as facilitating the downstream tasks, such as image matting, editing, and so on;</p><p>? the subsets of DIS5K can be used for fast prototyping of different segmentation tasks;</p><p>? providing materials and examples for shape and structure analysis in graphics and topology;</p><p>? high resolution fine-grained image classification;</p><p>? segmentation guided super-resolution and image processing;</p><p>? synthesizing more composite images with diversified backgrounds for more robust image segmentation;</p><p>? edge, boundary or contour detection, etc.</p><p>Thanks to the high resolution and accurate labeling, many samples in our DIS5K show high artistic and aesthetic values. <ref type="figure" target="#fig_8">Fig. 9</ref> shows the comparison between the original ship image with cluttered background and the backgroundremoved image with perspective transforms (See more samples in <ref type="figure">Fig. 17</ref>). As can be seen, compared with the original image, the background-removed image shows higher aesthetic values and good usability, which can even be directly used as:</p><p>? materials of art design, image and video editing;</p><p>? backgrounds of posters or slides, wall papers of cellphones, tablets, desktops;</p><p>? materials for 3D modeling, as shown in <ref type="figure" target="#fig_4">Fig. 16</ref> (A demo video is also attached).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations and Future Works</head><p>Failure Cases of Our Model. <ref type="figure" target="#fig_6">Fig.18</ref> shows some typical failure cases of our model. The first row shows the result of a sail ship image. Our model fails in segment two of the masts and the ropes because this region has a cluttered background (a building). The second row shows the segmentation result of a baby carriage. Our model fails in segmenting the mesh-like structure of the carriage since it is too meticulous (just one-pixel width), so that it is hard to be segmented by our model from the input images with the size of 1024 ? 1024. The third row illustrates the segmentation result of a key chain with a cluttered background. As can be seen, the color differences between the critical chain and the background are small, which significantly increases the difficulty of the segmentation. In summary, the highly accurate dichotomous image segmentation (DIS) is a highly challenging task. There is still a large room for improvement. Therefore, more powerful deep segmentation models are needed to handle larger size input for obtaining very detailed object structures. In contrast, the model size, memory occupation, training, and inference time costs are expected to be affordable on the mainstream GPUs. Limitations of Our DIS5K dataset. Although our DIS5K is currently the most complex dichotomous segmentation dataset, there is still a large room for improvement. For example, compared with the vast number of categories and the diversified general object classes in the real-world, 225 categories in our DIS5K dataset are far from enough. Therefore, more categories, more samples of specific categories, and more diversified image qualities are needed to further improve the diversity of this dataset. Besides, semiautomatic and highly accurate annotation tools are expected to simplify and boost the ground truth labeling processes. We will explore semi-supervised and weakly supervised methods for further reducing the labeling workloads. In addition, it also requires a set of standard criteria to control the labeling accuracy. Limitations of Our HCE metric. Our HCE metric provides direct measures of the human correction efforts needed for fixing faulty predictions under certain accuracy requirements. To leverage different accuracy requirements, the erosion <ref type="bibr" target="#b37">[38]</ref> and dilation <ref type="bibr" target="#b37">[38]</ref> operations are used to remove small false positive and false negative regions, while the skeleton extraction algorithm <ref type="bibr" target="#b111">[112]</ref> is used to preserve the structural information of the thin components in the ground truth masks. However, the skeleton extraction algorithm is slow when processing the large-size masks. Therefore, the evaluation of large-scale datasets takes a long time. This issue also happens when computing the weighted Fmeasure <ref type="bibr" target="#b63">[64]</ref>, which uses a distance transform algorithm <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b27">29]</ref> to calculate the weights. Therefore, more works need to be conducted on these conventional algorithms, such as skeleton extraction, distance transform, etc., to handle larger and more complicated inputs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Left: Correlations between different complexities. Right: Categories and groups of our DIS5K dataset. Zoom-in for better view. Please refer to ?3.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>GT masks of our DIS5K with diversified intercategorical complexities. The complexity relationships are only valid within each row or column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Proposed IS-Net baseline: (a) shows the image segmentation component, (b) illustrates the ground truth encoder built upon the intermediate supervision (IS) component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Feature maps produced by the last layer of the EN 2 stage of our GT encoder. "21", "23", "29" and "37" are the indices (start with 1) of the corresponding channels in the feature map.of high-dimensional intermediate feature maps f I D before producing the probability maps. Each feature map f I d has the same dimension with its corresponding GT intermediate feature map f G d : f I D = F ? sg (? sg , I), D = {1, 2, 3, 4, 5, 6}, where ? sg denotes the weights of the segmentation model. Then, the intermediate supervision (IS) via feature synchronization on the deep intermediate features can be conducted by the following high-dimensional feature consistency loss: L fs = D d=1 ? f s d f I d ? f G d 2 , where ? f s d denotes the weight of each FS loss. The training process of the segmentation model F sg can be formulated as the following optimization problem: argmin ?sg (L fs + L sg ), where L sg indicates the BCE loss of the side outputs of F sg :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>-c) enclosed by TP and the FP TN regions (Fig.7-e) enclosed by TN can be easily corrected by one-click region selection. Therefore, the HCE for correcting the faulty regions inFig.7(b-e) is 10 (six and two clicks needed in (b) and (d), one click needed in (c) and one click needed in (e)). The dominant point selection operations and the region selection operations are approximated by DP al-Input: P , G, ? = 5, epsilon = 2.0 Output: HCE? 1 G ske = skeletonize (G); 2 PorG, T P = or (P , G), and (P ,G); 3 F N , F P = (G -T P ), (P -T P ); 4 for (i = 0; i ? ?; i + +) do 5 PorG = erode (PorG, disk (1)); 6 end 7 F N , F P = and (F N ,PorG), and (F P ,PorG); 8 for (i = 0; i ? ?; i + +) do 9 F N = dilate (F N , disk (1)); 10 F N = and (F N , not P ); 11 F P = dilate (F P , disk (1)); 12 F P = and (F P , not G); 13 end 14 F N , F P = and (F N , F N ), and (F P , F P ); 15 F N = or (F N , xor (G ske , and (T P , G ske ))); 16 HCE? = compute HCE (F N , F P , T P , epsilon) Algorithm 1: Relax HCE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative comparisons of IS-Net with four baselines. Refer to the SM for more results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( b )</head><label>b</label><figDesc>Artistic figure based on the background removed image (a) High-resolution image with cluttered background</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Demo application: artistic figure generated based on a sample of our DIS5K dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10</head><label>10</label><figDesc>illustrates the number of images per-category and per-group. Our DIS5K contains 5,470 images from 225 categories divided into 22 groups. The average numbers of images per category and per group are around 24 and 249, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>(c) and (d) show two typical examples against that in different magnitudes. Fig. 12 (c) illustrates some bicycles with Number of images per-category and per-group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .</head><label>12</label><figDesc>Structure analysis of inter-and intra-category targets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 .</head><label>13</label><figDesc>Sample ground truth (GT) masks from DIS-TE1, DIS-TE2, DIS-TE3, and DIS-TE4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 .</head><label>14</label><figDesc>Qualitative comparisons of our model and four cutting-edge baselines. ducted every 1,000 iterations. If the validation results (in terms of maxF and M ) are improved, the hard disk drive saves the model weights. It is worth mentioning that the loss weights of the dense supervision in the ground truth encoder training and intermediate supervision of the segmentation component training are all set to 1.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 .</head><label>15</label><figDesc>Curves of the training loss computed on the last prediction probability map and the Mean Absolute Error (M ) on our validation set (DIS-VD).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 . 18 .</head><label>1718</label><figDesc>Comparisons between the original images and their backgrounds-removed correspondences generated from our DIS5K. Typical failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Data analysis of existing datasets. See ?3.2 for details.</figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell>Number</cell><cell></cell><cell>Image Dimension</cell><cell></cell><cell></cell><cell>Object Complexity</cell></row><row><cell></cell><cell></cell><cell>Inum</cell><cell>H ? ?H</cell><cell>W ? ?W</cell><cell>D ? ?D</cell><cell>IP Q ? ?IP Q</cell><cell>Cnum ? ?C</cell><cell>Pnum ? ?P</cell></row></table><note>etc.) of a large number of samples, guaranteeing the robust- ness and generalization of the category-agnostic segmen- tation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>), the image I is transformed to a set</figDesc><table><row><cell>CONV (3,64,k=3,s=2)</cell><cell>EN_1 RSU-7 (64,32,64)</cell><cell>DE_1 RSU-7 (128,16,64)</cell><cell>CONV (3,16,k=3,s=2) (16,16,64) EN_1 RSU-7</cell></row><row><cell>EN_2 RSU-6 (64,32,128)</cell><cell>DE_2 RSU-6 (256,32,64)</cell><cell></cell><cell>EN_2 RSU-6 (64,16,64)</cell></row><row><cell>EN_3 RSU-5 (128,64,256)</cell><cell>DE_3 RSU-5 (512,64,128)</cell><cell></cell><cell>EN_3 RSU-5 (64,32,128)</cell></row><row><cell>EN_4 RSU-4 (256,128,512)</cell><cell>DE_4 RSU-4 (1024,128,256)</cell><cell></cell><cell>EN_4 RSU-4 (128,32,256)</cell></row><row><cell>EN_5 RSU-4F (512,256,512)</cell><cell>DE_5 RSU-4F (1024,256,512)</cell><cell></cell><cell>EN_5 RSU-4F (256,64,512)</cell></row><row><cell></cell><cell>EN_6</cell><cell></cell><cell>EN_6</cell></row><row><cell cols="2">RSU-4F</cell><cell></cell><cell>RSU-4F</cell></row><row><cell cols="2">(512,256,512)</cell><cell></cell><cell>(512,64,512)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Image dimension and object complexity of the subsets of DIS5K. ? (?) is the standard deviation of the corresponding index.</figDesc><table><row><cell>Task Dataset</cell><cell>Number</cell><cell>Image Dimension</cell><cell>Object Complexity</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>5470 2513.37 ? 1053.40 3111.44 ? 1359.51 4041.93 ? 1618.26 107.60 ? 320.69 106.84 ? 436.88 1427.82 ? 3326.72 DIS-TR 3000 2514.15 ? 1052.45 3091.23 ? 1356.92 4028.09 ? 1612.45 69.32 ? 261.98 73.99 ? 367.81 1153.05 ? 2893.36 DIS-VD 470 2472.59 ? 963.43 3102.85 ? 1308.72 4006.49 ? 1526.56 156.85 ? 349.75 163.91 ? 650.42 1954.73 ? 5119.89 ? 1069.37 3527.81 ? 1412.89 4580.93 ? 1645.86 443.32 ? 667.01 482.98 ? 843.50 4858.80 ? 5618.87</figDesc><table><row><cell>DIS-TE1</cell><cell cols="2">500 2240.35 ? 1092.92 2678.50 ? 1291.11 3535.32 ? 1598.89</cell><cell>27.13 ? 29.07</cell><cell>6.94 ? 6.37</cell><cell>237.48 ? 96.27</cell></row><row><cell>DIS-TE2</cell><cell cols="2">500 2402.09 ? 1047.89 3032.25 ? 1298.45 3904.03 ? 1583.39</cell><cell>50.79 ? 69.85</cell><cell>21.20 ? 16.30</cell><cell>583.04 ? 120.90</cell></row><row><cell>DIS-TE3</cell><cell>500</cell><cell>2597.15 ? 988.88 3336.51 ? 1339.10 4263.78 ? 1571.21</cell><cell>92.68 ? 118.99</cell><cell>60.96 ? 40.32</cell><cell>1190.93 ? 255.00</cell></row><row><cell cols="3">DIS-TE4 500 2847.55 variant structures. Their differences are mainly caused by</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">components absence (out-of-view imaging, incomplete ar-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">chitecture), variations on the design, view angle changes,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">co-existence of multiple targets, etc. Although the struc-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">tures of these bicycles are different, they are still sharing</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">some common features, such as wheels, frames, etc. How-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ever, objects in some other categories may share no struc-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ture similarities. For example, the sculptures in</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Objects with stable and less variant structures (b) Objects sharing common structures Seadragon 2736x3648 Seadragon 2000x3008 Seadragon 2736x3648 Bicycle 1928x2952 Bicycle 3024x4032 Bicycle 1536x2048 (e) Targets with similar structure characteristics across DIFFERENT categories Crack 1920x2560 Lightning 1076x1434 Cable 1712x2560 Rope 1561x2340 Pipe 3072x2304 (d) Objects with very different structures from the SAME category</head><label></label><figDesc></figDesc><table><row><cell>(a) Sculpture 1944x2592 Bicycle 1200x1600</cell><cell>Sculpture 2448x3264 Bicycle 2448x3264</cell><cell>Sculpture 2736x3648 (c) Objects with different structures from the SAME category Sculpture 1200x1600 Sculpture 960x1280 Bicycle 3024x4032 Bicycle 4000x6000 Bicycle 2304x3072</cell><cell>Sculpture 2736x3648 Bicycle 2858x3811</cell></row><row><cell></cell><cell></cell><cell cols="2">optimizer with the default settings</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table Lamp</head><label>Lamp</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Ladder</cell><cell>Bicycle Stand</cell><cell>Ant</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3684x2736</cell><cell>2432x3648</cell><cell>1905x3000</cell></row><row><cell></cell><cell></cell><cell>Eyeglasses</cell><cell>Gym Equipment</cell><cell>Earphone</cell><cell>Drum</cell></row><row><cell></cell><cell>2848x4288</cell><cell>1514x2271</cell><cell>3648x2736</cell><cell>3959x5938</cell><cell>1200x1600</cell></row><row><cell>Gym Equipment</cell><cell>Boat</cell><cell>Bench</cell><cell>Fan</cell><cell>UAV</cell><cell>Boat</cell></row><row><cell>4032x3024</cell><cell>1635x2171</cell><cell>2433x3637</cell><cell>653x800</cell><cell>1063x1600</cell><cell>1276x1890</cell></row><row><cell>Headset</cell><cell></cell><cell>Jewelry</cell><cell>Chair</cell><cell>Cart</cell><cell></cell></row><row><cell>600x800</cell><cell></cell><cell>1362x1600</cell><cell>1944x2592</cell><cell cols="2">960x1280</cell></row><row><cell>Parachute</cell><cell></cell><cell>Bicycle</cell><cell>Bridge</cell><cell>Tractor</cell><cell></cell></row><row><cell>1157x1741</cell><cell></cell><cell>2588x3888</cell><cell>13000x4000</cell><cell cols="2">1500x2000</cell></row><row><cell cols="2">Transmission Tower</cell><cell>Gate</cell><cell>Fence</cell><cell>Parachute</cell><cell></cell></row><row><cell>4032x3024</cell><cell></cell><cell>3264x3264</cell><cell>3264x3264</cell><cell>1318x1980</cell><cell></cell></row><row><cell>Scaffold</cell><cell></cell><cell>Antenna</cell><cell>Sculpture</cell><cell>Bridge</cell><cell></cell></row><row><cell>3872x2592</cell><cell></cell><cell>3296x2472</cell><cell>3024x4032</cell><cell>2592x3872</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>PART-I: Quantitative evaluation on our validation, DIS-VD, and test sets, DIS-TE(1-4), based on groups. ResNet18=R-18. ResNet34=R-34. ResNet50=R-50. Res2Net50=R2-50. DeepLab-V3+=DLV3+. BiseNetV1=BSV1. STDC813=S-813. EffiNetB1=E-B1. MobileNetV3-Large=MBV3. HyperSeg-M=HySM.</figDesc><table><row><cell>Dataset</cell><cell>Metric</cell><cell>UNet [81]</cell><cell>BASNet [78]</cell><cell>GateNet [117]</cell><cell>F 3 Net [99]</cell><cell>GCPANet [10]</cell><cell>U 2 Net [77]</cell><cell>SINetV2 [24]</cell><cell>PFNet [66]</cell><cell>PSPNet [115]</cell><cell>DLV3+ [7]</cell><cell>HRNet [93]</cell><cell>BSV1 [107]</cell><cell>ICNet [114]</cell><cell>MBV3 [43]</cell><cell>STDC [27]</cell><cell>HySM [70]</cell><cell>Ours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 .</head><label>6</label><figDesc>PART-II: Quantitative evaluation on our validation, DIS-VD, and test sets, DIS-TE(1-4), based on groups. ResNet18=R-18. ResNet34=R-34. ResNet50=R-50. Res2Net50=R2-50. DeepLab-V3+=DLV3+. BiseNetV1=BSV1. STDC813=S-813. EffiNetB1=E-B1. MobileNetV3-Large=MBV3. HyperSeg-M=HySM.</figDesc><table><row><cell>Dataset</cell><cell>Metric</cell><cell>UNet [81]</cell><cell>BASNet [78]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.gimp.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">It is worth noting that only R-PASCAL and the BIG datasets are included here because they target highly accurate segmentation, and most of their images contain one or two objects, which is comparable to the listed tasks and datasets.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">One hundred years since the introduction of the set distance by dimitrie pompeiu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tiba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFIP SMO</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hausdorff&apos;s Grundz?ge der Mengenlehre</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blumberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="116" to="129" />
			<date type="published" when="1920" />
		</imprint>
	</monogr>
	<note>American</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distance transformations in digital images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Borgefors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Graph. Image Process</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ab initio particlebased object manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSS</title>
		<editor>D. A. Shell, M. Toussaint, and M. A. Hsieh</editor>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Global contextaware progressive aggregation network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020. 9</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boundary IoU: Improving object-centric image segmentation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boundary iou: Improving object-centric image segmentation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cascadepsp: Toward class-agnostic and very high-resolution segmentation via global and local refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MUC-4 evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chinchor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MUC, 1992</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">R3net: Recurrent residual refinement network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Relaxed precision and recall for ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Euzenat</surname></persName>
		</author>
		<editor>K-CapW</editor>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Concealed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cognitive vision inspired object segmentation metric and loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SSI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Camouflaged object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking bisenet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rethinking bisenet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distance transforms of sampled functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two linear time union-find strategies for image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fiorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gustedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCS</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="181" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Yet another survey on image segmentation: Region and boundary information integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Freixenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mu?oz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mart?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cuf?</surname></persName>
		</author>
		<editor>A. Heyden, G. Sparr, M. Nielsen, and P. Johansen</editor>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="652" to="662" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Context-aware saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1915" to="1926" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image analysis using mathematical morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Sternberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE TPAMI, PAMI-9</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Leipzig: Veit, ISBN 978-0-8284-0061-9Reprinted by Chelsea Publishing Company in 1949</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hausdorff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1914" />
			<pubPlace>Germany</pubPlace>
		</imprint>
	</monogr>
	<note>Grundz?ge der Mengenlehre</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporally distributed networks for fast video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Is a green screen really necessary for real-time portrait matting? ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Anabranch network for camouflaged object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dfanet: Deep feature aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep interactive thin object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Robust highresolution video matting with temporal guidance. CoRR, abs/2108.11515</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fully understanding generic objects: Modeling, segmentation, and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Visual saliency transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Semantic segmentation using adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08408</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Non-local deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Simultaneously localize, segment and rank the camouflaged objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">How to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Camouflaged object segmentation with distraction mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021. 9</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Machine Learning for Aerial Image Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning to detect roads in highresolution aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Design and perceptual validation of performance measures for salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Movahedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Hyperseg: Patch-wise hypernetwork for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11582</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">In defense of pre-trained imagenet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Osserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The isoperimetric inequality. BAM</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1182" to="1238" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Open-world entity segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.14228</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Boundary-aware segmentation network for mobile and web applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Diagne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Sant&amp;apos;anna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Su?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.04704</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">U2-net: Going deeper with nested ustructure for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">R</forename><surname>Zaiane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>PR</publisher>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">An iterative procedure for the polygonal approximation of plane curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CGIP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="244" to="256" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Multiple object extraction from aerial imagery with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>EI</publisher>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Automatic portrait segmentation for image stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sachs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGF</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Animal camouflage analysis: Chameleon database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Skurowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abdulameer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>B?aszczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Depta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kornacki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kozie?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Unpublished Manuscript</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Topological structural analysis of digitized binary images by border following</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="46" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">A method of establishing groups of equal amplitude in plant sociology based on similarity of species content and its application to analyses of the vegetation on Danish commons. K?benhavn, I kommission hos E</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>S?rensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
			<pubPlace>Munksgaard, Denmark</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Disentangled high quality salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Information retrieval. London:Butterworths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<ptr target="http://www.dcs.gla.ac.uk/Keith/Preface.html" />
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Detect globally, refine locally: A novel approach to saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Salient object detection in the deep learning era: An indepth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Perimetric complexity of binary digital images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math J</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">F 3 net: Fusion, feedback and focus for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020. 9</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Building outline extraction directly using the u2-net semantic segmentation model from high-resolution aerial images and a comparison study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RS</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page">3187</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Optimizing connected component labeling algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Otoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shoshani</surname></persName>
		</author>
		<editor>J. M. Fitzpatrick and J. M. Reinhardt</editor>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>MI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07181</idno>
		<title level="m">Meticulous object segmentation</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06613</idno>
		<title level="m">High-resolution deep image matting</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Towards high-resolution salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Salient object detection by lossless feature reflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Learning uncertain convolutional features for accurate saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">A fast parallel algorithm for thinning digital patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Road extraction by deep residual u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GRSL</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="749" to="753" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Icnet for realtime semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2017. 9</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Egnet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Suppress and balance: A simple gated network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 9</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
