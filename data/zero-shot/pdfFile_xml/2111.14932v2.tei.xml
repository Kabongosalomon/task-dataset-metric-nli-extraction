<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning with Noisy Labels by Efficient Transition Matrix Estimation to Combat Label Miscorrection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Seong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hyperconnect Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hyperconnect Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghee</forename><surname>Choi</surname></persName>
							<email>kwanghee.choi@hpcnt.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hyperconnect Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonyoung</forename><surname>Yi</surname></persName>
							<email>joonyoung.yi@hpcnt.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hyperconnect Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buru</forename><surname>Chang</surname></persName>
							<email>buru.chang@hpcnt.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hyperconnect Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning with Noisy Labels by Efficient Transition Matrix Estimation to Combat Label Miscorrection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Learning with noisy labels</term>
					<term>Label correction</term>
					<term>Transition ma- trix estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies on learning with noisy labels have shown remarkable performance by exploiting a small clean dataset. In particular, model agnostic meta-learning-based label correction methods further improve performance by correcting noisy labels on the fly. However, there is no safeguard on the label miscorrection, resulting in unavoidable performance degradation. Moreover, every training step requires at least three back-propagations, significantly slowing down the training speed. To mitigate these issues, we propose a robust and efficient method, Fas-TEN, which learns a label transition matrix on the fly. Employing the transition matrix makes the classifier skeptical about all the corrected samples, which alleviates the miscorrection issue. We also introduce a two-head architecture to efficiently estimate the label transition matrix every iteration within a single back-propagation, so that the estimated matrix closely follows the shifting noise distribution induced by label correction. Extensive experiments demonstrate that our FasTEN shows the best performance in training efficiency while having comparable or better accuracy than existing methods, especially achieving state-of-the-art performance in a real-world noisy dataset, Clothing1M.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the last decade, supervised learning has achieved great success by leveraging an abundant amount of annotated data to solve various classification tasks such as image classification <ref type="bibr">[37]</ref>, object detection <ref type="bibr">[24]</ref>, and face recognition <ref type="bibr">[89]</ref>. It has been proven both theoretically and empirically that the performance of supervised learning-based classification models steadily improves as the size of annotated data increases <ref type="bibr">[27,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr">21]</ref>. However, we cannot avoid noisy labels due to its coarse-grained annotation sources <ref type="bibr">[40,</ref><ref type="bibr" target="#b31">121]</ref>, resulting in performance degradation <ref type="bibr" target="#b13">[14]</ref>. Many methods have been proposed to build a classifier that is robust to noisy labels. Unlike traditional methods <ref type="bibr">[68,</ref><ref type="bibr">94,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr">75]</ref> which assume that all the given labels are potentially corrupted, recently proposed methods utilize an inexpensively obtained small clean dataset to improve performance further. Based on the clean data set, loss correction methods <ref type="bibr">[40,</ref><ref type="bibr">101]</ref> reduce the influence of noisy labels by modifying loss functions and re-weighting methods [80, 85, 3, 23] penalize samples that are likely to be noisy labels. Especially, recent label correction methods <ref type="bibr">[105,</ref><ref type="bibr" target="#b31">121]</ref> achieve remarkable performance based on model-agnostic meta-learning (MAML) <ref type="bibr">[20]</ref>. These methods relabel noisy labels to directly reduce the noise level, raising the theoretical upper bound of the predictive performance (See Appendix A.1).</p><p>However, there are two challenges for these MAML-based label correction methods: <ref type="bibr" target="#b0">(1)</ref> The label correction methods blindly trust the already miscorrected labels. Erroneously corrected labels are often kept throughout the training, which causes the model to learn the miscorrected labels as ground-truth labels. Several studies <ref type="bibr">[67,</ref><ref type="bibr">105]</ref> attempt to tackle this through training techniques such as soft labels, whereas it does not fundamentally solve the problem. (2) MAMLbased methods are inherently slow in training, resulting in excessive computational overhead. The inefficiency comes from multiple training steps per single iteration of MAML-based methods, including virtual updates with inner optimization loops.</p><p>To alleviate these issues, we propose a robust and efficient method called FasTEN (Fast Transition Matrix Estimation for Learning with Noisy Labels). FasTEN efficiently estimates a transition matrix to learn with noisy labels while continuously correcting them on-the-fly. It is theoretically proven that the correctly estimated label transition matrix is useful to obtain a statistically consistent classifier from noisy labels <ref type="bibr" target="#b18">[108,</ref><ref type="bibr" target="#b23">113]</ref> (See Appendix A.2), i.e., more robust to noisy labels. To efficiently estimate the transition matrix, we adopt a twohead architecture that consists of two classifiers, a noisy and a clean classifier, with a shared feature extractor. For every iteration, the noisy classifier estimates the label transition matrix shifted by the label correction. On the other hand, the clean classifier is trained to be statistically consistent by leveraging the estimated transition matrix. Using the output of the clean classifier, Fas-TEN relabels noisy labels to reduce the noise level. Our proposed FasTEN has a safeguard for the miscorrected labels since it adaptively estimates the transition matrix on every iteration, so that the clean classifier stays equally skeptical towards all the corrected labels. Furthermore, our efficient method jointly optimizes the two-head architecture with only a single back-propagation for each iteration, boosting training speed. In this paper, we focus on solving the problem of class-dependent noisy labels <ref type="bibr">[25,</ref><ref type="bibr">76,</ref><ref type="bibr" target="#b29">119]</ref> (i.e., p(?|y, x) = p(?|y)), although the problem of instance-dependent noisy labels <ref type="bibr" target="#b17">[107,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">123]</ref> remains an important problem to be addressed.</p><p>Experimental results show that our method achieves state-of-the-art performance by a large margin on both the synthetic and real-world noisy label datasets, various noise levels of CIFAR [49] and Clothing1M <ref type="bibr" target="#b19">[109]</ref>, respectively. We demonstrate the exceptional training speed of our proposed FasTEN while achieving better performance compared to baselines, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Especially, although our FasTEN assumes only class-dependent noisy labels, it also achieves state-of-the-art performance in the Clothing 1M dataset which contains instance-dependent noisy labels. This experimental result supports recent observations that leveraging the accurately estimated transition matrix with small clean data is helpful for alleviating instance-dependent noise <ref type="bibr">[66,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b33">123,</ref><ref type="bibr">45]</ref> (See Appendix A.2). Finally, we conduct a thorough analysis to understand the inner mechanisms of our proposed method.</p><p>Our contribution in this paper is threefold: <ref type="bibr" target="#b0">(1)</ref> We propose a robust and efficient method that learns a transition matrix to learn with noisy labels while continuously correcting them on the fly. To the best of our knowledge, this is the first attempt to improve the label correction with the transition matrix estimation. <ref type="bibr" target="#b1">(2)</ref> Our proposed method boosts training speed by employing a two-head architecture so that the label transition matrix can be learned with a single back-propagation. <ref type="bibr" target="#b2">(3)</ref> Extensive experiments validate the efficacy of our proposed method in terms of both training speed and predictive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Learning with noisy labels assumes that labels in all the training samples are potentially corrupted. They can be further categorized as follows: various loss functions <ref type="bibr">[71,</ref><ref type="bibr">94,</ref><ref type="bibr">75,</ref><ref type="bibr">22,</ref><ref type="bibr" target="#b30">120,</ref><ref type="bibr">99,</ref><ref type="bibr">94,</ref><ref type="bibr">62,</ref><ref type="bibr">63,</ref><ref type="bibr">61,</ref><ref type="bibr">57,</ref><ref type="bibr" target="#b22">112,</ref><ref type="bibr">45]</ref>, regularizations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">46,</ref><ref type="bibr">38,</ref><ref type="bibr">42,</ref><ref type="bibr">65,</ref><ref type="bibr">62,</ref><ref type="bibr">35,</ref><ref type="bibr">87,</ref><ref type="bibr">59,</ref><ref type="bibr">57,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr">39,</ref><ref type="bibr">73,</ref><ref type="bibr">74,</ref><ref type="bibr">53,</ref><ref type="bibr" target="#b27">117,</ref><ref type="bibr">36,</ref><ref type="bibr">55,</ref><ref type="bibr">64]</ref>, re-weighting training samples <ref type="bibr">[80,</ref><ref type="bibr">44,</ref><ref type="bibr">67,</ref><ref type="bibr">60,</ref><ref type="bibr">100,</ref><ref type="bibr">93,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">43,</ref><ref type="bibr">102,</ref><ref type="bibr">104,</ref><ref type="bibr">104,</ref><ref type="bibr">77]</ref>, and correcting noisy labels [90, <ref type="bibr" target="#b24">114,</ref><ref type="bibr">34,</ref><ref type="bibr">86,</ref><ref type="bibr" target="#b32">122,</ref><ref type="bibr">30,</ref><ref type="bibr">47,</ref><ref type="bibr">106,</ref><ref type="bibr" target="#b29">119]</ref>. However, different losses or regularizations yield inferior performance to state-of-the-art methods <ref type="bibr" target="#b27">[117,</ref><ref type="bibr">67,</ref><ref type="bibr">42,</ref><ref type="bibr">55]</ref>, and re-weighting methods often filter out noisy but helpful samples for extracting features to show sub-optimal performance <ref type="bibr">[86,</ref><ref type="bibr">105,</ref><ref type="bibr" target="#b31">121,</ref><ref type="bibr">67,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr">58,</ref><ref type="bibr">84]</ref>. Label correction methods circumvent their shortcomings by relabeling so that the feature extractor leverage the corrected labels. However, label correction methods also have a limitation in that they are prone to propagate the error when miscorrected labels are continuously accumulated <ref type="bibr">[67,</ref><ref type="bibr">105,</ref><ref type="bibr" target="#b31">121]</ref>. Others correct the training loss by estimating a label transition matrix <ref type="bibr">[68,</ref><ref type="bibr">79,</ref><ref type="bibr">88,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">76,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b21">111,</ref><ref type="bibr" target="#b18">108,</ref><ref type="bibr" target="#b23">113]</ref> to build a statistically consistent classifier, where the methods need multiple training stages; e.g., include a separate pretraining stage. In this paper, we join a simple label correction method with estimating the label transition matrix to alleviate the miscorrection issue caused by miscorrected noisy labels, which only requires a single training stage. Learning with Noisy Label via Small Clean Dataset. Unlike traditional methods that use noisy datasets only <ref type="bibr">[76,</ref><ref type="bibr" target="#b23">113,</ref><ref type="bibr">56]</ref>, several recent studies argue that a small clean dataset is easily obtained by techniques such as image retrieval [78]; hence one can further devise a method that effectively leverages it. Many studies have successfully adapted the idea and shown massive performance improvement compared to the traditional methods. Early methods [40, <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b28">118]</ref> require multiple training stages where it hinders the training efficiency. Recent studies widely adopt MAML [20] to various strategies discussed above: sample re-weighting <ref type="bibr">[97,</ref><ref type="bibr">50,</ref><ref type="bibr">44,</ref><ref type="bibr">80,</ref><ref type="bibr">54,</ref><ref type="bibr">85]</ref>, label correction <ref type="bibr">[105,</ref><ref type="bibr" target="#b31">121]</ref>, and label transition matrix estimation [101]. These approaches first perform a virtual update with the noisy dataset, find optimal parameters using the clean dataset, and update the actual parameters by the found parameters. This virtual update process requires three back-propagations per iteration, leading to at least three times the computational cost. Our proposed label correction method estimates the label transition matrix using a batch drawn from the clean small dataset in a single back-propagation, greatly enhancing the training speed while showing comparable or better performance to existing state-of-the-art methods. Additional related works are described in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Existing label correction methods try to find and fix noisy labels to utilize them as clean samples in model training, where they can improve the classification performance by reducing the noise level of the whole training samples. However, erroneously corrected samples, i.e., clean samples deemed noisy, or vice versa, are often kept throughout the model training. Since current label correction methods blindly trust these miscorrected labels, this behavior degrades the classification performance under the noisy label situation <ref type="bibr">( ? 4.4)</ref>.</p><p>In this section, we show that the accurately estimated label transition matrix with the clean dataset alleviates the miscorrection problem of existing label correction methods. Further, we describe our efficient method estimating the label transition matrix for every training iteration while correcting noise labels. Our proposed method is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> and summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Batch Formation</head><p>We estimate the transition matrix to track the shifted noisy label distribution caused by label correction using a clean batch. To ensure the effective estimation  of the label transition matrix, we formulate the batch to have the same number of samples per class. We first compose the clean batch d with randomly chosen K samples for the entire N classes in the clean dataset D to get benefits from having a certain amount of clean samples for each class, as follows</p><formula xml:id="formula_0">Noisy Batch ? ,? (?) ? , ( ) ,? ( ) ? ? ? ? ? ,<label>(?)</label></formula><formula xml:id="formula_1">d = {(x n , y n )} KN n=1</formula><p>where x is an input and y ? R N is the clean label of x. A noisy batchd is randomly sampled from the noisy datasetD, as followsd = {(x n ,? n )} M n=1 wher? y ? R N is the noisy label ofx and M is the size of the noisy batch which we set as M = KN for simplicity. It is different from other methods <ref type="bibr" target="#b31">[121,</ref><ref type="bibr">105,</ref><ref type="bibr">85,</ref><ref type="bibr">80]</ref> based on meta-learning which randomly compose the clean batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transition Matrix Estimation</head><p>Each element T ij of the label transition matrix T ? R N ?N is defined as the probability of a clean label i to be corrupted as a noisy label j, i.e. T ij = p(? = j|y = i). It is well-known that a robust classifier can be obtained with the accurately estimated label transition matrix <ref type="bibr">[88,</ref><ref type="bibr">76,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b18">108,</ref><ref type="bibr" target="#b23">113]</ref>. We choose a simple but accurate method that directly estimates the posterior with a clean dataset [40, <ref type="bibr" target="#b18">108,</ref><ref type="bibr" target="#b23">113]</ref>, whereas there are other more sophisticated methods that estimate the label transition matrix <ref type="bibr">[68,</ref><ref type="bibr">79,</ref><ref type="bibr">88,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">25,</ref><ref type="bibr">76]</ref>. Following the assumption of the previous work [40, <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">107]</ref>, we also assume conditional independence of? and y given x: p(?|y) = p(?|y) p(x|?, y)dx = p(?|y, x)p(x|y)dx = p(?|x)p(x|y)dx. <ref type="bibr" target="#b0">(1)</ref> We design the transition matrix to be class-dependent, i.e., p(?|y, x) = p(?|y), following recent state-of-the-art methods <ref type="bibr">[60,</ref><ref type="bibr">82,</ref><ref type="bibr" target="#b18">108,</ref><ref type="bibr" target="#b23">113]</ref>. By parameterizing a feature extractor? and a linear classifier?, we obtain p(?|x) = f? ,? (x) where f? ,? Algorithm 1 Fast Transition Matrix Estimation for Learning with Noisy Labels</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(FasTEN)</head><p>Input: Clean dataset D, noisy datasetD. Hyper-parameters: Label correction threshold ?, Controllable loss ratio for noisy classifier ?.</p><p>Output: Clean classifier f ?,? where linear classifier ? and feature extractor ?. Randomly initialize common feature extractor ?, linear classifiers ?, and? for clean labels and noisy labels.</p><formula xml:id="formula_2">for each epoch i = 0, ? ? ? do for each iteration in epoch i do Sample mini-batch d ? D,d ?D. T ? (x,y)?d yf ?,? (x) ? diag ?1 (x,y)?d y L clean ? (x,y)?d LCE (f ?,? (x), y) + (x,?)?d LCE T ? f ?,? (x),? Lnoisy ? (x,?)?d LCE f ?,? (x),? ) D ? D ?d ? x, ? * , if max(f ?,? (x))&lt;? ?f ?,? (x)/max(f ?,? (x))?, otherwise (x,?) ?d</formula><p>Update ?, ?,? using ? ?,?,? (L clean + ?Lnoisy) with a single back-propagation. end for end for is the noisy classifier that consists of the linear classifier and the feature extractor trained only with the noisy labels. If the noisy classifier f? ,? gives a perfect prediction for the noisy data, we can estimate the transition probability p(?|y) using the clean samples (x, y) ? d as follows (See Appendix A.3 for details):</p><formula xml:id="formula_3">T ? ( (x,y)?d yf? ,? (x) ? )diag ?1 ( (x,y)?d y).<label>(2)</label></formula><p>We emphasize the importance of the transition matrix estimation, as its accuracy determines the bounds of the generalization error of the classifier <ref type="bibr" target="#b18">[108]</ref>. However, the limited number of clean samples inside a single batch may yield an inaccurate transition matrix, even with the ideal f? ,? . We analyze the upper bound of the estimation error as follows: </p><formula xml:id="formula_4">p T ij ? T ij &gt; ? ? N LB( ? 2H log 2 + 1)?? H?1 h=1? i |D| + ? log(?) 2|D| + 2 exp ?2? 2 K .<label>(3)</label></formula><p>Proof. See Appendix A.4.</p><p>Although the upper bound of the estimation error of the transition matrix is affected by the batch size K, we empirically verify that small K does not necessarily harm the classification performance (See Appendix C.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning with Estimated Transition Matrix</head><p>A clean classifier f ?,? is trained with the estimated transition matrix T :</p><formula xml:id="formula_5">L clean = (x,y)?d L CE (f ?,? (x), y) + (x,?)?d L CE T ? f ?,? (x),? ,<label>(4)</label></formula><p>given the cross-entropy loss function L CE , where the feature extractor ? and the linear classifier ? form the clean classifier f ?,? which estimates clean labels. If T is correctly estimated, the clean classifier f ?,? becomes statistically consistent <ref type="bibr">[88,</ref><ref type="bibr">76,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b18">108,</ref><ref type="bibr" target="#b23">113]</ref>. This approach makes the clean classifier skeptical towards corrected labels, hence avoiding the miscorrection issue.</p><p>On the other hand, the noisy classifier f? ,? is trained to model the noisy label distribution.</p><formula xml:id="formula_6">L noisy = (x,?)?d L CE f? ,? (x),?<label>(5)</label></formula><p>We emphasize that updating the noisy classifier f? ,? every iteration is critical as it can adaptively model the ever-changing noisy label distribution on the fly, where the distribution constantly shifts as the noisy labels are actively corrected to reduce the noise level (See ? 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Efficient Training</head><p>Similar to <ref type="bibr">[98,</ref><ref type="bibr">44]</ref>, we propose an efficient training scheme through weight sharing via two-head architecture, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Where the architecture closely resembles the ones of [98, 44], our two-head architecture only shares the feature extractor ? =?. Unlike the shared feature extractor, our architecture does not share the linear classifier since modeling both noisy and clean data distribution with a single linear classifier is impractical. Based on the two-head architecture, the given samples require only a single inference on the feature extractor for (1) training classifiers, (2) estimating the transition matrix, and (3) correcting labels, which makes model training highly efficient. Thus, we define the clean and noisy classifier as f ?,? and f ?,? , respectively, to produce our final objective function L:</p><formula xml:id="formula_7">L = L clean + ?L noisy<label>(6)</label></formula><p>where ? is a loss balancing factor. In order to prevent over-fitting ond, we introduce ? to the final objective function. We search for the optimal hyperparameter ? for all of our experiments (See Appendix C.7  <ref type="figure" target="#fig_0">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Label Correction</head><p>In this paper, we focus on the efficient, on-the-fly estimation of the label transition matrix to combat label miscorrection. To further demonstrate the effectiveness of our method, we employ a na?ve label correction strategy where we feed each noisy set sample x ?d to the clean classifier f ?,? to produce a probability vector. If the maximum probability max(f ?,? (x)) is bigger than the threshold ?, we correct its label to a more probable label. This strategy relies only on the most recent prediction of the model mid-training, so the decision is prone to change. Formally, we can describe the relabeled?,</p><formula xml:id="formula_8">y = ? * , if max(f ?,? (x)) &lt; ? ?f ?,? (x)/ max(f ?,? (x))?, otherwise<label>(7)</label></formula><p>where ??? denotes floor function and? * denotes the original label fromd.? * differs from?; the former denotes the original label from the noisy dataset, whereas the latter is continuously corrected by the above strategy. Even with this simple strategy, our model shows better performance compared to the stateof-the-art methods. The experimental results suggest that replacing this strategy may further improve the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate our proposed learning method, FasTEN, in terms of predictive performance ( ? 4.1) and efficiency ( ? 4.2). We also validate the label correction performance to demonstrate that our method is better in correcting noisy labels ( ? 4.3 and Appendix C.6) and experimentally show the robustness of our proposed method towards miscorrected labels. ( ? 4.4). We further analyze whether our method successfully estimates the label transition matrix in the case where the label correction shifts the true label transition matrix ( ? 4.5) or not ( ? 4.6). Additional experimental results and further analyses are described in Appendix C. We provide the source codes 1 for the reproduction of the experiments conducted in this paper. Baselines using the small clean dataset. We deliberately choose the baselines that utilize the small clean dataset in learning with noisy labels. These baselines are categorized in the following three types.   <ref type="bibr">[49]</ref> have been widely adopted to assess the robustness of the methods to noisy labels. Since CIFAR-10/100 are known as clean datasets, labels are synthetically manipulated to contain noisy labels, injecting two types of noise: symmetric and asymmetric. Symmetric: The labels are randomly flipped with uniform distribution. Asymmetric: the labels are flipped with class-dependent distribution, following the evaluation protocol of <ref type="bibr">[76,</ref><ref type="bibr" target="#b21">111]</ref>. We claim that most studies report the performance highly overfitted to the test set without hyperparameter tuning on the validation set [105, 53, 73, 74]. Moreover, baseline models employ different backbone networks, making it challenging to dissect the performance improvement whether it originated from each method or the backbone networks. Therefore, we first extract 5K samples as the validation set from the training set containing 50K samples and further extract 1K samples as the clean dataset. Then, we unify the backbone network as ResNet-34 <ref type="bibr">[37]</ref>, which is widely adopted in various baselines <ref type="bibr">[105,</ref><ref type="bibr">59]</ref>. Note that we do our best to maintain the experimental settings of each method, including the hyperparameters written in the original paper. Detailed settings are deferred to Appendix B.</p><p>Results. <ref type="table" target="#tab_4">Table 1</ref> summarizes the evaluation results on CIFAR-10/100. For both CIFAR-10/-100, our proposed FasTEN achieves state-of-the-art performance on various noise levels within 95% confidence intervals. Especially, under a high noise level (80%), our FasTEN considerably outperforms the baselines with small variance on performance, which implies the robustness of our method [52, 51]. These results demonstrate that our proposed method performs well in learning with noisy labels, especially considering its training efficiency (See ? 4.2). Clothing1M with Real-world Noise. Clothing1M <ref type="bibr" target="#b19">[109]</ref> is a noisy real-world dataset that consists of one million samples with additional 47K human-annotated clean samples. We use its original splits of clean and noisy data. For a fair comparison, we employ ResNet-50 architecture pretrained with the ImageNet dataset [18] for the initial backbone architecture. Evaluation results on Cloth-ing1M are summarized in <ref type="table" target="#tab_5">Table 2</ref>.</p><p>Further baselines. We further compare our proposed FasTEN with additional baselines that have already reported their performance on Clothing1M dataset. Since the data split of Clothing1M dataset is the same for all the baselines, we simply obtain the performance of the baselines from their original papers and report the performance in Results. As shown in <ref type="table" target="#tab_5">Table 2</ref>, our proposed FasTEN achieves remarkable performance on Clothing1M which contains instance-dependent noisy labels, beating the baselines by a large margin. This evaluation result indicates that our proposed FasTEN is more applicable in real-world problems where label corruption frequently occurs, although it does not directly target to address the problem of instance-dependent noisy labels. Similar to previous observations [66, 40], we suspect that using the transition matrix seems to combat instance-dependent noise to some extent. Also, not only that our method shows superior performance over all the baselines that use the small clean set, but it also surpasses the semi-supervised learning-based methods (DivideMix and AugDesc) without any complex augmentation techniques. Finally, FasTEN shows better performance than T-Revision, causalNL, IF, and VolMinNet, which estimate the transition matrix without the small clean data (this is not a fair comparison). This result indicates that using the small clean data is effective in estimating the transition matrix accurately, leading to performance improvement eventually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Time Comparison</head><p>Setup. To verify the efficiency of our proposed FasTEN, we compare it with the baselines in terms of accuracy by total training time. Total training time is measured on CIFAR-10/-100, respectively, with a single RTX 2080Ti GPU. Test accuracy shows the predictive performance on CIFAR-10/-100 with 20% and 80% symmetric noise ratios, the mildest and most severe noise conditions, respectively. Since Deep kNN and GLC require multiple training stages, the summation of all the hours needed for each training phase is provided.</p><p>Results. <ref type="figure" target="#fig_0">Figure 1</ref> shows that our FasTEN, which learns the label transition matrix with the single back-propagation in the single-training stage, makes model training more efficient than other baselines that need multiple back-propagation or multiple training stages while showing better performance. <ref type="table" target="#tab_7">Table 3</ref> shows the total training hours of each baseline, including our FasTEN. Our method provides the training time speedup of minimum ?1.49 to maximum ?6.64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Label Correction Performance Comparison</head><p>We analyze the predictive performance of the baseline methods on all the training samples (Overall) and the wrongly labeled subset of them (Incorrect), respectively. <ref type="table">Table 4</ref> demonstrates that our method can successfully correct the noisy labels, where using the label correction further improves the correction performance. This also implies that our FasTEN may be helpful in further cleansing the noisy training set. We also compare the performance between Overall and Incorrect cases. Reweighting (L2RW, MW-Net, Deep kNN) and transition matrix estimation-based methods (GLC, MLoC) show similar performance between two cases: Overall and Incorrect. However, the performance of the meta-model of MLaC is worse for the Incorrect case, which indicates that the correction from the meta-model <ref type="table">Table 4</ref>: Label correction performance comparison on CIFAR-10 with symmetric 80% noise. Accuracy (%) and Negative Log Likelihood (NLL) loss are calculated using the true labels before the synthetic noise is injected. Performance of the trained model on all training samples (Overall) and incorrectly labeled training samples (Incorrect) is measured. ? denotes performance extracted from the meta model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Asymmetric 40%</head><p>(1/N )</p><formula xml:id="formula_9">N i=1 Tii (1/N ) N i=1 Tii 0 20 40 60 Epoch 0.4 0.6 0.8</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symmetric 80%</head><p>(1/N ) is less effective where the labels are wrong. Also, notable underperformance of the meta-model of MSLC may indicate the inefficacy of the meta-model. We also analyze the meta-model of the re-weighting methods in the Appendix C.6, where they do not distinguish the wrongly labeled samples well.</p><formula xml:id="formula_10">N i=1 Tii (1/N ) N i=1 Tii (b) Transition matrix estimation</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Robustness to Miscorrection: What Happens if Labels are Wrongly Corrected?</head><p>This subsection illustrates the robustness of our label correction method to miscorrected labels by comparing it with other label correction methods (MLaC and MSLC) which blindly trust the miscorrected labels as the ground-truth, where we verify the imperfect corrections (See ? 4.3). We examine how much this behavior deteriorates the predictive performance.</p><p>Setup. We experiment on CIFAR-10 with symmetric 80% noise where there are a maximum number of noisy labels to correct. To simulate the miscorrection, we perturb the corrected labels by injecting artificial noise. We control the degree of random perturbation to observe the robustness of each method on various levels of miscorrection. We further assess the robustness of our FasTEN and MSLC by comparing it with the performance obtained without label correction.</p><p>Results. <ref type="figure" target="#fig_2">Figure 3a</ref> shows our proposed FasTEN outperforms MLaC and MSLC on all the degrees of the random perturbation. MLaC shows steep performance degradation when perturbation worsens, i.e., there are more miscorrected labels. This observation reveals the susceptibility of MLaC. MSLC shows trivial performance gains when labels are corrected, implying that it is not using the full benefits of label correction. Furthermore, when highly perturbed, MSLC performance worsens if it attempts to correct the labels. In contrast, the label correction of our FasTEN improves performance even in harsh situations. FasTEN does not degrade performance even if the correction becomes useless (100% perturbation). These observations show that our FasTEN builds a more robust classifier to miscorrected labels through its efficient estimation of the label transition matrix, acting as a safeguard combating the miscorrected labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">On-the-fly Estimation of the Label Transition Matrix</head><p>Our proposed FasTEN newly estimates the label transition matrix on every iteration, where the matrix is constantly shifted by label correction. To assess the matrix estimation quality, we compare it with the true label transition matrix.</p><p>Setup. We train FasTEN on CIFAR-10 with symmetric 80% and asymmetric 40% noise, which are harsh conditions on symmetric and asymmetric noise injection, respectively. We compare the estimated label transition matrix T with the true label transition matrix T by observing the mean of diagonal term values for each epoch. The mean of the diagonal term in the transition matrix represents the average of the probability that a sample is mapped to a clean label.</p><p>Results. <ref type="figure" target="#fig_2">Figure 3b</ref> shows the overall tendency of the estimated transition matrix (red) to follow the true label matrix (blue). In the asymmetric 40% setting, diagonal term values of the true label transition matrix T gradually increases (blue), which indicates the dataset is cleansed by the label correction. However, in the symmetric 80% case, diagonal term values of the true transition matrix T decreases at the middle of the training. As we maintain the fixed threshold ?, the total number of corrected samples decreases. Nonetheless, we can conclude that the transition matrix is successfully estimated on shifting noise levels.</p><p>Additionally, we observe that the estimated transition matrix T shows higher mean values, i.e., being overconfident on the clean dataset samples. Theoretically, f ?,? should correctly approximate the noisy label distribution given enough number of clean samples (See Appendix A.4), but it seems to be overfitting to the clean dataset in practice. This observation is consistent with the popular belief that neural networks tend to learn clean samples first and noisy samples later <ref type="bibr" target="#b0">[1]</ref>. For better matrix estimation to yield a more robust classifier [31, <ref type="bibr" target="#b18">108,</ref><ref type="bibr">67,</ref><ref type="bibr" target="#b23">113]</ref>, it appears that we need to address the overfitting through additional components. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Empirical Convergence Analysis on Estimating the Label Transition Matrix</head><p>Setup. This section analyzes the convergence of estimation error between the true label transition matrix T and the estimated transition matrix T , comparing our FasTEN to other methods, MLoC and GLC, which learn the transition matrix. For fair comparison, we exclude the label correction for our method.</p><p>Results. <ref type="figure" target="#fig_3">Figure 4</ref> shows the difference between the probability distribution of the true label transition matrix T and the estimated transition matrix T for each iteration, where Pearson ? 2 -divergence is used to measure the discrepancy between the two matrices. GLC error remains fixed (dotted line) as it estimates the transition matrix only once in the entire learning process. The decrease of MLoC error is extremely slow (blue line), implying the high dependence of the initialization of T and its ineffectiveness on estimation. Although our FasTEN does not require multiple stages of training and produces the single mini-batch-based estimate every iteration, it shows fast convergence with a similar estimation error to GLC, which uses all the available data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a robust and efficient method, FasTEN, which efficiently learns a label transition matrix that mitigates the label miscorrection problem of existing label correction methods. Our proposed FasTEN accurately estimates the label transition matrix using a small clean dataset even if the samples are miscorrected. Moreover, our FasTEN is highly efficient compared to existing methods since it requires single back-propagation through two-head architecture and needs only a single training stage. Extensive experiments show that our method is the fastest and the most robust classifier. Especially, our method achieves remarkable performance on both the real-world noise dataset (Clothing1M) and the synthetic dataset on various noise levels (CIFAR). The detailed analysis shows that our method is robust to miscorrected labels by efficiently estimating the transition matrix shifted by the label correction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Theoretical Analysis</head><p>In this section, we provide a theoretical analysis of our proposed FasTEN. First, we formally establish the need for label correction (Appendix A.1). Then, we provide a formal background for a statistically consistent classifier (Appendix A.2) and show the detailed calculation on the estimation of transition matrix T through forward propagation (Appendix A.3). Finally, we prove that T estimation error of our method are upper bounded (Appendix A.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Motivation: Need for Label Correction</head><p>Reducing the noise level of a dataset is crucial in learning with noisy labels both empirically [19, 121, 105, 40] and theoretically <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12]</ref>. Following <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">80,</ref><ref type="bibr">44,</ref><ref type="bibr">64,</ref><ref type="bibr">33]</ref>, the true transition matrix T in symmetric noise with noise level ? (&lt; 1) is defined as follows:</p><formula xml:id="formula_11">T ij = 1 ? ( (N ? 1) /N) ?, for i = j. ? /N, otherwise.<label>(8)</label></formula><p>Correspondingly, the true transition matrix T in asymmetric noise with noise level ? (&lt; 1 /2) is defined as follows:</p><formula xml:id="formula_12">T ij = ? ? ? ? ? 1 ? ?, for i = j. ?, for some i ? = j. 0, otherwise.<label>(9)</label></formula><p>We employ these noise schemes in our CIFAR-10 experiments. Under the assumption that the label class is balanced, <ref type="bibr" target="#b13">[14]</ref> prove that the upper bound of test accuracy for the symmetric and asymmetric noise is as follows:</p><p>( (N ? 1) /N) ? 2 ? 2 ( (N ? 1) /N) ? + 1, for symmetric noise. 2? 2 ? 2? + 1, for asymmetric noise.</p><p>Eq. 10 shows quadratic (convex) functions of ?. In the case of the symmetric noise, test accuracy is minimized at the ? = 1. Similarly, with asymmetric noise, test accuracy is minimized at the ? = 1 /2. Without additional assumptions, asymmetric noise of ? &gt; 1 /2 cannot be learned as over half of the training data have wrong labels <ref type="bibr">[33]</ref>. Hence, the test accuracy always decreases as ? increases in the feasible bound of ?. Therefore, reducing the noise level of a dataset plays a vital role in increasing the achievable test accuracy.</p><p>How to Reduce the Noise Level of a Dataset Two approaches are commonly used to reduce the noise level of a dataset: re-weighting samples and label correction. Sample re-weighting reduces the noise level by eliminating noisy samples during the model training, whereas label correction directly cleans up the dataset. Recently, label correction methods have shown notable results compared to sample re-weighting methods. <ref type="bibr">[86,</ref><ref type="bibr">105,</ref><ref type="bibr" target="#b31">121,</ref><ref type="bibr">67,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr">58,</ref><ref type="bibr">84]</ref> claim that re-weighting might show sub-optimal performance by filtering out noisy samples, which might aid in training feature extractors.</p><p>Theoretical Inspired Explanation of the Superiority of Label Correction Here, we provide a more theoretically motivated explanation of the above claim. We explore the reason behind the superior performance of label correction compared to sample re-weighting. <ref type="bibr" target="#b13">[14]</ref> considers only the upper bound of test accuracy according to the noise level while ignoring the effect on the number of samples on a generalization error while <ref type="bibr" target="#b11">[12]</ref> does not consider deep networks. We aim to exhibit the superiority of label correction by presenting the generalization error considering both the number of samples and the noise level. We argue that both the noise level and the number of training samples are critical in determining the generalization error.</p><p>For simplicity, our explanation assumes binary classification with asymmetric noise with a level ?. We employ the VC dimension framework <ref type="bibr">[96,</ref><ref type="bibr">95,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">116]</ref> to describe the various methods for learning with noisy labels, although the framework provides a loose bound. Further investigation on a tighter bound using the Rademacher complexity or considering the multi-class classification is suggested for future research. Under clean training data distribution D and clean true data distribution D * , the VC dimension framework presents the following bound.</p><formula xml:id="formula_14">p (|E D (f ) ? E D * (f )| &gt; ?) ? 4 (2|D|) d V C exp ? 1 8 ? 2 |D| ,<label>(11)</label></formula><p>where d V C is the VC dimension and E D (f ) is the expectation of error for function f regarding the data distribution D. If the VC dimension d V C is bounded (or finite), convergence is guaranteed because the upper bound decreases exponentially as the size of the dataset increases. Now, we observe a noisy datasetD rather than a clean dataset D. With the triangular inequality and the definition of ?, the following inequalities hold.</p><formula xml:id="formula_15">p (|ED(f ) ? E D * (f )| &gt; ?) (12) = p (|ED(f ) ? E D (f ) + E D (f ) ? E D * (f )| &gt; ?) (13) ? p (|ED(f ) ? E D (f )| &gt; ?) + p (|E D (f ) ? E D * (f )| &gt; ?) (14) ? ? + 4 (2|D|) d V C exp ? 1 8 ? 2 |D|<label>(15)</label></formula><p>Even if this theoretical bound is loose, we argue that label correction shows better performance than re-weighting samples. As the dataset gets noisier, the number of filtered out samples by the re-weighting methods will also increase, resulting in a drastic reduction of the number of training samples. However, as aforementioned in Section 1, label correction holds an inherent problem of error propagation. We now explain the theoretical background on how the transition matrix acts as a safeguard for the label correction on our FasTEN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Background: Statistically Consistent Classification</head><p>It is well known that the label transition matrix T can be used to train statistically consistent classifiers in the presence of noisy labels [67, <ref type="bibr" target="#b18">108,</ref><ref type="bibr" target="#b23">113]</ref>. A statistically consistent classifier is a classifier which guarantees the convergence to an optimal classifier when the number of data samples increases indefinitely. <ref type="figure" target="#fig_0">Following [31, 108, 113</ref>, 67], we describe the consistency of empirical risk to yield the consistency of the classifier.</p><p>Statistically Consistent Empirical Risk Multi-class classification aims to train the hypothesis H, which estimates a label y given an input x. Given the deep neural network f ?,? , a hypothesis H is commonly defined as follows:</p><formula xml:id="formula_16">H(x) = arg max n?{1,...,N } f ?,? (x)| n .<label>(16)</label></formula><p>With the true sample distribution D * , the expected risk R for H is defined as follows:</p><formula xml:id="formula_17">R(H) = min H E (x,y)?D * [L(H(x), y)]<label>(17)</label></formula><p>Under the distribution D * is unknown, the optimal hypothesis H should minimize R. Since the risk of the optimal hypothesis is difficult to calculate, the empirical risk is usually used for approximation via training dataset D. The definition of empirical risk is as follows:</p><formula xml:id="formula_18">R |D| (H) = E (x,y)?D [L(H(x), y)] = 1 |D| (x,y)?D L(H(x), y).<label>(18)</label></formula><p>Following equation holds for the statistically consistent empirical risk :</p><formula xml:id="formula_19">R(H) = lim |D|?? R |D| (H),<label>(19)</label></formula><p>where it is common to assume that D is sampled from D * as independent and identically distributed (i.i.d) random variables <ref type="bibr" target="#b18">[108,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr">31,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Statistically Consistent Classifier Suppose an ideal zero-one loss function L * (where it cannot be used in reality because its differentiation is impossible) <ref type="bibr" target="#b4">[5]</ref>:</p><formula xml:id="formula_20">L * (H(x) = y) = 1 {H(x)? =y} .<label>(20)</label></formula><p>1 {?} is an indicator function that outputs 1 if H(x) ? = y and 0 otherwise. If the class of the hypothesis H is large enough [69], the optimal hypothesis to minimize the expected risk R(H) corresponds to the Bayes classifier <ref type="bibr" target="#b4">[5]</ref> as follows:</p><formula xml:id="formula_21">H(x) = arg max n?{1,...,N } p(y = n|x)<label>(21)</label></formula><p>Many classification loss functions in modern machine learning are proven to be classification-calibrated <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">83]</ref>, i.e., the classification-calibrated loss function leads to a similar prediction to that of L * when |D| is sufficiently large <ref type="bibr">[69,</ref><ref type="bibr">95]</ref>. For example, the hinge loss is proven to be classification calibrated <ref type="bibr" target="#b20">[110]</ref>, and the cross-entropy loss with softmax function is empirically classificationcalibrated <ref type="bibr">[29]</ref>. The classifier f ?,? (x) is said to be statistically consistent when the classifier converges to the probability p(y|x) by minimizing the empirical risk R |D| (H). Note that being risk consistent makes classifier consistent, but not vice versa <ref type="bibr" target="#b18">[108]</ref>.</p><p>Statistically Consistent Classifier in Noisy Labels The empirical risk R |D| (H) of a noisy datasetD is as follows.</p><formula xml:id="formula_22">R| D| (H) = E (x,y)?D [L(H(x), y)] = 1 |D| (x,y)?D L(H(x), y)<label>(22)</label></formula><p>Since the statistically consistent classifier f ?,? (x) converges to p(y|x), we can accept f ?,? (x) to approximate p(y|x). Given the definition of the transition matrix p(?|x) = T ? p(y|x), a hypothesis with a noisy datasetH is defined as follows:</p><formula xml:id="formula_23">H(x) = arg max n?{1,...,N } T ? f ?,? (x)| n (23)</formula><p>Hence, minimizing the following empirical risk R| D| (H) using only the noisy datasetD leads to a consistent classifier f ?,? (x) <ref type="bibr" target="#b18">[108]</ref>. </p><p>In other words, f ?,? converges to the optimal classifier for the clean data when the sample size of the noisy dataset becomes infinitely large. Although other lines of research guarantee that maximizing accuracy in noisy data distribution maximizes accuracy in clean data distribution even without the transition matrix <ref type="bibr" target="#b14">[15]</ref>, loss correction via the transition matrix is still an effective consistent classifier training scheme. For this reason, a line of work in learning with noisy labels via the transition matrix attempts to train a statistically consistent classifier by an additional layer modeling the transition matrix preceded by the softmax layer [25, 76, 92, 115, 68, 79, 88]. Incidentally, it is known that modifying the loss function using the transition matrix has a degree of handling instance-dependent label corruption <ref type="bibr">[66,</ref><ref type="bibr">40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistically Consistent Classifier in Noisy Labels with Small Clean Dataset</head><p>We exploit a small number of clean data as in <ref type="bibr">[20,</ref><ref type="bibr">97,</ref><ref type="bibr">50,</ref><ref type="bibr">44,</ref><ref type="bibr">80,</ref><ref type="bibr">54,</ref><ref type="bibr">40,</ref><ref type="bibr">85,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b28">118,</ref><ref type="bibr" target="#b31">121,</ref><ref type="bibr">105</ref>, 101] while disjointing the clean D and noisy datasetD. It is trivial that a statistically consistent classifier in exploiting a clean set can be obtained by minimizing the following empirical risk:  </p><formula xml:id="formula_25">R |D| (H) + R| D| (H) = E (x,</formula><formula xml:id="formula_26">= 1 |d i | (x,y)?di f ?,? (x) j (30) = 1 |d i | ? ? ? (x,y)?di yf? ,? (x) ? (i,j) ? ? ? (31) = ? ? ? (x,y)?di yf? ,? (x) ? (i,j) ? ? ? 1 |d i | (32) = ? ? ? (x,y)?di yf? ,? (x) ? (i,j) ? ? ? ? ? ? diag ?1 ? ? (x,y)?d y ? ? (i,j) ? ? ? (33)<label>(29)</label></formula><p>Without loss of generality, T can be written as follows:</p><formula xml:id="formula_27">T = ? ? (x,y)?d yf? ,? (x) ? ? ? diag ?1 ? ? (x,y)?d y ? ? .<label>(34)</label></formula><p>A.4 Proof of Theorem 1</p><p>In this section, we prove under strong assumptions (Theorem 2) followed by milder assumptions (Theorem 1). Theorem 2 estimates the upper bound of the error on transition matrix T , assuming the ideal situation where p(?|x) is perfectly parameterized to f? ,? (x).</p><formula xml:id="formula_28">Theorem 2. Assuming p(?|x) = f? ,? (x), for ? ? 0, p T ij ? T ij &gt; ? ? 2 exp ?2? 2 K .<label>(35)</label></formula><p>Proof. If p(?|x) = f? ,? (x), then p E T ? T &gt; ? = 0. With the triangular and Hoeffding inequality, the following holds:</p><formula xml:id="formula_29">p T ij ? T ij &gt; ? (36) = p T ij ? E T ij + E T ij ? T ij &gt; ? (37) ? p E T ij ? T ij &gt; ? + p T ij ? E T ij &gt; ? (38) = p E T ij ? T ij &gt; ? + 2 exp ?2? 2 K (39) = 2 exp ?2? 2 K .<label>(40)</label></formula><p>With Theorem 2 alone, we can see that the estimation error of transition matrix T decreases exponentially as K (the number of samples per class) increases, as we mentioned in Theorem 1 of Section 3.2.</p><p>We assume the hypothetical case that p(?|x) could flawlessly model f? ,? (x), but the assumption does not hold in practice. Several lemmas are established in order to prove Theorem 1 under more relaxed assumptions. If p(?|x) ? = f? ,? (x), then p(|E[ T ] ? T | &gt; ?) ? = 0. We focus on examining the upper bound of p(|E[ T ] ? T | &gt; ?) under the relaxed assumption. The upper bound of T (which is equivalent to f? ,? (x)) is strictly 1 since it is a probability. By applying Mc-Diarmid's concentration inequality <ref type="bibr" target="#b8">[9]</ref>, the following inequality is established:</p><formula xml:id="formula_30">p E T ij ? T ij &gt; ? ? E ? ? ? sup H 1 |D| (x,?)?D ? x L(H(x),?) ? ? + log(1/?) 2|D| (41)</formula><p>where ? is an i.i.d Rademacher random variable <ref type="bibr">[70]</ref> and H is a hypothesis. We estimate the upper bound of the estimation error of T by assuming H is constructed using deep neural networks. A deep neural networks hypothesis H ? is defined as follows.</p><formula xml:id="formula_31">H ? (x) =?A H?1 (? H?1 A H?2 (... A 1 (? 1 x))) ? R N<label>(42)</label></formula><p>where H is the depth of deep neural networks and A i is the i-th activation function. When the function class is limited with deep neural networks, the following lemma holds by borrowing the results of <ref type="bibr" target="#b18">[108]</ref>.</p><p>Lemma 1. Suppose ? is an i.i.d Rademacher random variable and L is the cross-entropy loss function which is L-Lipschitz continuous with respect to H ? ,</p><formula xml:id="formula_32">E ? ? ? sup H 1 |D| (x,?)?D ? x L(H(x),?) ? ? ? N LE ? ? ? sup H ? 1 |D| (x,?)?D ? x H ? (x) ? ? (43)</formula><p>where H ? is a hypothesis belonging to the function class of deep neural networks.</p><p>As opposed to using the VC dimension framework in Section 1, this section uses the Rademacher complexity framework <ref type="bibr" target="#b5">[6]</ref> to assess the upper bounds of our method. Hypothesis complexity of deep neural networks via Rademacher complexity is broadly studied in <ref type="bibr" target="#b18">[108,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr">26,</ref><ref type="bibr">72]</ref>. In particular, [26] proves the following lemma: Lemma 2. Assume the Frobenius norm of the weight matrices? 1 , ...,? H?1 ,? are at most? 1 , ...,? H?1 ,? for H-layer neural networks f? ,? . Let the activation functions be 1-Lipschitz, positive-homogeneous, and applied element-wise (such as the ReLU). Let ? is an i.i.d Rademacher random variable. Let x is upper bounded by B, i.e., for any x ? X , ?x? ? B. Then, for ? ? 0</p><formula xml:id="formula_33">E ? ? ? sup H ? 1 |D| (x,?)?D ? x H ? (x) ? ? ? B( ? 2H log 2 + 1)?? H?1 h=1? i |D| .<label>(44)</label></formula><p>Now, we can complete the proof of Theorem 1.</p><p>Proof. With the triangular inequality, Hoeffding inequality, Theorem 2, Lemma 1, and 2, the following holds.</p><formula xml:id="formula_34">p T ij ? T ij &gt; ? (45) = p T ij ? E T ij + E T ij ? T ij &gt; ? (46) ? p E T ij ? T ij &gt; ? + p T ij ? E T ij &gt; ? (47) = p E T ij ? T ij &gt; ? + 2 exp ?2? 2 K (48) ? E ? ? ? sup H 1 |D| (x,?)?D ? x L(H(x),?) ? ? + log( 1 /?) 2|D| + 2 exp ?2? 2 K (49) ? N LE ? ? ? sup H ? 1 |D| (x,?)?D ? x H ? (x) ? ? + log( 1 /?) 2|D| + 2 exp ?2? 2 K (50) ? N LB( ? 2H log 2 + 1)?? H?1 h=1? i |D| + log( 1 /?) 2|D| + 2 exp ?2? 2 K<label>(51)</label></formula><p>Theorem 1 and 2 state that the estimation error of transition matrix T is reduced with a larger K. However, we experimentally verify that K = 1 is enough for achieving comparable performance (See Appendix C.7). We end this section by enumerating the limitations of our theoretical analysis. (i) Although our method is based on a multi-head architecture, a clean and a noisy classifier are trained simultaneously, where only the training of the noisy classifier is considered in the theoretical analysis. (ii) We failed to make the upper bound tight for Theorem 1 and 2. Additional assumptions like <ref type="bibr" target="#b11">[12]</ref> may yield tighter bound. We conjecture that our method works empirically well for K = 1 since the upper bound is loose. (iii) The data distribution on a noisy classifier changes in every iteration due to simultaneously corrected labels. However, we assume that the data distribution is stationary in the proof. In order to make our theoretical assumption more adequate for our method, it is necessary to examine the situation under changing data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Datasets</head><p>As shown in <ref type="table" target="#tab_10">Table 5</ref>, we use bigger noisy dataset (noisy-train) and smaller clean dataset (clean-train) for training. Validation set is used for obtaining the best model. Since CIFAR datasets do not have the validation set, we split 10% of the entire training set as the validation set. Thus, experimental results may differ from the results of their papers. For Clothing1M, we use its original data split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Mini-batch Construction</head><p>We sample the mini-batches from both clean and noisy datasets. For the clean dataset, we construct the mini-batch to have the same number of instances per class for clean samplers, whereas the mini-batch of the noisy set is randomly sampled. We choose the batch size 100 for both CIFAR-10 and CIFAR-100, a total of 200 images used per iteration. As the number of classes is 10 and 100, 10 and 1 image(s) are used per class for the clean batch, respectively. We choose the batch size 42 for each noisy and clean set on Clothing1M dataset with 14 classes so that 84 images are used per iteration, where 3 samples are used per class for the clean batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Detailed Training Procedure</head><p>CIFAR-10/100 dataset Here, we describe the detailed training procedure of our baselines on CIFAR-10/100 dataset <ref type="bibr">[49]</ref>. For all baselines except Deep kNN, we use SGD optimizer with an initial learning rate of 1e-1. For Deep kNN <ref type="bibr" target="#b2">[3]</ref>, we use Adam optimizer [48] and set the initial learning rate to 1e-3. We follow the experimental settings described in each corresponding papers as much as possible to obtain performance fairly.</p><p>L2RW: When training the model, we decay the learning rate to 1e-2 and 1e-3 at 40 and 60 epochs, with a total of 80 epochs. MW-Net: For the total of 60 epochs, we decay the learning rate by a factor of 10 at 40 and 50 epochs, respectively. Deep kNN: Since it has multiple training stages, we first train two independent models with only the clean dataset D and sum of the clean and noisy dataset D ?D, respectively. Then, we filter out the suspicious samples from the noisy set to generate the filtered noisy setD filter using k-nearest neighbors algorithm (k-NN) of the logit outputs from one of the two trained models, where we choose the model with the better validation set accuracy. Finally, we train the model with the sum of the clean and filtered noisy set D ?D filter . For each phase, we train the model until 100 epochs without learning rate decay. GLC:</p><p>We first train the model with only the noisy setD and obtain the label transition matrix with the trained model. With the label transition matrix, we train the initialized model with the clean and noisy set D ?D while correcting the loss obtained from the noisy samples. MLoC: We first train the model with a warmup of 75 epochs, i.e., directly training with the noisy label dataset without bells and whistles. We then meta-train the model with the learning rate of 1e-4 for additional 75 epochs. MLaC: For a total of 120 epochs, we decay the learning rate at 80 and 100 epochs by a factor of 10. MSLC: Similar to MLoC, we first train the model with the warm-up of 80 epochs, then meta-train the model with the learning rate of 1e-2 and cut it to 1e-3 at 20 epochs, for 40 epochs. FasTEN (Ours): we train the model until 70 epochs and decay the learning rate at 50 and 60 epochs by a factor of 10.</p><p>Clothing1M dataset For the Clothing1M dataset <ref type="bibr" target="#b19">[109]</ref>, we borrow the baseline evaluation results from each corresponding paper except for L2RW, where we train the model ourselves as the original paper does not report the results. For a fair comparison, we use the same backbone network, ImageNet-pretrained ResNet-50. L2RW: We train the model for 10 epochs using the SGD optimizer with the initial learning rate of 1e-2. We decay the learning rate after 5 epochs by a factor of 10, where we follow the common training procedure borrowed from <ref type="bibr">[105,</ref><ref type="bibr">85]</ref>. FasTEN (Ours): Similarly, we use the SGD optimizer with the same initial learning rate, where we decay the learning rate after 1 epochs for total of 2 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Evaluation Details for Section 4.3</head><p>Considering the situation where we have to purify the existing noisy labels inside the training set automatically, predicting the correct labels of the train samples is crucial. We compare the accuracy on the noisy train dataset where we compare with the clean label, which is unknown to the model at training time. We show the accuracy on CIFAR-10 with symmetric noise of 80%; hence if the model is perfectly overfitted to the noisy set, it will yield 28% accuracy.</p><p>For each method, we use the model with the best validation accuracy, i.e., the best model that each has produced. For the meta-model of MLaC, we use the output of the label correction network (LCN), where the network is fed with the feature vector and the noisy label for every noisy dataset to yield a corrected soft label. The feature vector is extracted from the main model, where it is obtained using the features before the fully connected layer. For the meta-model of MSLC, we use the cached soft label from the last epoch. MSLC calculates the soft label with the linear combination of the previous cached soft label, the predicted label from the main model, and the given noisy label, where the weights are continuously learned during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Experiments on CIFAR-N with Real-world Noise</head><p>We conducted further experiments on a recently released dataset, CIFAR-N [103], which contains real-world noise from human annotators. CIFAR-N is constructed by relabeling the existing CIFAR dataset using the Amazon Mechanical  Turk, a crowdsourcing platform, to show the instance-dependent noise from the human annotators. From the original dataset, we extracted 1K clean samples from the original training samples to construct noisy and clean training subsets. We use the same training settings of CIFAR-10/100 experiments in Section 4.1 that does not use semi-supervised methods or sophisticated augmentation techniques to evaluate the methods fairly. <ref type="table" target="#tab_11">Table 6</ref> shows that our proposed Fas-TEN achieves better performance than the baselines, similar to the results on Clothing1M. These additional results demonstrate that FasTEN is more robust against real-world noise. <ref type="figure">Figure 5</ref> shows the effect of label correction on our method. When FasTEN does not correct samples at the end of each epoch, it degrades the predictive performance. Furthermore, as the noise gets severe, performance further degrades where we expect the effect of label correction to be larger <ref type="bibr" target="#b13">[14]</ref>. From these observations, we verify that label correction also contributes for improving performance. <ref type="figure">Figure 6</ref> demonstrates the robustness of our method when unreliable samples are also corrected by lowering the threshold of label correction to investigate how safe our method is. We observe the robustness of our method compared to other label correction methods, MLaC <ref type="bibr" target="#b31">[121]</ref> and MSLC <ref type="bibr">[105]</ref>. We observe how the performance of our model changes when labels are corrected more unreliably as we lower the threshold ?. The experiments are conducted on CIFAR-10 with the most severe noise level (symmetric 80%). As shown in <ref type="figure">Figure 6</ref>, we verify that our method is robust for miscorrected samples even if all samples in the noisy dataset are corrected when the threshold is under 0.5. Our method uses the transition matrix to avoid the error propagation problem even if unreliable samples are corrected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Effect of Label Correction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Robustness to Miscorrected Labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Is the Performance Improvement Due to Over-sampling on the Clean Dataset?</head><p>Unlike many MAML-based methods using the clean dataset as gradient guidance in the meta training step, our proposed method utilizes the dataset directly during the model training. One may suspect that the performance improvement of our method may come from over-sampling the clean dataset. Therefore, we compare our proposed model with an over-sampling method <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">41]</ref>. To see the effectiveness of our batch formation, we experiment with the standard crossentropy loss (Eq. 52) instead of our final objective (Eq. 6), using the same batch formation (Na?ve Oversampling). For a fair comparison, label correction is excluded. arg min <ref type="table" target="#tab_12">Table 7</ref> shows that our proposed method outperforms the over-sampling method. This observation indicates that our meta-learning method appropriately leverages the clean dataset to estimate the label corruption matrix.</p><formula xml:id="formula_35">?,? (x,y)?d L (f ?,? (x), y) + (x,?)?d L (f ?,? (x),?)<label>(52)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Comparison to Other Methods with the Transition Matrix</head><p>Although the transition matrix is initially introduced as a safeguard to mitigate the risk of label correction in the FasTEN, our FasTEN even shows better performance than other methods employing the transition matrix. This section illustrates that FasTEN, even without label correction, shows better performance than other methods using transition matrix with the clean dataset: GLC (Section C.5) and MLoC (Section C.5).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to Gold Loss Correction (GLC) [40]</head><p>Our proposed method is similar to GLC in estimating the label transition matrix, but it shows better performance than GLC even without label correction (See <ref type="table" target="#tab_13">Table 8</ref>). Additionally, instead of estimating the transition matrix, we directly use the oracle matrix to examine the effectiveness of the multi-head architecture more clearly. Even using the same oracle matrix for both methods, our FasTEN outperforms GLC. We conjecture that our multi-head architecture trains the model to extract features better than the two-stage training of GLC, which learns noisy classifier and clean classifier consecutively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to Meta Loss Correction (MLoC) [101]</head><p>Since our method does not directly parameterize the label transition matrix T , stable estimation of T and its theoretical analysis are possible (See Theorem 1). <ref type="table" target="#tab_14">Table 9</ref> shows that MLoC and our FasTEN without Label Correction (LC) shows comparable performance. MLoC uses several engineering techniques for stable training: a strong prior and gradient clipping, where it is not mentioned in the paper. However, our method shows good performance even without label correction, being robust to different hyperparameters, reducing the need for excessive engineering. We also emphasize that there is a significant gap in performance at a severe noise level.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Incorrect Label Detection Performance Comparison</head><p>We consider the case where we have to continuously purify the already-collected dataset with the existence of a human oracle, where the process can be accelerated by correctly detecting the candidates for the wrongly labeled samples. Hence, we regard the incorrect label detection problem as a binary classification problem where the model output probability of the noisy samples is used as the barometer for the correctness of the label.</p><p>Settings. We extract the probability values of the noisy labels per sample inside the noisy training set to be the negative score for the binary classification problem where we label 1 for the wrongly labeled sample and 0 otherwise. We additionally measure the performance of the meta-models. We use the metalearned sample weights for MSLC, MW-Net, and L2RW. For MLaC, we use the probability of the soft label obtained by the meta-model, which is described in detail in Appendix B.4. Finally, as Deep kNN filters out the doubtful samples while training the final model, we regard the process as weighting each sample by 0 or 1 depending on its doubtfulness. Note that we evaluate each method using all the training samples, including the correctly labeled, as each method may mistake those samples to be wrongly labeled.</p><p>Results.</p><p>[80, 85, 3] claim that using meta-learning or pre-training is able to tell whether a sample is mislabeled. Although our model is not directly aimed at finding noisy samples in the noisy set, <ref type="table" target="#tab_4">Table 10</ref> shows that our model achieves comparable or better performance in detecting noisy labels than the baselines.  Although <ref type="bibr">[80,</ref><ref type="bibr" target="#b2">3]</ref> claim that the performance has improved because the metamodel detects noisy samples through re-weighting, the actual performance of meta-models is generally lower than that of the final classifier. It implies that [80, 3] may operate with different dynamics than the original author intended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 Analysis on Our Method</head><p>How Many Clean Samples are Required? To verify the effect of the clean dataset size, we observe the performance differences while varying the size. As shown in <ref type="table" target="#tab_4">Table 11</ref>, our method consistently shows better performance on different sizes. Especially, even when our method only uses 100 clean samples, it outperforms all the baselines which utilize all the clean samples (1,000 samples). This observation demonstrates that our method could accurately estimate the label transition matrix with a small number of clean samples, which can be applicable to real-world scenarios where it is difficult to obtain a sufficient number of clean samples.</p><p>How Sensitive is to the Batch Size? In Section 3.2, we show that the accuracy of estimating the label transition matrix is upper-bounded by the number of samples in the mini-batch. As previous studies [40] mentioned, the quality of the estimated transition matrix affects the performance in learning with noisy labels. To verify the effect of the number of samples in the mini-batch, we observe the performance changes by varying the number of samples per class in the mini-batch from 1 to 10. As shown in <ref type="table" target="#tab_4">Table 12</ref>, there is little change in performance depending on the number of samples per class, although the performance degradation is predicted by Theorem 1 when the number of samples is small. From this observation, we believe that our proposed method shows practicality even in situations where the batch size cannot be increased due to the limited computing resources.</p><p>Searching the Optimal Hyperparameter ? We observe performance variance on the CIFAR-10/-100 datasets when we change the hyperparameter ? which is a loss balancing factor. The results are summarized in <ref type="table" target="#tab_4">Table 13</ref>. The hyperparameter is searched in {0.01, 0.05, 0.1, 0.2, 0.5, 1.0}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Comparison with Other Methods with Label Transition Matrix</head><p>Under the assumption that label corruption occurs class-dependently and instanceindependently, learning with noisy label methods exploiting the label transition matrix has shown admirable performance <ref type="bibr">[68,</ref><ref type="bibr">79,</ref><ref type="bibr">88,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">76,</ref><ref type="bibr">25]</ref>. It is well known that training a statistically consistent classifier is possible if the transition matrix is estimated accurately, but precise estimation is usually challenging [67, <ref type="bibr" target="#b18">108,</ref><ref type="bibr" target="#b23">113]</ref>. Various methods have been proposed to alleviate the issue: imposing strong prior [76, 32], designing a loss function using the ratio of the label transition matrix T <ref type="bibr" target="#b18">[108]</ref>, or factorization of the transition matrix <ref type="bibr" target="#b23">[113]</ref>.</p><p>However, it is still challenging to estimate the transition matrix with only the noisy dataset. Recently, approaches that improve the estimation accuracy of the transition matrix using a small clean dataset have shown remarkable results: Gold Loss Correction (GLC) <ref type="bibr">[40]</ref> and Meta Loss Correction (MLoC) <ref type="bibr">[101]</ref>. The clean dataset makes it possible to directly estimate the noisy label posterior, resulting in stable prediction of the transition matrix. Our FasTEN differs from the existing methods which try to find the fixed label transition matrix, as the oracle transition matrix continuously changes during label correction in our method. Our method shows novelty compared to the previous two methods even without the label correction.</p><p>Differences from Gold Loss Correction (GLC) [40] Similar to FasTEN, GLC models p(?|x) as f? ,? (x) for estimating the transition matrix T . However, GLC is more inefficient than our FasTEN because it requires multiple training phases (See ? 4.2). We introduce a multi-head architecture with to speed up the training. Furthermore, there is an additional performance advantage compared to GLC. The multi-head architecture is presumed to help obtain a better feature extractor by inducing corruption-independent feature extraction. Detailed experimental results can be found in Appendix C.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differences from Meta Loss Correction (MLoC) [101]</head><p>MLoC gradually finds the oracle transition matrix T via the MAML framework <ref type="bibr">[20]</ref>. As mentioned earlier, MLoC is very slow because it requires three back-propagations for a single iteration due to its nature of MAML (See ? 4.2). MLoC directly parameterizes the transition matrix T and learns it using various engineering techniques: strong prior and gradient clipping, which were not mentioned in original paper. In contrast, our method estimates T more accurately by sampling the posterior through a single forward propagation. We empirically validate that our method performs better or comparable to MLoC even without label correction (See Appendix C.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Methods using Multi-head Architecture for Noisy Labels</head><p>We propose a multi-head architecture to estimate the transition matrix efficiently: one is for the clean label distribution, and the other is for the noisy label distribution. A similar multi-head architecture has been used in situations dealing with crowdsourcing. Many crowdsourcing studies assume that multiple people label a single image [81, 28, 91], where training a reliable classifier is the goal of the crowdsourcing problem. They maintain separate heads for each annotator, and each head performs multi-task learning to learn each annotator's decisions directly. Then, the final decision is made by voting each head's decision. There is no component for estimating the label transition matrix in these methods and no primary head classifier to learn from the estimated label transition matrix.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Plotting accuracy (%) (y-axis) according to total training time (hours) (x-axis). Our proposed method (FasTEN) shows the best performance in training efficiency while having comparable or better accuracy on both CIFAR-10/100 with various noise levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Summarization of our proposed method (FasTEN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>(a) Robustness to miscorrected labels on CIFAR-10 with various perturbation strength. Test accuracy (%) of baselines and baselines without the label correction is provided. (b) The plot for the mean of the diagonal term in true transition matrix T and our estimated transition matrix T according to the epoch on CIFAR-10 dataset with symmetric 80% and asymmetric 40% noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Plot of transition matrix estimation error for every iteration. Pearson ? 2 -divergence of our FasTEN, MLoC and GLC is provided.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>1 28. Guan, M., Gulshan, V., Dai, A., Hinton, G.: Who said what: Modeling individual labelers improves classification. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 32 (2018) 38 29. Guo, C., Pleiss, G., Sun, Y., Weinberger, K.Q.: On calibration of modern neural networks. In: International Conference on Machine Learning. pp. 1321-1330.PMLR (2017) 25, 26 30. Guo, J., Gong, M., Liu, T., Zhang, K., Tao, D.: Ltf: A label transformation framework for correcting label shift. In: International Conference on Machine Learning. pp. 3843-3853. PMLR (2020) 3 31. Han, B., Niu, G., Yu, X., Yao, Q., Xu, M., Tsang, I., Sugiyama, M.: Sigua: Forgetting may make learning with noisy labels more robust. In: International Conference on Machine Learning. pp. 4006-4016. PMLR (2020) 13, 24 32. Han, B., Yao, J., Niu, G., Zhou, M., Tsang, I., Zhang, Y., Sugiyama, M.: Masking:A new perspective of noisy supervision. arXiv preprint arXiv:1805.08193 (2018) 37 33. Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I.,Sugiyama, M.: Coteaching: Robust training of deep neural networks with extremely noisy labels. arXiv preprint arXiv:1804.06872 (2018) 22 34. Han, J., Luo, P., Wang, X.: Deep self-learning from noisy labels. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5138-5147 (2019) 3 35. Han, K., Wang, Y., Xu, Y., Xu, C., Wu, E., Xu, C.: Training binary neural networks through learning with noisy supervision. In: International Conference on Machine Learning. pp. 4017-4026. PMLR (2020) 3 36. Harutyunyan, H., Reing, K., Ver Steeg, G., Galstyan, A.: Improving generalization by controlling label-noise information in neural network weights. In: International Conference on Machine Learning. pp. 4071-4081. PMLR (2020) 3 37. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016) 1, 9 38. Hendrycks, D., Lee, K., Mazeika, M.: Using pre-training can improve model robustness and uncertainty. In: International Conference on Machine Learning. pp. 2712-2721. PMLR (2019) 3 39. Hendrycks, D., Mazeika, M., Kadavath, S., Song, D.: Using self-supervised learning can improve model robustness and uncertainty. arXiv preprint arXiv:1906.12340 (2019) 3 40. Hendrycks, D., Mazeika, M., Wilson, D., Gimpel, K.: Using trusted data to train deep networks on labels corrupted by severe noise. Advances in neural information processing systems 31 (2018) 1, 2, 3, 4, 5, 7, 8, 10, 11, 12, 22, 25, 26, 34, 36, 38 41. Hong, Y., Han, S., Choi, K., Seo, S., Kim, B., Chang, B.: Disentangling label distribution for long-tailed visual recognition. arXiv preprint arXiv:2012.00321 (2020) 33 42. Hu, W., Li, Z., Yu, D.: Simple and effective regularization methods for training on noisily labeled data with generalization guarantee. arXiv preprint arXiv:1905.11368 (2019) 3 43. Huang, L., Zhang, C., Zhang, H.: Self-adaptive training: beyond empirical risk minimization. Advances in Neural Information Processing Systems 33 (2020) 3 44. Jiang, L., Zhou, Z., Leung, T., Lif, L.J., Fei-Fei, L.: Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels. In: International Conference on Machine Learning. pp. 2304-2313. PMLR (2018) 3, 4, 7, 22, 25 45. Jiang, Z., Zhou, K., Liu, Z., Li, L., Chen, R., Choi, S.H., Hu, X.: An information fusion approach to learning with instance-dependent label noise. In: International Conference on Learning Representations (2022), https://openreview. net/forum?id=ecH2FKaARUp 3, 10 46. Jindal, I., Nokleby, M., Chen, X.: Learning deep networks from noisy labels with dropout regularization. In: 2016 IEEE 16th International Conference on Data Mining (ICDM). pp. 967-972. IEEE (2016) 3 47. Kim, T., Ko, J., Choi, J., Yun, S.Y., et al.: Fine samples for learning with noisy labels. Advances in Neural Information Processing Systems 34 (2021) 3 48. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) 30 49. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny images (2009) 3, 9, 30 50. Lee, K.H., He, X., Zhang, L., Yang, L.: Cleannet: Transfer learning for scalable image classifier training with label noise. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 5447-5456 (2018) 4, 25 51. Li, D., Chen, C., Liu, W., Lu, T., Gu, N., Chu, S.M.: Mixture-rank matrix approximation for collaborative filtering. In: Proceedings of the 31st International Conference on Neural Information Processing Systems. pp. 477-485 (2017) 9 52. Li, D., Chen, C., Lv, Q., Yan, J., Shang, L., Chu, S.: Low-rank matrix approximation with stability. In: International Conference on Machine Learning. pp. 295-303. PMLR (2016) 9 53. Li, J., Socher, R., Hoi, S.C.: Dividemix: Learning with noisy labels as semisupervised learning. arXiv preprint arXiv:2002.07394 (2020) 3, 9, 10 54. Li, J., Wong, Y., Zhao, Q., Kankanhalli, M.S.: Learning to learn from noisy labeled data. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5051-5059 (2019) 4, 25 55. Li, M., Soltanolkotabi, M., Oymak, S.: Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. In: International Conference on Artificial Intelligence and Statistics. pp. 4313-4324. PMLR (2020) 3 56. Li, X., Liu, T., Han, B., Niu, G., Sugiyama, M.: Provably end-to-end label-noise learning without anchor points. In: International Conference on Machine Learning. PMLR (2021) 4, 10 57. Lienen, J., H?llermeier, E.: From label smoothing to label relaxation. In: Proceedings of the 35th AAAI Conference on Artificial Intelligence, AAAI, Online, February 2-9, 2021. AAAI Press (2021) 3 58. Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll?r, P.: Focal loss for dense object detection. In: Proceedings of the IEEE international conference on computer vision. pp. 2980-2988 (2017) 3, 22 59. Liu, S., Niles-Weed, J., Razavian, N., Fernandez-Granda, C.: Early-learning regularization prevents memorization of noisy labels. arXiv preprint arXiv:2007.00151 (2020) 3, 9 60. Liu, T., Tao, D.: Classification with noisy labels by importance reweighting. IEEE Transactions on pattern analysis and machine intelligence 38(3), 447-461 (2015) 3, 5 61. Liu, Y., Guo, H.: Peer loss functions: Learning from noisy labels without knowing noise rates. In: International Conference on Machine Learning. pp. 6226-6236. PMLR (2020) 3 62. Lukasik, M., Bhojanapalli, S., Menon, A., Kumar, S.: Does label smoothing mitigate label noise? In: International Conference on Machine Learning. pp. 6448-6458. PMLR (2020) 3 63. Ma, X., Huang, H., Wang, Y., Romano, S., Erfani, S., Bailey, J.: Normalized loss functions for deep learning with noisy labels. In: International Conference on Machine Learning. pp. 6543-6553. PMLR (2020) 3 64. Ma, X., Wang, Y., Houle, M.E., Zhou, S., Erfani, S., Xia, S., Wijewickrema, S., Bailey, J.: Dimensionality-driven learning with noisy labels. In: International Conference on Machine Learning. pp. 3355-3364. PMLR (2018) 3, 22 65. Menon, A.K., Rawat, A.S., Reddi, S.J., Kumar, S.: Can gradient clipping mitigate label noise? (2020) 3 66. Menon, A.K., Van Rooyen, B., Natarajan, N.: Learning from binary labels with instance-dependent corruption. arXiv preprint arXiv:1605.00751 (2016) 3, 10, 25 67. Mirzasoleiman, B., Cao, K., Leskovec, J.: Coresets for robust training of deep neural networks against noisy labels. Advances in Neural Information Processing Systems 33 (2020) 2, 3, 4, 13, 22, 23, 24, 37 68. Mnih, V., Hinton, G.E.: Learning to label aerial images from noisy data. In: Proceedings of the 29th International conference on machine learning (ICML-12). pp. 567-574 (2012) 2, 4, 5, 25, 37 69. Mohri, M., Rostamizadeh, A., Talwalkar, A.: Foundations of machine learning. MIT press (2018) 24 70. Montgomery-Smith, S.J.: The distribution of rademacher sums. Proceedings of the American Mathematical Society 109(2), 517-522 (1990) 28 71. Natarajan, N., Dhillon, I.S., Ravikumar, P., Tewari, A.: Learning with noisy labels. In: NIPS. vol. 26, pp. 1196-1204 (2013) 3 72. Neyshabur, B., Bhojanapalli, S., Srebro, N.: A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564 (2017) 28 73. Nishi, K., Ding, Y., Rich, A., H?llerer, T.: Augmentation strategies for learning with noisy labels. arXiv preprint arXiv:2103.02130 (2021) 3, 9, 10 74. Ortego, D., Arazo, E., Albert, P., O'Connor, N.E., McGuinness, K.: Multiobjective interpolation training for robustness to label noise. arXiv preprint arXiv:2012.04462 (2020) 3, 9 75. Patrini, G., Nielsen, F., Nock, R., Carioni, M.: Loss factorization, weakly supervised learning and label noise robustness. In: International conference on machine learning. pp. 708-717. PMLR (2016) 2, 3 76. Patrini, G., Rozza, A., Krishna Menon, A., Nock, R., Qu, L.: Making deep neural networks robust to label noise: A loss correction approach. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1944-1952 (2017) 3, 4, 5, 7, 9, 10, 25, 37 77. Pleiss, G., Zhang, T., Elenberg, E.R., Weinberger, K.Q.: Identifying mislabeled data using the area under the margin ranking. arXiv preprint arXiv:2001.10528 (2020) 3 78. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International Conference on Machine Learning. pp. 8748-8763. PMLR (2021) 4 79. Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., Rabinovich, A.: Training deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596 (2014) 4, 5, 25, 37 80. Ren, M., Zeng, W., Yang, B., Urtasun, R.: Learning to reweight examples for robust deep learning. In: International Conference on Machine Learning. pp. 4334-4343. PMLR (2018) 2, 3, 4, 5, 8, 10, 11, 12, 22, 25, 35, 36 81. Rodrigues, F., Pereira, F.: Deep learning from crowds. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 32 (2018) 38 82. Scott, C.: A rate of convergence for mixture proportion estimation, with application to learning from noisy labels. In: Artificial Intelligence and Statistics. pp. 838-846. PMLR (2015) 5 83. Scott, C., et al.: Calibrated asymmetric surrogate losses. Electronic Journal of Statistics 6, 958-992 (2012) 24 84. Shrivastava, A., Gupta, A., Girshick, R.: Training region-based object detectors with online hard example mining. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 761-769 (2016) 3, 22 85. Shu, J., Xie, Q., Yi, L., Zhao, Q., Zhou, S., Xu, Z., Meng, D.: Metaweight-net: Learning an explicit mapping for sample weighting. arXiv preprint arXiv:1902.07379 (2019) 2, 4, 5, 8, 11, 12, 25, 31, 35 86. Song, H., Kim, M., Lee, J.G.: Selfie: Refurbishing unclean samples for robust deep learning. In: International Conference on Machine Learning. pp. 5907-5915. PMLR (2019) 3, 22 87. Song, H., Kim, M., Park, D., Lee, J.G.: How does early stopping help generalization against label noise? arXiv preprint arXiv:1911.08059 (2019) 3 88. Sukhbaatar, S., Bruna, J., Paluri, M., Bourdev, L., Fergus, R.: Training convolutional networks with noisy labels. arXiv preprint arXiv:1406.2080 (2014) 4, 5, 7, 25, 37 89. Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Deepface: Closing the gap to human-level performance in face verification. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1701-1708 (2014) 1 90. Tanaka, D., Ikami, D., Yamasaki, T., Aizawa, K.: Joint optimization framework for learning with noisy labels. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 5552-5560 (2018) 3 91. Tanno, R., Saeedi, A., Sankaranarayanan, S., Alexander, D.C., Silberman, N.: Learning from noisy labels by regularized estimation of annotator confusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11244-11253 (2019) 38 92. Thekumparampil, K.K., Khetan, A., Lin, Z., Oh, S.: Robustness of conditional gans to noisy labels. arXiv preprint arXiv:1811.03205 (2018) 25 93. Thulasidasan, S., Bhattacharya, T., Bilmes, J., Chennupati, G., Mohd-Yusof, J.: Combating label noise in deep learning using abstention. arXiv preprint arXiv:1905.10964 (2019) 3 94. Van Rooyen, B., Menon, A.K., Williamson, R.C.: Learning with symmetric label noise: The importance of being unhinged. arXiv preprint arXiv:1505.07634 (2015) 2, 3 95. Vapnik, V.: The nature of statistical learning theory. Springer science &amp; business media (2013) 23, 24 96. Vapnik, V.N.: An overview of statistical learning theory. IEEE transactions on neural networks 10(5), 988-999 (1999) 23 97. Veit, A., Alldrin, N., Chechik, G., Krasin, I., Gupta, A., Belongie, S.: Learning from noisy large-scale datasets with minimal supervision. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 839-847 (2017) 4, 25 98. Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., Wierstra, D.: Matching networks for one shot learning. arXiv preprint arXiv:1606.04080 (2016) 7 99. Wang, Y., Ma, X., Chen, Z., Luo, Y., Yi, J., Bailey, J.: Symmetric cross entropy for robust learning with noisy labels. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 322-330 (2019) 3 100. Wang, Y., Kucukelbir, A., Blei, D.M.: Robust probabilistic modeling with bayesian data reweighting. In: International Conference on Machine Learning. pp. 3646-3655. PMLR (2017) 3 101. Wang, Z., Hu, G., Hu, Q.: Training noise-robust deep neural networks via metalearning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4524-4533 (2020) 2, 4, 8, 11, 12, 25, 34, 35, 38 102. Wang, Z., Zhu, H., Dong, Z., He, X., Huang, S.L.: Less is better: Unweighted data subsampling via influence function. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 34, pp. 6340-6347 (2020) 3 103. Wei, J., Zhu, Z., Cheng, H., Liu, T., Niu, G., Liu, Y.: Learning with noisy labels revisited: A study using real-world human annotations. In: ICLR (2022) 31 104. Wu, P., Zheng, S., Goswami, M., Metaxas, D., Chen, C.: A topological filter for learning with label noise. arXiv preprint arXiv:2012.04835 (2020) 3 105. Wu, Y., Shu, J., Xie, Q., Zhao, Q., Meng, D.: Learning to purify noisy labels via meta soft label corrector. arXiv preprint arXiv:2008.00627 (2020) 2,<ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref> 22, 25, 31, 33 106. Xia, X., Liu, T., Han, B., Gong, M., Yu, J., Niu, G., Sugiyama, M.: Sample selection with uncertainty of losses for learning with noisy labels. arXiv preprint arXiv:2106.00445 (2021) 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>R|</head><label></label><figDesc>D| (H) = E (x,y)?D L(H(x), y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>y)?D [L(H(x), y)] + E (x,y)?D L(H(x), y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>the cross-entropy loss surrogates the ideal zero-one loss function L * [29], minimizing the empirical risk R |D|+|D| (H,H) is equivalent to following optimization problem. arg min ?,? (x,y)?DL (f ?,? (x), y) + (x,?)?D L T ? f ?,? (x),? .(26)Without loss of generalization, the optimization problem can be rewritten by introducing an episodic batch formation in Section 3.1:arg min ?,? (x,y)?d L (f ?,? (x), y) + (x,?)?d L T ? f ?,? (x),? .(27) A.3 Calculation of the Estimated Transition Matrix T of FasTEN GLC [40] presents a method to estimate the transition matrix through a small clean dataset similar to our FasTEN. GLC adopts the slow calculation method via a FOR or WHILE loop since [40] only requires to obtain the transition matrix once in the entire training process. However, our FasTEN needs to estimate the transition matrix for every iteration as we correct the labels on the fly, ending up altering the ideal transition matrix. We speed up the estimation with a single forward propagation by using only matrix operations, avoiding the sluggish FOR or WHILE loop. Here, we show the derivation of Eq. 2. Let d i = {(x, y) ? d|y i = 1}. Then, T ij = p(? = j|y = i) (28) = 1 |d i | (x,y)?di p(? = j|y = i, x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Effect of label correction on CIFAR-10/100 with various symmetric noise ratio. Accuracy (%) of FasTEN (ours.) and FasTEN without label correction is provided. Robustness to Miscorrection of FasTEN. The corrected label amount (%) and model accuracy (%) according to the label correction threshold (?) are provided.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison on CIFAR-10/100 datasets under various noise level. Test accuracy (%) with 95% confidence interval of 5-runs is provided.</figDesc><table><row><cell></cell><cell>Method</cell><cell>20%</cell><cell>Symmetric Noise Level 40% 60%</cell><cell>80%</cell><cell>Asymmetric Noise Level 20% 40%</cell></row><row><cell></cell><cell>L2RW</cell><cell cols="4">88.26 ? 0.79 83.76 ? 0.54 74.54 ? 1.54 42.60 ? 1.71 88.79 ? 0.63 85.86 ? 0.87</cell></row><row><cell></cell><cell>MW-Net</cell><cell cols="4">89.76 ? 0.31 86.52 ? 0.28 81.68 ? 0.25 56.56 ? 3.07 91.31 ? 0.25 88.69 ? 0.37</cell></row><row><cell>CIFAR-10</cell><cell>Deep kNN GLC MLoC</cell><cell cols="4">90.02 ? 0.35 87.27 ? 0.39 82.80 ? 0.55 68.30 ? 1.21 89.97 ? 0.48 84.56 ? 0.87 89.66 ? 0.10 85.30 ? 0.73 80.34 ? 0.73 67.44 ? 1.50 91.56 ? 0.66 89.76 ? 0.89 90.50 ? 0.71 87.20 ? 0.35 81.95 ? 0.44 54.64 ? 4.04 91.15 ? 0.16 89.35 ? 0.45</cell></row><row><cell></cell><cell>MLaC</cell><cell cols="4">89.75 ? 0.62 86.63 ? 0.56 82.20 ? 0.81 71.94 ? 2.22 91.45 ? 0.32 90.26 ? 0.48</cell></row><row><cell></cell><cell>MSLC</cell><cell cols="4">90.94 ? 0.45 88.36 ? 0.80 83.93 ? 1.21 64.90 ? 4.84 91.45 ? 1.35 89.26 ? 0.52</cell></row><row><cell></cell><cell cols="5">FasTEN (ours.) 91.94 ? 0.28 90.07 ? 0.17 86.78 ? 0.31 79.52 ? 0.78 92.29 ? 0.10 90.43 ? 0.31</cell></row><row><cell></cell><cell>L2RW</cell><cell cols="4">57.79 ? 1.88 44.82 ? 4.30 30.01 ? 1.74 10.71 ? 1.79 59.11 ? 2.74 55.12 ? 3.40</cell></row><row><cell></cell><cell>MW-Net</cell><cell cols="4">66.73 ? 0.78 59.44 ? 0.91 49.19 ? 1.57 19.04 ? 1.21 67.90 ? 0.78 64.50 ? 0.34</cell></row><row><cell>CIFAR-100</cell><cell>Deep kNN GLC MLoC MLaC</cell><cell cols="4">59.60 ? 0.97 52.48 ? 1.37 39.90 ? 0.60 23.39 ? 0.75 57.71 ? 0.47 50.23 ? 1.12 60.99 ? 0.64 49.00 ? 4.33 33.38 ? 4.09 20.38 ? 1.35 64.43 ? 0.43 54.20 ? 0.86 68.16 ? 0.41 62.09 ? 0.33 54.49 ? 0.92 20.23 ? 1.86 69.20 ? 0.59 66.48 ? 0.56 49.81 ? 5.59 35.15 ? 5.75 20.15 ? 2.81 12.85 ? 0.87 56.46 ? 3.54 49.20 ? 3.23</cell></row><row><cell></cell><cell>MSLC</cell><cell cols="4">68.62 ? 0.60 63.30 ? 0.49 53.83 ? 0.70 21.07 ? 5.20 70.86 ? 0.30 66.99 ? 0.69</cell></row><row><cell></cell><cell cols="5">FasTEN (ours.) 68.75 ? 0.60 63.82 ? 0.33 55.22 ? 0.64 37.36 ? 1.15 70.35 ? 0.51 67.93 ? 0.53</cell></row><row><cell cols="4">4.1 Predictive Performance Comparison</cell><cell></cell></row><row><cell cols="5">CIFAR-10/100 with Synthetic Noise. CIFAR-10/100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy (%) comparison on Clothing1M dataset with real-world label noise. Rows with ? denote results directly borrowed from<ref type="bibr" target="#b31">[121]</ref> and ? denotes the result directly borrowed from[56]. All the other results except L2RW [80] are taken from original papers.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Top-1 Accuracy</cell></row><row><cell></cell><cell>Forward ?</cell><cell>69.91</cell></row><row><cell>Clean set X</cell><cell>T-Revision ? casualNL IF VolMinNet ? DivideMix</cell><cell>70.97 72.24 72.29 72.42 74.76</cell></row><row><cell></cell><cell>AugDesc</cell><cell>75.11</cell></row><row><cell>Clean set O</cell><cell>MLoC L2RW GLC  ? MW-Net  ? MSLC MLaC  ?</cell><cell>71.10 72.04 ? 0.24 73.69 73.72 74.02 75.78</cell></row></table><note>Ours FasTEN w/o LC 77.07 ? 0.52 FasTEN 77.83 ? 0.17</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2</head><label>2</label><figDesc></figDesc><table /><note>. DivideMix [53] and AugDesc [73] leverages semi-supervised learning with various data augmentation strategies. Forward [76], T-Revision [108], IF [45], and causalNL [112], and VolMin- Net [56] are transition matrix estimation methods that use certain data points without clean data points.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Training time comparison on CIFAR-10 dataset with 80% symmetric noise. Time (hours) per total training on a single RTX 2080Ti GPU are provided with the relative ratio compared to our method.</figDesc><table><row><cell>Method</cell><cell cols="3">L2RW MW-NET Deep kNN GLC MLoC MLaC MSLC FasTEN [80] [85] [3] [40] [101] [121] [105] (ours.)</cell></row><row><cell cols="2">Total Training Time 4.78 (Relative to Ours.) (3.11x) (1.71x) 2.63</cell><cell>2.32 (1.51x) (1.49x) (4.64x) (6.64x) (1.64x) 2.29 7.13 10.2 2.51</cell><cell>1.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>18. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A largescale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248-255. Ieee (2009) 10 19. Drory, A., Avidan, S., Giryes, R.: How do neural networks overcome label noise. arXiv Preprint (2018) 22 20. Finn, C., Abbeel, P., Levine, S.: Model-agnostic meta-learning for fast adaptation of deep networks. In: International Conference on Machine Learning. pp. 1126-1135. PMLR (2017) 2, 4, 25, 38 21. Floridi, L., Chiriatti, M.: Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines 30(4), 681-694 (2020) 1 22. Ghosh, A., Kumar, H., Sastry, P.: Robust loss functions under label noise for deep neural networks. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 31 (2017) 3 23. Ghosh, A., Lan, A.: Do we really need gold samples for sample weighting under label noise? In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 3922-3931 (2021) 2 24. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 580-587 (2014) 1 25. Goldberger, J., Ben-Reuven, E.: Training deep neural-networks using a noise adaptation layer (2016) 3, 4, 5, 25, 37 26. Golowich, N., Rakhlin, A., Shamir, O.: Size-independent sample complexity of neural networks. In: Conference On Learning Theory. pp. 297-299. PMLR (2018) 28 27. Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning. MIT Press (2016), http: //www.deeplearningbook.org</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Data split composition of dataset used in our experiments.</figDesc><table><row><cell cols="5">Dataset Noisy-train Clean-train Valid Test Image size # of classes</cell></row><row><cell>CIFAR-10 CIFAR-100</cell><cell>44K</cell><cell>1K</cell><cell>5K 10K 32 ? 32</cell><cell>10 100</cell></row><row><cell>Clothing1M</cell><cell>1M</cell><cell>47K</cell><cell>14K 10K 224 ? 224</cell><cell>14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Test accuracy (%) comparison on CIFAR-N dataset with real-world label noise.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Methods</cell><cell></cell><cell>CIFAR-10N CIFAR-100N</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">L2RW [79]</cell><cell>83.15</cell><cell>57.54</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">MW-Net [84]</cell><cell>83.82</cell><cell>61.12</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Deep kNN [3]</cell><cell>82.87</cell><cell>52.71</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GLC [40]</cell><cell></cell><cell>82.71</cell><cell>54.72</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">MLoC [100]</cell><cell>84.45</cell><cell>61.49</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">MLaC [119]</cell><cell>83.64</cell><cell>45.95</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">MSLC [103]</cell><cell>84.35</cell><cell>63.51</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">FasTEN (Ours)</cell><cell>87.01</cell><cell>63.53</cell></row><row><cell></cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy (%)</cell><cell>70 80</cell><cell></cell><cell></cell><cell>40 60</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.25</cell><cell>0.50</cell><cell>0.75</cell><cell>1.00</cell><cell>0.25</cell><cell>0.50</cell><cell>0.75</cell><cell>1.00</cell></row><row><cell></cell><cell></cell><cell cols="2">Noise level</cell><cell></cell><cell></cell><cell cols="2">Noise level</cell></row><row><cell></cell><cell cols="4">FasTEN w/o Label Correction</cell><cell></cell><cell cols="2">FasTEN (ours.)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>The effect of clean set oversampling on the performance of CIFAR-10/100 experiments. The accuracy (%) of na?ve oversampling and FasTEN (ours.) w/o label correction is provided. Label Correction 68.02 61.75 52.79 28.46 69.59 66.07</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Symmetric 20 % 40 % 60 % 80 % 20 % 40 % Asymmetric</cell></row><row><cell>CIFAR-10</cell><cell cols="2">Na?ve Oversampling FasTEN (ours.) w/o Label Correction 90.67 88.29 84.12 74.19 92.50 91.05 89.39 85.90 83.90 56.83 90.58 84.41</cell></row><row><cell>CIFAR-100</cell><cell>Na?ve Oversampling FasTEN (ours.) w/o</cell><cell>65.42 57.08 42.18 25.88 67.71 61.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>The effect of two-head architecture via oracle label transition matrix on CIFAR-10/100 dataset. Test accuracy (%) of GLC [40] and FasTEN (ours.) with and without oracle label transition matrix is provided. For a fair comparison, label corruption is excluded in FasTEN. 85.45 81.56 67.54 91.74 90.35 FasTEN (ours.) w/ Oracle w/o Label Correction 91.37 88.71 83.97 74.91 91.80 91.10 GLC [40] 89.66 85.30 80.34 67.44 91.56 89.76 FasTEN (ours.) w/o Label Correction 90.67 88.29 84.12 74.19 92.50 91.05</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Symmetric 20 % 40 % 60 % 80 % 20 % 40 % Asymmetric</cell></row><row><cell cols="3">CIFAR-10 89.06 CIFAR-100 GLC [40] w/ Oracle GLC [40] 60.99 49.00 33.38 20.38 64.43 54.20 FasTEN (ours.) w/o Label Correction 68.02 61.75 52.79 28.46 69.59 66.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Test accuracy (%) comparison between FasTEN without Label Correction and MLoC [101] on CIFAR-10/100 dataset. 87.20 81.95 54.64 91.15 89.35 FasTEN (ours.) w/o Label Correction 91.37 88.71 83.97 74.91 91.80 91.10 68.16 62.09 54.49 20.23 69.20 66.48 FasTEN (ours.) w/o Label Correction 68.02 61.75 52.79 28.46 69.59 66.07</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Symmetric 20 % 40 % 60 % 80 % 20 % 40 % Asymmetric</cell></row><row><cell cols="3">CIFAR-10 90.50 CIFAR-100 MLoC [101] MLoC [101]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Incorrect label detection performance comparison on CIFAR-10 with symmetric 80% noise. The Area Under the Receiver Operating Characteristic (AUROC) and The Area Under the Precision-Recall Curve (AUPRC) are provided. Note that pure random model will yield 0.5 AUROC and 0.72 AUPRC. ? denotes the performance of the sample weights obtained with the meta model. L2RW L2RW ? MW-NET Deep kNN Deep kNN ? GLC MLoC MLaC MLaC ? MSLC FasTEN</figDesc><table><row><cell>AUROC 0.8653 0.4898 0.9205</cell><cell>0.9019</cell><cell>0.8070 0.9324 0.9318 0.9640 0.9564 0.9303 0.9651</cell></row><row><cell>AUPRC 0.9412 0.7994 0.9631</cell><cell>0.9396</cell><cell>0.9326 0.9674 0.9695 0.9835 0.9791 0.9624 0.9835</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>The effect of the number of clean set on CIFAR-10 with symmetric 80% noise. Comparison with other label correction methods with meta-learning is provided.</figDesc><table><row><cell># of clean examples</cell><cell>MLaC MSLC</cell><cell>FasTEN (ours.)</cell></row><row><cell>100</cell><cell cols="2">32.92 69.00 76.48</cell></row><row><cell>250</cell><cell cols="2">42.15 63.52 79.36</cell></row><row><cell>500</cell><cell cols="2">50.70 63.35 77.82</cell></row><row><cell>1000</cell><cell cols="2">71.94 64.90 77.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 :</head><label>12</label><figDesc>The effect of the number of samples per class (K) in the mini-batch on the predictive performance (Accuracy (%)) of CIFAR-10 experiments. .85 89.96 87.00 81.68 92.43 91.11 4 91.79 89.85 86.92 82.18 92.14 91.18 6 92.16 90.07 86.77 79.57 92.14 90.98 8 92.10 89.82 84.81 79.55 92.41 90.74 10 91.72 89.30 84.63 77.88 91.95 90.25</figDesc><table><row><cell>K</cell><cell>Symmetric 20 % 40 % 60 % 80 % 20 % 40 % Asymmetric</cell></row><row><cell cols="2">2 91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc>Evaluation results varying the hyperparameter ?. Test accuracy (%) with 95% confidence interval of 5-runs is provided. 90.87 ? 0.35 89.03 ? 0.24 85.20 ? 1.00 75.95 ? 1.13 91.06 ? 0.36 89.40 ? 0.44 0.05 91.69 ? 0.17 89.20 ? 0.64 84.48 ? 0.89 76.39 ? 1.79 91.92 ? 0.31 89.58 ? 0.31 0.1 91.72 ? 0.20 89.30 ? 0.32 84.63 ? 0.70 77.88 ? 1.09 91.95 ? 0.22 90.25 ? 0.39 0.2 91.72 ? 0.11 89.61 ? 0.29 85.71 ? 0.24 77.55 ? 2.78 92.20 ? 0.19 90.51 ? 0.27 0.5 91.94 ? 0.28 90.07 ? 0.17 86.78 ? 0.31 79.52 ? 0.78 92.29 ? 0.10 90.43 ? 0.31 1.0 91.80 ? 0.20 89.70 ? 0.19 86.66 ? 0.48 80.95 ? 0.44 92.04 ? 0.40 90.54 ? 0.23 CIFAR-100 0.01 65.36 ? 0.77 57.79 ? 1.12 43.65 ? 1.06 26.95 ? 0.76 66.58 ? 0.71 62.19 ? 0.66 0.05 68.49 ? 0.27 62.47 ? 0.32 53.55 ? 0.86 35.53 ? 1.28 69.73 ? 0.18 65.63 ? 0.79 0.1 68.38 ? 0.29 62.53 ? 0.33 54.82 ? 0.46 35.35 ? 1.13 69.35 ? 0.13 66.34 ? 0.27 0.2 68.65 ? 0.09 63.07 ? 0.22 54.84 ? 0.30 35.65 ? 0.66 70.37 ? 0.15 66.93 ? 0.20 0.5 68.75 ? 0.60 63.82 ? 0.33 55.22 ? 0.64 37.36 ? 1.15 70.35 ? 0.51 67.93 ? 0.53 1.0 67.91 ? 0.59 62.78 ? 0.28 52.76 ? 1.15 31.45 ? 0.75 70.02 ? 0.60 67.11 ? 0.55</figDesc><table><row><cell>?</cell><cell>20%</cell><cell>Symmetric Noise Level 40% 60%</cell><cell>80%</cell><cell>Asymmetric Noise Level 20% 40%</cell></row><row><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/hyperconnect/FasTEN</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07069</idno>
		<title level="m">Auxiliary image regularization for deep cnns with noisy labels</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep k-nn for noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
	<note>PMLR (2020) 2, 4, 7, 8, 11, 12, 25, 30</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08498</idno>
		<title level="m">Spectrally-normalized margin bounds for neural networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convexity, classification, and risk bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">473</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training deep neural-networks based on unreliable labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bekker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berthon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03772</idno>
		<title level="m">Confidence scores make instance-dependent label-noise learning possible</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Concentration inequalities: A nonasymptotic theory of independence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boucheron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Massart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Oxford university press</publisher>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15766</idno>
		<title level="m">Heteroskedastic and imbalanced deep learning with adaptive regularization</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Active bias: Training more accurate neural networks by emphasizing high variance samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07433</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning from untrusted data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing</title>
		<meeting>the 49th Annual ACM SIGACT Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04193</idno>
		<title level="m">Robustness of accuracy metric and its inspirations in learning with noisy labels</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning with instancedependent label noise: A sample sieve approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning with bounded instance and label-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<title level="m">Part-dependent label noise: Towards instance-dependent label noise. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00189</idno>
		<title level="m">Are anchor points really indispensable in label-noise learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the consistency of top-k surrogate losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Safeguarded dynamic label regression for noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Instance-dependent labelnoise learning under a structural causal model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dual t: Reducing estimation error for transition matrix in label-noise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07805</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning with biased complementary labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<title level="m">Understanding deep learning requires rethinking generalization</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-paced robust learning for leveraging clean labels in noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning noise transition matrix from only noisy labels via total variation regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07836</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meta label correction for noisy label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 35th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Errorbounded correction of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A second-order approach to learning with instancedependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
