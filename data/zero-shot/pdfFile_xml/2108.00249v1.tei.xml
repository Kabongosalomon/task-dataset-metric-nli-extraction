<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SyDog: A Synthetic Dog Dataset for Improved 2D Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moira</forename><surname>Shooter</surname></persName>
							<email>m.shooter@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Surrey Stag Hill</orgName>
								<orgName type="institution" key="instit2">University Campus</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<settlement>Guildford</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Malleson</surname></persName>
							<email>charles.malleson@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Surrey Stag Hill</orgName>
								<orgName type="institution" key="instit2">University Campus</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<settlement>Guildford</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
							<email>a.hilton@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Surrey Stag Hill</orgName>
								<orgName type="institution" key="instit2">University Campus</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<settlement>Guildford</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SyDog: A Synthetic Dog Dataset for Improved 2D Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating the pose of animals can facilitate the understanding of animal motion which is fundamental in disciplines such as biomechanics, neuroscience, ethology, robotics and the entertainment industry. Human pose estimation models have achieved high performance due to the huge amount of training data available. Achieving the same results for animal pose estimation is challenging due to the lack of animal pose datasets. To address this problem we introduce SyDog: a synthetic dataset of dogs containing ground truth pose and bounding box coordinates which was generated using the game engine, Unity. We demonstrate that pose estimation models trained on SyDog achieve better performance than models trained purely on real data and significantly reduce the need for the labour intensive labelling of images. We release the SyDog dataset as a training and evaluation benchmark for research in animal motion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating the pose of animals from video <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22]</ref> helps to understand the animal motion and this supports many applications and disciplines such as veterinary science where lameness can be diagnosed early and recovery monitored; biomechanical applications where gait is analysed to improve animal performance in sports such as horse racing and dressage; neuroscience where motion is analysed to understand behaviour and/or relate motion to brain activity <ref type="bibr" target="#b17">[18]</ref>; robotics where robots learn from animal motion data <ref type="bibr" target="#b20">[21]</ref>; and in the entertainment industry to produce more natural and realistic animal animations and to create 3D representations of animals. The traditional and most accurate method to track the motion of subjects of interest is optical motion capture. This involves placing reflective markers on the subject and uses a system with multiple cameras to capture their 3D location. This method has its disadvantages in that it requires expertise and time to set up, it can be stressful to the animal, it can change the animal's behaviour, animals can be uncooperative and in some cases it is impossible to bring the animal into a lab. Another disadvantage is that the lighting conditions need to be fairly controlled, typically restricting such systems to laboratories. Non-contact video-based estimation of animal motion has the potential to overcome these limitations. Deep learning methods are known to perform well with huge amounts of data. The main focus in the literature has been on human pose estimation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> where large amounts of training data have allowed high accuracy to be obtained. It is challenging to achieve the same quality results for animals as there is less training data available <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13]</ref>. The standard way to create datasets is to annotate each image manually, but annotating several keypoints in thousands of images is both labour intensive and expensive. However, in recent years the generation of synthetic data has been an accelerator for machine learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>In this work we address the lack of animal datasets by creating a dataset consisting of images of dogs rendered using a real-time game engine. To add variation into the data, the dog's appearance and pose, the environment, the camera viewing points and the lighting conditions were modified. Using this approach, we generated a synthetic dataset containing 32k annotated images.We evaluate the pose estimation models trained with synthetic data on the StanfordExtra dataset <ref type="bibr" target="#b1">[2]</ref>. Because networks trained only on synthetic data often fail to generalize to real world examples (the domain gap) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>, two techniques were applied separately: fine-tuning the networks and training the networks with a combination of real and synthetic samples. We demonstrate that models trained on synthetic data increase the models' performances and reduce the need for labour intensive image annotation.</p><p>The main contributions of this work are: (i) We present a real-time system that generates 2D annotated images containing dogs. (ii) We release SyDog, a large scale annotated dataset of dogs with 2D keypoints and bounding box coordinates. (iii) We show that using the SyDog dataset improves the accuracy of pose estimation models and reduces the need for labour intensive labelling. <ref type="figure" target="#fig_0">Figure 1</ref>: Pipeline showing the process of generating the SyDog dataset. The dog's motion is controlled using keyboard inputs. A virtual camera follows and renders a frame of the dog with different appearance, pose, lighting (post-processing effects), environment, and camera view points. These parameters are randomly sampled to make the data more diverse. RGB images, 2D pose and bounding box coordinate annotation are simultaneously generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data Generation</head><p>In this section we present how the SyDog dataset was generated. <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates the pipeline overview for generating the synthetic data.</p><p>We generated the synthetic dataset using the game engine Unity3D. We built upon Zhang et al.'s project <ref type="bibr" target="#b27">[28]</ref> which produces natural animations for quadruped animals from real motion data using a novel neural network architecture which they call Adaptive Neural Networks. By using this system we were able to control the animal's motion using keyboard inputs and make the dataset more varied by transitioning the dog's pose from one state to another. To add more variety into the dataset, the dog's appearance, the environment, the camera viewing points and the lighting conditions (post-processing effects) were randomly modified. We produced 32k images along with annotations of 25 keypoints and bounding box coordinates. We refer the reader to the supplementary material for samples of the SyDog dataset. Dog models. We used 8 different type of dogs, 1 came with Zhang et al.'s project, which we will refer to as the default model, 5 were imported from the RGBD-Dog dataset <ref type="bibr" target="#b12">[13]</ref>, and 2 were a fat and a skinny version of the default model. The models represent dogs ranging from big to small sized breeds. The models were manually scaled and rigged based on the default model for the models to be correctly imported into the project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dog textures.</head><p>To create different types of textures quickly and without the need of manually UV-unwrapping the models and manually producing textures, the textures were generated procedurally using shaders and mapped onto the surfaces by applying triplanar mapping. Triplanar mapping is a technique which applies textures onto a model from three directions using the world space positions. The initial setup of the shader can take time, but once implemented many textures can be generated by modifying the parameters of the shader such as the colour, size and position of the spots and the main colour of the dog. In total we have generated 12 types of fur texture, which are randomly sampled when rendering the images. Post-processing effects. The post-processing effects from Unity were used to generate different lighting conditions and add noise to the renders. We added different types of grain which differ in particle size, intensity value, colour, and luminance contribution. We colour graded the image with saturation values which are randomly sampled between [-100, 100]; and with brightness values that range between <ref type="bibr">[-20, 35</ref>]. Camera. The camera was set to follow and look at the dog while being randomly positioned around the dog to capture it from different angles. The camera's field of view values were sampled uniformly at random between [50,100] degrees. Environment. Different environments were created by modifying the sky and terrain textures. To set the sky texture we randomly sampled 1341 images from the Kaggle Landscape Pictures dataset <ref type="bibr" target="#b23">[24]</ref>. Additionally, 10 different terrain textures were collected from the internet consisting of grass, autumn leaves (2x), dry mud (2x), cobble stone, pebbles, sand, snow and tiles. 2D annotation. To save the 2D annotations we located the 3D joint positions in world space and transform them into screen space. When the program runs, the 2D keypoints, the frame number and the bounding box coordinates with the 256x256 RGB image are saved. The bounding box coordinates were computed by adding 10 pixels to the minimum and maximum of the x-and y-coordinates. The average time to produce a synthetic is 33 milliseconds. By contrast, manual annotation of an image with these keypoints typically takes at least a minute. The data was generated on a MacBook Pro 2016 with a 2.9 GHz Quad-Core Intel Core i7 processor and a AMD Radeon Pro 460 4GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and Results</head><p>We trained a 2-stacked hourglass network with 2 blocks (2HG), an 8-stacked hourglass network with 1 block (8HG) and a pre-trained Mask R-CNN model with a ResNet50 as a backbone. We refer the reader to the supplementary material and <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref> for further details on the training set up and the networks, respectively.</p><p>Six experiments were conducted: Firstly, the networks were trained with solely synthetic data. Secondly, the networks were trained purely on the StanfordExtra dataset. Thirdly, the networks which were trained only on synthetic data were fine-tuned with the StanfordExtra dataset using the same parameters as the first experiment. Then, we repeated the third experiment but with a smaller learning rate. Finally, the networks were trained on a mixed dataset which is a dataset that contains both the StanfordExtra and (either the whole or a fraction of) the SyDog dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>The StanfordExtra dataset <ref type="bibr" target="#b1">[2]</ref> is based on the Stanford Dogs dataset <ref type="bibr" target="#b13">[14]</ref> and contains 12k real images which cover 120 different types of dogs. The 2D joint annotations were modified to reflect our synthetic data labels. Only the common joints were included in the annotations and the joints that differed between the StanfordExtra and the Synthetic Dog datasets, the keypoints' visibility were set to invisible. We used the StandfordExtra training-test split, which are publicly available <ref type="bibr" target="#b2">[3]</ref>. To train the networks on synthetic data, the SyDog dataset was divided by the different types of dog. 6 dogs were used for training, 1 for validation and 1 for testing. To train the networks with the mixed dataset, either the whole or a fraction of SyDog dataset was made available for training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation metrics</head><p>The networks were evaluated using the percentage of correct keypoints (PCK) and the mean per joint position error (MPJPE), which were both normalized with respect to the length of the bounding box diagonal. The PCK measures whether the predicted keypoints are within a threshold from the true keypoints. The threshold was set to 10% of the bounding box diagonal. The MPJPE is the mean of the per joint position error <ref type="bibr" target="#b14">[15]</ref>. The evaluation metrics are calculated for visible keypoints only. <ref type="table" target="#tab_0">Table 1</ref> shows the pose estimation results for the 2HG, 8H and Mask R-CNN. Some challenges do arise for the Mask R-CNN when it has to predict certain poses such as sitting and when it is presented with certain camera view points such as when the dog is far away.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results for SyDog test dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results for StanfordExtra test dataset</head><p>The average PCK and MPJPE for all experiments are shown in <ref type="table" target="#tab_2">Table 2</ref>. Isolated training The networks trained solely on synthetic data performed poorly on real data, which was expected due to the domain gap. Another possible reason could be that the SyDog dataset does not cover all breeds in the Stan-fordExtra dataset. The results from the networks trained only on real data were used as a baseline to evaluate the use of the SyDog dataset. Fine-tuning There's a significant increase in performance when fine-tuning the stacked hourglass networks with the same learning rate, and there is an even better performance when fine-tuning with a smaller learning rate. When finetuning the 2HG and the 8HG with a smaller learning rate the models' PCK performances are increased by 12.51% and 14.15%, respectively. The performance of the Mask R-CNN did not improve when fine-tuning with smaller learning rates, yet it performs better than the Mask R-CNN that was trained solely on real data. Training with mixed dataset The best performance for the stacked hourglass networks is when the mixed dataset contains the full synthetic dataset, this is different for the Mask R-CNN; the Mask R-CNN performs best when the mixed dataset contains only half of the synthetic dataset, however  using the full synthetic dataset in the mixed dataset produces better results than when the Mask R-CNN is trained solely with real data. Our results clearly demonstrate the benefits of using the synthetic data generated by our system. We show that using a mixed dataset when training gives a slight boost in the performance and that fine-tuning the networks results in a significant boost in the performance compared to the networks trained only on real data. The synthetic data generated by our system is not very photorealistic. However, it already improves the accuracy of the pose estimation models by 12.51% in the case of the 2-stacked hourglass network. We expect that improvements in photorealism would result in further improvements in pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>To solve the lack of animal datasets, we introduce SyDog a synthetic dataset containing dogs with 2D pose and bounding box annotation which was generated using real-time rendering technology. The dataset was made varied by modifying the dog's appearance, pose, environment, lighting conditions (post-process effects) and camera view points. To evaluate the use of the SyDog dataset we conducted extensive experiments on the real dataset, Stan-fordExtra. We bridged the domain gap by fine-tuning the networks trained on synthetic data with real data and training the networks with a mixed dataset (synthetic+real). We demonstrated that using the SyDog dataset increases the performance of pose estimation models trained solely on real data and significantly reduces the need for labour intensive labelling which in turn speeds up the process. The models trained with a mixed dataset return a slight increase in performance and models that were fine-tuned with real data return a significant increase in performance. The data generated does not look very photorealistic; however we showed that it already notably improves the accuracy of the pose estimation models. Future work would involve improving the photorealism of the data for yet further improvements in pose. In this work we focused on 2D pose estimation and generated data with 2D annotations but with further work, the system could be extended to generate 3D annotations and modified to produce scenes that handle occlusions, multiple dogs and interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material SyDog: A Synthetic Dog Dataset for Improved 2D Pose Estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Moira Shooter</head><p>Charles Malleson Adrian Hilton University of Surrey Stag Hill, University Campus, Guildford GU2 7XH {m.shooter,charles.malleson,a.hilton}@surrey.ac.uk </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Data Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Training Set Up</head><p>We used a GeForce RTX 2080 Ti for training. The code was implemented with Pytorch Lightning <ref type="bibr">[?]</ref>. In this paper we focus on the usage of synthetic data and not the architecture design therefore we trained a 2-Stacked Hourglass network with 2 blocks (2HG), an 8-Stacked Hourglass network with 1 block (8HG) and a pre-trained Mask R-CNN model with a ResNet50 as a backbone from the TorchVision library. For 2HG and 8HG RMSprop was used as an optimiser with a learning rate set to 1 ? 10 ?3 and for the Mask R-CNN we also used an RMSprop but with a learning rate set to 1 ? 10 ?5 . The batch size was set to 32 for the stacked hourglass networks and 16 for the Mask R-CNN, although when the networks were trained with the mixed dataset, which consists of both real and synthetic samples, the batch size was set to 8, 4 of which were real samples and 4 of which were synthetic samples. We applied early stopping to our networks, the networks stop training when the validation does not improve for 10 epochs. The model is saved when there is an improvement in the validation loss. Originally the loss function for the stacked hourglass network would be the mean squared error between the ground truth x i and all the heatmaps generated by the network y i but because we only care about the visible keypoints, we modified the loss function by multiplying the keypoints' ground truth visibility v i with the squared error such that only the visible keypoints contribute to the loss function.</p><formula xml:id="formula_0">M SE masked = 1 n n i=1 v i (y i ? x i ) 2 , v i = 0, 1<label>(1)</label></formula><p>For In order to train the stacked hourglass network we represented the joints as 2D heatmaps. The ground truth heatmaps are produced by generating 2D Gaussians with a standard deviation (std) of 3 pixels centered at the joint's location. The stacked hourglass network takes 256x256 RGBimages as input and returns 25 heatmaps of size 64x64. Because the StanfordExtra dataset contains different sized images, it was necessary to resize them to 256x256; there was no need to resize the synthetic images as they were generated to be size 256x256. To train the Mask R-CNN the joints were represented as a list containing the joint's coordinates and visibility. The Mask R-CNN takes 256x256 images as input and returns the predicted bounding boxes, labels, scores of each prediction and the locations of the predicted keypoints. Before feeding the data to the networks, we made sure that each pixel had the same similar data distrubtion by normalizing the data.    <ref type="table" target="#tab_0">Table 1</ref> shows the pose estimation results from the Mask R-CNN when fine-tuned with smaller learning rates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>shows some example images from the SyDog dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The StanfordExtra dataset was normalised with a mean=[0.4822, 0.4621, 0.3972] and a std=[0.2220, 0.2172, 0.2167]; while the Syn-theticDog dataset was normalised with a mean=[0.6528, 0.4980, 0.5418] and a std=[0.1827, 0.1970, 0.1946].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Examples of the SyDog dataset. The data is made varied using different lightning conditions (post-process effects), environments, dog's appearance and camera viewing points.(a) Bar graph of the percentage of correct keypoints (PCK) between various pose estimation models and training data. (b) Bar graph of the mean per joint per error (MPJPE) between various pose estimation models and training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Quantitative comparison on StanfordExtra test dataset for the 2HG, 8HG and Mask R-CNN trained purely on real data, fine-tuned with real data and trained with the mixed dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative comparison on StanfordExtra between the N-stacked hourglass networks trained purely on real data, fine-tuned with real data and trained with the mixed dataset. The ground truth (green) and predicted (yellow) pose with the mean per joint per error (MPJPE) are displayed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average PCK@0.1 and MPJPE from the 2HG, 8HG and the Mask R-CNN on the SyDog test dataset.</figDesc><table><row><cell>Network</cell><cell cols="2">PCK (%) MPJPE (%)</cell></row><row><cell>2HG 8HG Mask R-CNN</cell><cell>77.76 77.57 68.98</cell><cell>6.51 6.56 11.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results on the StanfordExtra test dataset. Results are shown from the 2-and 8-stacked hourglass (2HG, 8HG) and the Mask R-CNN trained solely on the StanfordExtra dataset (Real) and solely on the SyDog dataset (Synthetic) together with the fine-tuned (FT) models and the models trained with a mixed dataset (Mixed@fraction). The performance is evaluated using the percentage of correct keypoints (PCK) with a threshold set to 0.1 and the mean per joint per error (MPJPE) which are both w.r.t. the length of the ground truth bounding box diagonal.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Pose estimation results from the Mask R-CNN on the StanfordExtra test dataset when fine-tuned with a smaller learning rate. The performance is evaluated using the percentage of correct keypoints (PCK) with a threshold set to 0.1 and the mean per joint per error (MPJPE) which are both normalised w.r.t. the length of the ground truth bounding box diagonal.</figDesc><table><row><cell>the Mask R-CNN's loss function we sum the classifica-tion, regression and keypoint loss which our returned by the Mask R-CNN during training.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Aleksander Rognhaugen, and Theoharis Theoharis. Looking beyond appearances: Synthetic training data for deep cnns in re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><forename type="middle">Barros</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<idno>abs/1701.03153</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Who left the dogs out? 3d animal reconstruction with expectation maximization in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Biggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Boyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Who left the dogs out?: 3D animal reconstruction with expectation maximization in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Biggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Boyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Creatures great and SMAL: Recovering the shape and motion of animals from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Biggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Cross-domain adaptation for animal pose estimation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoshu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<idno>abs/1812.08008</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhe</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1604.02703</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">RMPE: regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoshu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno>abs/1612.00137</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Liftpose3d, a deep learning-based approach for transforming 2d to 3d pose in laboratory animals. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Gosztolai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semih</forename><surname>G?nel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><forename type="middle">Pietro</forename><surname>Abrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Victor Lobato R?os</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramdya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deepposekit, a software toolkit for fast and robust animal pose estimation using deep learning. eLife, 8:e47994</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Graving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemal</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><forename type="middle">D</forename><surname>Blair R Costelloe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Couzin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Mask</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Transfer learning from synthetic to real images using variational autoencoders for precise position detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadanobu</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhajit</forename><surname>Chaudhury</surname></persName>
		</author>
		<idno>abs/1807.01990</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Giovanni De Magistris, and Sakyasingha Dasgupta</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rgbd-dog: Predicting canine pose from rgbd sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinead</forename><surname>Kearney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang</forename><forename type="middle">In</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Cosker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nityananda</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Colorado Springs, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeplabcut: markerless pose estimation of user-defined body parts with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Mamidanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">M</forename><surname>Cury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiga</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mackenzie</forename><forename type="middle">Weygandt</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1281" to="1289" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning from synthetic animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiteng</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichao</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1912.08265</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An integrated brain-machine interface platform with thousands of channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elon</forename><surname>Musk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Med Internet Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">16194</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1603.06937</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving rubik&apos;s cube with a robot hand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilge</forename><surname>Openai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciek</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Chociej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Petron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Ribas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schneider</surname></persName>
		</author>
		<idno>abs/1910.07113</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
			<pubPlace>Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning agile robotic locomotion skills by imitating animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xue Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingnan</forename><surname>Coumans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsang-Wei Edward</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<biblScope unit="page" from="7" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fast animal pose estimation using deep neural networks. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Aldarondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Willmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kislin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Shaevitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno>abs/1608.02192</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Landscape pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Rougetet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
		<idno>abs/1411.4280</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deeppose: Human pose estimation via deep neural networks. CoRR, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4659</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mode-adaptive neural networks for quadruped motion control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Three-d safari: Learning to estimate zebra pose, shape, and texture from images &quot;in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><forename type="middle">Y</forename><surname>Berger-Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>abs/1908.07201</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
