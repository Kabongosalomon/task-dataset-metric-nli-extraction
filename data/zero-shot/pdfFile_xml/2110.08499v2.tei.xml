<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Xiao</surname></persName>
							<email>xiaowen3@cs.ubc.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
							<email>beltagy@allenai.org</email>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
							<email>carenini@cs.ubc.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Arman Cohan ? ? ? University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Paul G</orgName>
								<orgName type="institution" key="instit1">Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents. It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot, few-shot and full-supervised settings, PRIMERA outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-Document Summarization is the task of generating a summary from a cluster of related documents. State-of-the-art approaches to multi-document summarization are primarily either graph-based <ref type="bibr" target="#b4">(Liao et al., 2018;</ref><ref type="bibr" target="#b12">Pasunuru et al., 2021)</ref>, leveraging graph neural networks to connect information between the documents, or hierarchical <ref type="bibr" target="#b7">(Liu and Lapata, 2019a;</ref><ref type="bibr">Fabbri et al., 2019;</ref><ref type="bibr">Jin et al., 2020)</ref>, building intermediate representations of individual documents and then aggregating information across. While effective, these models either require domain-specific additional information e.g. Abstract Meaning Representation <ref type="bibr" target="#b4">(Liao et al., 2018)</ref>, or discourse graphs <ref type="bibr">(Christensen et al., 2013;</ref>, or use dataset-specific, customized architectures, making it difficult to leverage pretrained language models. Simultaneously, recent pretrained language models (typically encoder-decoder transformers) * Work mainly done during an internship at AI2. <ref type="bibr">1</ref> The code and pre-trained models can be found at https: //github.com/allenai/PRIMER  have shown the advantages of pretraining and transfer learning for generation and summarization <ref type="bibr" target="#b13">(Raffel et al., 2020;</ref><ref type="bibr">Lewis et al., 2020;</ref><ref type="bibr" target="#b1">Beltagy et al., 2020;</ref><ref type="bibr" target="#b18">Zaheer et al., 2020)</ref>. Yet, existing pretrained models either use single-document pretraining objectives or use encoder-only models that do not work for generation tasks like summarization (e.g., <ref type="bibr">CDLM, Caciularu et al., 2021)</ref>.</p><p>Therefore, we argue that these pretrained models are not necessarily the best fit for multi-document summarization. Alternatively, we propose a simple pretraining approach for multi-document summarization, reducing the need for dataset-specific architectures and large fine-tuning labeled data (See <ref type="figure" target="#fig_0">Figure 1</ref> to compare with other pretrained models). Our method is designed to teach the model to identify and aggregate salient information across a "cluster" of related documents during pretraining. Specifically, our approach uses the Gap Sentence Generation objective (GSG) <ref type="bibr" target="#b19">(Zhang et al., 2020)</ref>, i.e. masking out several sentences from the input document, and recovering them in order in the decoder. We propose a novel strategy for GSG sentence masking which we call, Entity Pyramid, inspired by the Pyramid Evaluation method <ref type="bibr" target="#b11">(Nenkova and Passonneau, 2004)</ref>. With Entity Pyramid, we mask salient sentences in the entire cluster then train the model to generate them, encouraging it to find important information across documents and aggregate it in one summary.</p><p>We conduct extensive experiments on 6 multidocument summarization datasets from 3 different domains. We show that despite its simplic-  ity, PRIMERA achieves superior performance compared with prior state-of-the-art pretrained models, as well as dataset-specific models in both few-shot and full fine-tuning settings. PRIMERA performs particularly strong in zero-and few-shot settings, significantly outperforming prior state-of-the-art up to 5 Rouge-1 points with as few as 10 examples. Our contributions are summarized below: 1. We release PRIMERA, the first pretrained generation model for multi-document inputs with focus on summarization. 2. We propose Entity Pyramid, a novel pretraining strategy that trains the model to select and aggregate salient information from documents. 3. We extensively evaluate PRIMERA on 6 datasets from 3 different domains for zero-shot, few-shot and fully-supervised settings. We show that PRIMERA outperforms current state-of-the-art on most of these evaluations with large margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>In this section, we discuss our proposed model PRIMERA, a new pretrained general model for multi-document summarization. Unlike prior work, PRIMERA minimizes dataset-specific modeling by simply concatenating a set of documents and processing them with a general efficient encoderdecoder transformer model ( ?2.1). The underlying transformer model is pretrained on an unlabeled multi-document dataset, with a new entity-based sentence masking objective to capture the salient information within a set of related documents ( ?2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Architecture and Input Structure</head><p>Our goal is to minimize dataset-specific modeling to leverage general pretrained transformer models for the multi-document task and make it easy to use in practice. Therefore, to summarize a set of related documents, we simply concatenate all the documents in a single long sequence, and process them with an encoder-decoder transformer model. Since the concatenated sequence is long, instead of more standard encoder-decoder transformers like <ref type="bibr">BART (Lewis et al., 2020)</ref> and T5 <ref type="bibr" target="#b13">(Raffel et al., 2020)</ref>, we use the Longformer-Encoder-Decoder (LED) Model <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref>, an efficient transformer model with linear complexity with respect to the input length. 2 LED uses a sparse local+global attention mechanism in the encoder self-attention side while using the full attention on decoder and cross-attention.</p><p>When concatenating, we add special document separator tokens (&lt;doc-sep&gt;) between the documents to make the model aware of the document boundaries ( <ref type="figure" target="#fig_1">Figure 2</ref>). We also assign global attention to these tokens which the model can use to share information across documents (Caciularu et al., 2021) (see ?5 for ablations of the effectiveness of this input structure and global attention).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pretraining objective</head><p>In summarization, task-inspired pretraining objectives have been shown to provide gains over general-purpose pretrained transformers (PEGASUS; <ref type="bibr" target="#b19">Zhang et al., 2020)</ref>. In particular, PE-GASUS introduces Gap Sentence Generation (GSG) as a pretraining objective where some sentences are masked in the input and the model is tasked to generate them. Following PEGASUS, we use the GSG objective, but introduce a new masking strategy designed for multi-document summarization. As in GSG, we select and mask out m summary-like sentences from the input documents we want to summarize, i.e. every selected sentence is replaced by a  <ref type="figure">Figure 3</ref>: An example on sentence selection by Principle vs our Entity Pyramid strategy. Italic text in red is the sentence with the highest Principle ROUGE scores, which is thereby chosen by the Principle Strategy. Most frequent entity 'Colorado' is shown with blue, followed by the Pyramid ROUGE scores in parenthesis. The final selected sentence by Entity Pyramid strategy is in italic. which is a better pseudo-summary than the ones selected by the Principle strategy. single token [sent-mask] in the input, and train the model to generate the concatenation of those sentences as a "pseudo-summary" <ref type="figure" target="#fig_1">(Figure 2</ref>). This is close to abstractive summarization because the model needs to reconstruct the masked sentences using the information in the rest of the documents.</p><p>The key idea is how to select sentences that best summarize or represent a set of related input documents (which we also call a "cluster"), not just a single document as in standard GSG. <ref type="bibr" target="#b19">Zhang et al. (2020)</ref> use three strategies -Random, Lead (first m sentences), and "Principle". The "Principle" method computes sentence salience score based on ROUGE score of each sentence, s i , w.r.t the rest of the document (D/{s i }), i.e. Score(s i ) = ROUGE(s i , D/{s i }). Intuitively, this assigns a high score to the sentences that have a high overlap with the other sentences.</p><p>However, we argue that a naive extension of such strategy to multi-document summarization would be sub-optimal since multi-document inputs typically include redundant information, and such strategy would prefer an exact match between sentences, resulting in a selection of less representative information.</p><p>For instance, <ref type="figure">Figure 3</ref> shows an example of sentences picked by the Principle strategy <ref type="bibr" target="#b19">(Zhang et al., 2020)</ref> vs our Entity Pyramid approach. The figure shows a cluster containing three news articles discussing a wildfire happened in Corolado, and the pseudo-summary of this cluster should be related to the location, time and consequence of the wildfire, but with the Principle strategy, the non-salient sentences quoting the words from an officer are assigned the highest score, as the exact same sentence appeared in two out of the three articles. In comparison, instead of the quoted words, our strategy selects the most representative sentences in the cluster with high frequency entities.</p><p>To address this limitation, we propose a new masking strategy inspired by the Pyramid Evaluation framework <ref type="bibr" target="#b11">(Nenkova and Passonneau, 2004)</ref> which was originally developed for evaluating summaries with multiple human written references. Our strategy aims to select sentences that best represent the entire cluster of input documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Entity Pyramid Masking</head><p>Pyramid Evaluation The Pyramid Evaluation method <ref type="bibr" target="#b11">(Nenkova and Passonneau, 2004)</ref> is based on the intuition that relevance of a unit of information can be determined by the number of references (i.e. gold standard) summaries that include it. The unit of information is called Summary Content Unit (SCU); words or phrases that represent single facts. These SCUs are first identified by human annotators in each reference summary, and they receive a score proportional to the number of reference summaries that contain them. A Pyramid Score for a candidate summary is then the normalized mean of the scores of the SCUs that it contains. One advantage of the Pyramid method is that it directly assesses the content quality.</p><p>Entity Pyramid Masking Inspired by how content saliency is measured in the Pyramid Evaluation, we hypothesize that a similar idea could be applied in multi-document summarization to identify salient sentences for masking. Specifically, for a cluster with multiple related documents, the more documents an SCU appears in, the more salient that information should be to the cluster. Therefore, it should be considered for inclusion in the pseudosummary in our masked sentence generation objective. However, SCUs in the original Pyramid Evaluation are human-annotated, which is not feasible for large scale pretraining. As a proxy, we explore leveraging information expressed as named entities, since they are key building blocks in extracting information from text about events/objects and the relationships between their participants/parts <ref type="bibr">(Jurafsky and Martin, 2009</ref> selected.append(cur_sent) 7:</p><p>if |selected| == m then 8:</p><p>Break 9: end if 10: end for 11: Return selected framework, we use the entity frequency in the cluster as a proxy for saliency. Concretely, as shown in <ref type="figure">Fig. 4</ref>, we have the following three steps to select salient sentences in our masking strategy: 1. Entity Extraction. We extract named entities using SpaCy <ref type="bibr">(Honnibal et al., 2020)</ref>. 3 2. Entity Pyramid Estimation. We then build an Entity Pyramid for estimating the salience of entities based on their document frequency, i.e. the number of documents each entity appears in. 3. Sentence Selection. Similar to the Pyramid evaluation framework, we identify salient sentences with respect to the cluster of related documents. Algorithm 1 shows the sentence selection procedure. As we aim to select the entities better representing the whole cluster instead of a single document, we first remove all entities from the Pyramid that appear only in one document. Next, we iteratively select entities from top of the pyramid to bottom (i.e., highest to lowest frequency), and then select sentences in the document that include the entity as the initial candidate set. Finally, within this candidate set, we find the most representative sentences to the cluster by measuring the content overlap of the sentence w.r.t documents other than the one it appears in. This final step supports the goal of our pretraining objective, namely to reconstruct sentences that can be recovered using information from other documents in the cluster, which encourages the model to better connect and aggregate information across multiple documents. Following <ref type="bibr" target="#b19">Zhang et al. (2020)</ref> we use ROUGE scores <ref type="bibr" target="#b5">(Lin, 2004)</ref> as a proxy for content overlap. For each sentence s i , we specifically define a Cluster ROUGE score as Score(s i ) = {doc j ?C,s i ? doc j } ROUGE(s i , doc j ) Where C is the cluster of related documents. Note that different from the importance heuristic defined in PEGASUS <ref type="bibr" target="#b19">(Zhang et al., 2020)</ref>, Entity Pyramid strategy favors sentences that are representative of more documents in the cluster than the exact matching between fewer documents (See <ref type="figure">Figure 3</ref> for a qualitative example.) . The benefit of our strategy is shown in an ablation study ( ?5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment Goals</head><p>We aim to answer the following questions:  (Beltagy et al., 2020) large as our model initialization, The length limits of input and output are 4096 and 1024, respectively, with sliding window size as w = 512 for local attention in the input. (More implementation details of pretraining process can be found in Appx ?A) Pretraining corpus For pretraining, our goal is to use a large resource where each instance is a set of related documents without any ground-truth summaries. The Newshead dataset (Gu et al., 2020) (row 1, <ref type="table" target="#tab_5">Table 1</ref>) is an ideal choice; it is a relatively large dataset, where every news event is associated with multiple news articles. Evaluation Datasets We evaluate our approach on wide variety of multi-document summarization datasets plus one single document dataset from various domains (News, Wikipedia, and Scientific literature). See <ref type="table" target="#tab_5">Table 1</ref> for dataset statistics and Appx. ?B for details of each dataset. Evaluation metrics Following previous works <ref type="bibr" target="#b19">(Zhang et al., 2020)</ref>, we use ROUGE scores (R-1, -2, and -L), which are the standard evaluation metrics, to evaluate the downstream task of multi-document summarization. 4 For better readability, we use AVG ROUGE scores (R-1, -2, and -L) for evaluation in the few-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Zero-and Few-shot Evaluation</head><p>Many existing works in adapting pretrained models for summarization require large amounts of finetuning data, which is often impractical for new domains. In contrast, since our pretraining strategy is mainly designed for multi-document summarization, we expect that our approach can quickly adapt to new datasets without the need for significant fine-tuning data. To test this hypothesis, we first provide evaluation results in zero and few-shot settings where the model is provided with no, or only a few <ref type="formula">(10 and</ref>  Comparison To better show the utility of our pretrained models, we compare with three state-of-theart pretrained generation models: BART (Lewis et al., 2020) 5 , PEGASUS <ref type="bibr" target="#b19">(Zhang et al., 2020)</ref> and Longformer-Encoder-Decoder(LED) <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref>. These pretrained models have been shown to outperform dataset-specific models in summarization <ref type="bibr">(Lewis et al., 2020;</ref><ref type="bibr" target="#b19">Zhang et al., 2020)</ref>, and because of pretraining, they are expected to also work well in the few-shot settings. As there is no prior work doing few-shot and zeroshot evaluations on all the datasets we consider, and also the results in the few-shot setting might be influenced by sampling variability (especially with only 10 examples) <ref type="bibr">(Bragg et al., 2021)</ref>, we run the same experiments for the compared models five times with different random seeds (shared with all the models), with the publicly available checkpoints . <ref type="bibr">6</ref> Similar to <ref type="bibr" target="#b12">Pasunuru et al. (2021)</ref>, the inputs of all the models are the concatenations of the documents within the clusters (in the same order), each document is truncated based on the input length limit divided by the total number of documents so Models Multi-News <ref type="formula">(256)</ref> Multi-XSci <ref type="formula">(128)</ref> WCEP <ref type="formula">(50)</ref> WikiSum <ref type="formula">(128)</ref> arXiv <ref type="formula">(300)</ref> DUC2004 <ref type="formula">(128)</ref> R <ref type="bibr" target="#b19">(Zhang et al., 2020)</ref>   <ref type="table">Table 2</ref>: Zero-shot results. The models in the first block use the full-length attention (O(n 2 )) and are pretrained on the single document datasets. The numbers in the parenthesis following each dataset indicate the output length limit set for inference. PEGASUS means results taken exactly from PEGASUS <ref type="bibr" target="#b19">(Zhang et al., 2020)</ref>, where available. that all documents are represented in the input. <ref type="bibr">7</ref> To preserve the same format as the corresponding pretrained models, we set the length limit of output for BART and PEGASUS exactly as their pretrained settings on all of the datasets (except for the zero-shot experiments, the details can be found in Sec.4.3). Regarding length limit of inputs, we tune the baselines by experimenting with 512, 1024, 4096 on Multi-News dataset in few-shot setting (10 data examples), and the model with length limit 512(PEGASUS)/1024(BART) achieves the best performance, thus we use this setting (detailed experiment results for different input lengths can be found in Appx. ?C.1). We use the same length limit as our model for the LED model, i.e. 4096/1024 for input and output respectively, for all the datasets.</p><formula xml:id="formula_0">-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L PEGASUS</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Zero-Shot Results</head><p>For zero-shot 8 abstractive summarization experiments, since the models have not been trained on the downstream datasets, the lengths of generated summaries mostly depend on the pretrained settings. Thus to better control the length of generated summaries and for a fair comparison between all models, following <ref type="bibr" target="#b20">Zhu et al. (2021)</ref>, we set the 7 Pilot experiments show simple truncation results in inferior performance, which is in line with <ref type="bibr" target="#b12">Pasunuru et al. (2021)</ref>. <ref type="bibr">8</ref> For clarity, by zero-shot we mean using the pretrained model directly without any additional supervision. length limit of the output at inference time to the average length of gold summaries. 9 Exploring other approaches to controlling length at inference time (e.g., <ref type="bibr" target="#b16">Wu et al., 2021)</ref> is an orthogonal direction, which we leave for future work. <ref type="table">Table 2</ref> shows the performance comparison among all the models. Results indicate that our model achieves substantial improvements compared with all the three baselines on most of the datasets. As our model is pretrained on clusters of documents with longer input and output, the benefit is stronger on the dataset with longer summaries, e.g. Multi-News and arXiv. Comparing PEGASUS and BART models, as the objective of PEGASUS is designed mainly for summarization tasks, not surprisingly it has relatively better performances across different datasets. Interestingly, LED underperforms other models, plausibly since part of the positional embeddings (1k to 4k) are not pretrained. Encouragingly, our model performs the best, demonstrating the benefits of our pretraining strategy for multi-document summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Few Shot Evaluation</head><p>Compared with the strict zero-shot scenario, fewshot experiments are closer to the practical scenarios, as it is arguably affordable to label dozens of examples for almost any application.</p><p>We fine-tune all of the four models on different subsets with 10 and 100 examples, and the results are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. (hyperparameter settings in Appx. ?D.1) Since R-1, -2, and -L show the same trend, we only present the average of the three metrics in the figure for brevity (full ROUGE scores can be found in Appx. <ref type="table" target="#tab_15">Table 8</ref>) To show the generality, all the results of few-shot experiments are the average over 5 runs on different subsets (shared by all the models).</p><p>The result of each run is obtained by the 'best' model chosen based on the ROUGE scores on a randomly sampled few-shot validation set with the same number of examples as the training set, which is similar with <ref type="bibr" target="#b19">Zhang et al. (2020)</ref>. Note that their reported best models have been selected based on the whole validation set which may give PEGA-SUS some advantage. Nevertheless, we argue that sampling few-shot validation sets as we do here is closer to real few-shot scenarios <ref type="bibr">(Bragg et al., 2021)</ref>.</p><p>Our model outperforms all baselines on all of the datasets with 10 and 100 examples demonstrating the benefits of our pretraining strategy and input structure. Comparing the performances of our model with the different number of training data fed in, our model converges faster than other models with as few as 10 data examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Fully Supervised Evaluation</head><p>To show the advantage of our pretrained model when there is abundant training data, we also train the model with the full training set (hyperparameter settings can be found in Appx. ?D.2). <ref type="table" target="#tab_8">Table 3</ref> shows the performance comparison with previous state-of-the-art 10 , along with the results of previous SOTA. We observe that PRIMERA achieves stateof-the-art results on Multi-News, WCEP, and arXiv, while slightly underperforming the prior work on Multi-XScience (R-1). One possible explanation is that in Multi-XScience clusters have less overlapping information than in the corpus on which PRIMERA was pretrained. In particular, the source documents in this dataset are the abstracts of all the publications cited in the related work paragraphs, which might be less similar to each other and the target related work(i.e., their summary) . PRIMERA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Previous SOTA PRIMERA    outperforms the LED model (State-of-the-art) on the arXiv dataset while using a sequence length 4x shorter (4K in PRIMERA v.s. 16K in LED), further showing that the pretraining and input structure of our model not only works for multi-document summarization, but can be also effective for summarizing single documents having multiple sections.</p><formula xml:id="formula_1">R-1 R-2 R-L R-1 R-2 R-L Multi-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Study</head><p>We conduct ablation studies on the Multi-News dataset in few-shot setting, to validate the contribution of each component in our pretrained models. Input structure: In <ref type="figure" target="#fig_5">Figure 6</ref> (a) we observe the effectiveness of both pretraining and the input structure (&lt;doc-sep&gt; tokens between documents and global attention on them). Sentence masking strategy: To isolate the effect of our proposed pretraining approach, we compare with a model with exactly the same architecture when pretrained on the same amount of data but using the PEGASUS <ref type="bibr" target="#b19">(Zhang et al., 2020)</ref> masking strategy instead of ours. In other words, we keep all the other settings the same (e.g., data, length limit of input and output, pretraining dataset, input structure, as well as the separators) and only modify the pretraining masking strategy. We run the same experiments under zero-/few-shot scenar-ios on the Multi-News dataset as in ?4.2, and the results are shown in <ref type="figure" target="#fig_5">Figure 6</ref> (b). The model pretrained with our Entity Pyramid strategy shows a clear improvement under few-shot scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Human Evaluation</head><p>We also conduct human evaluations to validate the effectiveness of PRIMERA on DUC2007 and TAC2008 <ref type="bibr">(Dang and Owczarzak, 2008)</ref> datasets in the few-shot setting (10/10/20 examples for train/valid/test). Both datasets consist of clusters of news articles, and DUC2007 contains longer inputs (25 v.s. 10 documents/cluster) and summaries (250 v.s. 100 words). Since the goal of our method is to enable the model to better aggregate information across documents, we evaluate the content quality of the generated summaries following the original Pyramid human evaluation framework <ref type="bibr" target="#b11">(Nenkova and Passonneau, 2004)</ref>. In addition, we also evaluate the fluency of generated summaries following the DUC guidelines. 12</p><p>Settings Three annotators 13 are hired to do both Pyramid Evaluation and Fluency evaluation, they harmonize the standards on one of the examples. Specifically, for each data example, we provide three anonymized system generated summaries, along with a list of SCUs. The annotators are asked to find all the covered SCUs for each summary, and score the fluency in terms of Grammaticality, Referential clarity and Structure &amp; Coherence, according to DUC human evaluation guidelines, with a scale 1-5 (worst to best). They are also suggested to make comparison between three generated summaries into consideration when scoring the fluency. To control for the ordering effect of the given summaries, we re-order the three summaries for each data example, and ensure the chance of their appearance in different order is the same (e.g. BART appears as summary A for 7 times, B for 7 times and C for 6 times for both datasets). The instruction for human annotation can be found in <ref type="figure" target="#fig_7">Figure 7</ref> and <ref type="figure" target="#fig_8">Figure 8</ref> in the appendix. Annotators were aware that annotations will be used solely for computing aggregate human evaluation metrics and reporting in the scientific paper.</p><p>Compared Models We compare our model with LED and PEGASUS in human evaluations. Be-12 https://www-nlpir.nist.gov/projects/ duc/duc2007/quality-questions.txt <ref type="bibr">13</ref> We recruited expert annotators with payment above average of the participants' demographics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>DUC2007 <ref type="formula">(20)</ref> TAC2008(20) Sr R P F Sr R P F PEGASUS 6.0 2.5 2.4 2.4 8.7 9.1 9.4 9.1 LED 9.6 3.9 4.0 3.8 6.9 7.1 10.8 8.4 PRIMERA 12.5 5.1 5.0 5.0 8.5 8.9 10.0 9.3  cause PEGASUS is a task-specific model for abstractive summarization, and LED has the same architecture and length limits as our model with the parameters inherited from BART, which is more comparable with our model than vanilla BART.</p><p>Pyramid Evaluation Both TAC and DUC datasets include SCU (Summary Content Unit) annotations and weights identified by experienced annotators. We then ask 3 annotators to make a binary decision whether each SCU is covered in a candidate summary. Following <ref type="bibr" target="#b11">Nenkova and Passonneau (2004)</ref>, the raw score of each summary is then computed by the sum of weights of the covered SCUs, i.e. S r = SCU w i I(SCU i ), where I(SCU i ) is an indicator function on whether SCU i is covered by the current summary, and w i is the weight of SCU i . In the original pyramid evaluation, the final score is computed by the ratio of S r to the maximum possible weights with the same number of SCUs as in the generated summaries. However, the total number of SCUs of generated summaries is not available in the simplified annotations in our design. To take consideration of the length of generated summaries and make a fair comparison, instead, we compute Recall, Precision and F-1 score regarding lengths of both gold references and system generated summaries as R= S r len(gold)</p><p>; P= S r len(sys)</p><p>; F1= 2 ? R ? P (R + P )</p><p>Fluency Evaluation Fluency results can be found in <ref type="table" target="#tab_10">Table 5</ref>, and PRIMERA has the best performance on both datasets in terms of all aspects. Only for Grammaticality PRIMERA's top performance is matched by PEGASUS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Neural Multi-Document Summarization These models can be categorized into two classes, graph-based models <ref type="bibr" target="#b17">(Yasunaga et al., 2017;</ref><ref type="bibr" target="#b4">Liao et al., 2018;</ref><ref type="bibr" target="#b12">Pasunuru et al., 2021)</ref> and hierarchical models <ref type="bibr" target="#b7">(Liu and Lapata, 2019a;</ref><ref type="bibr">Fabbri et al., 2019;</ref><ref type="bibr">Jin et al., 2020)</ref>. Graph-based models often require auxiliary information (e.g., AMR, discourse structure) to build an input graph, making them reliant on auxiliary models and less general. Hierarchical models are another class of models for multi-document summarization, examples of which include multi-head pooling and inter-paragraph attention <ref type="bibr" target="#b7">(Liu and Lapata, 2019a)</ref>, MMR-based attention <ref type="bibr">(Fabbri et al., 2019;</ref><ref type="bibr" target="#b10">Mao et al., 2020)</ref>, and attention across representations of different granularity (words, sentences, and documents) <ref type="bibr">(Jin et al., 2020)</ref>. Prior work has also shown the advantages of customized optimization in multi-document summarization (e.g., RL; <ref type="bibr" target="#b15">Su et al., 2021)</ref>. Such models are often dataset-specific and difficult to develop and adapt to other datasets or tasks.</p><p>Pretrained Models for Summarization Pretrained language models have been successfully applied to summarization, e.g., BERTSUM <ref type="bibr" target="#b8">(Liu and Lapata, 2019b)</ref>, BART (Lewis et al., 2020), T5 <ref type="bibr" target="#b13">(Raffel et al., 2020)</ref>. Instead of regular language modeling objectives, PEGASUS <ref type="bibr" target="#b19">(Zhang et al., 2020)</ref> introduced a pretraining objective with a focus on summarization, using Gap Sentence Generation, where the model is tasked to generate summaryworthy sentences, and Zou et al. <ref type="formula">(2020)</ref> proposed different pretraining objectives to reinstate the original document, specifically for summarization task as well. Contemporaneous work by <ref type="bibr" target="#b14">Rothe et al. (2021)</ref> argued that task-specific pretraining does not always help for summarization, however, their experiments are limited to single-document summarization datasets. Pretraining on the titles of HTMLs has been recently shown to be useful for few-shot short-length single-document summarization as well <ref type="bibr" target="#b0">(Aghajanyan et al., 2021)</ref>. Goodwin et al. (2020) evaluate three state-of-the-art models (BART, PEGASUS, T5) on several multi-document summarization datasets with low-resource settings, showing that abstractive multi-document summarization remains challenging. Efficient pretrained transformers (e.g., Longformer <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref> and BigBird <ref type="bibr" target="#b18">(Zaheer et al., 2020</ref>) that can process long sequences have been also proven successful in summarization, typically by the ability to process long inputs, connecting information across the entire sequence. CDLM <ref type="figure" target="#fig_0">(Caciularu et al., 2021)</ref> is a follow-up work for pretraining the Longformer model in a cross-document setting using global attention on masked tokens during pretraining. However, this model only addresses encoder-specific tasks and it is not suitable for generation. In this work, we show how efficient transformers can be pretrained using a task-inspired pretraining objective for multi-document summarization. Our proposed method is also related to the PMI-based token masking Levine et al. (2020) which improves over random token masking outside summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>In this paper, we present PRIMERA a pre-trained model for multi-document summarization. Unlike prior work, PRIMERA minimizes dataset-specific modeling by using a Longformer model pretrained with a novel entity-based sentence masking objective. The pretraining objective is designed to help the model connect and aggregate information across input documents. PRIMERA outperforms prior state-of-the-art pre-trained and datasetspecific models on 6 summarization datasets from 3 different domains, on zero, few-shot, and full fine-tuning setting. PRIMERA's top performance is also revealed by human evaluation.</p><p>In zero-shot setting, we can only control the output length of generated summaries at inference time by specifying a length limit during decoding. Exploring a controllable generator in which the desired length can be injected as part of the input is a natural future direction. Besides the summarization task, we would like to explore using PRIMERA for other generation tasks with multiple documents as input, like multi-hop question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Concern</head><p>While there is limited risk associated with our work, similar to existing state-of-the-art generation models, there is no guarantee that our model will always generate factual content. Therefore, caution must be exercised when the model is deployed in practical settings. Factuality is an open problem in existing generation models. A Implementation details of pre-training</p><p>As the multi-document summarization task has a higher compression ratio, defined as len(Summary)/len(Input), (e.g. 12% for Multi-News dataset and 15% for Multi-Xscience dataset), we use 15% as the ratio of masked sentences for generation. In addition to this 15% masked sentences, following PEGASUS <ref type="bibr" target="#b19">(Zhang et al., 2020)</ref>, we also copy an additional 15% of the input sentences to the output without masking them in the input. This allows the model to also learn to copy information from the source directly and found to be useful by <ref type="bibr" target="#b19">Zhang et al. (2020)</ref>.</p><p>We pretrain the model for 100K steps, with early stopping, batch size of 16, Adam optimizer with a learning rate of 3e?5 following <ref type="bibr" target="#b1">Beltagy et al. (2020)</ref>, with 10K warmup steps and linear decay. The pretraining process takes likely 7 days on 4 A100 GPUs.</p><p>As the backbone of PRIMERA is the Longformer Encoder Decoder model (LED), it has the same number of parameters with LED (447M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Detailed Description on the Evaluation Datasets</head><p>The details of evaluation datasets can be found below.</p><p>Multi-News (Fabbri et al., 2019): A multidocument dataset with summaries written by professional editors from the newser.com.</p><p>Wikisum  Each summary is a Wikipedia article, and the source documents are either citations in the reference section or the Web Search results of section titles. 14 In our experiments, we use the data crawled by <ref type="bibr" target="#b7">Liu and Lapata (2019a)</ref>.</p><p>WCEP (Gholipour Ghalandari et al., 2020) is built based on news events from Wikipedia Current Events Portal and the references are obtained similar to Wikisum. There are at most 100 documents within each cluster in the original dataset, thus we remove all the duplicates and only keep up to 10 documents for each cluster based on the relevance score in the original dataset, which is similar to the WCEP-10 variant in the original paper.</p><p>Multi-X-Science <ref type="bibr" target="#b9">(Lu et al., 2020)</ref> a multidocument summarization dataset created from scientific articles, the summaries are paragraphs of related work section, while source documents include the abstracts of the query and referred papers.</p><p>DUC benchmarks <ref type="bibr">(Dang, 2005)</ref> include multidocument summarization datasets in the news domain, with 10-30 documents and 3-4 humanwritten summaries per cluster. Since these datasets are small, we use them primarily for a few-shot evaluation. We use DUC2003 for training (only one of the reference summaries for each document is used for training) and DUC2004 as test.</p><p>ArXiv <ref type="bibr">(Cohan et al., 2018</ref>) is a single document summarization dataset in the scientific paper domain. Each document is a scientific paper, and the summary is the corresponding abstract. As each scientific paper consists of multiple sections, we treat each section as a separate document within a cluster in our experiments. This is to evaluate our model's effectiveness on summarizing single documents having multiple sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details on Compared models</head><p>The details of compared models in the zero-/fewshot setting can be found below.</p><p>BART (Lewis et al., 2020) an encoder-decoder transformer model pretrained on the objective of reconstructing the corrupted documents in multiple ways, e.g. Token Deletion, Text Infilling, Sentence Rotation and etc.</p><p>PEGASUS <ref type="bibr" target="#b19">(Zhang et al., 2020)</ref> a pretrained model designed for abstractive summarization as the downstream task, especially for the single document input. It is trained on the objective of Gap Sentence Generation on C4 <ref type="bibr" target="#b13">(Raffel et al., 2020)</ref> and Hugenews datasets (Note that the pretraining data size in PEGASUS is magnitudes larger than ours). As it is only evaluated on one multi-document summarization dataset (Multi-news), we rerun the model on all the datasets. To verify the quality of our reproduction, the average ROUGE scores of our re-run model vs. 42.3 13.7 19.7 37.6 10.7 18.8 4096 37.9 11.0 17.5 34.9 8.7 17.6 <ref type="table">Table 6</ref>: The ROUGE score (R-1/R-2/R-3) for pretrained models (BART and PEGASUS) with different input length limit in few-shot setting (10 data example) on the multi-news dataset. The results are the average over 5 runs on different subsets (the same seeds shared with all the other models in this paper).</p><p>Longformer Encoder-Decoder (LED) <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref> is the initial state of our model before pretraining. The parameters of LED are inherited from the BART model, and to enable the model to deal with longer input, the position embeddings are repeatedly copied from BART's 1K position embeddings. It is different from our model with respect to both pretraining and input structure (document separators and global attentions), with global attention on the (&lt;s&gt;) token only and no document separators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Detailed Experiment for Input Length Limit</head><p>We run an experiment to select the proper length limit for compared pretrained models, i.e. BART and PEGASUS. Specifically, we train both models with different input length limits (512/1024/4096) in the few-shot setting (with 10 data examples) on the multi-news dataset. Similar as the few-shot experiments described in ?4.2, we train each model with each specific input length limit for 5 times on different subsets, which are shared by all the models. As shown in <ref type="table">Table 6</ref>, BART with length limit 1024 performs the best and PEGASUS with length limit 512 performs the best, thus in all our experiments, we use 1024 as the input length limit for BART and 512 for PEGASUS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Hyperparameters in Few-shot and Full Supervised Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Few-shot Experiments</head><p>We use Adam as the optimizer with linear scheduled learning rate 3e ? 5 for BART, LED and our model, and use the default optimization settings of the few-shot experiments from <ref type="bibr" target="#b19">Zhang et al. (2020)</ref>, i.e. AdaFactor optimizer with scheduled learning rate 5e ? 4. For all the experiments with 10 examples, the batch size is 10, the models are trained for 200 steps, with warm-up as 20 steps. For the experiments with 100 examples, we use the same batch size, with the total step and warm-up step set to be 1000 and 100, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Fully Supervised Experiments</head><p>We use Adam as the optimizer with linear scheduled learning rate 3e ? 5, and batch size as 16 for all the datasets in the full supervised experiments. The number of steps and warm-up steps are set based on the size of the datasets. The details can be found in <ref type="table" target="#tab_13">Table 7</ref> Dataset  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Detailed Results in Few-shot Setting</head><p>The exact ROUGE scores in <ref type="figure" target="#fig_3">Figure 5</ref> are shown in <ref type="table" target="#tab_15">Table 8</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Detailed Analysis on Fully Supervised Experiments</head><p>To show the advantage of our pre-trained model when there is sufficient data, we also train the model with the full training set, and the results can be found in <ref type="table">Table 9</ref>-12 15 , along with the results from previous works. Differently from the zero-/few-shot experiments, here we report the state-of-the-art results on different datasets, as they were presented in the corresponding original papers. Since we use the same train/valid/test set as in those prior works, we can perform a fair comparison , without re-running all those extremely time-consuming experiments . Overall, our model achieves state-of-the-art on Multi-News (see <ref type="table">Table 9</ref> , WCEP dataset (see <ref type="table" target="#tab_5">Table 11</ref>) and arXiv dataset (see <ref type="table" target="#tab_5">Table 12</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>ROUGE-1 ROUGE-2 ROUGE-L PEGASUS <ref type="bibr" target="#b19">(Zhang et al., 2020)</ref> 47.52 18.72 24.91 BART-Long-Graph <ref type="bibr" target="#b12">(Pasunuru et al., 2021)</ref> 49.03 19.04 24.04 BART-Long-Graph(1000) <ref type="bibr" target="#b12">(Pasunuru et al., 2021)</ref> 49.24 18.99 23.97 BART-Long(1000) <ref type="bibr" target="#b12">(Pasunuru et al., 2021)</ref> 49  <ref type="table">Table 9</ref>: ROUGE scores of the previous models and our fully supervised model on the Multi-News dataset. The results of PEGASUS is from <ref type="bibr" target="#b19">Zhang et al. (2020)</ref>, and the other results are from <ref type="bibr" target="#b12">Pasunuru et al. (2021)</ref> Multi-News The experiment results on Multi-News dataset can be found in <ref type="table">Table 9</ref>. Specifically, the PEGASUS model <ref type="bibr" target="#b19">(Zhang et al., 2020)</ref> is pretrained on a large-scale single-document dataset with the Gap Sentence Generation objective, which is the same as ours, but with a different masking strategy, BART-Long <ref type="bibr" target="#b12">(Pasunuru et al., 2021)</ref> uses the same model structure as ours , and BART-Long-Graph <ref type="bibr" target="#b12">(Pasunuru et al., 2021)</ref> additionally has discourse graph injected. Comparing the results with the BART-Long model, our model is around 1 ROUGE point higher, which may result from either better model structure or pre-training. Interestingly, in one of the ablation studies in <ref type="bibr" target="#b12">Pasunuru et al. (2021)</ref>, they find that the BART-Long model achieves its best performance with the length limit of 1000, and no further improvement is found when the length limit is greater than that. Thus we may conclude the gap between the performances is mainly from our design on the model, i.e. the document separators, proper global attention as well as the pre-training on a multi-document dataset. <ref type="bibr">15</ref> Due to the lack of computational resources, we do not train the model on Wikisum.   WCEP As for the WCEP dataset, BERTREG (Gholipour Ghalandari et al., 2020) is a Regressionbased sentence ranking system with BERT embedding, which is used as extractive summarization method, while Submodular+Abs is a simple two-step abstractive summarization model with a submodular-based extractive summarizer followed by a bottom-up abstractive summarizer <ref type="bibr">(Gehrmann et al., 2018)</ref>. DynE is a BART-based abstractive approach, which is to ensemble multiple input, allowing single document summarization models to be directly leveraged on the multi-document summarization task. Our model outperforms all the models by a large margin, including the SOTA model DynE, and it may indicate that the plain structure is more effective than purely ensembling the output of single documents.</p><p>arXiv In addition to the experiments on multidocument summarization datasets, we also compare our fully supervised model with previous works on the arXiv dataset, with each section treated as a single document. All the models to be compared with are based on pre-trained models, and Bigbird-PEGASUS and LED utilize the pre-training of PEGASUS <ref type="bibr" target="#b18">(Zaheer et al., 2020)</ref> and <ref type="bibr">BART (Lewis et al., 2020)</ref>, respectively. However, both Bigbird and LED apply more efficient attentions, which make the models able to take longer   <ref type="bibr">heer et al., 2020)</ref>, and the results of LED are from <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref>. The number in the parenthesis indicates the length limit of the input.</p><p>input (3k for BigBird, 4K and 16k for LED). Our model has a better performance than all the models, including LED(16K), which allows for the input 4 times longer than ours. It is worth mentioning that LED(4K) has the same structure as our model, with the same length limit of the input, and with the pre-training on multi-document datasets, our model is more than 3 ROUGE point better than it, which shows that the strategy not only works for multi-document summarization but can also effectively improve single-document summarization for long documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Examples of Generated Summaries</head><p>We show an example (from Multi-News) of generated summaries by PRIMERA and compared models trained with different number of examples in <ref type="table" target="#tab_5">Table 13</ref>. And we show an example from DUC2007 (which is one of the examples used for human evaluation) with generated summaries by PRIMERA and two compared models in I Annotation Instructions for Human Evaluation <ref type="figure" target="#fig_7">Figure 7</ref> and <ref type="figure" target="#fig_8">Figure 8</ref> shows the annotation instruction for human annotators.   Model Summaries PRIMERA-0 BALTIMORE (AP) -The U.S. Coast Guard says it's looking for a man who has cost the service about $500,000 after responding to nearly 30 of his fake distress calls.. The Coast Guard also said such false distress alerts detract from their ability to respond to actual alerts.Anyone with information about the caller is asked to contact the Coast Guard's regional command center at 410-576-2525 or email investigators at CGIS-Baltimore@uscg.mil..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PRIMERA-10</head><p>The U.S. Coast Guard says it's looking for a man who has cost the service about $500,000 after responding to nearly 30 of his fake distress calls. In a press release published Friday, the Coast Guard says the 28 calls have originated from around the area of Annapolis, Maryland. Each call involved the same male voice and used an emergency radio channel. The two most recent calls were made on the night of July 21 and the early morning of July 22. The Coast Guard also says hoax calls distract rescuers from real emergencies, putting both the public and the responding crews at risk. "Calls like these not only put our crews at risk, but they put the lives of the public at risk," said Lt. Cmdr. Sara Wallace, who heads the response sector in Maryland. "Calls like these not only put our crews at risk, but they put the lives of the public at risk." Anyone with information about the caller is asked to contact the Coast Guard's regional command center at 410-576-2525 or email investigators at CGIS-Baltimore@uscg.mil.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PRIMERA-100</head><p>The US Coast Guard is looking for a man who has cost the service about $500,000 after making 28 false distress calls from Maryland. The 28 calls have involved the same male voice and used an emergency radio channel, the AP reports. The Coast Guard says it has responded to nearly 30 of the fake calls and that the estimated cost of the responses is $500,000. The caller has been making the calls since July 2014. He could be facing a six-year prison sentence, a $10,000 civil fine, and a $250,000 criminal fine, the AP reports. "A hoax call is a deadly and serious offense," says Lt. Cmdr. Sara Wallace, head of the response sector in Maryland. "Calls like these not only put our crews at risk, but they put the lives of the public at risk."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PEGASUS-100</head><p>The US Coast Guard has responded to 28 false distress calls in two years-and it's a man who's been making the calls, not the waves. The unidentified caller has made the calls in Annapolis, Maryland, since July 2014, the Coast Guard reports. Each call involved the same male voice and used an emergency radio channel. The two most recent calls were made on the night of July 21 and early morning of July 22. The Coast Guard says the calls were made in the hope that the public would get into the water. Rather than responding to actual alerts, the calls were made to divert the attention of rescue crews from real emergencies, the AP reports. The Coast Guard says the cost of the responses to these false alarms is about $500,000. The caller made 28 false distress alerts from Annapolis, Maryland, beginning in July 2014, according to the Coast Guard. The estimated cost of the responses to these false alarms is $500,000. The Coast Guard says the calls were made in the hope that the public would get into the water. Rather than responding to actual alerts, the calls were made to divert the attention of rescue crews from real emergencies, the AP reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fully Supervised</head><p>The US Coast Guard says it's looking for a man who has cost the service about $500,000 after responding to nearly 30 of his fake distress calls, the AP reports. In a press release published Friday, the Coast Guard says the 28 calls have originated from around the area of Annapolis, Maryland. Each call involved the same male voice and used an emergency radio channel. The two most recent calls were made on the night of July 21 and the early morning of July 22. The Coast Guard also says hoax calls distract rescuers from real emergencies, putting both the public and the responding crews at risk. "A hoax call is a deadly and serious offense," says Lt. Cmdr. Sara Wallace, who heads the response sector in Maryland. "Calls like these not only put our crews at risk, but they put the lives of the public at risk." Making false distress calls is a felony, punishable by a maximum of six years in prison, a $10,000 civil fine, $250,000 criminal fine, and a reimbursement to the Coast Guard for their efforts, the Coast Guard says.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>The US Coast Guard says it's looking for a man who has cost the service about $500,000 after responding to nearly 30 of his fake distress calls, reports the AP. In a press release published Friday, the Coast Guard says the 28 calls have originated from around the area of Annapolis, Maryland. Each call involved the same male voice and used an emergency radio channel. He's been making the calls since July 2014. The two most recent calls were made on the night of July 21 and the early morning of July 22. "A hoax call is a deadly and serious offense," a Coast Guard rep tells NBC4, which notes that such calls are a felony that carry six years in prison, $10,000 civil fine, $250,000 criminal fine, and reimbursement to the Coast Guard. "Calls like these not only put our crews at risk, but they put the lives of the public at risk." </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PEGASUS</head><p>In 1996, Congress passed the Line-Item Veto Act, which gave the president the power to cut individual projects from tax and spending bills without vetoing the entire legislation. The act was followed by the President's line-item veto, which he used to trim 144 million dollars from a 248 billion dollars defense spending bill. He also used the veto power to block a congressional rejection of his line-item veto on 38 military construction projects. The bill was passed by the House and the President signed it into law. The veto was challenged by members of both parties who said it was unconstitutional because it gave the president unchecked power to rewrite legislation. The Supreme Court agreed on Friday to hear argument and decide the constitutionality of the president line-item veto. In 1998 the President used his line-item veto to cut $38 million from a military construction bill. In 1999 the President used his line-item veto to cut $54 million from a military spending bill. In 2000 the President used his line-item veto to cut $54 million from a defense spending bill. In January the President vetoed a tax and spending bill, which gave him the power to cut individual projects from tax and spending bills without vetoing the entire legislation. In February the President vetoed a spending bill, which gave him the power to cut individual projects from tax and spending bills without vetoing the entire legislation. In September the President used his line-item veto to cut $54 million from a defense spending bill. The bill was rejected by the House and the President vetoed it. In November the President used his line-item veto to trim 144 million dollars from a defense spending bill. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LED</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PRIMERA</head><p>In 1996, Congress gave the President the power to veto specific items of tax and spending bills. Before the law's enactment in 1996, the president could veto an entire spending measure but not individual items. The court ruled that such a specialized veto can be authorized only through a constitutional amendment. In January 1997, the line-item veto law was passed. It was passed under the Republican Party's "Contract with Congress". It was passed after President Clinton vetoed thirteen relatively obscure research and spending programs, almost all of the military spending increases approved by Congress. In October 1998, Clinton used his line-item veto authority to have trimmed 144 million U.S. dollars from a 248 billion defense spending bill. In November 1998, Clinton vetoed 38 military construction projects, worth 287 million U.S. dollars. In February 1999, the Justice Department appealed the line-item veto law to the Supreme Court, which agreed to hear argument and decide the constitutionality of the law. Earlier this month, a federal judge struck down the line-item veto law as unconstitutional. The highest court's review will yield a momentous balance of powers ruling. The case is scheduled to be argued before the justices on April 27. The line item veto, strongly supported by President Bill Clinton and a number of his predecessors, was passed in 1996 under the Republican Party's "Contract with Congress". It was passed in January 1997. Before the law's enactment, the only way presidents could reject spending laws was to veto whole budget bills. In 1996, Congress gave the president the power to cancel individual items in tax and spending bills. In January 1997, the line-item veto law was passed. It was passed under the Republican Party's "Contract with Congress". It was passed in January 1997. In 1998, President Clinton threatened to veto some items of the military construction bill because of the increased funding. In November 1998, Clinton used his line-item veto power to delete 38 projects in 24 states worth 287 million U.S. dollars. In February 1999, the Justice Department appealed the line-item veto law to the Supreme Court, which agreed to hear a case about its constitutionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>In 1996 a Republican congress overwhelmingly passed a Line Item Veto Act allowing presidents (including the incumbent Democratic president), to strike individual tax or spending items within 5 days after signing a bill into law. Congress could restore those items in a new bill passed by majority vote. If the president vetoed that bill, Congress could override that veto with a two-thirds majority. Proponents argued that the law preserved the integrity of federal spending, saved billions of dollars, and that it did not repeal any portion of a law, but was simply a delegated spending authorization from Congress. In January 1997, the first year of the law, the president vetoed 163 line-items in six bills, and in 1998 82 line-items in 11 bills. In October 1997 Congress overrode the president's line-item veto against 36 of 38 military construction projects. Initial 1997 efforts by congressmen to challenge the law in the Supreme Court were rejected due to lack of standing. On June 25, 1998 after lower courts rejected the Line Item Veto Act as unconstitutional, on appeal by the White House the Supreme Court ruled 6-3 that Congress unconstitutionally violated the principle of separation of powers, because that procedure allows the president to create a law that was not voted on by either house of Congress in violation of the Constitution's Article I "presentment" clause. A constitutional amendment would be required to institute line item vetoes. Justices Breyer and Scalia argued similar dissenting opinions that separation of powers was not violated. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>PRIMERA vs existing pretrained models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Model Structure of PRIMERA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>100) training examples. Obtaining such a small number of examples should be viable in practice for new datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The AVG ROUGE scores (R-1, R-2 and R-L) of the pretrained models with 0, 10 and 100 training data with variance. All the results of few-shot experiments (10 and 100) are obtained by the average of 5 random runs (with std, and the same set of seeds shared by all the models). Note that DUC2004 only has 50 examples, we use 20/10/20 for train/valid/test in the few-shot experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Ablation study with the few-shot setting on the Multi-News dataset regarding to (a) input Structure (&lt;doc-sep&gt; tokens between documents and global attention on them) and pretraining, (b) pretraining using PEGASUS vs our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(the ones reported on the paper) with 10 examples and 100 examples fed are 23.81 ? 0.79 vs. (24.13) and 25.86 ? 0.41 vs. (25.48), with minor differences plausibly resulting from different samplings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Annotation instruction for human annotators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Annotation instruction for human annotators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>In 1996, the Republican-led Congress passed the Line Item Veto Act, giving the president the power to delete individual items of spending and tax bills. Clinton used the power to cut individual projects from tax and spending bills. In February 1999, the President Clinton vetoed a congressional rejection of his line-item veto on 38 military construction projects. In May 1999, the President Clinton used the line-item veto to cut individual items of spending and tax breaks. In 2000, the President Clinton used the line-item veto to cancel individual items of spending and tax breaks. In May 2000, the President Clinton threatened to use the line-item veto to cancel all military spending and tax breaks. In June 2000, the President Clinton used the line-item veto to cut individual items of spending and tax breaks. In August 2000, the President Clinton used the line-item veto to cut individual items of spending and tax breaks. In September 2000, the President Clinton used the line-item veto to cut individual items of spending and tax breaks. In 2000, the President Clinton used the line-item veto to cut individual items of spending and tax breaks. In 2001, the President Clinton used the line-item veto to cut individual items of spending and tax breaks. In June 2000, the President Clinton used the line-item veto to cut individual items of spending and tax breaks. In August 2000, the President Clinton used the line-item veto to cut individual items of spending and tax breaks. In September 2000, the President Clinton used the line-item veto to cut individual items of spending and tax breaks. In 2000, the President Clinton used the line-item veto to cancel individual items of spending and tax breaks. In 2001, the President Clinton used the line-item veto to cut individual items of spending and tax breaks. In June 2000, the President Clinton used the line-item veto to cut individual items of spending and tax breaks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Six wildfires were ... Wildfires continued ... [sent mask] &lt;doc-sep&gt; The Buffalo ... &lt;/s&gt; Wildfires have burned ... . In Colorado 's Documents are separated with &lt;doc-sep&gt; tokens and they are assigned global attention. Other tokens except for &lt;s&gt; have local attention only. Selected sentences are replaced with a special [sent mask] token The model is trained to generate the masked sentences</figDesc><table><row><cell></cell><cell>Sent 1</cell><cell>Sent 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Masked Sentence</cell><cell>&lt;s&gt;</cell><cell></cell><cell>...</cell><cell>&lt;/s&gt;</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tokens with</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Global Attention</cell></row><row><cell>Global Attention Local Attention</cell><cell cols="2">Longformer Encoder Decoder (LED)</cell><cell></cell><cell></cell><cell></cell><cell>Tokens with Local Attention</cell></row><row><cell>&lt;s&gt; [sent mask]</cell><cell>&lt;doc-sep&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>&lt;doc-sep&gt;</cell></row><row><cell>Doc 1</cell><cell>w/ Global Attention</cell><cell>Doc 2</cell><cell cols="2">w/ Global Attention</cell><cell>Doc 3</cell><cell>w/ Global Attention</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Document #1Wildfires have burned across tens of thousands of acres of parched terrain in Colorado, spurring thousands of evacuations ...(0.107)..., residents have sought shelter in middle schools, and local officials fear tourists usually drawn to the region for the summer may not come. Document #2 ... In Colorado's southwest, authorities have shuttered the San Juan National Forest in southwestern Colorado and residents of more than 2,000 homes were forced to evacuate.(0.187) No homes had been destroyed ... "Under current conditions, one abandoned campfire or spark could cause a catastrophic wildfire, ..., with human life and property," said San Juan National Forest Fire Staff Officer Richard Bustamante... Document #3 The Buffalo Fire west of Denver is ... Several wildfires in Colorado have prompted thousands of home evacuations ...(0.172)... Nearly 1,400 homes have been evacuated in Summit County, Colorado, ...(0.179)... "Under current conditions, one abandoned campfire or spark could cause a catastrophic wildfire, ... , with human life and property," said Richard Bustamante, SJNF forest fire staff officer ... Entities with High Frequency Colorado, 416, Tuesday, Wildfires, San Juan National Forest,...</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="4">: The statistics of all the datasets we explore in</cell></row><row><cell cols="4">this paper. *We use subsets of Wikisum (10/100, 3200)</cell></row><row><cell cols="2">for few-shot training and testing only.</cell><cell></cell><cell></cell></row><row><cell cols="4">strategy, compared with the strategy used in PEGA-</cell></row><row><cell>SUS? See  ?5.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">? Q5: Is PRIMERA able to capture salient informa-</cell></row><row><cell cols="3">tion and generate fluent summaries? See  ?6.</cell><cell></cell></row><row><cell cols="4">With these goals, we explore the effectiveness of</cell></row><row><cell cols="4">PRIMERA quantitatively on multi-document sum-</cell></row><row><cell cols="4">marization benchmarks, verify the improvements</cell></row><row><cell cols="4">by comparing PRIMERA with multiple existing pre-</cell></row><row><cell cols="4">trained models and SOTA models, and further vali-</cell></row><row><cell cols="4">date the contribution of each component with care-</cell></row><row><cell cols="4">fully controlled ablations. An additional human</cell></row><row><cell cols="4">evaluation is conducted to show PRIMERA is able</cell></row><row><cell cols="4">to capture salient information and generate more</cell></row><row><cell>fluent summaries.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4 Experiments</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4.1 Experimental Setup</cell><cell></cell><cell></cell></row><row><cell>Implementation</cell><cell>Details We</cell><cell>use</cell><cell>the</cell></row><row><cell cols="3">Longformer-Encoder-Decoder (LED)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>10.1 16.7 27.6 4.6 15.3 33.2 12.7 23.8 24.6 5.5 15.0 29.5 7.9 17.1 32.7 7.4 17.6 BART (our run) 27.3 6.2 15.1 18.9 2.6 12.3 20.2 5.7 15.3 21.6 5.5 15.0 29.2 7.5 16.9 24.1 4.0 15.3 15.0 3.1 10.8 16.6 3.0 12.0 PRIMERA (our model) 42.0 13.6 20.8 29.1 4.6 15.7 28.0 10.3 20.9 28.0 8.0 18.0 34.6 9.4 18.3 35.1 7.2 17.9</figDesc><table><row><cell cols="2">36.5 10.5 18.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-28.1 6.6 17.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PEGASUS (our run) 32.0 LED (our run) 17.3</cell><cell cols="3">3.7 10.4 14.6 1.9</cell><cell cols="2">9.9 18.8</cell><cell cols="4">5.4 14.7 10.5 2.4</cell><cell>8.6</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>News 49.2 19.6 24.5 49.9 21.1 25.9 Multi-XScience 33.9 6.8 18.2 31.9 7.4 18.0 WCEP 35.4 15.1 25.6 46.1 25.2 37.9 arXiv 46.6 19.6 41.8 47.6 20.8 42.6</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Fully supervised results. Previous SOTA are from Pasunuru et al. (2021) for Multi-News, Lu et al. (2020) for Multi-XScience 11 , Hokamp et al. (2020) for WCEP, and Beltagy et al. (2020) for arXiv.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Pyramid Evaluation results: Raw scores S r , (R)ecall, (P)recision and (F)-1 score. For readability, Recall, Precision and F-1 scores are multiplied by 100. Ref. Str.&amp;Coh. Gram. Ref. Str.&amp;Coh.</figDesc><table><row><cell cols="3">DUC2007(20) Gram. PEGASUS 4.45 4.35 Model 1.95</cell><cell cols="2">TAC2008(20) 4.40 4.20 3.20</cell></row><row><cell>LED</cell><cell>4.35 4.50</cell><cell>3.20</cell><cell>3.10 3.80</cell><cell>2.55</cell></row><row><cell cols="2">PRIMERA 4.70 4.65</cell><cell>3.70</cell><cell>4.40 4.45</cell><cell>4.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>The results of Fluency Evaluation on two datasets, in terms of the Grammaticality , Referential clarity and Structure &amp; Coherence.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Gholipour Ghalandari, Chris Hokamp, Nghia The Pham, John Glover, and Georgiana Ifrim. 2020. A large-scale multi-document summarization dataset from the Wikipedia current events portal. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1302-1308, Online. Association for Computational Linguistics. Xingxing Zhang, Wei Lu, Furu Wei, and Ming Zhou. 2020. Pre-training for abstractive document summarization by reinstating source text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3646-3660, Online. Association for Computational Linguistics.</figDesc><table><row><cell>Yanyan Zou,</cell><cell></cell><cell cols="2">Demian Travis Goodwin, Max Savery, and Dina Demner-</cell></row><row><cell></cell><cell></cell><cell cols="2">Fushman. 2020. Flight of the PEGASUS? compar-</cell></row><row><cell></cell><cell></cell><cell cols="2">ing transformers on few-shot and zero-shot multi-</cell></row><row><cell></cell><cell></cell><cell cols="2">document abstractive summarization. In Proceed-</cell></row><row><cell></cell><cell></cell><cell cols="2">ings of the 28th International Conference on Com-</cell></row><row><cell></cell><cell></cell><cell cols="2">putational Linguistics, pages 5640-5646, Barcelona,</cell></row><row><cell cols="2">Avi Caciularu, Arman Cohan, Iz Beltagy, Matthew Pe-</cell><cell cols="2">Spain (Online). International Committee on Compu-</cell></row><row><cell cols="2">ters, Arie Cattan, and Ido Dagan. 2021. CDLM:</cell><cell>tational Linguistics.</cell></row><row><cell cols="2">Cross-document language modeling. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2648-2662, Punta Cana, Do-minican Republic. Association for Computational Linguistics.</cell><cell cols="2">Xiaotao Gu, Yuning Mao, Jiawei Han, Jialu Liu, You Wu, Cong Yu, Daniel Finnie, Hongkun Yu, Jiaqi Zhai, and Nicholas Zukoski. 2020. Generating rep-resentative headlines for news stories. In Proceed-ings of The Web Conference 2020, WWW '20, page</cell></row><row><cell cols="2">Janara Christensen, Mausam, Stephen Soderland, and</cell><cell cols="2">1773-1784, New York, NY, USA. Association for</cell></row><row><cell>Oren Etzioni. 2013.</cell><cell>Towards coherent multi-</cell><cell>Computing Machinery.</cell></row><row><cell cols="2">document summarization. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, pages 1163-1173, At-lanta, Georgia. Association for Computational Lin-</cell><cell cols="2">Chris Hokamp, Demian Gholipour Ghalandari, Nghia The Pham, and John Glover. 2020. Dyne: Dynamic ensemble decoding for multi-document summarization. CoRR, abs/2006.08748.</cell></row><row><cell>guistics.</cell><cell></cell><cell cols="2">Matthew Honnibal, Ines Montani, Sofie Van Lan-</cell></row><row><cell cols="2">Arman Cohan, Franck Dernoncourt, Doo Soon Kim,</cell><cell>deghem, and Adriane Boyd. 2020.</cell><cell>spaCy:</cell></row><row><cell cols="2">Trung Bui, Seokhwan Kim, Walter Chang, and Na-</cell><cell cols="2">Industrial-strength Natural Language Processing in</cell></row><row><cell cols="2">zli Goharian. 2018. A discourse-aware attention</cell><cell>Python.</cell></row><row><cell cols="2">model for abstractive summarization of long docu-ments. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-nologies, Volume 2 (Short Papers), pages 615-621, New Orleans, Louisiana. Association for Computa-tional Linguistics.</cell><cell cols="2">Hanqi Jin, Tianming Wang, and Xiaojun Wan. 2020. Multi-granularity interaction network for extractive and abstractive multi-document summarization. In Proceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 6244-6254, Online. Association for Computational Lin-guistics.</cell></row><row><cell cols="2">Hoa Trang Dang. 2005. Overview of duc 2005. In Pro-ceedings of the document understanding conference, volume 2005, pages 1-12.</cell><cell cols="2">Dan Jurafsky and James H. Martin. 2009. Speech and language processing : an introduction to natural language processing, computational linguistics, and</cell></row><row><cell cols="2">Hoa Trang Dang and Karolina Owczarzak. 2008.</cell><cell cols="2">speech recognition. Pearson Prentice Hall, Upper</cell></row><row><cell cols="2">Overview of the tac 2008 update summarization task.</cell><cell>Saddle River, N.J.</cell></row><row><cell cols="2">Theory and Applications of Categories.</cell><cell cols="2">Yoav Levine, Barak Lenz, Opher Lieber, Omri</cell></row><row><cell cols="2">Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and</cell><cell cols="2">Abend, Kevin Leyton-Brown, Moshe Tennenholtz,</cell></row><row><cell cols="2">Dragomir Radev. 2019. Multi-news: A large-scale</cell><cell cols="2">and Yoav Shoham. 2020. Pmi-masking: Princi-</cell></row><row><cell cols="2">multi-document summarization dataset and abstrac-</cell><cell cols="2">pled masking of correlated spans. arXiv preprint</cell></row><row><cell cols="2">tive hierarchical model. In Proceedings of the 57th</cell><cell>arXiv:2010.01825.</cell></row><row><cell cols="2">Annual Meeting of the Association for Computa-</cell><cell></cell></row><row><cell cols="2">tional Linguistics, pages 1074-1084, Florence, Italy.</cell><cell cols="2">Mike Lewis, Yinhan Liu, Naman Goyal, Mar-</cell></row><row><cell cols="2">Association for Computational Linguistics.</cell><cell cols="2">jan Ghazvininejad, Abdelrahman Mohamed, Omer</cell></row><row><cell></cell><cell></cell><cell cols="2">Levy, Veselin Stoyanov, and Luke Zettlemoyer.</cell></row><row><cell></cell><cell></cell><cell cols="2">2020. BART: Denoising sequence-to-sequence pre-</cell></row><row><cell></cell><cell></cell><cell cols="2">training for natural language generation, translation,</cell></row><row><cell></cell><cell></cell><cell cols="2">and comprehension. In Proceedings of the 58th An-</cell></row><row><cell></cell><cell></cell><cell cols="2">nual Meeting of the Association for Computational</cell></row><row><cell></cell><cell></cell><cell cols="2">Linguistics, pages 7871-7880, Online. Association</cell></row><row><cell></cell><cell></cell><cell>for Computational Linguistics.</cell></row></table><note>Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. 2018. Bottom-up abstractive summarization. In Proceedings of the 2018 Conference on Em- pirical Methods in Natural Language Processing, pages 4098-4109, Brussels, Belgium. Association for Computational Linguistics.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Details of total steps and warm-up steps used in the Full Supervised experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: Detailed ROUGE scores (R-1/R-2/R-L) on all</cell></row><row><cell>the datasets in the few-shot setting (corresponds to Fig-</cell></row><row><cell>ure 5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell cols="2">: ROUGE scores of the previous models</cell></row><row><cell cols="2">and our fully supervised model on the Multi-Xscience</cell></row><row><cell cols="2">dataset. All the results are from Lu et al. (2020). * The</cell></row><row><cell cols="2">ROUGE-L is not comparable as we have different set-</cell></row><row><cell cols="2">tings on the settings of evaluation, see the gap between</cell></row><row><cell>LEAD and LEAD(ours).</cell><cell></cell></row><row><cell>Models</cell><cell>R1 R2 RL</cell></row><row><cell>BERTREG (Gholipour Ghalandari et al., 2020)</cell><cell>35.0 13.5 25.5</cell></row><row><cell cols="2">SUBMODULAR+ABS(Gholipour Ghalandari et al., 2020) 30.6 10.1 21.4</cell></row><row><cell>DynE (Hokamp et al., 2020)</cell><cell>35.4 15.1 25.6</cell></row><row><cell>Ours</cell><cell>46.08 25.21 37.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 11 :</head><label>11</label><figDesc>ROUGE scores of the previous models and our fully supervised model on the WCEP dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 12 :</head><label>12</label><figDesc>ROUGE scores of the previous models and our fully supervised model on the arXiv dataset. The result of PEGASUS and BigBird-PEGASUS are from (Za</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 14</head><label>14</label><figDesc></figDesc><table><row><cell>, with all</cell></row><row><cell>the models trained on 10 data examples from</cell></row><row><cell>DUC2007.</cell></row><row><cell>H Software and Licenses</cell></row><row><cell>Our code is licensed under Apache License 2.0.</cell></row><row><cell>Our framework dependencies are:</cell></row><row><cell>? HuggingFace Datasets 16 , Apache 2.0</cell></row><row><cell>? NLTK 17 , Apache 2.0</cell></row><row><cell>? Numpy 18 , BSD 3-Clause "New" or "Revised"</cell></row><row><cell>? Spacy 19 , MIT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 13 :</head><label>13</label><figDesc>Generated summaries from PRIMERA and best baseline model (according ROUGE score on this example) trained with different number of training examples. The data used here is the #10 in the test set of Multi-News dataset on Huggingface.</figDesc><table /><note>Model Summaries</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 14 :</head><label>14</label><figDesc>Generated summaries from PRIMERA, PEGASUS and LED trained with 10 training examples, along with one (out of four) ground-truth summary. The data used here is D0730 in DUC2007.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use LED and not other efficient transformers like Bigbird-PEGASUS<ref type="bibr" target="#b18">(Zaheer et al., 2020)</ref> for two reasons, the first is that BigBird's global attention can't be assigned to individual tokens in the middle of the sequence, which is important for the representation of long documents as shown inCaciularu et al. (2021). Second, because pretrained checkpoints are available for LED, while BigBird-PEGASUS released the already fine-tuned checkpoints.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that entity information is only used at pretraining time. This is unlike some prior work (e.g.<ref type="bibr" target="#b12">Pasunuru et al. (2021)</ref>) that utilize additional information (like named entities, coref, discourse, or AMR) at fine-tuning and inference time.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use https://github.com/google-research/googleresearch/tree/master/rouge with default stemmer settings.5  Pilot experiments comparing BART and T5 showed BART to outperform T5 on the few-shot evaluation of Multi-News (with AVG ROUGE of 23.5/26.4 (T5) v.s. 25.2/26.7 (BART) for 10/100 training examples, respectively). Thus, we are using BART as one of the baselines.6  Checkpoints from https://huggingface.co/models</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">In practice, it is reasonable to assume knowing the approximate length of the expected summary for a given task/domain.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">We re-evaluate the generated summaries of the models from<ref type="bibr" target="#b9">Lu et al. (2020)</ref> for Multi-XScience, as we use a different version of ROUGE.10  Due to the lack of computational resources, we do not train the model on Wikisum.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">Due to the large size of the dataset, we evaluate all the models on the first 3200 data in the test set. And in the fewshot experiments, we randomly choose few examples (10 or 100) from the training set and validation set.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">HTLM: hyper-text pre-training and prompting of language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/2107.06955</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno>abs/2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07170</idno>
		<title level="m">and Iz Beltagy. 2021. Flex: Unifying evaluation for few-shot nlp</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Leveraging graph to improve abstractive multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.555</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6232" to="6243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Lebanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1178" to="1190" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating wikipedia by summarizing long sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical transformers for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1500</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5070" to="5081" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1387</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3730" to="3740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-XScience: A large-scale dataset for extreme multidocument summarization of scientific articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.648</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8068" to="8074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-document summarization with maximal marginal relevance-guided reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1737" to="1751" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluating content selection in summarization: The pyramid method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficiently summarizing text and graph encodings of multidocument clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.380</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4768" to="4779" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A thorough evaluation of task-specific pretraining for summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="140" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pobrl: Optimizing multi-document summarization by blending reinforcement learning policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mulvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent Poor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08244</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Controllable abstractive dialogue summarization with sketch supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.454</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5108" to="5122" />
		</imprint>
	</monogr>
	<note>Pontus Stenetorp, and Caiming Xiong</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph-based neural multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitijh</forename><surname>Meelu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-1045</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="452" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Leveraging Lead Bias for Zero-Shot Abstractive News Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gmyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3462846</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1462" to="1471" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
