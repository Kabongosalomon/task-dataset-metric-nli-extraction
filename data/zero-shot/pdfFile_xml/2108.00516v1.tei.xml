<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Wen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Bekris</surname></persName>
						</author>
						<title level="a" type="main">BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tracking the 6D pose of objects in video sequences is important for robot manipulation. Most prior efforts, however, often assume that the target object's CAD model, at least at a category-level, is available for offline training or during online template matching. This work proposes BundleTrack, a general framework for 6D pose tracking of novel objects, which does not depend upon 3D models, either at the instance or category-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It leverages the complementary attributes of recent advances in deep learning for segmentation and robust feature extraction, as well as memory-augmented pose graph optimization for spatiotemporal consistency. This enables long-term, low-drift tracking under various challenging scenarios, including significant occlusions and object motions. Comprehensive experiments</head><p>given two public benchmarks demonstrate that the proposed approach significantly outperforms state-of-art, category-level 6D tracking or dynamic SLAM methods. When compared against state-of-art methods that rely on an object instance CAD model, comparable performance is achieved, despite the proposed method's reduced information requirements. An efficient implementation in CUDA provides a real-time performance of 10Hz for the entire framework. Code is available at: https: //github.com/wenbowen123/BundleTrack</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Robot manipulation often requires information about the pose of the manipulated object. In some cases, this can be achieved through forward kinematics (FK), assuming the object's motion equivalent to the end-effector's motion. Frequently, however, FK is insufficient to accurately estimate the object's pose <ref type="bibr" target="#b0">[1]</ref>. This can be due to slippage during grasping or in-hand manipulation <ref type="bibr" target="#b1">[2]</ref>, or during handoffs or due to the compliance of a suction cup ( <ref type="figure">Fig. 1</ref>). In these cases, dynamically estimating an object's pose from visual data is desirable. Single-image 6D pose estimation methods have been studied extensively <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Some of them are fast and can re-estimate poses from scratch for every new frame <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Nevertheless, this is redundant, less efficient, leading to less coherent estimations over consecutive frames and negatively impacts planning and control. On the other hand, given an initial pose estimate, tracking 6D object poses over image sequences can improve estimation speed while providing coherent and accurate poses by leveraging temporal consistency <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>.</p><p>Most existing 6D object pose estimation or tracking approaches assume access to an object instance's 3D model <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Having access to such instance 3D models complicates generalization to novel, unseen instances. To overcome this limitation, recent efforts have relaxed this assumption and The authors are with the Computer Science Dept. of Rutgers in NJ, USA. Email: {bw344,kostas.bekris}@cs.rutgers.edu. This work is supported by NSF NRI award 1734492. The results do not express the sponsor's positions. t <ref type="figure">Fig. 1</ref>: Top: NOCS Dataset <ref type="bibr" target="#b12">[13]</ref> example: The target object exits the camera's frustum during tracking but BundleTrack maintains its estimate without re-initialization. Bottom: YCBInEOAT Dataset <ref type="bibr" target="#b21">[22]</ref> example: The object is successfully tracked during pick and place manipulation by a robotic arm, despite the lack of texture, severe selfocclusion and motions due to the arm and the compliant suction cup. Computing object pose from forward kinematics is unreliable in this setup due to the end-effector. require only category-level 3D models for 6D pose estimation <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref> or tracking <ref type="bibr" target="#b16">[17]</ref>. They often achieve this by training over a large number of CAD models from the same category. While promising results have been demonstrated for previously seen object categories, there are still limitations. These methods are constrained by the variety of categories in the training database. Popular 3D model databases, such as ShapeNet <ref type="bibr" target="#b17">[18]</ref> and ModelNet40 <ref type="bibr" target="#b18">[19]</ref>, contain 55 and 40 categories respectively. This is still far from sufficient to cover diverse object categories present in the real world. Furthermore, 3D model databases often require nontrivial manual effort and expert domain knowledge to build, involving steps such as scanning <ref type="bibr" target="#b19">[20]</ref>, mesh refinement <ref type="bibr" target="#b20">[21]</ref> or CAD design.</p><p>Another line of work from the SLAM literature has moved to address dynamic, object-aware challenges <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b25">[26]</ref>, where dynamic objects are being reconstructed on-thefly while being tracked without the need for object 3D models beforehand. However, tracking-via-reconstruction <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref> tends to accumulate errors when fusing observations with erroneous pose estimates into the global model. These errors adversely impact model tracking in subsequent frames.</p><p>Motivated by the above limitations, this work aims for accurate, robust 6D pose tracking that is generalizable to novel objects without instance or category-level 3D models. It exploits recent advances in video segmentation as well as learning-based keypoint detection and matching for a coarse pose estimate, followed by a memory-augmented pose-graph optimization step to achieve spatiotemporal consistent pose output. Instead of aggregating into a global model, representative historical observations are maintained as keyframes in a memory pool, providing candidate nodes for future graphs so as to enable multi-pair data association together with the latest observation. An efficient implementation of this framework in CUDA allows to achieve competitive running times. Extensive experiments have been conducted on two large-scale public benchmarks, shown in <ref type="figure">Fig. 1</ref>. Both qualitative and quantitative results demonstrate a significant improvement over existing state-of-art approaches, including methods using instance or category-level 3D models or SLAM-like methods.</p><p>In summary, this work's contributions are the following: 1) A novel integration of methods that result in a 6D pose tracking framework that generalizes to novel objects without access to instance or category-level 3D models.</p><p>2) A memory-augmented pose graph optimization for low-drift accurate 6D object pose tracking. In particular, augmenting the memory pool with historical observations enables multi-hop data association and ameliorate the dearth of correspondences between a pair of consecutive frames. Additionally, maintaining keyframes as raw nodes instead of aggregating into a global model significantly reduces tracking drift.</p><p>3) An efficient CUDA implementation, which allows to execute online the computationally-heavy multi-pair feature matching as well as pose-graph optimization for 6D object pose tracking (for the first time to the best of the authors' knowledge).</p><p>These contributions result in a new state-of-art performance by boosting the previous best accuracy from 33.3% to 87.4% under the "5?5cm" metric in the NOCS Dataset <ref type="bibr" target="#b12">[13]</ref>, even when compared against approaches utilizing categorylevel 3D models for training. They also result in comparable performance on the YCBInEOAT dataset <ref type="bibr" target="#b21">[22]</ref>, even when compared against approaches utilizing instance-level 3D models <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>6D Object Pose Tracking -For setups where object CAD models are available, significant progress has been made in 6D pose tracking. This includes techniques based on handcrafted probabilistic filtering <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, optimization <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>, and machine learning <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b21">[22]</ref>. The requirements, however, of such instance-level 3D models, either for training offline or model-frame registration during tracking, complicate generalization to novel instances. More recently, a 6D pose tracking approach <ref type="bibr" target="#b16">[17]</ref> relaxed the assumption to category-level 3D models using 3D object CAD model databases for training <ref type="bibr" target="#b17">[18]</ref>. During testing, the target object category needs to be identified and the corresponding network for that category is utilized for tracking. Instead of being limited to the number of categories such database is able to include, this work employs deep features that in principle can be trained on arbitrary 2D images. It allows generalization to diverse novel objects, as shown in the accompanying experiments.</p><p>Dynamic Object-aware SLAM -In order to track dynamic objects' pose and decouple them from static background, frame-model Iterative Closest Point (ICP) combined with color [23]- <ref type="bibr" target="#b25">[26]</ref>, probabilistic data association <ref type="bibr" target="#b31">[32]</ref>, or 3D level-set likelihood maximization <ref type="bibr" target="#b32">[33]</ref> has been applied. Object models are simultaneously reconstructed on-the-fly by aggregating the observed RGB-D data with the newly tracked pose. Nevertheless, frame-model tracking can be challenging for object reconstruction, since errors in pose estimation transfer to the reconstructed model and adversely affect the subsequent tracking <ref type="bibr" target="#b33">[34]</ref>. This work does not fuse observed frames but instead maintains them as nodes in a pose graph, allowing to correct previously erroneous estimates, and reduces drift in long-term tracking. The aforementioned SLAM-family approaches may also face challenges in robot manipulation setups that involve small, textureless, flat or shiny objects due to the dearth of sufficient correspondences between the pair of consecutive frames. To ameliorate this issue, BundleTrack searches correspondences among current and multiple historical frames, consisting of both feature and geometric terms, as the edges in the pose graph. Its effectiveness has been shown in extensive experiments including for such challenging manipulation scenarios.</p><p>3D Hand-held Object Scanning -Promising results have been demonstrated in scanning dynamic hand-held objects <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b38">[39]</ref>, where the object's motion needs to be taken into account similar to the current setup. In particular, a framework for robot manipulation <ref type="bibr" target="#b36">[37]</ref> performs simultaneous object reconstruction and tracking, which leads to similar issues as the aforementioned dynamic SLAM methods. In addition, forward kinematics is required in its Kalman Filtering framework, preventing generalization in scenarios when objects are not held by the robotic manipulator. While estimating object poses is part of the scanning process, there are key differences from online 6D pose tracking. For the scanning application, external assistance including human interaction or deliberate motion is acceptable <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> but it is not assumed in the current work. Furthermore, time consuming global-optimization steps are often adopted at the end of scanning to polish the models and their poses while intermediate erroneous pose estimations and associated frames can be discarded and not fused into the global model <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. In contrast, this work aims to provide fast and accurate pose tracking output online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM FORMULATION</head><p>? Assume a rigid body for which there is no its corresponding 3D model, nor its categorylevel 3D model database for training. The objective is to continuously track its 6D pose change relative to the start of tracking, i.e., the relative transformation 0? ? (3), ? {1, 2, ..., } in the camera's frame . The input is the following:</p><p>? : A sequence of RGB-D data , ? {0, ..., }. ? 0 : A binary mask on the first image 0 , indicating the target object region to track in the image space. ? 0 (optional): The initial pose in the camera's frame .</p><p>Used if the objective is to recover the object's absolute pose in , otherwise set to identity. The initial mask 0 can be obtained in multiple different ways to initialize tracking. For instance, via semantic segmentation <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b41">[42]</ref> or non-semantic methods, such as image . . . . . .</p><formula xml:id="formula_0">(1) (2)<label>(3)</label></formula><p>(4) (5) (6) <ref type="figure">Fig. 2</ref>: BundleTrack framework from left to right: (1) an image segmentation network returns the object mask given the prior one; (2) a network detects keypoints and their descriptors; (3) keypoints are matched and coarse registration is performed between consecutive frames to estimate an initial relative transformT ; (4) keyframes are selected from a memory pool to participate in the pose graph optimization; (5) online pose graph optimization outputs a refined spatiotemporal consistent pose T ; and (6) the latest frame is included in the memory pool, if it is a novel view to enrich diversity. segmentation, <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b44">[45]</ref>, point cloud segmentation/clustering <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, or plane fitting and removal <ref type="bibr" target="#b45">[46]</ref>, etc.</p><p>The object's pose in the camera's frame can be recovered at any timestamp by applying the relative transformation 0? in the camera's frame T = =</p><formula xml:id="formula_1">0 [( 0 ) ?1 0? 0 ] = 0? 0 ? (3)</formula><p>. For simplicity, the rest of this document will refer to T as the output of the process but 0? is what is actually computed as tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPROACH</head><p>An overview of the proposed BundleTrack framework is depicted in <ref type="figure">Fig. 2</ref>. The currently observed RGB-D frame and the object segmentation mask computed during the last timestamp ?1 are forwarded to a video segmentation network to compute the current object mask . Based on and ?1 respectively, the target object regions in both and ?1 are cropped, resized and sent to a keypoint detection network to compute keypoints and feature descriptors. A data association process consisting of feature matching and outlier pruning in the manner of RANSAC <ref type="bibr" target="#b47">[48]</ref> identifies feature correspondences. Based on these correspondences, a registration between ?1 and can be solved in closedform, which is then used to provide a coarse estimateT for the transform between the two snapshots. The estimat? T is used to initialize the current node T as part of a pose graph optimization step. To define the rest of the nodes of the pose graph, no more than K keyframes are selected from a memory pool to participate in the optimization. The choice of K is made to balance an efficiency vs. accuracy tradeoff. Pose graph edges include both feature and geometric correspondences, which are computed in parallel on GPU. Given this information, the pose graph step outputs online the optimized pose for the current timestamp T ? (3). If the last frame corresponds to a novel view, then it is also included in the memory pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Propagating Object Segmentation</head><p>The first step is to segment the object's image region from the background. Prior work <ref type="bibr" target="#b23">[24]</ref> used Mask-RCNN <ref type="bibr" target="#b48">[49]</ref> to compute the object mask in every frame of the video. It deals with each new frame independently, which is less efficient and results in temporal inconsistencies.</p><p>To avoid these limitations, this work adopts an off-theshelf transductive-VOS network <ref type="bibr" target="#b49">[50]</ref> for video object segmentation, which is trained on the Davis 2017 <ref type="bibr" target="#b50">[51]</ref> and Youtube-VOS <ref type="bibr" target="#b51">[52]</ref> datasets. The network uses dense longterm similarity dependencies between current and past feature embeddings to propagate the previous object mask to the latest frame. The object mask needed by BundleTrack is simply binary, i.e., = {0, 1} ? , ? {0, 1, ..., } and distinguishes the object region from the background. The only requirement is an initial mask 0 of interest. Neither the transductive-VOS network nor the following steps of BundleTrack require 0 to come from semantic/instance segmentation. Therefore, it can also be obtained in alternative ways depending on the application, e.g., low-level image segmentation <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b52">[53]</ref>, point cloud segmentation/clustering <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, or plane fitting and removal <ref type="bibr" target="#b45">[46]</ref>, etc.</p><p>While the current implementation uses transductive-VOS, the following techniques do not depend on this specific network. If the object mask can be computed via simpler means, such as computing a region of interest (ROI) from forward kinematics followed by point cloud filtering in robot manipulation scenarios <ref type="bibr" target="#b1">[2]</ref>, the segmentation module can be replaced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Keypoint Detection, Matching and Local Registration</head><p>Local registration is performed between consecutive frames ?1 and to compute a initial poseT . To do so, correspondence between keyframes detected on each image is performed. Different from prior work <ref type="bibr" target="#b16">[17]</ref>, which relies on category-level 3D models to learn a fixed number of category-level semantic keypoints, this work aims to use generalizable features not specific to certain instances or categories. The LF-Net <ref type="bibr" target="#b53">[54]</ref> is chosen given its satisfactory balance between performance and inference speed. It only requires training on general 2D images, such as the ScanNet dataset <ref type="bibr" target="#b54">[55]</ref> used here, and generalizes to novel scenes. During testing, for the newly observed frame , LF-Net receives the segmented image (Sec. IV-A) as input. It then outputs keypoints , ? {0, 1, ..., ? 1} along with the feature descriptor ? 128 , where is 500 in all experiments. Due to the potentially imperfect segmentation in previous step, outlier keypoints can arise from the background. It is thus critical to perform feature matching and outlier pruning via RANSAC <ref type="bibr" target="#b47">[48]</ref>, executed in parallel on GPU in this work. Each registration sample consists of 3 pairs of keypoints matched between the two images. A pose hypothesis is generated from a sample via least squares <ref type="bibr" target="#b55">[56]</ref>. When evaluating samples, inlier correspondences have a distance between transformed point pairs below a threshold and an angle formed by the normals within a threshold . The values of and are empirically set to 5 and 45?in all experiments. After RANSAC, a preliminary pose is computed byT = T ?1 ?1 where ?1 is the best sampled correspondence hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Keyframe Selectio?</head><p>T is then refined during a pose graph optimization step. The number of keyframes participating in the optimization is limited to K for the sake of efficiency, where K = 15 is the number used in the experiments. When the size of the keyframe memory pool N is larger than K, the objective is to find the set of keyframes with the largest mutual viewing overlap to make good use of multi-view consistency. This challenge can be formulated as the minimum H-subgraph of an edge-weighted graph problem <ref type="bibr" target="#b56">[57]</ref>:</p><formula xml:id="formula_2">argmin ?? ?N ?? ?N, ? ? ( ) ? 1 2 so that : ?? ?N = K and ? {0, 1}, ? N ,</formula><p>where is the rotation matrix of the corresponding keyframe's pose. The goal is to find the optimal binary vector ? that indicates the selections. The weight of the edge between frame pair ( , ) is the geodesic distance of their rotations. Mutual viewing overlap is maximized when the mutual rotation difference relative to the camera is minimized. Combinatorial optimization algorithms for solving this problem have a complexity of (N K / N ) <ref type="bibr" target="#b56">[57]</ref>. In practice, an iterative greedy selection is followed by starting with the keyframe set { 0 } until the number of selected keyframes reaches K. 0 is chosen since the initial frame does not suffer from any tracking drift and serves as the reference frame. In each iteration, the keyframe with the smallest sum of geodesic distances against as well as all previously selected keyframes is added. This reduces complexity to (N K 3 + N K 2 ), making the selection practical (under a millisecond) without degrading performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Online Pose Graph Optimization</head><p>The pose graph can be denoted as = { , }, | | = + 1, where each node corresponds to the object pose in the camera's frame at the current and selected timestamps ? { , ? 1 , ? 2 , ..., ? }. For simplicity, the subscripts of graph nodes will be denoted as simple indices ? | | instead of the actual timestamp ? . Each node's pose can then be denoted as T , ? | |. Inspired by <ref type="bibr" target="#b57">[58]</ref>, for the edges between each pair of nodes, two types of energies E and E are considered. The energy E relates to the residuals computed from feature correspondences and E relates to the geometric residuals measured by dense pixel-wise pointto-plane distance. The spatiotemporal consistency is achieved when the total energy of the graph E is minimized:</p><formula xml:id="formula_3">E = ? | | ? | |, ? ( 1 E ( , ) + 2 E ( , ))<label>(1)</label></formula><formula xml:id="formula_4">E ( , ) = ( , ) ? , T ?1 ? T ?1 2<label>(2)</label></formula><p>In order to compute E , feature correspondences , between each pair of nodes ( , ) are determined. If , has been built during a previous pose graph optimization, it is reused. Otherwise, the data association process of Sec. IV-B is performed to compute , . These multi-pair feature correspondences are built in parallel on GPU. In Eq. <ref type="formula" target="#formula_4">(2)</ref> and <ref type="formula" target="#formula_0">(3)</ref>, represents the unprojected 3D points in the camera's frame, is the M-estimator, where Huber loss is used.</p><formula xml:id="formula_5">E ( , ) = ? | | ( ) ? (T T ?1 ?1 ( (T T ?1 )) ? ) 2 (3)</formula><p>For E , dense pixel-wise correspondences are associated by point re-projection, while outliers are filtered based on the distance between the point pair and the angle formed by their normals; (?) is the perspective projection operation; ?1 (?) denotes the unprojection mapping, which recovers a 3D point in the camera's frame by looking up the depth value on the pixel location; (?) returns the normal of the pixel on the frame , ? | |. In Eq. (1), 1 and 2 are the weights balancing E and E . To emphasize the lack of sensitivity to the choice of these values, 1 and 2 are set to 1 in all experiments unless otherwise specified. Then, the goal is to find the optimal poses, such that:</p><formula xml:id="formula_6">* = argmin (?( ))</formula><p>where?( ) is the stacked energy residual vector, = ( , ? 1 , ? 2 , ..., ? ) ? 6?( +1) is the stacked pose vector corresponding to the current frame and selected past keyframes, while the pose corresponding to the initial frame 0 is kept constant as reference. Each block = (T ) ? (3) is parametrized in Lie Algebra <ref type="bibr" target="#b58">[59]</ref>, consisting of 3 parameters for translation and 3 parameters for rotation. A common approach is to apply first-order Taylor expansion around , such that the iteratively re-weighted nonlinear least squares can be solved by a Gauss-Newton update:</p><p>(J WJ)? = J W? where J is the Jacobian matrix with respect to , W is a diagonal weight matrix computed by the M-estimator and residual, which is updated in each iteration. To better take advantage of the sparsity of J and W, inside each Gauss-Newton step, an iterative PCG (Preconditioned Conjugate Gradient) <ref type="bibr" target="#b59">[60]</ref> solver is leveraged, where the diagonal matrix J WJ is used as the preconditioner. Incremental pose updates are accumulated in the tangent space after each iteration ? ? . The entire pose graph optimization is implemented in CUDA for parallel computation.</p><p>At the end of the optimization, the object pose corresponding to each graph node is obtained by T = ( ) ? (3), ? | |. The one corresponding to the current timestamp becomes the output tracked pose T , while poses corresponding to the historical keyframes are updated in the memory pool. The entire process is causal, i.e. past frames' corrected poses cannot be updated in the output. However, their corrected pose estimates provide better initialization in following pose graph optimization steps to benefit the solution of new observations. This significantly reduces long-term drift compared against tracking-via-reconstruction <ref type="bibr" target="#b23">[24]</ref>, where any intermediate erroneous pose estimation introduces noise when fused into the global model and adversely affects the subsequent tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Augmenting the Keyframe Memory Pool</head><p>The initial frame 0 is always selected as it does not suffer from any tracking drift. For later frames, once the current object pose T is determined, its rotation geodesic distance against each existing keyframe in the pool is compared. If all pair-wise distances are larger than ( (10?) in all experiments), is added into the keyframe memory pool. This encourages to add frames from novel views, such that multi-view diversity is enriched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>This section evaluates the proposed approach and compares against state-of-the-art 6D pose tracking and estimation methods on two public benchmarks, the NOCS dataset <ref type="bibr" target="#b12">[13]</ref> and the YCBInEOAT dataset <ref type="bibr" target="#b21">[22]</ref>. Experiments are performed over diverse types of objects and various tracking scenarios (e.g., moving camera or moving objects). Both quantitative and qualitative results demonstrate that Bundle-Track achieves comparable or even superior performance relative to alternatives, although it does not require instance or category-level 3D models. Concretely, no CAD models or training data from a 3D object database are used by BundleTrack. All experiments are conducted on a standard desktop with Intel Xeon(R) E5-1660 v3@3.00GHz processor and a single NVIDIA RTX 2080 Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NOCS dataset [13]:</head><p>Among existing datasets, this is the closest to the setup here, where instance 3D models are not provided during evaluation. The dataset contains 6 object categories: bottle, bowl, camera, can, laptop, and mug. The training set consists of: (1) 7 real videos containing 3 instances of each category in total, annotated with ground truth poses; and (2) 275K frames of synthetic data generated using 1085 instances from the above 6 categories using a 3D model database ShapeNetCore <ref type="bibr" target="#b17">[18]</ref> with random poses and object combinations in each scene. The testing set has 6 real videos containing 3 different unseen instances within each category, resulting in 18 different object instances and 3,200 frames in total. YCBInEOAT dataset <ref type="bibr" target="#b21">[22]</ref>: This dataset helps verify the effectiveness of 6D pose tracking during robot manipulation. It was originally developed to evaluate approaches relying on CAD models. The available CAD models, however, are not used by BundleTrack. In contrast to the NOCS dataset where objects are statically placed on a tabletop and captured by a moving camera, YCBInEOAT contains 9 video sequences captured by a static RGB-D camera, while objects are dynamically manipulated. There are three types of manipulation: (1) single arm pick-and-place, (2) within-hand manipulation, and (3) pick to hand-off between arms to placement. These scenarios and the end-effectors used make directly computing   <ref type="table" target="#tab_1">Table I</ref> and <ref type="figure" target="#fig_2">Fig. 3</ref> present the quantitative and qualitative results of state-of-art methods on the NOCS dataset respectively. The comparison points include learning-based methods relying on a category-level prior, such as NOCS <ref type="bibr" target="#b12">[13]</ref>, KeypointNet <ref type="bibr" target="#b61">[62]</ref>, and 6-PACK with or without temporal prediction <ref type="bibr" target="#b16">[17]</ref>. These methods are offline trained on both real and synthetic training sets, which are rendered with 3D object models extracted from the same categories of ShapeNetCore <ref type="bibr" target="#b17">[18]</ref>. In contrast, ICP <ref type="bibr" target="#b62">[63]</ref>, MaskFusion <ref type="bibr" target="#b23">[24]</ref>, TEASER++ * <ref type="bibr" target="#b63">[64]</ref> and the proposed BundleTrack have no access to any training data based on 3D models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on the NOCS Dataset</head><p>The evaluation protocol is the same as in prior work <ref type="bibr" target="#b16">[17]</ref>. A perturbed ground-truth object pose is used for initialization. The perturbation adds a uniformly sampled random translation within a 4cm range to evaluate robustness against a noisy initial pose <ref type="bibr" target="#b16">[17]</ref>. No re-initialization is allowed during tracking. To evaluate robustness against missing frames, the same uniformly sampled 450 frames out of 3200 in the testing videos are dropped <ref type="bibr" target="#b16">[17]</ref>. Four metrics are adopted: 1) 5?5cm: percentage of estimates with orientation error &lt; 5?and translation error &lt; 5cm -the higher the better; 2) IoU25 (Intersection over Union): percentage of cases where the overlapping prediction and ground-truth 3D bounding box volume is larger than 25% of their union -the higher the better; 3) R err : mean orientation error in degrees -the lower the better; and 4) T err : mean translation error in centimetersthe lower the better. For R err and T err , estimates with IoU?25 are not counted when computing averages <ref type="bibr" target="#b16">[17]</ref>. https://github.com/j96w/6-PACK/blob/master/benchmark.py  The results of comparison points other than MaskFusion and TEASER++ * come from the literature <ref type="bibr" target="#b16">[17]</ref>. The opensourced code of MaskFusion is used for evaluation, where the global SLAM module is disabled to avoid inferring object poses from the camera's estimated ego-motion. The dynamic object tracking module is kept to solely evaluate object pose tracking effectiveness. Its original segmentation module Mask-RCNN <ref type="bibr" target="#b48">[49]</ref> is fine-tuned on the real training data provided in the NOCS dataset for better performance while the synthetic data rendered using category-level 3D models are not used, as this method is also agnostic to any 3D models <ref type="bibr" target="#b23">[24]</ref>. In addition to ICP reported in <ref type="bibr" target="#b16">[17]</ref>, another state-ofart 3D registration approach <ref type="bibr" target="#b63">[64]</ref> is included for comparison and denoted as TEASER++ * , which is robust to outlier correspondences and agnostic to 3D models. It takes as input the segmented point cloud and feature correspondences that are computed using the same modules proposed in BundleTrack. For BundleTrack, an initial mask 0 is required as input to the framework and is provided via the aforementioned Mask-RCNN. During execution, BundleTrack does not require external mask input nor any form of re-initialization. As exhibited in <ref type="table" target="#tab_1">Table I</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results on YCBInEOAT Dataset</head><p>Evaluation exclusively on static objects captured by a moving camera cannot completely reflect the properties of a 6D pose tracking method <ref type="bibr" target="#b21">[22]</ref>. For this reason, the YCBI-nEOAT dataset is chosen to evaluate tracking in scenarios where objects are moving in front of the camera. The same evaluation protocol is followed as in prior work <ref type="bibr" target="#b21">[22]</ref>. Results are computed from accuracy-threshold AUC (Area Under Curve) measured by = 1 ? || + ? (?+ )||, which performs exact model matching, and -= 1 1 ? min 2 ? || 1 + ? (?2 +?)|| <ref type="bibr" target="#b2">[3]</ref> designed for evaluating symmetric objects. Similar to prior work <ref type="bibr" target="#b21">[22]</ref>, the ground-truth object's pose in the camera's frame is provided as initialization. No re-initialization is allowed during the tracking process.</p><p>Quantitative and qualitative results are shown in <ref type="table" target="#tab_1">Table II</ref> and <ref type="figure" target="#fig_3">Fig. 4</ref> respectively. Comparison points include state-ofart 6D pose tracking methods that use object CAD models, such as RGF <ref type="bibr" target="#b27">[28]</ref>, dbot PF <ref type="bibr" target="#b10">[11]</ref> and (3)-TrackNet <ref type="bibr" target="#b21">[22]</ref>. 6-PACK <ref type="bibr" target="#b16">[17]</ref> is a state-of-art 6D pose tracking approach relying on category-level 3D models. Its evaluation on objects "021_bleach_cleanser", "006_mustard_bottle" and "005_tomato_soup_can" are performed by using the officially released networks trained on "bottle" and "can" category respectively . For the rest of the objects "003_cracker_box" and "004_sugar_box", no suitable corresponding category can be found in existing 3D model database <ref type="bibr" target="#b17">[18]</ref> and thus 6-PACK is not able to be retrained and evaluated on them. For 6-PACK, 3D bounding box of the object model, computed from forward kinematics, is provided in every frame to crop ROI from point cloud, since it is more reliable than its default module of extrapolating the 3D bounding box by estimated motion. For MaskFusion <ref type="bibr" target="#b23">[24]</ref> and BundleTrack, https://github.com/j96w/6-PACK Assumption Methods 003_cracker_box 021_bleach_cleanser 004_sugar_box 005_tomato_soup_can 006_mustard_bottle ALL ADD ADD-S ADD ADD-S ADD ADD-S ADD ADD-S ADD ADD-S ADD ADD-S   , <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b63">[64]</ref> for tracking drift study.</p><p>the initial object mask is obtained by table fitting and removal, followed by Euclidean Clustering implemented in PCL <ref type="bibr" target="#b45">[46]</ref>. The original MaskFusion's segmentation module Mask-RCNN cannot be retrained on this benchmark due to the lack of training set. Therefore, during tracking, the target object mask is computed by segmenting out the region of robot arm and end-effector from forward kinematics. For instances of irregular shapes or colors ("021_bleach_cleanser", "006_mustard_bottle") within the "bottle" category that 6-PACK has been trained on, it struggles to get satisfactory result. Nevertheless, BundleTrack consistently demonstrates high quality tracking without any retraining or fine-tuning. This establishes generalizability of BundleTrack to novel object instances regardless of their out-of-distribution properties within the category. BundleTrack also achieves comparable or superior performance even when compared against methods relying on object instance CAD models <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Analysis</head><p>Ablations Study: An ablation study investigates the effectiveness of the online global pose graph optimization and each energy term, presented in <ref type="figure" target="#fig_4">Fig. 5 (a)</ref>.</p><p>Sensitivity to Initial Pose: As mentioned, random translation noise within 4cm range is added to the initial pose. This part further investigates robustness under different translation and rotation noise levels, shown in <ref type="figure" target="#fig_4">Fig. 5 (b)</ref>. Computation Time: The average running time of modules are given in <ref type="figure" target="#fig_4">Fig. 5 (c)</ref>. The entire framework runs at 10Hz on average including video segmentation. The 6-PACK <ref type="bibr" target="#b16">[17]</ref>, TEASER++ * <ref type="bibr" target="#b63">[64]</ref> and MaskFusion <ref type="bibr" target="#b23">[24]</ref> methods from related work run at 4Hz, 11Hz and 17Hz respectively on the same machine. Tracking Drift Analysis: <ref type="figure" target="#fig_4">Fig. 5 (d)</ref> presents the rotation and translation error w.r.t. timestamps compared against representative related works <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b63">[64]</ref>. Results are averaged across all videos on the NOCS Dataset. Generalization: The neural networks' weights and hyperparameters in BundleTrack are fixed without any retraining or fine-tuning across all evaluations (Sec. V-B, V-C). When applied to novel instances, the framework does not require access to instance or category-level 3D models for training or registration.  <ref type="figure">Fig. 6</ref>: Some of the most challenging cases for BundleTrack on the NOCS Dataset. Top: Severe self-occlusion prevents data association around the mug's handle, introducing challenges for solving the orientation around the green axis. Nevertheless, with better visibility in subsequent frames, BundleTrack is able to recover from drifts and continue tracking, thanks to the memory-augmented pose graph optimization. Bottom: Near the end of video, noisy segmentation (purple mask) falsely ignores the side of the bowl, preventing relevant feature extraction and leads to slight translation offset. With future development of more advanced segmentation module, the overall tracking performance is expected to be boosted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This work presents BundleTrack, a general framework for tracking the 6D pose of novel objects without any assumptions on instance or category-level 3D models. Extensive experiments demonstrate that it is able to perform long-term accurate tracking under various challenging scenarios. It even achieves comparable performance to state-of-art methods that depend on the target object's CAD model. Future research includes the exploration of combining BundleTrack with model-free grasping methods <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>, to perform robust pick-and-place <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref> or in-hand dexterous manipulation for a wide variety of novel objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Example qualitative results of BundleTrack and representative comparison points on NOCS Dataset. In all methods, each object is tracked individually and depicted in the same image for visualization. Methods' names are colored in blue and green to denote assumption on category-level 3D model and no model respectively. For more qualitative results, please refer to the supplementary video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Example qualitative results of BundleTrack and representative comparison points on YCBInEOAT Dataset. Methods' names are colored in red, blue and green to denote assumption on instance 3D model, category-level 3D model and no model respectively. For more qualitative results, please refer to the supplementary video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Experimental analysis performed on NOCS dataset as described in Sec V-D. (a) Ablation study investigating effectiveness of pose graph optimization and each energy term. (b) Sensitivity of BundleTrack to inaccurate initial pose by deliberately introducing different translation and rotation noise levels. (c) Average running time decomposition of different modules. (d) Rotation and translation error w.r.t. timestamps compared against representative related works [17]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Failure Cases:</head><label></label><figDesc>While BundleTrack is able to robustly keep tracking in all experiments without lost or re-initialization, intermediate imprecise estimates are observed, such as the cases illustrated in Fig. 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Results on the NOCS dataset<ref type="bibr" target="#b12">[13]</ref>. For the metrics of 5?5cm and IoU25, a higher value is preferable. For the metrics of R err and T err , a lower value is preferable. Under each type of 3D model assumption, the best results are highlighted in bold font. TEASER++</figDesc><table /><note>* denotes TEASER++ [64] operating over the same segmented point cloud and feature correspondences as in the proposed BundleTrack.poses from forward kinematics unreliable. The manipulation videos involve 5 YCB Objects [61]: mustard bottle, tomato soup can, sugar box, bleach cleanser and cracker box.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>,</head><label></label><figDesc>BundleTrack significantly outperforms the comparison points under all metrics and over all object categories, despite not accessing instance or category-level 3D models.</figDesc><table><row><cell>Time</cell></row><row><cell>se(3)-TrackNet</cell></row><row><cell>6-PACK</cell></row><row><cell>TEASER++*</cell></row><row><cell>BundleTrack(Ours)</cell></row><row><cell>Ground-truth</cell></row></table><note>https://github.com/martinruenz/maskfusion</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Results of AUC measured by ADD and ADD-S metrics on YCBInEOAT Dataset<ref type="bibr" target="#b21">[22]</ref>. Under each type of 3D model assumption, best results are in bold. TEASER++ * denotes TEASER++<ref type="bibr" target="#b63">[64]</ref> operating over the same segmented point cloud and feature correspondences as in the proposed BundleTrack.</figDesc><table><row><cell cols="3">5?5cm IoU25 Rerr Proposed 87.4 99.9 2.4 w/o Pose Graph 39.9 99.9 9.2 w/o 78.5 99.3 5.3</cell><cell>Terr 2.1 2.4 2.5</cell><cell>5?5cm Percentage</cell><cell>84 88 92</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o</cell><cell>86.3 99.9</cell><cell>2.5</cell><cell>2.2</cell><cell></cell><cell>80</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell></cell><cell cols="2">(a) Ablation Study</cell><cell></cell><cell cols="2">(b)</cell><cell cols="4">trans_noise (cm) rot_noise (deg)</cell></row><row><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(d)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time perception meets reactive motion generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kappler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE RAL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust, occlusion-aware pose estimation for objects grasped by adaptive hands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Scene-level pose estimation for multiple instances of densely packed objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mitash</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep object pose estimation for semantic robotic grasping of household objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Poserbpf: A raoblackwellized particle filter for 6d object pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Probabilistic object tracking using a range camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>W?thrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dart: Dense articulated real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Normalized object coordinate space for categorylevel 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Latentfusion: End-to-end differentiable reconstruction and rendering for unseen object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Category level object pose estimation via neural analysis-by-synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08145</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning canonical shape space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">6-pack: Category-level 6d pose tracker with anchor-based keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">Shapenet: An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Kinectfusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meshlab: an open-source mesh processing tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cignoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Italian chapter conference</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">se (3)-tracknet: Data-driven 6d pose tracking by calibrating image residuals in synthetic domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Mid-fusion: Octree-based object-level multi-instance dynamic slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Maskfusion: Real-time recognition, tracking and reconstruction of multiple moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Runz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ISMAR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Simultaneous localization, mapping, and manipulation for unsupervised object discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Co-fusion: Real-time segmentation, tracking and fusion of multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>R?nz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<editor>ICRA. IEEE</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4471" to="4478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rgb-d object tracking: A particle filter approach on gpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Depth-based object tracking using a robust gaussian filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Issac</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A versatile learningbased 3d temporal tracker: Scalable, robust, online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D. Joseph</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A robust monocular 3d object tracking method combining statistical and photometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A region-based gauss-newton approach to realtime monocular multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tjaden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Em-fusion: Dynamic object-level slam with probabilistic data association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Strecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stuckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Star3d: Simultaneous tracking and reconstruction of 3d objects using rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sdf-2-sdf: Highly accurate 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slavcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d object reconstruction from hand-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="729" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Online loop closure for real-time interactive 3d scanning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Manipulator and object tracking for in hand model acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krainin</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">In-hand object scanning via rgb-d video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hauser</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Accurate and robust registration for in-hand modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep contextual recurrent residual networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H N</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Color image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing and its Applications</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Segmenting unknown 3d objects from real depth images using mask r-cnn trained on synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danielczuk</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning rgb-d feature embeddings for unseen object instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">3d is here: Point cloud library (pcl),&quot; in ICRA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cousins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Voxel cloud connectivity segmentation -supervoxels for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Papon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A transductive approach for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">The 2017 davis challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Lf-net: learning local features from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6234" to="6244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Least-squares fitting of two 3-d point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Blostein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="698" to="700" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Finding the smallest h-subgraph in real weighted graphs and related problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vassilevska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Colloquium on Automata, Languages, and Programming</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Bundlefusion: Real-time globally consistent 3d reconstruction using on-the-fly surface reintegration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM ToG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bourbaki</surname></persName>
		</author>
		<title level="m">Lie groups and Lie algebras: chapters 7-9</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Introduction to numerical analysis. Courier Corporation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Hildebrand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Benchmarking in manipulation research: The ycb object and model set and benchmarking protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Calli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Magazine</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Discovery of latent 3d keypoints via endto-end geometric reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Open3d: A modern library for 3d data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">TEASER: Fast and Certifiable Point Cloud Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carlone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robotics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Grasp pose detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">6-dof grasping for target-driven object manipulation in clutter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murali</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Vision-driven compliant manipulation for reliable, high-precision assembly tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morgan</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Task-driven perception and manipulation for constrained placement of unknown objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mitash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE RAL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
