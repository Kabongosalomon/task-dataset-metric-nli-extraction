<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Emergence of Objectness: Learning Zero-Shot Segmentation from Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runtao</forename><surname>Liu</surname></persName>
							<email>runtao219@gmail.comstellayu@berkeley.eduwuzhiron</email>
							<affiliation key="aff1">
								<orgName type="institution">John Hopkins University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<settlement>Berkeley / ICSI 3</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">John Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">John Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Emergence of Objectness: Learning Zero-Shot Segmentation from Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Humans can easily segment moving objects without knowing what they are. That objectness could emerge from continuous visual observations motivates us to model grouping and movement concurrently from unlabeled videos. Our premise is that a video has different views of the same scene related by moving components, and the right region segmentation and region flow would allow mutual view synthesis which can be checked from the data itself without any external supervision. Our model starts with two separate pathways: an appearance pathway that outputs feature-based region segmentation for a single image, and a motion pathway that outputs motion features for a pair of images. It then binds them in a conjoint representation called segment flow that pools flow offsets over each region and provides a gross characterization of moving regions for the entire scene. By training the model to minimize view synthesis errors based on segment flow, our appearance and motion pathways learn region segmentation and flow estimation automatically without building them up from low-level edges or optical flows respectively. Our model demonstrates the surprising emergence of objectness in the appearance pathway, surpassing prior works on zero-shot object segmentation from an image, moving object segmentation from a video with unsupervised test-time adaptation, and semantic image segmentation by supervised fine-tuning. Our work is the first truly end-to-end zero-shot object segmentation from videos. It not only develops generic objectness for segmentation and tracking, but also outperforms prevalent image-based contrastive learning methods without augmentation engineering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, contrastive learning <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> has become a powerful model for obtaining high-level representations on generic images <ref type="bibr" target="#b3">[4]</ref>. Despite their encouraging performance, contrastive models have two critical limitations. First, they heavily rely on hand-crafted image augmentations <ref type="bibr" target="#b4">[5]</ref> to induce invariance, yet fail to account for complex variations such as object deformations and 3D viewpoints. Second, they still require additional labeled data and a fine-tuning stage for downstream applications, preventing use in a standalone fashion.</p><p>In this paper, we seek a zero-shot image model for detecting and segmenting objects by learning from unlabeled videos with minimal augmentation. As opposed to static images, a dynamic sequence of observations provides information about what is moving in the scene and how it moves. Such patterns not only reveal the boundary segment of an object, but also indicate hierarchical part organizations and even object semantics. Thus, as an easily accessible source of unlabeled data, videos provide rich natural supervision for learning image representations.  <ref type="figure">Figure 1</ref>: Our zero-shot object segmentation is learned from an unsupervised factorization of images into segments and their motions, whereas past work segments objects based on dense pixel-wise optical flows, which are brittle in the presence of noise, articulated movement, and abrupt motion.</p><p>A popular objective for self-supervised learning from videos is view synthesis. Concretely, given a source frame, a function is learned to warp the source frame to the target frame using photometric consistency as supervision. Dense optical flow <ref type="bibr" target="#b5">[6]</ref> can be self-supervised in this manner. One could also learn a monocular depth network from videos with additional camera parameters <ref type="bibr" target="#b6">[7]</ref>. The key idea is to find an appropriate representation which not only parameterizes the warping function, but also transfers to the target task. For example, the multi-plane image representation <ref type="bibr" target="#b7">[8]</ref> is proposed to extrapolate between stereo pairs using view synthesis.</p><p>Unlike prior works that use view synthesis for low-level vision tasks, our goal is to tackle object segmentation which involves mid-level and high-level visual recognition. To this end, dense optical flow field which represents the low-level correspondence in a local manner would not suffice (see <ref type="figure">Figure 1</ref>). We therefore seek a new representation which could capture a gross characterization of moving regions for the entire scene. Deriving a representation explicitly for moving regions would allow the model to localize and segment objects.</p><p>Our approach decomposes view synthesis into two visual pathways: an appearance pathway to model "what is moving" by segmenting a static RGB image into separate regions, and a motion pathway to model "how it moves" by extracting motion features on a pair of images. The motion features are then used to predict flow offsets for individual regions assuming common fate <ref type="bibr" target="#b8">[9]</ref> for all pixels within a region. The segment masks as well as their flow vectors jointly reconstruct a new representation called segment flow, which is used for view synthesis. In this way, object appearance and motion are decoupled, such that the appearance model for predicting segmentation would benefit from rich RGB signals. By conditioning on a region, the motion pathway is also tasked to solve a much simpler problem than dense flow. The two pathways are jointly trained with a reconstruction loss.</p><p>After self-supervised pretraining, we find that generic objectness detection and object segmentation automatically emerge from the model. Our model has the versatility for a variety of applications. First, the appearance pathway can be directly applied to novel images for dominant object segmentation in a zero-shot fashion. Second, it can also be fine-tuned for semantic segmentation on a small labeled dataset. Finally, with unsupervised test-time adaptation, the overall model can be transferred to novel videos for moving object segmentation without labels. Experimentally, we demonstrate strong performance on all of these applications, showing considerable improvements against the baselines.</p><p>The contributions of this work can be summarized as follows: 1) the first truly end-to-end zero-shot object segmentation from unlabeled videos; 2) a conceptually novel segment flow representation which goes beyond traditional dense optical flow; 3) a versatile model that can be applied to various image and video segmentation tasks. Our code is available at https://github.com/rt219/The-Emergence-of-Objectness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Video object segmentation. Segmentation of moving objects requires finding correspondences along the time dimension. A dominant line of work focuses on learning a representation for temporally propagating segmentation masks. Such a representation may be learned with pixel-level object masks in videos with long-term relations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, or learned through self-supervision such as colorization <ref type="bibr" target="#b11">[12]</ref> and cycle-consistency <ref type="bibr" target="#b12">[13]</ref>. Given the annotation of object masks in the initial frame, the model tracks the object and propagates the segmentation through the remaining frames.</p><p>Fully unsupervised video object segmentation, without initial frame annotations, has received relatively little attention. NLC <ref type="bibr" target="#b13">[14]</ref> and ARP <ref type="bibr" target="#b14">[15]</ref> take a temporal clustering approach to this problem. Though they do not require segmentation annotations, elements of these algorithms depend on edge and saliency labels, and thus are not completely unsupervised. FTS <ref type="bibr" target="#b15">[16]</ref> calculates the segmentation by obtaining a motion boundary from the optical flow map between frames. SAGE <ref type="bibr" target="#b16">[17]</ref> takes into account multiple cues of edges, motion segmentation, and image saliency for video object segmentation. Contextual information separation <ref type="bibr" target="#b17">[18]</ref> segments moving objects by exploiting the motion independence between the foreground and the background. A concurrent work based on motion grouping <ref type="bibr" target="#b18">[19]</ref> clusters pixels with similar motion vectors. Both of these works rely on an off-the-shelf optical flow representation, which may be trained with <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> or without <ref type="bibr" target="#b5">[6]</ref> supervision.</p><p>Motion Segmentation. Classical methods for motion segmentation <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref> cluster distinctive motion regions from the background based on two-frame optical flow. Supervised learning approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> map the optical flow field to segmentation masks. The requirement of dense and accurate optical flow may be problematic when the flow vectors are not smooth over time and vulnerable to articulated objects with inhomogeneous motion <ref type="bibr" target="#b26">[27]</ref>. we turn our attention to modeling appearance on RGB representations, which provide rich cues (e.g. texture, color and edges) for perceptual organization, alleviating the need for dense pixel correspondence.</p><p>Motion Trajectory Segmentation. Moving object segmentation has been shown to be effective when motion is considered over a large time interval <ref type="bibr" target="#b26">[27]</ref>. An approach based on trajectory clustering <ref type="bibr" target="#b27">[28]</ref> builds point trajectories over hundreds of frames, extracts descriptors for the point trajectories, and clusters them to obtain segmentation results. Though promising, such a global approach is computationally demanding.</p><p>Layered representations. A simple linear model <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> can factorize a video into layers of foreground objects and background, assuming independence among the objects and background. This layered representation was used to derive better optical flow estimates <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref> and also for view-interpolation and time retargeting applications <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>. Different from prior works, our work demonstrates the emergence of objectness through such layered representations.</p><p>Unsupervised learning for segmentation. Human annotation of pixel-level segmentation is not only time-consuming, but also often inaccurate along object boundaries. Learning segmentation without labels is thus of great interest in practice. Segsort <ref type="bibr" target="#b36">[37]</ref> predicts segmentation by learning to group super-pixels of similar appearance and context from static images. Later work <ref type="bibr" target="#b37">[38]</ref> contrasts holistic mask proposals obtained from traditional bottom-up grouping.</p><p>A related line of work focuses on learning part segmentation from images and videos of the same object category, such as humans and faces. SCOPS <ref type="bibr" target="#b38">[39]</ref> is a representative method learned in a self-supervised fashion. The general idea follows unsupervised landmark detection <ref type="bibr" target="#b39">[40]</ref>, where geometric invariance, representation equivariance and perceptual reconstructions are considered. Co-part segmentation <ref type="bibr" target="#b40">[41]</ref> is also explored in videos, where motion provides a strong cue for part organization. A motion-supervised approach <ref type="bibr" target="#b41">[42]</ref> models part motion between adjacent frames via affine parameters. Another work <ref type="bibr" target="#b42">[43]</ref> implements a similar idea in capsule networks. Our work differs significantly in studying learning from generic videos instead of from a single visual category.</p><p>Image representation learning using motions. Motion contains rich cues about object location, shape, and part hierarchy. Motion segmentation has been used as a self-supervision signal for learning image-level object representations <ref type="bibr" target="#b43">[44]</ref>. Motion propagation <ref type="bibr" target="#b44">[45]</ref> predicts a dense optical flow field from sparse optical flow vectors, conditioned on an RGB image. Our work also produces an image representation from unlabeled videos. Unlike prior works, our image representation is a by-product of our full framework for video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Segmentation by Appearance-Motion Decomposition</head><p>The goal of this work is to learn a zero-shot model to detect and segment objects by merely exposing it to unlabeled videos. We are only interested in detecting the objectness in this paper instead of We learn a single-image segmentation network and a dual-frame motion network with an unsupervised image reconstruction loss. We sample two frames, i and j, from a video. Frame i goes through the segmentation network and outputs a set of masks, whereas frames i and j go through the motion network and output a feature map. The feature is pooled per mask and a flow is predicted.</p><p>All the segments and their flows are combined into a segment flow representation from frame i ? j, which are used to warp frame i into j, and compared against frame j to train the two networks.</p><p>further categorizing the objects into specific classes. We assume that a single moving object appears in one video. When multiple moving objects occur, the model needs to group these objects as one.</p><p>We take a learning-based approach to this problem. During training, we are given a collection of unlabeled videos for self-supervised learning. The pretrained model should be directly applicable for inference on a novel image or a video to produce the object segmentation masks. The overall pipeline for training our model is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Our approach appearance-motion decomposition (AMD) takes a pair of RGB frames X i and X j sampled from a video for learning. The model consists of an appearance pathway f A (X i ) and a motion pathway f M (X i , X j ). The two pathways jointly construct a segment flow representation F , which is used to warp frame X i into X j . The overall model is self-supervised by the reconstruction objective on the frame X j . In the following, we describe the details for each module in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Appearance Pathway for Segmentation</head><p>The appearance pathway is a fully convolutional neural network for segmenting a static RGB image into separate regions. Formally, given the image X i ? R 3?h?w , it is segmented into c regions by</p><formula xml:id="formula_0">S = f A (X i ) ? R c?h?w .</formula><p>(1) In practice, the mask S is a soft probability distribution normalized across c channels. c is an important hyper-parameter of our approach. A large c may lead to over-segmentation, and a small c may not locate the object. Empirically, we use a default value of c = 5, and this is examined later in an ablation study.</p><p>We note that our segmentation network is designed to operate on static images and thus the network can be transferred to downstream image-based vision tasks. In Section 4.2, we demonstrate that the pretrained segmentation network can be used to detect salient objects in a zero-shot fashion. Fine-tuning the appearance pathway on a labeled dataset is examined in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Motion Pathway for Correspondence</head><p>The purpose of the motion pathway is to extract pixel-wise motion features between a pair of images in order to predict the region flow vector detailed in the next subsection. We follow the network architecture of PWC-Net <ref type="bibr" target="#b19">[20]</ref> for predicting dense optical flow, where the feature for each pixel describes the perceptual similarity to its spatial neighbors in the other frame. Formally, given input frames X i and X j , the network extracts features V by</p><formula xml:id="formula_1">V = f M (X i , X j ) ? R dv?h?w (2) where d v is the dimension of motion features.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Segment Flow Representation</head><p>Given the decoupled appearance pathway and motion pathway, the segment and its motion can be binded for view synthesis. Concretely, we pool the pixel-wise motion features within each segmentation mask to obtain the mask motion feature as a single vector,</p><formula xml:id="formula_2">V m = (V S m ) S m ? R dv , m = 1, ..., c<label>(3)</label></formula><p>where the summation operation is taken across the spatial coordinates, and m is used to index the segmentation masks. The optical flow vector for each segmentation mask is read out from the motion feature by</p><formula xml:id="formula_3">F m = g(V m ) ? R 2 , m = 1, ..., c<label>(4)</label></formula><p>where the head network g(?) is chosen as a two-layered multilayer perceptron (MLP).</p><p>So far, we decompose a pair of images X i ? X j into a set of segmentation masks S m and their associated flow vectors F m . This decomposition is based on the assumption that pixels within a mask share the same motion, a condition that simplifies optical flow estimation. This assumption may not hold for articulated objects and inhomogeneous motion. However, it becomes less problematic when all views in a video are taken for optimization, with the appearance pathway able to aggregate a smoothly moving region into a meaningful segment.</p><p>We reconstruct a novel flow representation for the full image by composing the layers of segments with their motion vectors,</p><formula xml:id="formula_4">F = m F m S m , m = 1, ..., c,<label>(5)</label></formula><p>where denotes the outer product. Since the flow representation F is segment-based, we refer to it as segment flow. This decoupled representation allows each component to cross-supervise each other. Given an optical flow offset, the segmentation network could be supervised to find pixels that share this offset. Given a segmentation mask, the correspondence network could be supervised to find the flow offset for this mask.</p><p>This approach for supervising object segmentation using motion information is fundamentally different from motion segmentation methods. Our segmentation mask is predicted from a static appearance model that does not require dense and accurate flow for supervision. It utilizes flow at the region level, which can be approximated from sparse and noisy pixel-level estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Reconstruction Objective</head><p>With the segment flow offset map, we are able to warp frame X i to X j b?</p><formula xml:id="formula_5">X j (p) = X i (p + F (p)),<label>(6)</label></formula><p>where p is a spatial location index. The ground-truth frame X j provides supervision for reconstructed frameX j through the following objective,</p><formula xml:id="formula_6">L = D(X j ,X j ),<label>(7)</label></formula><p>where D is a metric defining distance between two images. Among the numerous choices for D, such as photometric losses <ref type="bibr" target="#b45">[46]</ref>, deep-feature-based losses <ref type="bibr" target="#b46">[47]</ref> <ref type="bibr" target="#b47">[48]</ref>, and contrastive losses <ref type="bibr" target="#b48">[49]</ref>, we adopt the pixel-wise photometric loss of SSIM <ref type="bibr" target="#b45">[46]</ref> in this work for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Object Segment Selection</head><p>Since our model outputs c masks for each image, the mask corresponding to the object instead of the background needs to be determined. We have empirically observed that the primary moving objects all appear in a particular mask channel across the training videos. This channel can be heuristically identified as the one whose segmentation mask has the maximum averaged segment motion. The object segment from this mask layer is used for evaluating zero-shot downstream tasks. movable objects stationary objects <ref type="figure">Figure 3</ref>: Qualitative salient object detection results. We directly transfer our pretrained segmentation network to novel images on the DUTS dataset without any finetuning. Surprisingly, we find that the model pretrained on videos to segment moving objects can generalize to detect stationary unmovable objects in a static image, e.g. the statue, the plate, the bench and the tree in the last column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We demonstrate that AMD model can be transferred to three downstream applications. First, the appearance pathway is directly applied on static images for salient object detection in a zero-shot fashion. Second, both the appearance and the motion pathway are transferred to video object segmentation in novel videos with zero human labels. Third, we fine-tune the appearance pathway on labeled data for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training and Implementation Details</head><p>AMD is pretrained on the large object-centric video dataset Youtube-VOS <ref type="bibr" target="#b49">[50]</ref>. The training split for Youtube-VOS contains about 4,000 videos covering 94 categories of objects. The total duration for the dataset is 334 minutes. We train the model on the data with a sampling rate 24 frames per second, without using the original segmentation labels.</p><p>We train all model parameters from scratch without external pretraining. For the segmentation network, we use ResNet50 <ref type="bibr" target="#b50">[51]</ref> as our backbone followed by a fully convolutional head containing two convolutional blocks. For the motion network, we adopt PWC-Net <ref type="bibr" target="#b19">[20]</ref> architecture because of its effectiveness in estimating optical flows. We resize the short edge of the input image to 400 pixels, and random crop a square image of size 384 ? 384 with random horizontal flipping augmentation. No other augmentations is engineered. We adopt the symmetric loss that considers either frame as the target frame and sums the two reconstruction errors. For training the overall model, we use the Adam optimizer with a learning rate of 1 ? 10 ?4 and a weight decay of 1 ? 10 ?6 . We train AMD on eight V100 GPUs, with each processing two pairs of sampled adjacent frames. The network is optimized for 400K iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Zero-Shot Saliency Detection</head><p>Once pretrained, AMD's appearance pathway can be directly transferred to object segmentation in novel stationary images without any downstream fine-tuning. To evaluate the quality of the segmentation, we benchmark the results on salient object detection benchmark.</p><p>The salient object detection performance is measured on the DUTS [52] benchmark, which contains 5,019 test images with pixel-level ground truth annotations. We follow two widely used metrics in this area: the F ? score and the per-pixel mean squared errors (MAE). F ? is defined as the weighted harmonic mean of the precision (P ) and recall (R) scores: F ? = (1+? 2 )P ?R ? 2 P +R , with ? 2 = 0.3. MAE is simply the per-pixel averaged error of the soft prediction scores.</p><p>Experimental results. We compare our saliency estimation results against several traditional methods based on low-level cues. Useful low-level cues and priors include background priors <ref type="bibr" target="#b52">[53]</ref>,  objectness <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>, and color contrast <ref type="bibr" target="#b55">[56]</ref>. As shown in <ref type="table" target="#tab_0">Table 1</ref>, our method achieves an F ? score 60.2 and an MAE score of 0.13, outperforming all traditional approaches by a notable margin. We note that AMD is not designed specifically for this task nor for this particular dataset, and its strong performance demonstrates the generalization ability of the model.</p><p>In related work on unsupervised learning of saliency detection <ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref>, the priors of traditional low-level methods are ensembled. Though they do not use saliency annotations, their models are pretrained on ImageNet classification and even semantic segmentation with pixel-level annotations. These methods are thus not fully unsupervised, so they are omitted from the comparisons.</p><p>In <ref type="figure">Figure 3</ref>, we show some qualitative results on salient object detection. Surprisingly, we find that our model pretrained on videos to segment moving objects not only detects movable objects in images, but also generalizes to detect stationary unmovable objects, such as statues, benches, trees and plates shown in the last column. This suggests that our model learns a generic objectness prior from the unlabeled videos. We hypothesize that our model may learn objectness from the camera motion as well. Camera motion may cause the object and the background at various depth to have different observed projective 2D optical flow even though the objects are static.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Zero-shot Video Object Segmentation</head><p>We transfer the pretrained AMD model to object segmentation on novel videos. Since the segmentation prediction from our model is based on static images, inference on images sequentially in a video essentially estimates objectness. In order to exploit motion information, we use a test-time adaptation approach. Concretely, given a novel video, we optimize the training objective in Eq. 7 on pairs of frames sampled from the novel testing video. The adaptation takes 100 iterations per video.</p><p>We evaluate zero-shot video object segmentation on three testing datasets. DAVIS 2016 <ref type="bibr" target="#b62">[63]</ref> contains 20 validation videos with 1,376 annotated frames. SegTrackv2 <ref type="bibr" target="#b63">[64]</ref> contains 14 videos with 976 annotated frames. Following prior works, we combine multiple foreground objects in the annotation into a single object for evaluation. FBMS59 <ref type="bibr" target="#b26">[27]</ref> contains 59 videos with 720 annotated frames. The dataset is relatively challenging because the object may be static for a period of time. We pre-process the ground truth labels following prior work <ref type="bibr" target="#b17">[18]</ref>. For evaluation, we report the Jaccard score, which is equivalent to the intersection over union (IoU) between the prediction and the ground truth segmentation.</p><p>Experimental results. We consider baseline methods claiming to be unsupervised for the full pipeline, including traditional non-learning-based approaches <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b14">15]</ref> and recent selfsupervised learning methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. In <ref type="table" target="#tab_2">Table 3</ref>, we summarize the results for all the methods on the three datasets. Among these methods, NLC <ref type="bibr" target="#b13">[14]</ref> actually relies on an edge model trained with human-annotated edge boundaries, and ARP <ref type="bibr" target="#b14">[15]</ref> depends on a segmentation model trained on a human-annotated saliency dataset. We thus gray their entries in the table. For all the traditional methods, since their original papers do not report results on most of these benchmarks, we simply provide the performance values reported in the CIS paper <ref type="bibr" target="#b17">[18]</ref>.</p><p>We evaluate the performance for AMD with and without test-time adaptation. No adaptation boils down to per-image saliency estimation using only the appearance pathway. Adaptation transfers both appearance and motion pathways. On DAVIS 2016, our method achieves a Jaccard score of 57.8%, surpassing all traditional unsupervised models. For CIS <ref type="bibr" target="#b17">[18]</ref>, their best performing model uses a dense flow CIS <ref type="bibr" target="#b17">[18]</ref> segment flow AMD <ref type="figure">Figure 4</ref>: Qualitative comparisons to motion segmentation based method CIS <ref type="bibr" target="#b17">[18]</ref> with its input dense flow and our segmentation results with segment flow representation. CIS is prone to noise, articulated motion, and camera motion in the dense flow estimations. By decomposing appearance from motion, our model AMD suffers less from these vulnerabilities of dense optical flow. This leads to results much better and more robust than the motion segmentation based approach. significant amount of post-processing, including model ensembing, multi-crop, temporal smoothing and spatial smoothing. We thus refer to their performance obtained from a single model without postprocessing. Our model is slightly worse than CIS on DAVIS, by 1.4%. However, on SegTrackv2 and FBMS59, our method outperforms CIS by large margins of 11.4% and 10.7%, respectively. Motion grouping <ref type="bibr" target="#b18">[19]</ref> is a work concurrent with ours. It is essentially a motion segmentation approach which relies on an off-the-shelf pre-computed dense optical flow model. Motion grouping performs worse than our method on DAVIS2016 and SegTrackv2 when a low-performance unsupervised optical flow model ARFlow is used <ref type="bibr" target="#b5">[6]</ref>. With a state-of-the-art supervised optical flow model <ref type="bibr" target="#b20">[21]</ref> that is trained on ground truth flow, their performance improves significantly. Among all the discussed methods, ours is the first end-to-end self-supervised learning approach which does not require a pretrained optical flow model.</p><p>In <ref type="figure">Figure 4</ref>, we show qualitative comparisons to the baseline CIS <ref type="bibr" target="#b17">[18]</ref>. We display dense flow from pretrained PWC-Net, the CIS results, our segment flows, and our segmentation results. For most of these examples, our segment flow only coarsely reflects the true pixel-level optical flow. However, our segmentation results are significantly better and less noisy, as our model is relatively insensitive to optical flow quality. In the first and the third examples, our model produces high-quality object segmentations even though the motion cue for the objects is very subtle. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Semantic Segmentation</head><p>Given that our pretrained segmentation network can produce meaningful generic object segmentations, we further examine its semantic modeling ability on semantic segmentation. We conduct this experiment on the Pascal VOC 2012 <ref type="bibr" target="#b69">[70]</ref> dataset. The dataset contains 20 object categories with 10,582 training images and 1,449 validation images. Given a pretrained model, we finetune the model on the training set and evaluate the performance on the validation set. The finetuning takes 40,000 iterations with a batch size of 16 and an initial learning rate of 0.01. The learning rate undergoes polynomial decay with a power parameter of 0.9.</p><p>Experimental results. We compare the pretrained model to a image-based contrastive model, MoCo-v2 <ref type="bibr" target="#b1">[2]</ref> and a self-supervised video pretraining model, TimeCycle <ref type="bibr" target="#b61">[62]</ref>. TimeCycle <ref type="bibr" target="#b61">[62]</ref> is pretrained on the VLOG dataset, which is larger than our Youtube-VOS dataset. For MoCo-v2, we also pretrain the contrastive model on the Youtube-VOS dataset, to ablate the role of pretraining datasets. Since the base version of our method does not utilize heavy augmentations as in contrastive models, we also study the effects of data augmentations. The results are reported in <ref type="table" target="#tab_1">Table 2</ref>. Our method outperforms the video pretraining approach TimeCylce significantly by 9.2%. Compared with MoCo-v2, when light augmentation (resizing, cropping) is used, our model slightly outperforms MoCo-v2 by 0.5%. However, when heavy data augmentation (color jitter, grayscale, blurring) is applied, our method underperforms MoCo-v2 by 0.7%. This is possibly because our model is non-contrastive in nature, and thus unable to take advantage of information effectively in augmentations. MoCo-v2 performs much stronger when pretrained on ImageNet, possibly because the semantic distribution of ImageNet is well aligned with VOC2012. Overall, our model outperforms a prior self-supervised video model TimeCycle and compares favorably with contrastive model MoCo-v2 under the same data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>The variable c, the number of segmentation channels, is an important hyper-parameter of our model. We vary the value of c (5, 6, 8) for pretraining the model, and examine its transfer performance for video object segmentation on DAVIS2016. In <ref type="figure" target="#fig_2">Figure 5</ref>, we visualize the model predictions under different number of segments. We observe that a large c tends to lead to over-segmentation, and a small c tends to lead to large regions. The car and the swan is split into multiple regions even if the motion for separated regions are very close. The model trained with c = 5 segments a full object, while the model trained with c = 8 separates the object into parts. When pretraining the model with c &lt; 4, the training becomes unstable. Quantitatively, the video object segmentation performance on DAVIS2016 decreases as we increase the number of segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary</head><p>In this paper, we show that objectiveness could emerge from a computational model by exposing it to unlabeled videos. We present a novel model which decouples the appearance pathway and the motion pathway, and later binds them into a joint segment flow representation. As opposed to prior works that rely heavily on accurate dense optical flows for predicting object segmentation, our method learns only from raw pixel observations. The motion representation in our model is a lot weaker, however the object segmentation is more robust. The proposed model AMD is the first end-to-end learning approach for zero-shot object segmentation without using any pretrained modules. Its performance is validated on a number of image and video object segmentation benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary</head><p>In this supplementary, we provide the network architecture details in Section A.1. In Section A.2, we present more qualitative video object segmentation results on 3 datasets, DAVIS 2016 <ref type="bibr" target="#b62">[63]</ref>, FBMS59 <ref type="bibr" target="#b26">[27]</ref> and SegTrackv2 <ref type="bibr" target="#b63">[64]</ref>. We also present the per class quantitative results on these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Network Details</head><p>The network details are shown in <ref type="table" target="#tab_3">Table 4</ref> and <ref type="table" target="#tab_4">Table 5</ref>. <ref type="table" target="#tab_3">Table 4</ref> shows the detailed network architecture for the segment prediction head of our segmentation network. Our correspondence network adopt the similar framework as PWCNet <ref type="bibr" target="#b19">[20]</ref> which contains a feature extractor, a flow estimator and a context network. The feature extractor is the same as that of PWCNet while we don't use the context network in our correspondence network. <ref type="table" target="#tab_4">Table 5</ref> shows the detailed layers of the flow estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 More Results</head><p>More video object segmentation results are shown in <ref type="figure">Figure 6</ref>, <ref type="figure">Figure 7</ref> and <ref type="figure">Figure 8</ref> for SegTrackv2, DAVIS 2016 and FBMS59 correspondingly. We choose those samples from different videos as many as possible. In <ref type="figure">Figure 9</ref>, more saliency detection results from DUTS <ref type="bibr" target="#b51">[52]</ref> dataset are represented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Broader Impact</head><p>We proposed a self-supervised pretraining method for zero-shot object segmentation. The central idea of decomposing appearance and motion can be implemented with other network architectures, and even training losses. However, we have not studied the implications of these variations of the approach. There will also be unpredictable failures, where the generalization of the self-supervised framework still needs deeper understanding. This method is data-driven thus the data bias problem should be careful during data collection in both pretraining and downstream tasks. As this method can be applied to a wide range of videos without annotation, privacy should be also careful during the data utilization.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: We learn a single-image segmentation network and a dual-frame motion network with an unsupervised image reconstruction loss. We sample two frames, i and j, from a video. Frame i goes through the segmentation network and outputs a set of masks, whereas frames i and j go through the motion network and output a feature map. The feature is pooled per mask and a flow is predicted. All the segments and their flows are combined into a segment flow representation from frame i ? j, which are used to warp frame i into j, and compared against frame j to train the two networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Ablation study on different number of segments. Two examples with segmentation masks and segment flows are shown. The object region is split over multiple masks when c becomes large. The over-segmentation decreases the performance of video object segmentation on DAVIS2016.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :Figure 7 :Figure 8 :Figure 9 :</head><label>6789</label><figDesc>Qualitative Qualitative results of DAVIS 2016 Qualitative results of FBMS59 Qualitative salient object detection results. Our model can detect multiple primary objects and even static object like the chair and the rocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Salient object detection performance on the DUTS dataset. Our model outperforms traditional low-level methods by notable margins.</figDesc><table><row><cell>Model</cell><cell>F ?</cell><cell>MAE</cell></row><row><cell>RBD[53]</cell><cell>51.0</cell><cell>0.20</cell></row><row><cell>HS[60]</cell><cell>52.1</cell><cell>0.23</cell></row><row><cell>MC[54]</cell><cell>52.9</cell><cell>0.19</cell></row><row><cell>DSR[61]</cell><cell>55.8</cell><cell>0.14</cell></row><row><cell>DRFI[55]</cell><cell>55.2</cell><cell>0.15</cell></row><row><cell>AMD</cell><cell>60.2</cell><cell>0.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Transfer performance for semantic segmentation on VOC2012. Our method outperforms TimeCycle and compares favorably with contrastive methods.</figDesc><table><row><cell>Model</cell><cell>Data</cell><cell>Aug.</cell><cell>mIoU</cell></row><row><cell>Scratch</cell><cell>-</cell><cell>-</cell><cell>48.0</cell></row><row><cell>TimeCyle[62]</cell><cell>VLOG</cell><cell>light</cell><cell>52.8</cell></row><row><cell>MoCo-v2[2]</cell><cell>YTB</cell><cell>light</cell><cell>61.5</cell></row><row><cell>AMD</cell><cell>YTB</cell><cell>light</cell><cell>62.0</cell></row><row><cell>MoCo-v2[2]</cell><cell>YTB</cell><cell>heavy</cell><cell>62.8</cell></row><row><cell>AMD</cell><cell>YTB</cell><cell>heavy</cell><cell>62.1</cell></row><row><cell>MoCo-v2[2]</cell><cell>IMN</cell><cell>heavy</cell><cell>72.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance evaluations for unsupervised video object segmentation on DAVIS 2016, SegTrackv2 and FBMS59 datasets. The numbers are measured in terms of Jaccard score. The table is split into traditional non-learning-based and recent self-supervised learning methods. The model results which rely on other kinds of human supervisions (Sup.) are grayed. Dependence for pretrained dense flow method is also listed for each model. MG's results on SegTrackv2 and FMBS59 using ARFlow are reproduced by ours and marked with * . We evaluate AMD with appearance pathway only and with both pathways for test time adaptation. AMD performs favorably to the baseline CIS on DAVIS 2016, while showing large gains on the other two benchmarks.</figDesc><table><row><cell></cell><cell>Model</cell><cell>e2e</cell><cell>Sup.</cell><cell>Flow</cell><cell cols="2">DAVIS 2016 SegTrackv2 FBMS59</cell></row><row><cell>traditional</cell><cell>SAGE[65] NLC[14] CUT[28] FTS[16] ARP[15]</cell><cell></cell><cell cols="2">LDOF[66] SIFTFlow[67] LDOF[66] LDOF[68] saliency CPMFlow[69] edge</cell><cell>42.6 55.1 55.2 55.8 76.2</cell><cell>57.6 67.2 54.3 47.8 57.2</cell><cell>61.2 51.5 57.2 47.7 59.8</cell></row><row><cell>learning</cell><cell>CIS[18] MG[19] AMD (per-img) AMD (per-vid)</cell><cell></cell><cell></cell><cell>PWC[20] ARFlow[6]</cell><cell>59.2 53.2 45.7 57.8</cell><cell>45.6 37.8  *  28.7 57.0</cell><cell>36.8 50.4  *  42.9 47.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Details about the prediction head in our segmentation network. Our segmentation network consists of a backbone, ResNet50, and a prediction head which predicts the segments through the features from the backbone. Here c is a hyperparameter which represents the segment number.? 3, 2048 ? 256) + BN + ReLU 256 ? 48 ? 48 Conv(3 ? 3, 256 ? 256) + BN + ReLU 256 ? 48 ? 48 Conv(3 ? 3, 256 ? c) c ? 48 ? 48</figDesc><table><row><cell>Layer</cell><cell>Output size</cell></row><row><cell>Input Feature</cell><cell>2048 ? 48 ? 48</cell></row><row><cell>Conv(3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Architecture details about our correspondence network. As it processes the input at different pyramid levels, here H and W represents the size of input in a certain level. And c is a hyperparameter about the segment number.</figDesc><table><row><cell>Index</cell><cell>Layer</cell><cell>Output size</cell></row><row><cell>1.</cell><cell>Input Feature</cell><cell>115 ? H ? W</cell></row><row><cell>2.</cell><cell cols="2">Conv(3 ? 3, 115 ? 128) + ReLU 128 ? H ? W</cell></row><row><cell>3.</cell><cell cols="2">Conv(3 ? 3, 128 ? 128) + ReLU 128 ? H ? W</cell></row><row><cell>4.</cell><cell>Concatenate 2. and 3.</cell><cell>256 ? H ? W</cell></row><row><cell>5.</cell><cell>Conv(3 ? 3, 256 ? 96) + ReLU</cell><cell>96 ? H ? W</cell></row><row><cell>6.</cell><cell>Concatenate 3. and 5.</cell><cell>224 ? H ? W</cell></row><row><cell>7.</cell><cell>Conv(3 ? 3, 224 ? 64) + ReLU</cell><cell>64 ? H ? W</cell></row><row><cell>8.</cell><cell>Concatenate 5. and 7.</cell><cell>160 ? H ? W</cell></row><row><cell>9.</cell><cell>Conv(3 ? 3, 160 ? 32) + ReLU</cell><cell>32 ? H ? W</cell></row><row><cell>10.</cell><cell>Concatenate 7. and 9.</cell><cell>96 ? H ? W</cell></row><row><cell>11.</cell><cell>Average Pooling</cell><cell>96 ? c</cell></row><row><cell>12.</cell><cell>FC (96 ? 2)</cell><cell>2 ? c</cell></row><row><cell>image</cell><cell></cell><cell></cell></row><row><cell>segment</cell><cell></cell><cell></cell></row><row><cell>segment flow</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Self-supervised pretraining of visual features in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01988</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning by analogy: Reliable supervision from transformations for unsupervised optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6489" to="6498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3828" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Stereo magnification: Learning view synthesis using multiplane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09817</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Untersuchungen zur lehre von der gestalt. ii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Wertheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychologische forschung</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="301" to="350" />
			<date type="published" when="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9226" to="9235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A transductive approach for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6949" to="6958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="391" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14613</idno>
		<title level="m">Space-time correspondence as a contrastive random walk</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7417" to="7425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anestis</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1777" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3395" to="3402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised moving object detection via contextual information separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Self-supervised video object segmentation by motion grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charig</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hala</forename><surname>Lamdouar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07658</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Motion segmentation and tracking using normalized cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Computer Vision (IEEE Cat. No. 98CH36271)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="1154" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning layered motion segmentations of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>M Pawan Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="319" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Layered segmentation and optical flow estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1768" to="1775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3386" to="3394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to segment moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="282" to="301" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1187" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Motion trajectory segmentation via minimum cost multicuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3271" to="3279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Layered representation for motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edward H Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="361" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Representing moving images with layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edward H Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="625" to="638" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A fullyconnected layered model of foreground and background flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2451" to="2458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Layered image motion with explicit occlusions, temporal consistency, and depth ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2226" to="2234" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Motion based decompositing of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irfan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh IEEE International Conference on Computer Vision</title>
		<meeting>the Seventh IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The visual centrifuge: Model-free layered video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2457" to="2466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High-quality video view interpolation using a layered representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>C Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="600" to="608" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Layered neural rendering for retiming people in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubinstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07833</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Segsort: Segmentation by discriminative sorting of segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7334" to="7344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Unsupervised semantic segmentation by contrasting object mask proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06191</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scops: Self-supervised co-part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="869" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Unsupervised learning of object landmarks through conditional image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07823</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05136</idno>
		<title level="m">Unsupervised discovery of parts, structure, and dynamics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Motion-supervised co-part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhankar</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Lathuili?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03234</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroosh</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13920</idno>
		<title level="m">Unsupervised part representation by flow capsules</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2701" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-supervised learning via conditional motion propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1881" to="1889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Contrastive learning for unpaired image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="319" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangjiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2814" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Saliency detection via absorbing markov chain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1665" to="1672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2083" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="569" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep unsupervised saliency detection: A multiple noisy labeling perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9029" to="9038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multisource weak supervision for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6074" to="6083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Deepusps: Deep robust unsupervised saliency prediction with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Duc Tam Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dax</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13055</idno>
		<editor>Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong Nguyen, Zhongyu Lou, and Thomas Brox</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Harf: Hierarchy-associated rich features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Saliency detection via dense and sparse reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2976" to="2983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycleconsistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2566" to="2576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2192" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Saliency-aware video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="20" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="500" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Beyond pixels: exploring new representations and applications for motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Dense point trajectories by gpuaccelerated large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="438" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Efficient coarse-to-fine patchmatch for large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
