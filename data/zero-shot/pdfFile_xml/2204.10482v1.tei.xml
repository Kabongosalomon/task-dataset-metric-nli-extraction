<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Affine Transformation for Text-to-image Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senmao</forename><surname>Ye</surname></persName>
							<email>senmaoy@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
							<email>feiliu@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkui</forename><surname>Tan</surname></persName>
							<email>minkuitan@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Affine Transformation for Text-to-image Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-to-image synthesis aims to generate natural images conditioned on text descriptions. The main difficulty of this task lies in effectively fusing text information into the image synthesis process. Existing methods usually adaptively fuse suitable text information into the synthesis process with multiple isolated fusion blocks (e.g., Conditional Batch Normalization and Instance Normalization). However, isolated fusion blocks not only conflict with each other but also increase the difficulty of training (see first page of the supplementary). To address these issues, we propose a Recurrent Affine Transformation (RAT) for Generative Adversarial Networks that connects all the fusion blocks with a recurrent neural network to model their long-term dependency. Besides, to improve semantic consistency between texts and synthesized images, we incorporate a spatial attention model in the discriminator. Being aware of matching image regions, text descriptions supervise the generator to synthesize more relevant image contents. Extensive experiments on the CUB, Oxford-102 and COCO datasets demonstrate the superiority of the proposed model in comparison to state-of-the-art models 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Synthesizing images conditioned on descriptive sentences is a popular research topic in the cross-field of vision and language processing. Due to its various potential applications such as photo-editing, computer-aided design and virtual scene generation, many methods have been proposed to address this problem such as Generative Adversarial Networks (GANs) <ref type="bibr">[Reed et al., 2016b;</ref><ref type="bibr">Fang et al., 2019]</ref> and variational auto-encoders <ref type="bibr" target="#b1">[Gregor et al., 2015;</ref><ref type="bibr" target="#b1">Mansimov et al., 2015]</ref>. Recently, GAN-based methods achieve promising results on this task <ref type="bibr">[Qiao et al., 2019;</ref><ref type="bibr">Tao et al., 2020;</ref><ref type="bibr" target="#b1">Hu et al., 2021;</ref><ref type="bibr" target="#b4">Zhu et al., 2019;</ref><ref type="bibr" target="#b3">Ruan et al., 2021]</ref>.</p><p>GANs usually adaptively fuse suitable text information into the synthesis process with multiple isolated fusion blocks such as Conditional Batch Normalization (CBN) and Instance Normalization <ref type="bibr">(CIN)</ref>. <ref type="bibr">CIN et al [Dumoulin et al., 2017]</ref> is first proposed for style transfer. Afterwards, <ref type="bibr">BigGAN [Brock et al., 2019]</ref> and Style- <ref type="bibr">GAN [Karras et al., 2021]</ref> synthesize natural images with impressing visual quality based on <ref type="bibr">CBN and CIN, respectively. Recently, DF-GAN [Tao et al., 2020]</ref>, <ref type="bibr">DT-GAN [Zhang and Schomaker, 2021]</ref> and <ref type="bibr">SSGAN [Hu et al., 2021]</ref> use CIN and CBN to fuse text information into synthesized images. Despite their popularity, CIN and CBN suffer from a serious drawback in that they are isolated in different layers, which ignores the global assignment of text information fused in different layers. Furthermore, isolated fusion blocks are hard to optimize since they do not interact with each other.</p><p>In this paper, we propose a Recurrent Affine Transformation (RAT) for controlling all the fusion blocks consistently. RAT expresses different layers' outputs with standard context vectors of the same shape to achieve unified control of different layers. The context vectors are then connected using Recurrent Neural Network (RNN) in order to detect long-term dependencies. With skipping connections in RNN, fusion blocks are not only consistent between neighbouring blocks but also reduce training difficulty.</p><p>Besides, to improve semantic consistency between texts and synthesized images, we incorporate a spatial attention model in the discriminator. Being aware of matching image regions, text descriptions supervise the generator to synthesize more relevant image contents. Nevertheless, based on extensive experiments, we find that vanilla spatial attention with softmax function leads to model collapse since softmax suppresses most probabilities to be near zero, which in turn leads to increased instability of GAN. To overcome this, we use soft threshold function in place of softmax function, which prevents small attention probabilities from being close to zero. With spatial attention, our discriminator can focus its attention on image regions that are pertinent to the text description, which allows it to supervise the generator more effectively.</p><p>The contributions of this paper are the following:</p><p>? We propose a recurrent affine transformation that connects all the fusion blocks to allow global assignment of text information in the synthesis process.</p><p>? We incorporate spatial attention into the discriminator to focus on relevant image regions, so the generated images are more relevant to the text description. ? It is evident that the proposed model improves the visual quality and evaluation metrics on the CUB, Oxford-102, and COCO datasets.  <ref type="bibr">SSGAN [Hu et al., 2021]</ref> as a way of making CBN aware of spatial regions relevant to text description. To better fuse text information into the synthesis process, DF- <ref type="bibr">GAN [Tao et al., 2020]</ref> proposed a deep fusion method that have multiple affine layers in a single block. Different from previous work, DF-GAN discards normalization operation without decrease in performance, which reduces computational occupation and limitation from large batch-size. Similarly to DF-GAN, our RAT also discards normalization operations since normalization has no influence on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Some previous work <ref type="bibr">Reed et al., 2016b]</ref> showed that extra spatial structure information can lead to a great improvement in image quality. The method in <ref type="bibr">[Reed et al., 2016b]</ref> uses key points and bounding boxes to determine what and where to draw in the image. They represented the key points and bounding boxes as a feature map to feed them into the generator. But labelling key points and bound boxes need a lot of human labour and key points are very subjective. Although they also tried to generate key points with an extra GAN, key points are still needed during training. Similar to <ref type="bibr">[Reed et al., 2016b]</ref>, Wang et al. <ref type="bibr" target="#b3">[Wang and Gupta, 2016</ref>] generated images conditioned on surface maps which can reflect rough sketches of the original images. They also use an extra GAN to synthesize surface maps. The above two models learn to generate handcraft annotations so that they can synthesize images from scratch with an explicit notion of image spatial structure. But they both need additional manual annotations to train their model.</p><p>Matching Text and Image Feature. Graves et al.  first proposed an attention model in the context of handwriting synthesis. Another attention model was proposed by <ref type="bibr" target="#b0">[Bahdanau et al., 2015]</ref> for machine translation.</p><p>It also uses a weighted summation of local annotations as a context feature. But they computed the weights by softmax function. Following <ref type="bibr" target="#b0">[Bahdanau et al., 2015]</ref>, <ref type="bibr" target="#b3">[Xu et al., 2015]</ref> proposed a soft attention model and a hard attention model for image captioning. The soft attention model uses a weighted summation of local region features as a context vector. The hard attention model uses a single local feature as a context feature. The context feature captures visual information relevant to words in the caption. Our attention model aims to find image regions relevant to the entire caption. Different from <ref type="bibr" target="#b1">[Mansimov et al., 2016]</ref>, our model does not directly use attention to generate images. We only apply attention in the discriminator to facilitate the comparison between images and captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we aim to construct a GAN that can better control image details with the proposed RAT (see <ref type="figure">Figure 1</ref>). We begin with an introduction to the pre-training of text encoder in section 3.1. We then discuss how to build a generator with recurrent affine transformations in section 3.2. The final sectionn 3.3 describes how to build a discriminator based on spatial attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Contrastive Text Embedding Pre-training</head><p>We use a bidirectional long-short-term-memory network (LSTM) to encode each text description into a sentence-level feature vector s ? R d and a convolution network to encode each image into an image-level feature vector f ? R d . To train these two feature extractors, we adopt a contrastive loss that maximizes the image-text similarity among a batch of training samples. Following <ref type="bibr">AttnGAN [Xu et al., 2018]</ref>, we first calculate the similarity matrix for all possible text-image pairs by</p><formula xml:id="formula_0">M = [s 1 , s 2 , . . . , s n ] T [f 1 , f 2 , . . . , f n ] ,<label>(1)</label></formula><p>where n is the image number in a batch. s i and f i denote the i th text feature and image feature, respectively. M i,j denotes the dot-product similarity between the i th text feature and j th image feature. The similarity matrix M is turned into match probabilities as follows:</p><formula xml:id="formula_1">M i,j = exp (M i,j ) n j=1 exp (M i,j )</formula><p>.</p><p>(</p><p>To maximize the similarity of text and image features belonging to the same pair, we minimize the contrastive loss:</p><formula xml:id="formula_3">L = ? n i=1 logM i,i .<label>(3)</label></formula><p>The pre-trained text feature extractor maps texts into text feature vectors, which reduces the training difficulty of conditional GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recurrent Affine Transformation</head><p>In this section, we aim to use recurrent affine transformation in the generator to enhance the consistency between fusion  <ref type="figure">Figure 1</ref>: (a) Generator with the proposed recurrent affine transformation for text-to-image synthesis. The fusion blocks are connected by a RNN in order to ensure global assignment of text information. (b) Discriminator with spatial attention can focus on image regions relevant to the text description, which helps to judge the authenticity of the input image. blocks of different layers. We begin by introducing the proposed RAT.</p><formula xml:id="formula_4">(b)</formula><p>RAT first conducts channel-wise scaling operation on an image feature vector c with the scaling parameter, then it applies the channel-wise shifting operation on c with the shifting parameter. This process can be formally expressed as follows:</p><formula xml:id="formula_5">Affine (c | h t ) = ? i ? c + ? i ,<label>(4)</label></formula><p>where h t is the hidden state of a RNN and ?, ? are parameters predicted by two one-hidden-layer MLPs conditioned on h t .</p><formula xml:id="formula_6">? = MLP 1 (h t ), ? = MLP 2 (h t ).<label>(5)</label></formula><p>When applied to an image feature map composed of w ? h feature vectors, the same affine transformation is repeated for every feature vector. For increasing network depth and nonlinearity, as depicted in <ref type="figure">Figure 1</ref>, several RNN units, RATs and convolutions are stacked to form a RAT block. Recurrent Controller. We use a RNN to model the temporal structure within the RAT blocks in order to assign text information on a global basis. Instead of a vanilla RNN, we use the widely used LSTM variant. The initial states of the LSTM is computed from the noise vector z:</p><formula xml:id="formula_7">h 0 = MLP 3 (z), c 0 = MLP 4 (z).<label>(6)</label></formula><p>The update rule of RAT is formulated as follows:</p><formula xml:id="formula_8">? ? ? i t f t o t u t ? ? ? = ? ? ? ? ? ? tanh ? ? ? T s h t?1 ,<label>(7)</label></formula><p>c</p><formula xml:id="formula_9">t = f t c t?1 + i t u t ,<label>(8)</label></formula><formula xml:id="formula_10">h t = o t tanh(c t ), (9) ? t , ? t = MLP t 1 (h t ), MLP t 2 (h t ),<label>(10)</label></formula><p>where i t , f t , o t are the input gate, forget gate and output gate, respectively. T : R D+d ? R d is an affine transformation, where d is the dimensionality of the text embedding and D is the quantity of the RNN hidden state units. Furthermore, two layer-specific MLPs are used to convert each hidden state h t into ? t and ? t because each convolution layer has different channel dimensions. Different to conditional instance normalization, conditional batch normalization and conditional deep fusion <ref type="bibr">[Tao et al., 2020]</ref>, our work no longer takes affine transformation as isolated modules. In contrast, we employ RNN to model the long-term dependency between fusion blocks, which not only forces the fusion blocks to be consistent with one another but also reduces the difficulty of training with skip connections.</p><p>Finally, as depicted in <ref type="figure">Figure 1 (a)</ref>, we use a one-stage generator consisted of 6 up-sample blocks to synthesize fake images. The noise vector z ? N (0, 1) is sampled from standard Gaussian distribution and fed into the generator at the beginning. Each up-sample block is followed by a RAT block to control the image contents. The last 256 ? 256 feature map is turned into a fake image of size 256?256?3 by a hyperbolic tangent function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Matching-Aware Discriminator with Spatial Attention</head><p>To improve the semantic consistency between synthesized image and text description, we incorporate spatial attention into our discriminator. Being aware of matching image regions, text descriptions supervise the generator to synthesize more relevant image contents. As depicted in <ref type="figure">Figure 1 (b)</ref>, several down-sample blocks are used to encode the image into an image feature map P . Combing the information in the image feature map P and sentence vector s, spatial attention produces an attention map ? that suppresses sentence vectors for irrelevant regions. The precise formulation of this attention model is:</p><formula xml:id="formula_11">x w,h = MLP(P w,h , s),<label>(11)</label></formula><formula xml:id="formula_12">? w,h = 1 1+e ?x w,h W,H w=1,h=1 1 1+e ?x w,h ,<label>(12)</label></formula><formula xml:id="formula_13">S w,h = s ? ? w,h ,<label>(13)</label></formula><p>where S w,h is the feature channels of S at location {w, h}. P w,h and s are fed into a multi-layer perception with one hidden layer to compute an energy value x w,h , then this energy value is converted into attention probabilities ? w,h . Finally, S is concatenated with P and fed into the latter down-sample blocks to produce the global feature presentation. For stabilizing GAN training, attention probabilities ? are predicted by soft threshold function <ref type="bibr" target="#b4">[Ye et al., 2018]</ref>:</p><formula xml:id="formula_14">p(x k ) = 1 1+e ?x k K j=1 1 1+e ?x j .<label>(14)</label></formula><p>Soft threshold uses a standard logistic function to squash the negative energy values between (0, 1) before normalization. We do not adopt popular softmax function since it maximizes the biggest probability and suppresses other probabilities to be near 0. The extremely small probabilities hamper the backpropagation of the gradients, which worsens the instability of GAN training. In contrast, the soft threshold function prevents attention probabilities from being close to zero and increases the efficiency of back propagation. The spatial attention model assigns more text features to relevant image regions, which helps the discriminator to determine whether the text image pair matches. In adversarial training, the stronger discriminator forces the generator to synthesize more relevant image content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Objective Functions</head><p>Similar to <ref type="bibr">[Reed et al., 2016b]</ref>, the training objective for the discriminator takes the synthesized image and the mismatched images as negative samples. To keep gradient smooth, we use hinge loss with MA-GP [Tao et al., 2020] on real and match text pair. The precise formulation of the training objective of the discriminator is formulated as follows:</p><formula xml:id="formula_15">L D adv =E x?pdata [max(0, 1 ? D(x, s))] + 1 2 E x?p G [max(0, 1 + D(x, s))] + 1 2 E x?pdata [max(0, 1 + D(x,?))],<label>(15)</label></formula><p>where s is the given text description,? is a mismatched text description. The corresponding training objective of the generator is :   In this section, we present results on the CUB dataset of bird images, MS COCO dataset of common object images and Oxford-102 dataset of flower images in <ref type="table" target="#tab_3">Table 1</ref>. On the CUB dataset, our model achieves the highest Inception scores (5.36) and lowest FID scores (13.91) compared with other state-of-the-art models. On the Oxford dataset, our model achieves the highest Inception scores (4.09). Results of DF-GAN on the Oxford dataset are based on its public available code on the Github. The superiority of our RAT-GAN is more obvious on the COCO dataset where images are more complicated. According to the results, the proposed recurrent fusion strategy performs better in complex scenes. Noticeably, our model has no extra supervision such as DAMSM loss (SS-GAN) or cycle consisted loss (MirrorGAN). We also do not fine-tune the text encoder or truncate the image feature space or the noise vector space as DF-GAN. Extensive results well prove the effectiveness of the proposed RAT.</p><formula xml:id="formula_16">L G adv = E x?p G [min(D(x, s))].<label>(16</label></formula><p>Qualitative Results. In this section, we present qualitative results on the CUB dataset of bird images and the Oxford-102 dataset of flower images. We compare the visualization results of stackGAN, DF-GAN and the proposed RAT-GAN in <ref type="figure" target="#fig_0">Figure 2</ref>. StackGAN is a classical multi-stage method and DF-GAN is a popular one-stage method for text-to-image synthesis. On the CUB dataset, we observe that our model performs much better than DF-GAN and stackGAN with clear details such as feathers, eyes and feet. What's more, the background is also more regular in the results of our RAT-GAN. On the Oxford dataset, our RAT-GAN has better texture and more relevant colors than others. With the proposed RAT and the spatial attention model, our model has fewer distorted shapes and more relevant contents than the other two models. In addition, we observe that stackGAN suppresses the image pixels to be 0.5/1 in 128 ? 128 and 256 ? 256 branches (the first 64 ? 64 branch is normal), which leads to vaguer images than the other two models. As depicted in <ref type="figure" target="#fig_1">Figure 3</ref>, the qualitative superiority of RAT-GAN is more obvious on the COCO dataset compared with DF-GAN because the recurrent interactions between fusion blocks make them as an unit, which enables RAT to realize more complicated control over synthesized images. More synthesized images are included in the supplementary.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>Generator. For ablation study, we show quantitative evaluation results of model components in <ref type="table" target="#tab_4">Table 2</ref>. The baseline is a DF-GAN without latent space truncation. ID 1 directly stacks six MLP blocks to substitute RAT blocks, which decreases the performance because stacked MLPs are hard to optimize. Comparing ID 0 and 3, we can see that RAT improves the model performance obviously. In ID 2, fewer affine layers decrease the performance slightly.</p><p>Discriminator. By equipping the discriminator with spatial attention, the performance increases from 5.25 to 5.36, which reveals that enhancing the discriminator helps the generator to synthesize better images. Spatial attention with softmax function leads to model collapse (the generated pixels are all zero), hence the IS and FID are not available. In contrast, spatial attention with soft threshold avoids such model collapse This flower is dark red in color, with petals that are curled closely around the center. by protecting small probabilities from being zero.</p><p>Diversity. To qualitatively evaluate the diversity of the proposed DF-GAN, we generate random images conditioned on the same text description and different noise vectors. In <ref type="figure">Figure 4</ref>, we display 10 images from the same text. The images share similar foreground with high diversity in spatial structure, which demonstrates that the RAT-GAN can well control the image contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visualization of Attention Maps</head><p>To verify the effectiveness of our spatial attention model, we visualize the attention map ?. The attention map is of size 8? 8 with attention probabilities distributed in (0,1). For better visualize the attention probabilities in the discriminator, we simply up-sample the weights by a factor of 32 with bi-linear interpolation and normalize the attention map by:</p><formula xml:id="formula_17">? = (? ? min(?)) (max(?) ? min(?))</formula><p>.</p><p>Some attention maps are visualized in <ref type="figure">Figure 5</ref>. It's evident that spatial attention can identify regions relevant to the caption, thus enabling the discriminator to make a better comparison between the image and caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future work</head><p>In this paper, we address the text-to-image problem by GAN with the proposed recurrent affine transformation. The main difficulty of this task lies in effectively fusing text information into the image synthesis process. Previous models usually use isolated fusion blocks to adaptively fuse suitable information. In contrast, RAT successfully improves image quality by adding interactions among fusion blocks through RNN. The mutual interactions not only ensure consistent between neighbouring blocks but also reduce training difficulty. Besides, to improve semantic consistency, we incorporate a spatial attention model into the discriminator. Extensive experiments on different datasets demonstrate that our model improves the image quality obviously. In the future, it's interesting to apply RAT to other conditional image synthesis tasks. . Probabilities in input gate It and output gate Ot are regarded as the activations of corresponding channels. Each color stands for a random selected channel. We display 30 input gates and 30 output gate in total. It's obvious that each channel of ht are preferred by different layers. What's more, the variation of preference between each layer is smooth, which proves that consistency is naturally required by neighbouring layers   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Qualitative comparison on the CUB and Ox-ford dataset. The input text descriptions are given in the first row and the corresponding generated images from different methods are shown in the same column. Best view in color and zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative comparison between DF-GAN and our model on the COCO dataset. Best view in color and zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Randomly generated images conditioned on the same text description with different noise vectors. Attention maps predicted by the spatial attention model.Best view in color and zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>activation variation (d) Output activation variation (e) Input activation variation (f) Output activation variation Figure 6: Variation of channel activations in different layers (28 in total)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>The variation of Inception Scores during training on the CUB dataset. The proposed RAT-GAN, MLP and the baseline are compared. We observe that connecting CAT blocks with RNN (RAT-GAN) or MLP blocks (MLP) leads to faster convergence than the baseline because isolated fusion blocks are connected as an unit. But stacked MLP blocks are hard to optimize, hence MLP soon saturates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>More visualization samples from the Oxford dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>More visualization samples from the COCO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>TheOxford-102 dataset contains 102 different categories with 8,189 images of flowers in total. As in[Reed et al., 2016a;<ref type="bibr" target="#b2">Reed et al., 2016c]</ref>, we split images into class-disjoint training and test sets. The CUB dataset has 150 train classes and 50 test classes, while the Oxford-102 dataset has 82 train+val and 20 test classes. For both datasets, 10 captions are used per image. The MS COCO dataset totally consists of 123,287 images with 5 sentence annotations. The official training split of COCO is used for training and the official validation split of COCO is used for testing. During mini-batch selection for training, a random image view (e.g. crop, flip) is chosen for one of the captions. Heusel et al., 2017] to quantify the quantitative performance. On the MS COCO dataset, an Inception-v3 network pre-trained on Image-net is used to compute the KLdivergence between the conditional class distribution (generated images) and the marginal class distribution (real images). The presence of a large IS indicates that the generated images are of high quality. The FID computes the Fr?chet Distance between the image features distribution of the generated and real-world images. The image features are extracted by the same pre-trained Inception v3 network. A lower FID implies the generated images are closer to the real images. To evaluate the IS and FID scores, 30k images are generated by each model from the test dataset as previous works<ref type="bibr" target="#b1">[Hu et al., 2021;</ref> Tao et al., 2020;  Zhang and Schomaker, 2021]  reported that the IS metric completely fails in evaluating the synthesized images. Therefore, we only compare the FID on the COCO dataset. On the CUB and Oxford-102 dataset, pre-trained Inception models are fine-tuned on two fine-grained classification tasks.</figDesc><table><row><cell>A small bird with</cell><cell>This bird is white</cell><cell>This bird is mainly</cell><cell>A bird with blue</cell><cell>This flower has a lot</cell><cell>This flower has</cell><cell>This flower has large</cell><cell>A pale purple five</cell></row><row><cell>blue-grey wings,</cell><cell>from crown to belly,</cell><cell>grey, it has brown</cell><cell>head, white belly</cell><cell>of dark red petals</cell><cell>petals that are</cell><cell>smooth white petals</cell><cell>petaled flower with</cell></row><row><cell>rust colored sides</cell><cell>with gray wingbars</cell><cell>on the feathers and</cell><cell>and breast, and the</cell><cell>and no visible outer</cell><cell>purple and bunched</cell><cell>that turn yellow</cell><cell>yellow stamen and</cell></row><row><cell>and white collar.</cell><cell>and retrices.</cell><cell>back of the tail.</cell><cell>bill is pointed.</cell><cell>stigma or stamen.</cell><cell>together.</cell><cell>toward the center.</cell><cell>green stigma.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Training Details. The text encoder is frozen during the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">training of GAN with an output of size 256. The random</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">noise is sampled from a 100-dimensional standard Gaussian</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">distribution. Adam optimizer is used to optimize the net-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">work with base learning rates of 0.0001 for the generator and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">0.0004 for the discriminator. We used a mini-batch size of 24</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">to train the model for 600 epochs on the CUB and Oxford-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">102 datasets. The training of the CUB dataset takes around 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">days on a single NVIDIA RTX 3090 Ti GPU. To accelerate</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">training, We used a mini-batch size of 48 and train the model</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">for 300 epochs on the COCO dataset. With two RTX 3090 Ti</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">GPUs, the COCO dataset takes approximately two weeks to</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>train.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Compared Models. We compare our model with recent</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">state-of-the-art methods: StackGAN++ [Zhang et al., 2019],</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">DM-GAN [Zhu et al., 2019], DF-GAN [Tao et al., 2020],</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">DAE-GAN [Ruan et al., 2021], SSACN [Hu et al., 2021],</cell></row><row><cell></cell><cell></cell><cell></cell><cell>)</cell><cell cols="4">AttnGAN [Xu et al., 2018], DTGAN [Zhang and Schomaker,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>2021].</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4 Experiments</cell><cell></cell><cell></cell><cell cols="3">4.1 Comparisons with Others</cell><cell></cell></row></table><note>Datasets. We report results on the popular CUB, Oxford- 102 and MS COCO datasets. The CUB dataset has 200 dif- ferent categories with 11,788 images of birds in total.Evaluation Metrics. We adopt the widely used Inception Score (IS) [Salimans et al., 2016] and Fr?chet Inception Dis- tance (FID) [Quantitative Results. For simplicity, we denote GAN with the proposed RAT as RAT-GAN.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Performance of IS and FID of StackGAN++, AttnGAN, SS-GAN, DM-GAN, DTGAN, DF-GAN and our method on the CUB, Oxford and COCO test set. The results are taken from the authors' own papers. The best results are in bold.</figDesc><table><row><cell>Methods</cell><cell>IS ?</cell><cell></cell><cell>FID ?</cell></row><row><cell></cell><cell>CUB</cell><cell cols="2">Oxford CUB COCO</cell></row><row><cell cols="2">StackGAN++ 4.04 ? .06</cell><cell>3.26</cell><cell>15.30 81.59</cell></row><row><cell>AttnGAN</cell><cell>4.36 ? .03</cell><cell>-</cell><cell>23.98 35.49</cell></row><row><cell>DAE-GAN</cell><cell>4.42 ? .04</cell><cell>-</cell><cell>15.19 28.12</cell></row><row><cell>DM-GAN</cell><cell>4.75 ? .07</cell><cell>-</cell><cell>16.09 32.64</cell></row><row><cell>DTGAN</cell><cell>4.88 ? .03</cell><cell>-</cell><cell>16.35 23.61</cell></row><row><cell>DF-GAN</cell><cell>5.10 ? .--</cell><cell>3.80</cell><cell>14.81 21.42</cell></row><row><cell>SSAG</cell><cell>5.17 ? .08</cell><cell>-</cell><cell>15.61 19.37</cell></row><row><cell>Ours</cell><cell>5.36 ? .20</cell><cell>4.09</cell><cell>13.91 14.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of RAT and spatial attention in our model on the test set of the CUB dataset. MLPs means replacing RAT blocks with stacked MLP blocks and shallow means removing half of the affine layers from a RAT block.</figDesc><table><row><cell>ID</cell><cell cols="2">Components RAT Spatial Att</cell><cell>IS ?</cell><cell>FID ?</cell></row><row><cell>0</cell><cell>-</cell><cell>-</cell><cell cols="2">4.86 ? 0.04 19.24</cell></row><row><cell>1</cell><cell>MLPs</cell><cell>-</cell><cell cols="2">4.51 ? 0.19 22.57</cell></row><row><cell>2</cell><cell>(shallow)</cell><cell>-</cell><cell cols="2">5.20 ? 0.11 14.90</cell></row><row><cell>3</cell><cell></cell><cell>-</cell><cell cols="2">5.25 ? 0.22 14.86</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell cols="2">5.36 ? 0.20 13.91</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/senmaoy/Recurrent-Affine-Transformationfor-Text-to-image-Synthesis.git</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shancheng Fang, Hongtao Xie, Jianjun Chen, Jianlong Tan, and Yongdong Zhang</title>
		<editor>Sarit Kraus</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Neural turing machines. arXiv: Neural and Evolutionary Computing</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gregor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02793</idno>
	</analytic>
	<monogr>
		<title level="m">Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao</title>
		<editor>Scott E Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee</editor>
		<imprint>
			<publisher>Mirza and Osindero</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="217" to="225" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generative adversarial text to image synthesis. international conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
	</analytic>
	<monogr>
		<title level="m">Generative image modeling using style and structure adversarial networks. european conference on computer vision</title>
		<editor>Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio</editor>
		<imprint>
			<publisher>Wang and Gupta</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="318" to="335" />
		</imprint>
	</monogr>
	<note>CVPR. Computer Vision Foundation / IEEE Computer Society</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Zhenxing Zhang and Lambert Schomaker. DTGAN: dual attention generative adversarial networks for text-to-image generation</title>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1947" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
