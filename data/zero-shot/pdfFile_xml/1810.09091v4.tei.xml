<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON CYBERNETICS 1 SG-One: Similarity Guidance Network for One-Shot Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Life Fellow, IEEE</roleName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON CYBERNETICS 1 SG-One: Similarity Guidance Network for One-Shot Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Few-shot Learning</term>
					<term>Image Segmentation</term>
					<term>Neural Networks</term>
					<term>Siamese Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One-shot image semantic segmentation poses a challenging task of recognizing the object regions from unseen categories with only one annotated example as supervision. In this paper, we propose a simple yet effective Similarity Guidance network to tackle the One-shot (SG-One) segmentation problem. We aim at predicting the segmentation mask of a query image with the reference to one densely labeled support image of the same category. To obtain the robust representative feature of the support image, we firstly adopt a masked average pooling strategy for producing the guidance features by only taking the pixels belonging to the support image into account. We then leverage the cosine similarity to build the relationship between the guidance features and features of pixels from the query image. In this way, the possibilities embedded in the produced similarity maps can be adapted to guide the process of segmenting objects. Furthermore, our SG-One is a unified framework which can efficiently process both support and query images within one network and be learned in an end-to-end manner. We conduct extensive experiments on Pascal VOC 2012. In particular, our SG-One achieves the mIoU score of 46.3%, surpassing the baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>O BJECT Semantic Segmentation (OSS) aims at predicting the class label of each pixel. Deep neural networks have achieved tremendous success on the OSS tasks, such as U-net <ref type="bibr" target="#b0">[1]</ref>, FCN <ref type="bibr" target="#b1">[2]</ref> and Mask R-CNN <ref type="bibr" target="#b2">[3]</ref>. However, these algorithms trained with full annotations require many investments to the expensive labeling tasks. To reduce the budget, a promising alternative approach is to apply weak annotations for learning a decent network of segmentation. For example, previous works have implemented image-level labels <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, scribbles <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>, bounding boxes <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and points <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref> as cheaper supervision information. Whereas the main disadvantage of these weakly supervised methods is the lack of the ability for generalizing the learned models to unseen classes. For instance, if a network is trained to segment dogs using thousands of images containing various breeds of dogs, it will not be able to segment bikes without retraining the network using many images containing bikes.</p><p>In contrast, humans are very good at recognizing things with a few guidance. For instance, it is very easy for a X. <ref type="bibr">Zhang</ref>  <ref type="figure">Fig. 1</ref>. An overview of the proposed SG-One approach for testing a new class. Given a query image of an unseen category, e.g.. cow, its semantic object is precisely segmented with the reference to only one annotated example of this category.</p><p>child to recognize various breeds of dogs with the reference to only one picture of a dog. Inspired by this, one-shot learning dedicates to imitating this powerful ability of human beings. In other words, one-shot learning targets to recognize new objects according to only one annotated example. This is a great challenge for the standard learning methodology. Instead of using tremendous annotated instances to learn the characteristic patterns of a specific category, our target is to learn one-shot networks to generalize to unseen classes with only one densely annotated example. Concretely, one-shot image segmentation is to discover the object pixels of a query image with the reference to only one support image. The target objects in the support image is densely annotated. Current existing methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> are all based on the Siamese framework <ref type="bibr" target="#b16">[17]</ref>. Briefly, a pair of parallel networks is trained for extracting the features of labeled support images and query images, separately. These features are then fused to generate the probability maps of the target objects. The purpose of the network is actually to learn the relationship between the annotated support image and the query image within the highlevel feature space. These methods provide an advantage that the trained parameters of observed classes can be directly utilized for testing unseen classes without finetuning. Nevertheless, there are some weaknesses with these methods: 1) The parameters of using the two parallel networks are redundant, which is prone to overfitting and leading to the waste of computational resources; 2) combining the features of support and query images by mere multiplication is inadequate for guiding the query network to learn high-quality segmentation masks.</p><p>To overcome the above mentioned weaknesses, we propose a Similarity Guidance Network for One-Shot Semantic Segmentation (SG-One) in this paper. The fundamental idea of SG-One is to guide the segmentation process by effectively arXiv:1810.09091v4 [cs.CV] 12 May 2020 incorporating the pixel-wise similarities between the features of support objects and query images. Particularly, we firstly extract the highlevel feature maps of the input support and query images. Highlevel features are usually abstract and the embeddings of pixels belonging to the same category objects are close. The embeddings with respect to the background pixels are usually depressed and these embeddings are distant from the object embeddings. Therefore, we propose to obtain the representative vectors from support images by applying the masked average pooling operation. The masked average pooling operation can extract the object-related features by excluding the influences from background noises. Then, we get the guidance maps by calculating cosine similarities between the representative vectors and the features of query images at each pixel. The feature vectors corresponding to the pixels of objects in query images are close to the representative vectors extracted from support images, hence the scores in the guidance maps will be high. Otherwise, the scores in guidance maps will be low if the pixels belonging to the background. The generated guidance maps are applied to supply the guidance information of desired regions to the segmentation process. In detail, the position-wise feature vectors of query images are multiplied by the corresponding similarity values. Such a strategy can effectively contribute to activating the target object regions of query images following the guidance of support images and their masks. Furthermore, we adopt a unified network for producing similarity guidance and predicting segmentation masks of query images. Such a network is more capable of generalizing to unseen classes than the previous methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>Our approach offers multiple appealing advantages over the previous state-of-the-arts, e.g.OSLSM <ref type="bibr" target="#b14">[15]</ref> and co-FCN <ref type="bibr" target="#b15">[16]</ref>. First, OSLSM and co-FCN incorporate the segmentation masks of support images by changing the input structure of the network or the statistic distribution of input images. Differently, we extract the representative vector from the intermediate feature maps with the masked average pooling operation instead of changing the inputs. Our approach does not harm the input structure of the network, nor harm the statistics of input data. Averaging only the object regions can avoid the influences from the background. Otherwise, when the background pixels dominate, the learned features will bias towards the background contents. Second, OSLSM and co-FCN directly multiply the representative vector to feature maps of query images for predicting the segmentation masks. SG-One calculates the similarities between the representative vector and the features at each pixel of query images, and the similarity maps are employed to guide the segmentation branch for finding the target object regions. Our method is superior in the process of segmenting the query images. Third, both OSLSM and co-FCN adopt a pair of VGGnet-16 networks for processing support and query images, separately. We employ a unified network to process them simultaneously. The unified network utilizes much fewer parameters, so as to reduce the computational burden and increase the ability to generalize to new classes in testing.</p><p>The overview of SG-One is illustrated in <ref type="figure">Figure 1</ref>. We apply two branches, i.e., similarity guidance branch, and segmenta-tion branch, to produce the guidance maps and segmentation masks. We forward both the support and query images through the guidance branch to calculate the similarity maps. The features of query images also forward through the segmentation branch for prediction segmentation masks. The similarity maps act as guidance attention maps where the target regions have higher scores while the background regions have lower scores. The segmentation process is guided by the similarity maps for getting the precise target regions. After the training phase, the SG-One network can predict the segmentation masks of a new class without changing the parameters. For example, a query image of an unseen class, e.g., cow, is processed to discover the pixels belong to the cow with only one annotated support image provided.</p><p>To sum up, our main contributions are three-fold:</p><p>? We propose to produce robust object-related representative vectors using masked average pooling for incorporating contextual information without changing the input structure of networks. ? We produce the pixel-wise guidance using cosine similarities between representative vectors and query features for predicting the segmentation masks. ? We propose a unified network for processing support and query images. Our network achieves the cross-validate mIoU of 46.3% on the PASCAL-5 i dataset in one-shot segmentation setting, surpassing the baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Object semantic segmentation (OSS) aims at classifying every pixel in a given image for distinguishing different objects or contents. OSS with dense annotations as supervision has achieved great success in precisely identifying various kinds of objects <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b24">[25]</ref>. Recently, most of the works with impressive performance are based on deep convolutional networks. FCN <ref type="bibr" target="#b1">[2]</ref> and U-Net <ref type="bibr" target="#b0">[1]</ref> abandon fully connected layers and propose to only use convolutional layers for preserving relative positions of pixels. Based on the advantages of FCN, DeepLab proposed by Chen et al. <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, is one of the best algorithms for segmentation. It employs dilated convolution operations to increase the receptive field, and meanwhile to save parameters in comparison with the large convolutional kernel methods. He et al. <ref type="bibr" target="#b2">[3]</ref> proposes segmentation masks and detection bounding boxes can be predicted simultaneously using a unified network.</p><p>Weakly object segmentation seeks an alternative approach for segmentation to reduce the expenses in labeling segmentation masks <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b31">[32]</ref>. Zhou <ref type="bibr" target="#b32">[33]</ref> and Zhang <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> propose to discover precise object regions using a classification network with only image-level labels. Wei <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> apply a twostage mechanism to predict segmentation masks. object pixels according to the similarity of adjacent pixels and ground-truth scribble lines. Video Object Segmentation is also a challenging problem of segmenting a specific object in a video clip given merely the annotation of the first frame <ref type="bibr" target="#b35">[36]</ref>. The categories of training and testing sets are disjoint, which makes the task similar to our one-shot image segmentation. OSVOS <ref type="bibr" target="#b36">[37]</ref> adopts a direct approach of learning a segmentation network on the training set, and then finetunes the trained network on the augmented first frames of testing sets. Although OSVOS achieves good segmentation performance, the main drawback is the latency of finetuning during testing a new video clip. PLM <ref type="bibr" target="#b37">[38]</ref> applies a more sophisticated network to learn better feature embeddings by involving intermediate feature maps of both search and query frames. A simple crop method is also applied by estimating the approximate location of target objects according to the relationship between successive frames. SegFlow <ref type="bibr" target="#b38">[39]</ref> leverages optical flow of moving objects to assist the segmentation process. Flownet <ref type="bibr" target="#b39">[40]</ref> are internally embedded to the framework of SegFlow, and updated in an end-to-end way. Consequently, the segmentation network and flow network can benefit from each other to learn better segmentation masks as well as optical flows. VideoMatch <ref type="bibr" target="#b40">[41]</ref> learns the represents of both foreground and background regions, because the successive video clips usually maintain similar or static background environments. Therefore, the learned robust represents can be easily applied to retrieve the target object regions of query images.</p><p>Few-shot learning algorithms dedicates to distinguishing the patterns of classes or objects with only a few labeled samples <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Networks should generalize to recognize new objects with few images based on the parameters of base models. The base models are trained using entirely different classes without any overlaps with the testing classes. Finn et al. <ref type="bibr" target="#b43">[44]</ref> tries to learn some internal transferable representations, and these representations are broadly applicable to various tasks. Vinyals et al. <ref type="bibr" target="#b44">[45]</ref> and Annadani et al. <ref type="bibr" target="#b45">[46]</ref> propose to learn embedding vectors. The vectors of the same categories are close, while the vectors of the different categories are apart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Definition</head><p>Suppose we have three datasets: a training set</p><formula xml:id="formula_0">L train = {(I i , Y i )} Ntrain i=1 , a support set L support = {(I i , Y i )} Nsupport i=1</formula><p>and a testing set</p><formula xml:id="formula_1">L test = {I i } Ntest i=1 , where I i is an image, Y i</formula><p>is the corresponding segmentation mask and N is the number of images in each set. Both the support set and training set have annotated segmentation masks. The support set and testing set share the same types of objects which are disjoint with the training set. We denote l ? Y as the semantic class of the mask Y . Therefore, we have {l train } ? {l support } = ?, where ? denotes the intersection of the two sets. If there are K annotated images for each class, the target few-shot problem is named K-shot. Our purpose is to train a network on the training set L train , which can precisely predict segmentation masks? test on testing set L test according to the reference of the support set L support . Specifically, the predicted masks contains two classes, i.e., object class and background. If the objects in query images share the same category label with the annotated objects in support images, the corresponding values in the predicted masks are supposed to be 1 for indicating object pixels. Otherwise, the corresponding values should be 0 for indicating background pixels.</p><p>In order to better learn the connection between the support and testing set, we mimic this mechanism in the training process. For a query image I i , we construct a pair {(I i , Y i ), (I j , Y j )} by randomly selecting a support image I j whose mask Y j has the same semantic class as Y i . We are supposed to estimate the segmentation mask? i with a function? j = f ? ((I i , Y i ), I j ), where ? is the parameters of the function. In testing, (I i , Y i ) is picked from the support set L support and I j is an image from testing set L test .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed Model</head><p>In this section, we firstly present the masked average pooling operation for extracting the object-related representative vector of annotated support images. Then, the similarity guidance method is introduced for combining the representative vectors and features of query images. The generated similarity guidance maps supply the information for precisely predicting the segmentation masks. Masked Average Pooling The pairs of support images and their masks are usually encoded into representative vectors. OSLSM <ref type="bibr" target="#b14">[15]</ref> proposes to erase the background pixels from the support images by multiplying the binary masks to support images. co-FCN <ref type="bibr" target="#b15">[16]</ref> proposes to construct the input block of five channels by concatenating the support images with their positive and negative masks. However, there are two disadvantages of the two methods. First, erasing the background pixels to zeros will change the statistic distribution of the support image set. If we apply a unified network to process both the query images and the erased support images, the variance of the input data will greatly increase. Second, concatenating the support images with their masks <ref type="bibr" target="#b15">[16]</ref> breaks the input structure of the network, which will also prevent the implementation of a unified network.</p><p>We propose to employ masked average pooling for extracting the representative vectors of support objects. Suppose we have a support RGB image I ? R 3?w?h and its binary segmentation mask Y ? {0, 1} w?h , where w and h are the width and height of the image. If the output feature maps of I is F ? R c?w ?h , where c is the number of channels, w and h are width and height of the feature maps. We firstly resize the feature maps to the same size as the mask Y via bilinear interpolation. We denote the resized feature maps as F ? R c?w?h . Then, the i th element v i of the representative vector v is computed by averaging the pixels within the object regions on the i th feature map.</p><formula xml:id="formula_2">v i = w,h x=1,y=1 Y x,y * F i,x,y w,h x=1,y=1 Y x,y ,<label>(1)</label></formula><p>As the discussion in FCN <ref type="bibr" target="#b1">[2]</ref>, fully convolutional networks are able to preserve the relative positions of input pixels. Therefore, through masked average pooling, we expect to extract the features of object regions while disregarding the background contents. Also, we argue that the input of contextual regions in our method is helpful to learn better object features. This statement has been discussed in DeepLab <ref type="bibr" target="#b25">[26]</ref> which incorporates contextual information using dilated convolutions. Masked average pooling keeps the input structure of the network unchanged, which enables us to process both the support and query images within a unified network. Similarity Guidance One-shot semantic segmentation aims to segment the target object within query images given a support image of the reference object. As we have discussed, the masked average pooling method is employed to extract the representative vector v = (v 1 , v 2 , ..., v c ) of the reference object, where c is the number of channels. Suppose the feature maps of a query image I que is F que ? R c?w ?h . We employ the cosine distance to measure the similarity between the representative vector v and each pixel within F que following Eq. (2)</p><formula xml:id="formula_3">s x,y = v * F que x,y ||v|| 2 * ||F que x,y || 2 ,<label>(2)</label></formula><p>where s x,y ? [?1, 1] is the similarity value at the pixel (x, y). F que x,y ? R c?1 is the feature vector of query image at the pixel (x, y). As a result, the similarity map S integrates the features of the support object and the query image. We use the map S = {s x,y } as guidance to teach the segmentation branch to discover the desired object regions. We do not explicitly optimize the cosine similarity. In particular, we element-wisely multiply the similarity guidance map to the feature maps of query images from segmentation branch. Then, we optimize the guided feature maps to fit the corresponding ground truth masks. Similarity Guidance Branch is fed the extracted features of both query and support images. We apply this branch to produce the similarity guidance maps by combining the features of reference objects with the features of query images. For the features of support images, we implement three convolutional blocks to extract the highly abstract and semantic features, followed by a masked averaged pooling layer to obtain representative vectors. The extracted representative vectors of support images are expected to contain the highlevel semantic features of a specific object. For the features of query images, we reuse the three blocks and employ the cosine similarity layer to calculate the closeness between the representative vector and the features at each pixel of the query images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Similarity Guidance Method</head><p>Segmentation branch is for discovering the target object regions of query images with the guidance of the generated similarity maps. We employ three convolutional layers with the kernel size of 3?3 to obtain the features for segmentation. The inputs of the last two convolutional layers are concatenated with the paralleling feature maps from Similarity Guidance Branch. Through the concatenation, Segmentation Branch can borrow features from the paralleling branch, and these two branches can communicate information during the forward and backward stages. We fuse the generated features with the similarity guidance maps by multiplication at each pixel. Finally, the fused features are processed by two convolutional layers with the kernel size of 3 ? 3 and 1 ? 1, followed by a bilinear interpolation layer. The network finally classifies each pixel to be the same class with support images or to  <ref type="table" target="#tab_4">I  TESTING CLASSES FOR 4-FOLD CROSS-VALIDATION TEST. THE TRAINING  CLASSES OF PASCAL-5 I , I={0,1,2,3} ARE DISJOINT WITH THE TESTING</ref> CLASSES.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Test classes PASCAL-5 0 aeroplane,bicycle,bird,boat,bottle PASCAL-5 <ref type="bibr" target="#b0">1</ref> bus,car,cat,chair,cow PASCAL-5 <ref type="bibr" target="#b1">2</ref> diningtable,dog,horse,motorbike,person PASCAL-5 <ref type="bibr" target="#b2">3</ref> potted plant,sheep,sofa,train,tv/monitor be background. We employ the cross-entropy loss function to optimize the network in an end-to-end manner.</p><p>One-Shot Testing One annotated support image for each unseen category is provided as guidance to segment the target semantic objects of query images. We do not need to finetune or change any parameters of the entire network. We only need to forward the query and support images through the network for generating the expected segmentation masks. K-Shot Testing Suppose there are K(K &gt; 1) support images I i sup , i = {1, 2..., K} for each new category, we propose to segment the query image I que using two approaches. The first one is to ensemble the segmentation masks corresponding to the K support images following OSLSM <ref type="bibr" target="#b14">[15]</ref> and co-</p><formula xml:id="formula_4">FCN [16] based on Eq. (3) Y x,y = max(? 1 x,y ,? 2 x,y , ...,? K x,y ),<label>(3)</label></formula><p>where? i x,y , i = {1, 2, ..., K} is the predicted semantic label of the pixel at (x, y) corresponding to the support image I i sup . Another approach is to average the K representative vectors, and then use the averaged vector to guide the segmentation process. It is notable that we do not need to retrain the network using K-shot support images. We use the trained network in a one-shot manner to test the segmentation performance using K-shot support images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Dataset and Metric</head><p>Following the evaluation method of the previous methods OSLSM <ref type="bibr" target="#b14">[15]</ref> and co-FCN <ref type="bibr" target="#b15">[16]</ref>, we create the PASCAL-5 i using the PASCAL VOC 2012 dataset <ref type="bibr" target="#b46">[47]</ref> and the extended SDS dataset <ref type="bibr" target="#b47">[48]</ref>. For the 20 object categories in PASCAL VOC, we use cross-validation method to evaluate the proposed model by sampling five classes as test categories L test = {4i+1, ..., 4i+5} in <ref type="table">Table I</ref>, where i is the fold number, while the left 15 classes are the training label-set L train . We follow the same procedure of the baseline methods e.g.OSLSM <ref type="bibr" target="#b14">[15]</ref> to build the training and testing set. Particularly, we randomly sample image pairs from the training set. Each image pair have one common category label. One image is fed into the network as a support image accompanied by its annotated mask. Another image is treated as a query image and its mask is applied to calculate the loss. For a fair comparison, we use the same test set as OSLSM <ref type="bibr" target="#b14">[15]</ref>, which has 1,000 supportquery tuples for each fold.</p><p>Suppose the predicted segmentation mask is {M } Ntest i=1 and the corresponding ground-truth annotation is {M } Ntest i=1 , given a specific class l. We define the Intersection over Union (IoU l ) of class l as T P l T P l +F P l +F N l , where the T P, F P and F N are the number of true positives, false positives and false negatives of the predicted masks. The mIoU is the average of IoUs over different classes, i.e.(1/n l ) l IoU l , where n l is the number of testing classes. We report the averaged mIoU on the four cross-validation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>We implement the proposed approach based on the VGG-16 network following the previous works <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Stem takes the input of RGB images to extract middle-level features, and downsamples the images by the scale of 8. We use the first three blocks of the VGG-16 network as Stem. For the first two convolutional blocks of Similarity Guidance Branch, we adopt the structure of conv4 and conv5 of VGGnet-16 and remove the maxpooling layers to maintain the resolution of feature maps. One conv3?3 layer of 512 channels are added on the top without using ReLU after this layer. The following module is masked average pooling to extract the representative vector of support images. In Segmentation Branch, all of the convolutional layers with 3 ? 3 kernel size have 128 channels. The last layer of conv1?1 kernel has two channels corresponding to categories of either object or background. All of the convolutional layers except for the third and the last one are followed by a ReLU layer. We will justify this choice in section IV-E.</p><p>Following the baseline methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, we use the pretrained weights on ILSVRC <ref type="bibr" target="#b48">[49]</ref>. All input images remain the original size without any data augmentations. The support and query images are fed into the network, simultaneously. The difference from them is the support image just go through the guidance branch to obtain the representative vector. The query images goes through both the guidance branch for calculating the guidance maps and go through the segmentation branch for predicting the segmentation masks. We implement the network using PyTorch <ref type="bibr" target="#b49">[50]</ref>. We train the network with the learning rate of 1e ? 5. The batch size is 1, and the weight decay is 0.0005. We adopt the SGD optimizer with the momentum of 0.9. All networks are trained and tested on NVIDIA TITAN X GPUs with 12 GB memory. Our source code is available at https://github.com/xiaomengyc/SG-One.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison</head><p>One-shot <ref type="table" target="#tab_2">Table II</ref> compares the proposed SG-One approach with baseline methods in one-shot semantic segmentation. We observe that our method outperforms all baseline models. The mIoU of our approach on the four divisions achieves 46.3%, which is significantly better than co-FCN by 5.2% and OSLSM by 5.5%. Compared to the baselines, SG-One earns the largest gain of 7.8% on PASCAL-5 1 , where the testing classes are bus, car, cat, chair and cow. co-FCN <ref type="bibr" target="#b15">[16]</ref> constructs the input of the support network by concatenating the support images, positive and negative masks, and it obtains 41.1%. OSLSM <ref type="bibr" target="#b14">[15]</ref> proposed to feed only the object pixels as input by masking out the background regions, and this method obtains 40.8%.</p><p>OSVOS <ref type="bibr" target="#b36">[37]</ref> adopts a strategy of finetuning the network using the support samples in testing, and it achieves only 32.6%.  <ref type="figure">Fig. 3</ref>. Segmentation results on unseen classes with the guidance of support images. For the failure pairs, the ground-truth is on the left side while the predicted is on the right side. To summarize, SG-One can effectively predict segmentation masks on new classes without changing the parameters. Our similarity guidance method is better than the baseline methods in incorporating the support objects for segmenting unseen objects. <ref type="figure">Figure 3</ref> shows the one-shot segmentation results using</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One-Shot</head><p>Five-Shot SG-One on unseen classes. We observe that SG-One can precisely distinguish the object regions from the background with the guidance of the support images, even if some support images and query images do not share much appearance similarities. We also show some failure cases to benefit the sup pred similarity horse v.s. person With the reference to the support objects, the objects in the query images of the same categories will be highlighted, while the distracting objects and the background are depressed. The predicted mask can precisely segment the target objects under the guidance of the similarity maps.</p><p>future researches. We ascribe the failure to 1) the target object regions are too similar to background pixels, e.g.the side of the bus and the car; 2) the target region have very uncommon features with the discovered discriminative regions, e.g.the vest of the dog, which may far distant with the representative feature of support objects. <ref type="figure">Figure 5</ref> illustrates the similarity maps of cosine distance between the support objects and the query images. We try to segment the objects of the query images in the second row corresponding to the annotated objects of the support images in the first row. Note there exist distracting classes in the given query images. We only expect to segment the objects whose categories are consistent with the support images. With the reference to the extracted features of support objects, the corresponding regions in the query images are highlighted while the distracting regions and the background are depressed. The predicted masks can be precisely predicted with the guidance to the similairty maps. Five-shot <ref type="table" target="#tab_2">Table III</ref> illustrates the five-shot segmentation results on the four divisions. As we have discussed, we apply two approaches to five-shot semantic segmentation. The approach of averaging the representative vectors from the five support images achieves 47.1% which significantly outperforms the current state-of-the-art, co-FCN, by 5.7%. This result is also better than the corresponding one-shot mIoU of 46.3%. Therefore, the averaged support vector has a better expressiveness of the features in guiding the segmentation process. Another approach is to solely fuse the final segmentation results by combining all of the detected object pixels. We do not observe any improvement of this approach, comparing to the oneshot result. It is notable that we do not specifically train a new network for five-shot segmentation. The trained network in a one-shot way is directly applied to predict the fiveshot segmentation results. <ref type="figure" target="#fig_2">Figure 4</ref> compares the predicted segmentation masks of one-shot and five-shot. The segmentation masks of five-shot are slightly better than that from <ref type="bibr" target="#b0">1</ref> The details of baseline methods e.g.1-NN, LogReg and Siamese, refer to OSLSM <ref type="bibr" target="#b14">[15]</ref>. The results for co-FCN <ref type="bibr" target="#b15">[16]</ref> are for our re-implemented version. <ref type="table" target="#tab_3">Table IV</ref> reports the evaluation results regarding the same metric adopted in <ref type="bibr" target="#b15">[16]</ref> for a fairer comparison. one-shot prediction. As we have also observed that five-shot testing can only improve the mIoU by 0.8 which is a marginal growth. We think the reason for this phenomenon is that the highlevel features for different objects sharing the same class labels are very close. Hence, averaging these features from different objects can only improve a little in terms of feature expressiveness which causes the five-shot increase is limited. On the other side, our target of the similarity learning is exactly to produce the aligned features for each category. So, the fiveshot results can only improve a little under the current one-shot segmentation settings.</p><p>For a fairer comparison, we also evaluate the proposed model regarding the same metric in co-FCN <ref type="bibr" target="#b15">[16]</ref> and PL+SEG <ref type="bibr" target="#b50">[51]</ref>. This metric firstly calculates the IoU of foreground and background, and then obtains the mean IoU of the foreground and background pixels. We still report the averaged mIoU on the four cross-validation datasets. <ref type="table" target="#tab_3">Table IV</ref> compares SG-One with the baseline methods regarding this metric in terms of one-shot and five-shot semantic segmentation. It is obvious that the proposed approach outperforms all previous baselines. SG-One achieves 63.1% of one-shot segmentation and 65.9% of five-shot segmentation, while the most competitive baseline method PL+SEG only obtains 61.2% and 62.3%. The proposed network is trained end-to-end, and our results do not require any pre-processing or post-processing steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multi-Class Segmentation</head><p>We conduct experiments to verify the ability of SG-One in segmenting images with multiple classes. We randomly select 1000 entries of the query and support images. Query images may contain objects of multiple classes. For each entry, we sample five annotated images from the five testing classes as support images. For every query image, we predict its segmentation masks with the images of different support classes. We fuse the segmentation masks of the five classes by comparing the classification scores. The mIoU on the four datasets is 29.4%. We conduct the same experiment to test the co-FCN algorithm <ref type="bibr" target="#b15">[16]</ref>, and co-FCN only obtains the mIoU of 11.5%. Therefore, we can tell that SG-One is much more robust in dealing with multi-cluass images, although it is trained with images of single class. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>Masked Average Pooling The masked average pooling method employed in the proposed SG-One network is superior in incorporating the guidance masks of support images. Shaban et al. <ref type="bibr" target="#b14">[15]</ref> proposed to multiply the binary masks to the input support RGB images, so that the network can only extract features of target objects. co-FCN <ref type="bibr" target="#b15">[16]</ref> proposed by Rakelly et al.concatenates the support RGB images with the corresponding positive masks, i.e.object pixels are 1 while background pixels are 0, and negative binary masks i.e.object pixels are 1 and background pixels are 0, constituting the inputs of 5 channels. We follow the instructions of these two methods and compare with our masked average pooling approach. Concretely, we firstly replace the masked average pooling layer by a global average pooling layer. Then, we implement two networks. 1) SG-One-masking adopts the methods in OSLSM <ref type="bibr" target="#b14">[15]</ref>, in which support images are multiplied by the binary masks to solely keep the object regions. 2) SG-One-concatenate adopts the methods in co-FCN <ref type="bibr" target="#b15">[16]</ref>, in which we concatenate the positive and negative masks to the support images forming an input with 5 channels. We add an extra input block (VGGnet-16) with 5 convolutional channels for adapting concatenated inputs, while the rest networks are exactly the same as the compared networks. <ref type="table" target="#tab_4">Table V</ref> compares the performance of different methods in processing support images and masks. Our masked average pooling approach achieves the best results on every dataset. The mIoU of the four datasets is 46.3% using our method. The masking method (SG-One-masking) proposed in OSLSM <ref type="bibr" target="#b14">[15]</ref> obtains 45.0% of the mIoU. The approach of co-FCN (SG-One-concat) only obtains 41.75%, which ascribes the modification of the input structure of the network. The modified input block cannot benefit from the pre-trained weights of processing low-level information. We also implement a network using the general GAP layer to extract representative vectors instead of using the binary masks of the support images. The network under such a setting achieves the mIoU of 42.2%, which is inferior to the proposed MAP method. So, it is necessary to mask out the pixels corresponding to the background regions for better representative vectors. In total, we can conclude that 1) a qualified method of using support masks is crucial for extracting high-quality object features; 2) the proposed masked average pooling method provides a superior way to reuse the structure of well-designed classification network for extracting object features of support pairs; 3) networks with 5-channel input cannot benefit from the pre-trained weights and the extra input block cannot be jointly trained with the query images. 4) the masked average pooling layer has superior generalization ability in segmenting unseen classes. Guidance Similarity Generating Methods We adopt the cosine similarity to calculate the distance between the object feature vector and the feature maps of query images. The definition of the cosine distance is to measure the angle between two vectors, and its range is in [?1, 1]. Correspondingly, we abandon the ReLU layers after the third convolutional layers of both guidance and segmentation branches. By doing so, we increase the variance of the cosine measurement, and the cosine similarity is not partly bounded in [0, 1], but in [? <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b0">1]</ref>. For comparison, we add the ReLU layers after the third convolutional layers. The mIoU on the four datasets drops to 45.5% comparing to the non-ReLU approach of 46.3%.</p><p>We also train a network using the 2-norm distance as the guidance, and obtain 30.7% on the four datasets. This result is far poor than the proposed cosine similarity method. Hence, the 2-norm distance is not a good choice for guiding the query images to discover target object regions.</p><p>The Unified Structure We adopt the proposed unified structure between the guidance and segmentation branches. This structure can benefit each other during the forward and backward stages. We implement two networks for illustrating the effectiveness of this structure. First, we remove the first three convolutional layers of Segmentation Branch, and then multiply the guidance similarity maps directly to the feature maps from Similarity Guidance Branch. The final mIoU of the four datasets decreases to 43.1%. Second, we cut off the connections between the two branches by removing the first and second concatenation operations between the two branches. The final mIoU obtains 45.7%. Therefore, Segmentation Branch in our unified network is very necessary for getting high-quality segmentation masks. Also, Segmentation Branch can borrow some information via the concatenation operation between the two branches.</p><p>We also verify the functionality of the proposed unified network in the demand of computational resources and generalization ability. In <ref type="table" target="#tab_4">Table VI</ref>, we observe that our SG-One model has only 19.0M parameters, while it achieves the best segmentation results. Following the methods in OSLSM <ref type="bibr" target="#b14">[15]</ref> and co-FCN <ref type="bibr" target="#b15">[16]</ref>, we use a separate network (SG-Oneseparate) to process support images. This network has slightly more parameters (36.1M) than co-FCN(34.2M). The mIoU of SG-One-separate obtains 44.8%, and this result is far better than the 41.1% of co-FCN. This comparison shows that the approach we applied for incorporating the guidance information from support image pairs is superior to OSLSM and co-FCN in segmenting unseen classes. Surprisingly, the proposed unified network can even achieve higher performance of 46.3%. We attribute the gain of 1.5% to the reuse of the network in extracting support and query features. The reuse strategy not only reduces the demand of computational resources and decreases the risk of over-fitting, but offers the network more opportunities to see more training samples. OSLSM requires the most parameters (272.6M), whereas it has the lowest score. It is also worth mentioning that the number of OSLSM is from the officially released source code, while the number of co-FCN is based on our reimplemented version. Both of the two baseline methods do not share parameters in processing query and support images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Relationship with Video Object Segmentation</head><p>One-shot video segmentation is to segment specified objects in video clips with only the first frame densely annotated <ref type="bibr" target="#b35">[36]</ref>. Similar to our one-shot image semantic segmentation problem, the testing categories of the video segmentation problem are inconsistent with the trianing categories. So, for the both tasks, the underline mission is to learn the relationship between the feature embeddings of the support and query images. Siamese network <ref type="bibr" target="#b16">[17]</ref> is designed to learn the relationship of such a pair of support and query images by applying two parallel networks for extracting high-level abstract features of them, separately. Both the proposed method and a wealth of the video segmentation methods <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b51">[52]</ref> are derivatives of the Siamese network <ref type="bibr" target="#b16">[17]</ref>.</p><p>However, the key difference of the two problems is the source of information between support and query. First, in the video task, the contents of the target objects and background remain consistent in one sequence. For example, given a video clip of a girl dancing on the grass land, the foreground target (the girl) and the background environment (the grass land) do not change very seriously between different frames. In contrast, one-shot image semantic segmentation task does not exist sequential cues in targeting objects nor the background environments. The objects and background in query images are seriously different from the support images . For instance, for our one-shot image segmentation task, we may be required to segment an old man standing on grass with the reference to a little girl lying in bed, as they both belong to the same category, namely, person. Second, benefiting from sequential cues in videos, the video segmentation methods <ref type="bibr" target="#b40">[41]</ref> can calculate the frame-to-frame similarities from successive frames and to boost the performance by online updating. <ref type="figure">Figure 6</ref> illustrates the differences between the two tasks. In the video segmentation task, the target objects and the background environment maintain consistent in the whole video clip. In contrast, the objects and environments are totally different between the support images and the query image in our image segementation task. Compared to our task, none of the background nor successive information can be applied. We apply our SG-One network to the one-shot video segmentation task on DAVIS2016 <ref type="bibr" target="#b52">[53]</ref>. We try our best seeking the fair comparison results in video segmentation papers, which do not apply background similarities nor successive object cues between frames. In <ref type="table" target="#tab_2">Table VII</ref>, the results of the baseline models, OSVOS <ref type="bibr" target="#b36">[37]</ref>, VideoMatch <ref type="bibr" target="#b40">[41]</ref> and RGMP <ref type="bibr" target="#b51">[52]</ref> are obtained by excluding background features and successive frame-to-frame consistencies. These models are merely trained on the training set of DAVIS2016 by randomly selecting image pairs, excluding the finetuning step on testing set and any sequential cues between frames. In Tab. VII, we compare the mIoU of these different algorithms on DAVIS2016 testing set, and our SG-One achieves the best accuracy of 57.3%, surpassing the baseline methods. The proposed model is more robust and better in segmenting the given query images with the reference to only one annotated image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>We present that SG-One can effectively segment semantic pixels of unseen categories using only one annotated example. We abandon the previous strategy <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> and propose the masked average pooling approach to extract more robust object-related representative features. Extensive experiments show the masked average pooling approach is more convenient and capable of incorporating contextual information to learn better representative vectors. We also reduce the risks of the overfitting problem by avoiding the utilization of extra parameters through a unified network. We propose that a welltrained network on images of a single class can be directly applied to segment multi-class images. We present a pure endto-end network, which does not require any pre-processing or post-processing steps. More importantly, SG-One boosts the performance of one-shot semantic segmentation and surpasses the baseline methods. At the end, we analyze the relationship between one-shot video segmentation and our one-shot image semantic segmentation problems. The experiments show the superiority of the proposed SG-One in terms of segmenting video objects under fair comparison conditions. Code has been made available. We hope our simple and effective SG-One can serve as a solid baseline and help ease future research on one/few shot segmentation. There are still two problems and we will try to solve them in the future. First, due to the challenging settings of the one-shot segmentation problem, the latent distributions between the training classes and testing classes do not align, which prevents us from obtaining better features for input images. Second, the predicted masks sometimes can only cover part of the target regions and may include some background noises if the target object is too similar to the background. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Concretely, confident regions of objects and background are firstly extracted via the methods of Zhou et al.or Zhang et al.Then, a segmentation network, such as DeepLab, can be trained for segmenting the target regions. An alternative weakly segmentation approach is to use scribble lines to indicate the rough positions of objects and background. Lin et al. [7] and Tang et al. [8] adopted spectral clustering methods to distinguish the The network of the proposed SG-One approach. A query image and a labeled support image are fed into the network. Guidance Branch is to extract the representative vector of the target object in the support image. Segmentation Branch is to predict the segmentation masks of the query image. We calculate the cosine distance between the vector and the intermediate features of the query image. The CosineSimilarty maps are then employed to guide the segmentation process. The blue arrows indicate data streams of support images, while the black are for query images. Stem is the conv1 to conv3 of VGG16. Interp refers to the bilinear interpolation operation. Conv is a convoluational block. Conv k ? k is the convolutional filter with a kernel size of k ? k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>depicts the structure of our proposed model. SG-One includes three components, i.e., Stem, Similarity Guidance Branch, and Segmentation Branch. Different components have different structures and functionalities. Stem is a fully convolutional network for extracting intermediate features of both support and query images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Qualititive illustration of the one-shot and five-shot segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Yi</head><label></label><figDesc>Yang received the Ph.D. degree in computer science from Zhejiang University, Hangzhou, China, in 2010. He is currently a professor with University of Technology Sydney, Australia. He was a Post-Doctoral Research with the School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA. His current research interest include machine learning and its applications to multimedia content analysis and computer vision, such as multimedia indexing and retrieval, surveillance video analysis and video semantics understanding. Thomas S. Huang received Sc.D. from the Massachusetts Institute of Technology in 1963. He was a tenured faculty member at MIT from 1963 to 1973. He was director of the Information and Signal Processing Laboratory at Purdue University from 1973 to 1980. He is currently Research Professor and Swanlund Endowed Chair Professor Emeritas with the University of Illinois at Urbana-Champaign, and Director of Image Formation and Processing (IFP) group at Beckman Institute. He has co-authored over 20 books and over 1300 papers. He is a leading researcher in computer vision, image processing and multi-modal signal processing. He is a member of the National Academy of Engineering. He is a fellow of IEEE, IAPR, and OSA. He was a recipient of the IS&amp;T and SPIE Imaging Scientist of the Year Award in 2006. He has received numerous awards, including the IEEE Jack Kilby Signal Processing Medal, the King-Sun Fu Prize of the IAPR, and the Azriel Rosenfeld Life Time Achievement Award at ICCV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Guidance Segmentation CNN Similarity Map Multiplication CNN que img sup img sup mask</head><label></label><figDesc>, Y. Wei, Y. Yang are with the ReLER lab, Centre for Artificial Intelligence, University of Technology Sydney, Ultimo, NSW 2007, Australia. ( e-mail: Xiaolin.Zhang-3@student.uts.edu.au, Yunchao.Wei@uts.edu.au, Yi.Yang@uts.edu.au ) T. S. Huang is with the Department of Electrical and Computer Engineering and Beckman Institute, University of Illinois at Urbana-Champaign, Urbana, IL, 61901 USA. ( e-mail:t-huang1@illinois.edu )</figDesc><table><row><cell>Similarity</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II MEAN</head><label>II</label><figDesc>IOU RESULTS OF ONE-SHOT SEGMENTATION ON THE PASCAL-5 I DATASET. THE BEST RESULTS ARE IN BOLD.TABLE III MEAN IOU RESULTS OF FIVE-SHOT SEGMENTATION ON THE PASCAL-5 I DATASET. THE BEST RESULTS ARE IN BOLD.</figDesc><table><row><cell>Methods 1</cell><cell>PASCAL-5 0</cell><cell>PASCAL-5 1</cell><cell>PASCAL-5 2</cell><cell>PASCAL-5 3</cell><cell>Mean</cell></row><row><cell>1-NN</cell><cell>25.3</cell><cell>44.9</cell><cell>41.7</cell><cell>18.4</cell><cell>32.6</cell></row><row><cell>LogReg</cell><cell>26.9</cell><cell>42.9</cell><cell>37.1</cell><cell>18.4</cell><cell>31.4</cell></row><row><cell>Siamese</cell><cell>28.1</cell><cell>39.9</cell><cell>31.8</cell><cell>25.8</cell><cell>31.4</cell></row><row><cell>OSVOS [37]</cell><cell>24.9</cell><cell>38.8</cell><cell>36.5</cell><cell>30.1</cell><cell>32.6</cell></row><row><cell>OSLSM [15]</cell><cell>33.6</cell><cell>55.3</cell><cell>40.9</cell><cell>33.5</cell><cell>40.8</cell></row><row><cell>co-FCN [16]</cell><cell>36.7</cell><cell>50.6</cell><cell>44.9</cell><cell>32.4</cell><cell>41.1</cell></row><row><cell>SG-One(Ours)</cell><cell>40.2</cell><cell>58.4</cell><cell>48.4</cell><cell>38.4</cell><cell>46.3</cell></row><row><cell>Methods</cell><cell>PASCAL-5 0</cell><cell>PASCAL-5 1</cell><cell>PASCAL-5 2</cell><cell>PASCAL-5 3</cell><cell>Mean</cell></row><row><cell>1-NN</cell><cell>34.5</cell><cell>53.0</cell><cell>46.9</cell><cell>25.6</cell><cell>40.0</cell></row><row><cell>LogReg</cell><cell>35.9</cell><cell>51.6</cell><cell>44.5</cell><cell>25.6</cell><cell>39.3</cell></row><row><cell>OSLSM [15]</cell><cell>35.9</cell><cell>58.1</cell><cell>42.7</cell><cell>39.1</cell><cell>43.9</cell></row><row><cell>co-FCN [16]</cell><cell>37.5</cell><cell>50.0</cell><cell>44.1</cell><cell>33.9</cell><cell>41.4</cell></row><row><cell>SG-One-max(Ours)</cell><cell>40.8</cell><cell>57.2</cell><cell>46.0</cell><cell>38.5</cell><cell>46.2</cell></row><row><cell>SG-One-avg(Ours)</cell><cell>41.9</cell><cell>58.6</cell><cell>48.6</cell><cell>39.4</cell><cell>47.1</cell></row><row><cell>sup</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>que gt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pred</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>failure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV MEAN</head><label>IV</label><figDesc>IOU RESULTS OF ONE-SHOT SEGMENTATION REGARDING THE EVALUATION METHOD IN CO-FCN<ref type="bibr" target="#b15">[16]</ref> AND PL+SEG<ref type="bibr" target="#b50">[51]</ref> </figDesc><table><row><cell>Methods</cell><cell>one-shot</cell><cell>five-shot</cell></row><row><cell>FG-BG [16]</cell><cell>55.1</cell><cell>55.6</cell></row><row><cell>OSLSM [15]</cell><cell>55.2</cell><cell>-</cell></row><row><cell>co-FCN [16]</cell><cell>60.1</cell><cell>60.8</cell></row><row><cell>PL+SEG [51]</cell><cell>61.2</cell><cell>62.3</cell></row><row><cell>SG-One(Ours)</cell><cell>63.1</cell><cell>65.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>table v</head><label>v</label><figDesc></figDesc><table><row><cell>.s. person person v.s. table</cell><cell>car v.s. bus</cell><cell>bottle v.s. cat</cell><cell>cat v.s. sofa</cell><cell>plant v.s. cat</cell><cell>bottle v.s. table</cell></row></table><note>Fig. 5. Similarity maps of different categories.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V COMPARISON</head><label>V</label><figDesc>IN DIFFERENT METHODS OF EXTRACTING REPRESENTATIVE VECTORS. (i DENOTES THE PASCAL-5 I DATASET.)</figDesc><table><row><cell>Methods</cell><cell>i = 0</cell><cell>i = 1</cell><cell>i = 2</cell><cell>i = 3</cell><cell>Mean</cell></row><row><cell>SG-One-concat</cell><cell>38.4</cell><cell>51.4</cell><cell>44.8</cell><cell>32.5</cell><cell>41.75</cell></row><row><cell>SG-One-masking</cell><cell>41.9</cell><cell>55.3</cell><cell>47.4</cell><cell>35.5</cell><cell>45.0</cell></row><row><cell>SG-One-ours</cell><cell>40.2</cell><cell>58.4</cell><cell>48.4</cell><cell>38.4</cell><cell>46.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>IN THE NUMBER OF NETWORK PARAMETERS AND ONE-SHOT SEGMENTATION MEAN IOUS.</figDesc><table><row><cell>Methods</cell><cell>Parameters</cell><cell>Mean</cell></row><row><cell>OSLSM [15]</cell><cell>272.6M</cell><cell>40.8</cell></row><row><cell>co-FCN [16]</cell><cell>34.2M</cell><cell>41.1</cell></row><row><cell>SG-One-separate</cell><cell>36.1M</cell><cell>44.8</cell></row><row><cell>SG-One-unified</cell><cell>19.0M</cell><cell>46.3</cell></row><row><cell cols="2">Support</cell><cell>Query</cell></row><row><cell>Video Image</cell><cell></cell><cell></cell></row><row><cell>frame 1</cell><cell>frame 10</cell><cell>frame 20</cell></row><row><cell cols="3">Fig. 6. Comparison between the few-shot image segmentation and few-</cell></row><row><cell cols="3">shot video segmentation tasks. The object and background environment keep</cell></row><row><cell cols="3">consistent in between video frames, while both objects and environment are</cell></row><row><cell cols="2">greatly various in the image segmentation task.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII MIOU</head><label>VII</label><figDesc>ON DAVIS2016 VALIDATION SET. SEQUENTIAL CUES AND ONLINE FINETUNING ARE DISABLED FOR A FAIR COMPARISON, AS THE SEQUENTIAL CUES ARE UNAVAILABLE IN OUR TASK .</figDesc><table><row><cell>OSVOS [37]</cell><cell>VideoMatch [41]</cell><cell>RGMP [52]</cell><cell>SG-One</cell></row><row><cell>52.5</cell><cell>52.7</cell><cell>55.0</cell><cell>57.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work is supported by ARC DECRA DE190101315 and ARC DP200100938.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="7268" to="7277" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Stc: A simple to complex framework for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribblesupervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Normalized cut loss for weakly-supervised cnn segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Boundary perception guidance: a scribble-supervised semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3663" to="3669" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Whats the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly supervised salient object detection with spatiotemporal cascade neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weakly supervised scene parsing with point-based distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8843" to="8850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">One-shot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional networks for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR workshop</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Consistent depth video segmentation using adaptive surface models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCYB</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="278" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scale and orientation invariant text segmentation for born-digital compound images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCYB</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="519" to="533" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High-order energies for stereo segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCYB</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1616" to="1627" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">3-d fully convolutional networks for multimodal isointense infant brain image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note type="report_type">TCYB</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A hybrid level set with semantic shape constraint for object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
	<note type="report_type">TCYB</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Coupled bilinear discriminant projection for cross-view gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spgnet: Semantic prediction guidance for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-M</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02611</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weakly supervised facial action unit recognition with domain knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCYB</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3265" to="3276" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Instance annotation via optimal bow for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCYB</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1313" to="1324" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weakly supervised deep learning for brain disease prognosis using mri and incomplete clinical scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCYB</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-erasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="549" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Integral object mining via online attention accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2070" to="2079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-produced guidance for weakly-supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017. 3, 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pixel-level matching for video object segmentation using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="686" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Progressive learning for person re-identification with one example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2872" to="2881" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Asymptotic soft filter pruning for deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCYB</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Preserving semantic relations for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Annadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7603" to="7612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with prototype learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">He focuses on weakly object localization and segmentation, one-shot image semantic segmentation</title>
	</analytic>
	<monogr>
		<title level="m">Xiaolin Zhang is a Ph.D. student at the University of Technology Sydney, Australia. He received the B.S. degree and M.S. degree from the Lanzhou University</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">respectively</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">He was a Postdoctoral Researcher at Beckman Institute, UIUC</title>
	</analytic>
	<monogr>
		<title level="m">Yunchao Wei is currently an Assistant Professor with the University of Technology Sydney. He received his Ph.D. degree from Beijing Jiaotong University</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>He is ARC Discovery Early Career Researcher Award Fellow from 2019 to 2021. His current research interests include computer vision and machine learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
