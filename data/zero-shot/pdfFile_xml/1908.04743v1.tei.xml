<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMS-SPEECH: A SPEECH TO TEXT TOOL</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-08-13">13 Aug 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Denisov</surname></persName>
							<email>pavel.denisov|thang.vu@ims.uni-stuttgart.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Natural Language Processing (IMS)</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Natural Language Processing (IMS)</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IMS-SPEECH: A SPEECH TO TEXT TOOL</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-08-13">13 Aug 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the IMS-Speech, a web based tool for German and English speech transcription aiming to facilitate research in various disciplines which require accesses to lexical information in spoken language materials. This tool is based on modern open source software stack, advanced speech recognition methods and public data resources and is freely available for academic researchers. The utilized models are built to be generic in order to provide transcriptions of competitive accuracy on a diverse set of tasks and conditions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There is a considerable amount of spoken language materials in form of audio recordings, which researchers in e.g. humanities and social science could incorporate into their studies. However, to be able to access to their content, one needs to automatically transcribe these recordings. While all needed resources for building of an automatic speech recognition (ASR) system are typically available for academic usage, their utilization requires specialized knowledge and technical experience <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Therefore, in order to provide people easy accesses to information in spoken language materials, a speech to text tool with a user interface should be helpful.</p><p>This paper presents the IMS-Speech 1 , a web based tool for German and English speech transcription aiming to facilitate research in various disciplines. We are willing to provide a speech transcription service with an intuitive web interface accessible with a wide range of computing devices and to people with various backgrounds. The service is based on modern open source software stack, advanced speech recognition methods and public data resources and is freely available for academic researchers. The utilized models are built to be generic in order to provide transcriptions of competitive accuracy on a diverse set of tasks and conditions. In addition to that, they can serve as a strong base for customized task specific applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System description</head><p>In order to produce a meaningful transcription for the most of recordings that might be uploaded by users, two tasks must be performed for every recording sequentially. First, a recording must be split to segments not exceeding some short duration and corresponding to speech intervals. Second, actual ASR must be performed over each speech segment for finding the most probable sequence of words being said in the segment and thus constructing final transcription.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Speech Segmentation</head><p>Speech segmentation is performed with a speech activity detection (SAD) system based on Time-Delay Neural Network (TDNN) <ref type="bibr" target="#b2">[3]</ref> with statistics pooling <ref type="bibr" target="#b3">[4]</ref> for long-context information. TDNN is trained to estimate probability of 3 classes, Silence, Speech and Garbage, for each frame. Training targets are assigned based on lattices produced by Gaussian Mixture Model (GMM) based acoustic models and predefined lists of phones for each class. GMM is used for forced alignment as well as for unconstrained decoding. Training targets are obtained from both procedures separately and consequently merged by weighted summing, while samples with high disagreement between two methods are discarded. During the decoding, 3 estimated probabilities are transformed to pseudo-likelihoods of 2 states, Silence and Speech, using priors of 3 classes and manually chosen proportions of 2 states in 3 classes. Decoding is performed with Viterbi algorithm <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">End-to-end ASR</head><p>End-to-end approach implements ASR system as a single neural network based model that takes a T -length sequence of d dimensional feature vectors X = {x t ? R d |t = 1, . . ., T } in the input and provides a U -length sequence of output labels Y = {y u ? U |u = 1, . . . ,U }, where U is a set of distinct output labels and usually U &lt; T . Common architecture for such models is attention-based encoder-decoder network trained to minimize cross-entropy loss:</p><formula xml:id="formula_0">L att = ? log p att (Y |X ) (1) p att (Y |X ) = ? u p(y u |X , y 1:u?1 ) (2) p(y u |X , y 1:u?1 ) = Decoder(r u , q u?1 , y u?1 ) (3) h t = Encoder(X ) (4) a ut = Attention({a u?1 } t , q u?1 , h t ) (5) r u = ? t a ut h t .<label>(6)</label></formula><p>Here, Encoder(?) and Decoder(?) are recurrent neural networks, Attention(?) is an attention mechanism and h t , q u?1 and r u are the hidden vectors. Attention mechanism has been developed in the context of machine translation problem <ref type="bibr" target="#b5">[6]</ref> and provides a means to model correspondence of all elements of hidden representations sequence to all elements of output sequence in the decoder. Attention mechanism allows to learn non-sequential mapping between its inputs and outputs, meaning that order of output elements is not always the same as order of input elements corresponding to them, what can be an advantage in case of machine translation task, as word order sometimes differs between languages. However, this property of attention mechanism makes training of speech recognition suboptimal, because it is known in advance that word order is the same in audio and in transcription. Connectionist Temporal Classification (CTC) sequence level loss function <ref type="bibr" target="#b6">[7]</ref> has been adopted as a secondary learning objective for end-to-end ASR models in order to suppress this drawback:</p><formula xml:id="formula_1">L = ? L ctc + (1 ? ? )L att ,<label>(7)</label></formula><p>where 0 ? ? ? 1. Encoder output followed by a single linear layer serves as estimated output label sequence in CTC loss calculation, while target is set to be all possible T -length sequences of an extended output labels set Z = {z t ? U ? &lt;blank&gt;|t = 1, . . . , T }, corresponding to the original output labels sequence Y :</p><formula xml:id="formula_2">L ctc = ? log p ctc (Y |X ) (8) p ctc (Y |X ) ? Z ? t p(z t |z t?1 ,Y )p(z t |X ) (9) p(z t |X ) = Softmax(Lin(h t )).<label>(10)</label></formula><p>It has been found that CTC output can also improve decoding results when combined with the main attention-based probabilities during the search:</p><formula xml:id="formula_3">Y = arg max Y {? log p ctc (Y |X ) + (1 ? ? ) log p att (Y |X )}.<label>(11)</label></formula><p>External language model (LM) is commonly-used technique to improve ASR results. LMs are trained on text corpora, which usually contain order of magnitude more examples of written language compared to acoustic corpora, and therefore provide a reliable source of information for selection of well formed transcriptions from hypotheses. In end-to-end ASR, this information is used during the decoding by adding LM probability of hypothetical output label sequence with scaling factor ? to probabilities obtained from the main model:</p><formula xml:id="formula_4">Y = arg max Y {? log p ctc (Y |X ) + (1 ? ? ) log p att (Y |X ) + ? log p lm (Y )}.<label>(12)</label></formula><p>Encoder-decoder architecture allows output sequence (transcription) to have any length that does not exceed length of input sequence (audio recording). Consequently, it is possible to employ different kinds of output units, for example words or characters. In case of words, transcription hypotheses are limited by words presenting in vocabulary, what causes out of vocabulary problems and requires large dimensionality of final layers. In case of characters, output sequences become very long for alphabetical languages, what leads to high number of hypothetical transcriptions and slows down the decoding. Sub-word units have been suggested first as a trade-off solution in machine translation <ref type="bibr" target="#b7">[8]</ref> and recently have been adopted in speech recognition <ref type="bibr" target="#b8">[9]</ref>. Sub-word units include single characters and can be used to encode any word. In addition to that, sub-word units include combinations of several characters and encode words to shorter sequences compared to single characters. Unigram language model algorithm <ref type="bibr" target="#b9">[10]</ref> performs segmentation of a string X by searching for the most probable sequence of sub-word units composing the string:</p><formula xml:id="formula_5">x * = arg max x?S (X) P(x),<label>(13)</label></formula><p>where probability P(x) of a sequence of sub-word units x = (x 1 , . . . , x M ) is defined as the product of occurrence probabilities of sub-word units:</p><formula xml:id="formula_6">P(x) = M ? i=1 p(x i ),<label>(14)</label></formula><p>?i</p><formula xml:id="formula_7">x i ? V , ? x?V p(x) = 1.</formula><p>Sub-word units vocabulary V is derived during the training of segmentation model by starting from some large set of frequent in the training data substrings and iterative elimination of certain percent of substrings having lowest impact on total likelihood of all possible sequences of subword units for all sentences until some predefined size of vocabulary is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation</head><p>The frontend is implemented as a Node.js/React application and utilizes WebSocket protocol to communicate with the backend. Users can sign in and upload their recordings for transcription. We plan to add the users' feedback with the main focus on customization and fine tuning. Speech segmentation is performed with Kaldi toolkit <ref type="bibr" target="#b0">[1]</ref>. We use the pretrained SAD model downloaded from http://kaldi-asr.org/models/m4. The model is trained on Fisher-English corpus <ref type="bibr" target="#b10">[11]</ref> augmented with room impulses and additive noise from Room Impulse Response and Noise Database <ref type="bibr" target="#b11">[12]</ref>. The input features of SAD model are 40-dimensional Mel Frequency Cepstral Coefficients (MFCC) without cepstral truncation with a frame length 25 ms and shift of 10 ms. We use the segmentation parameters suggested in aspire Kaldi recipe, but extend maximum speech segment duration from 10 to 30 seconds and enable consecutive speech segments merging when duration of merged segment does not exceed 10 seconds.</p><p>Speech recognition is implemented with ESPnet end-to-end speech recognition toolkit <ref type="bibr" target="#b1">[2]</ref> with PyTorch backend. We follow LibriSpeech ESPnet recipe and use 80-dimensional log Mel filterbank coefficients concatenated with 3-dimensional pitch having a frame length of 25 ms and shift of 10 ms as acoustic features and sub-word units as output labels. Kaldi toolkit is used to extract and normalize input features. Normalization to zero mean and unit variance is done with global statistics from the training data. SentencePiece unsupervised text tokenizer 2 is used to generate list of sub-word units based on the language model training data and to segment all kinds of text data. We evaluated several sizes of sub-word unit vocabulary between 50 and 5000 and found that 100 resulted in better results for both English and German systems. The ASR model is an encoder-decoder neural network. The encoder network consists of 2 VGG <ref type="bibr" target="#b12">[13]</ref> blocks followed by 5 Bidirectional Long Short-Term Memory Network (BLSTM) layers <ref type="bibr" target="#b13">[14]</ref> with 1024 units in each layer and direction. The decoder network consists of 2 Long Short-Term Memory Network (LSTM) <ref type="bibr" target="#b14">[15]</ref> layers with 1024 units and location based attention mechanism with 1024 dimensions, 10 convolution channels and 100 convolution filters. CTC weight ? is set to 0.5 for both training and decoding. Training is performed with AdaDelta optimizer <ref type="bibr" target="#b15">[16]</ref> and gradient clipping on 4 Graphics Processing Units (GPUs) in parallel with a batch size of 24 for 10 epochs. The optimizer is initialized with ? = 0.95 and ? = 10 ?8 . ? is halved after an epoch if performance of the model did not improve on validation set. The model with the highest accuracy on validation set is used for the decoding with beam size of 20.</p><p>External LM for the English system contains 2 layers of 650 LSTM units and is trained with stochastic gradient descent optimizer with batch size 256 for 60 epochs. LM scaling factor ? is set to 0.5 during decoding for the English system. External LM for the German system contains 2 layers of 3000 LSTM units and is trained with Adam optimizer <ref type="bibr" target="#b16">[17]</ref> with batch size 128 for 10 epochs. LM scaling factor ? is set to 1.1 during decoding for the German system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Resources</head><p>Both English and German systems are trained on multiple speech databases, which are summarized in <ref type="table" target="#tab_0">Table 1</ref>. We use data preparation scripts from multi_en Kaldi recipe and German ASR recipe <ref type="bibr" target="#b17">[18]</ref>. German system is additionally improved by data augmentation, applied to 3 datasets (marked with (*) in the table) with Acoustic Simulator 3 package. This procedure gives an augmented dataset that is 10 times larger than original dataset.</p><p>External LM for the English system is trained with on transcriptions from the training speech databases except of Common Voice. External LM for the German system is trained on all transcriptions form the training speech databases and additional text corpus 4 containing 8 millions of preprocessed read sentences from the German Wikipedia, the European Parliament Proceedings Parallel Corpus and a crawled corpus of direct speech.  <ref type="bibr" target="#b19">[20]</ref> Spontaneous 317 TED-LIUM 3 <ref type="bibr" target="#b20">[21]</ref> Spontaneous 450 AMI <ref type="bibr" target="#b21">[22]</ref> Spontaneous 229 WSJ <ref type="bibr" target="#b22">[23]</ref> Read 81 Common Voice <ref type="bibr" target="#b4">5</ref> Read 240 Total 2277 German Tuda-De <ref type="bibr" target="#b23">[24]</ref> Read 109 SWC <ref type="bibr" target="#b24">[25]</ref> Read 245 M-AILABS 6 (*) Read 2336 Verbmobil 1 and 2 <ref type="bibr" target="#b25">[26]</ref> (*) Mixed 417 VoxForge 7 (*)</p><p>Read 571 RVG 1 <ref type="bibr" target="#b26">[27]</ref> Mixed 100 PhonDat 1 <ref type="bibr" target="#b27">[28]</ref> Mixed 19 Total 3797 5 ASR Performance 5.1 Results <ref type="table" target="#tab_1">Table 2</ref> compares the results of IMS-Speech on several testing datasets with the best results for the corresponding datasets which we could find in various sources. In summary, these results suggest that our generic systems can compete with task specific systems and in some cases even outperform them, possibly due to better generalization from larger amount of training data.  <ref type="bibr" target="#b30">[31]</ref> We evaluate the recognition speech with different beam widths and batched recognition with inference using CPU and GPU. The results in <ref type="table" target="#tab_2">Table 3</ref> show that batched recognition can significantly increase speed of recognition without any impact on WER. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparisions with Google API</head><p>We use the ASR benchmark framework <ref type="bibr" target="#b31">[32]</ref> to compare performance of IMS-Speech and Google API. The results of Google API were retrieved on 8.01.2019. As the framework uses custom WER computation method instead of NIST sclite utility used in ESPnet recipes, we had to perform scoring of IMS-Speech output with the framework as well. We excluded all utterances for which Google API transcriptions contained digits, because WER would be high for them even if transcriptions were correct (a couple of examples are given in <ref type="table" target="#tab_3">Table 4</ref>), and also utterances for which Google API transcriptions were empty. The results are shown in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>The numbers suggest that that Google API models may be optimized for certain speech domain and recording conditions that differ significantly from the ones tested by us.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented IMS-Speech, a web based speech transcription tool for English and German languages that can be used by non-technical researchers in order to utilize the information from audio recordings in their studies. The comparison of the IMS-Speech results with the results of specialized systems in terms of WER showed that the described service can perform decently in a diverse set of tasks and conditions. In the future, we plan to allow the users to customize the system for their needs as well as to constantly improve our ASR system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 -</head><label>1</label><figDesc>English and German training data covering data sets with different styles</figDesc><table><row><cell cols="2">Language Corpus</cell><cell>Style</cell><cell>Hours</cell></row><row><cell>English</cell><cell>LibriSpeech [19]</cell><cell>Read</cell><cell>960</cell></row><row><cell></cell><cell>Switchboard</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 -</head><label>2</label><figDesc>ASR performance comparison with state of the art results (WER, %)</figDesc><table><row><cell cols="2">Language Dataset</cell><cell cols="2">IMS-Speech State of the art</cell></row><row><cell>English</cell><cell>WSJ eval'92</cell><cell>3.8</cell><cell>3.5 [29]</cell></row><row><cell></cell><cell cols="2">LibriSpeech test-clean 4.4</cell><cell>3.2 [30]</cell></row><row><cell></cell><cell cols="2">LibriSpeech test-other 12.7</cell><cell>7.6 [30]</cell></row><row><cell></cell><cell>TED-LIUM 3 test AMI IHM eval AMI SDM eval AMI MDM eval</cell><cell>12.8 17.4 38.5 34.1</cell><cell>6.7 [21] 19.2 8 36.7 9 34.2 10</cell></row><row><cell>German</cell><cell>Tuda-De dev</cell><cell>11.1</cell><cell>13.1 [18]</cell></row><row><cell></cell><cell>Tuda-De test</cell><cell>12.0</cell><cell>14.4 [18]</cell></row><row><cell></cell><cell>Verbmobil 1 dev</cell><cell>6.7</cell><cell>18.2 [18]</cell></row><row><cell></cell><cell>Verbmobil 1 test</cell><cell>7.3</cell><cell>12.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 -</head><label>3</label><figDesc>Beam width effect on recognition performance and speed on Tuda-De test set</figDesc><table><row><cell>Beam width</cell><cell cols="2">Inference on 1 CPU core with batch size of 1</cell><cell cols="2">Inference on 1 GPU with batch size of 23</cell></row><row><cell></cell><cell cols="2">WER, % RT factor</cell><cell cols="2">WER, % RT factor</cell></row><row><cell>20</cell><cell>12.0</cell><cell>14.2</cell><cell>12.0</cell><cell>0.7</cell></row><row><cell>15</cell><cell>12.2</cell><cell>11.3</cell><cell>12.2</cell><cell>0.5</cell></row><row><cell>10</cell><cell>12.6</cell><cell>8.8</cell><cell>12.6</cell><cell>0.4</cell></row><row><cell>5</cell><cell>13.7</cell><cell>7.0</cell><cell>13.7</cell><cell>0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 -</head><label>4</label><figDesc>Examples of some perfect IMS-Speech transcriptions and Google API transcriptions</figDesc><table><row><cell>Utterance</cell><cell>System</cell><cell>Transcription</cell></row><row><cell>LibriSpeech test-other, 2609-157645-0010</cell><cell>IMS-Speech</cell><cell>then let them sing to the hundred and nineteenth replied the curate</cell></row><row><cell></cell><cell>Google API</cell><cell>then let them sing the 119th repository</cell></row><row><cell>Verbmobil 1 test, w007dxx0_001_BFG</cell><cell>IMS-Speech</cell><cell>Ich w?rde Ihnen den einundzwanzigsten August bis zum vier f?nfundzwanzigsten vorschlagen</cell></row><row><cell></cell><cell>Google API</cell><cell>ich w?rde Ihnen den 21. August bis den 425 vorschlagen</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 -</head><label>5</label><figDesc>ASR performance comparison with Google API in term of WER (%)</figDesc><table><row><cell cols="2">Language Dataset</cell><cell cols="3">IMS-Speech Google API Scored utterances</cell></row><row><cell>English</cell><cell>LibriSpeech test-clean</cell><cell>4.3</cell><cell>15.9</cell><cell>2444 of 2620 (93%)</cell></row><row><cell></cell><cell>LibriSpeech test-other</cell><cell>12.5</cell><cell>28.0</cell><cell>2708 of 2939 (92%)</cell></row><row><cell></cell><cell cols="2">Common Voice valid-test 4.5</cell><cell>19.2</cell><cell>3772 of 3995 (94%)</cell></row><row><cell>German</cell><cell>Tuda-De test</cell><cell>10.0</cell><cell>12.4</cell><cell>3481 of 4100 (85%)</cell></row><row><cell></cell><cell>Verbmobil 1 test</cell><cell>8.7</cell><cell>19.5</cell><cell>334 of 631 (53%)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.ims.uni-stuttgart.de/forschung/ressourcen/werkzeuge/IMS-Speech.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/google/sentencepiece 3 https://github.com/idiap/acoustic-simulator 4 http://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/German_sentences_8mil_filtered_maryfied.tx</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://voice.mozilla.org/en/datasets 6 http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/ 7 http://www.voxforge.org/de/Downloads 8 https://github.com/kaldi-asr/kaldi/blob/4bdb05ae78a842a07cae326aeb32aea87328fb2c/egs/ami/s5b/RESU 9 https://github.com/kaldi-asr/kaldi/blob/4bdb05ae78a842a07cae326aeb32aea87328fb2c/egs/ami/s5b/RESU 10 https://github.com/kaldi-asr/kaldi/blob/4bdb05ae78a842a07cae326aeb32aea87328fb2c/egs/ami/s5b/RESU</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00015</idno>
		<title level="m">ESPnet: End-to-End Speech Processing Toolkit</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Phoneme recognition using time-delay neural networks. In Readings in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Acoustic Modelling from the Signal Domain Using CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03294</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10959</idno>
		<title level="m">Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Fisher Corpus: a Resource for the Next Generations of Speech-to-Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A study on data augmentation of reverberant speech for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE ICASSP</title>
		<meeting>of IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bidirectional LSTM networks for improved phoneme classification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="799" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">ADADELTA: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Open Source Automatic Speech Recognition for German</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Milde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>K?hn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ITG</title>
		<meeting>of ITG</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE ICASSP</title>
		<meeting>of IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SWITCHBOARD: Telephone speech corpus for research and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Holliman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE ICASSP</title>
		<meeting>of IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Esteve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04699</idno>
		<title level="m">TED-LIUM 3: twice as much data and corpus repartition for experiments on speaker adaptation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carletta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The design for the Wall Street Journal-based CSR corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the workshop on Speech and Natural Language</title>
		<meeting>of the workshop on Speech and Natural Language</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Open source german distant speech recognition: Corpus and acoustic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Radeck-Arneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Milde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gouv?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Radomski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?hlh?user</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text, Speech, and Dialogue</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mining the Spoken Wikipedia for Speech Data and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>K?hn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Verbmobil: foundations of speech-to-speech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wahlster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SCHIEL: RVG 1 -A Database for Regional Variants of Contemporary German</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Phondat-verbmobil speech corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01482</idno>
		<title level="m">Deep recurrent neural networks for acoustic modelling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chandrashekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00059</idno>
		<title level="m">The CAPIO 2017 conversational speech recognition system</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Comparing open-source speech recognition toolkits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gaida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Petrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Proba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malatawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suendermann-Oeft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2014" />
			<publisher>DHBW Stuttgart</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<title level="m">A Framework for Speech Recognition Benchmarking. Proc. of Interspeech</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
