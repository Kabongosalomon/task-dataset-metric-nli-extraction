<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEEPER-GXX: DEEPENING ARBITRARY GNNS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqi</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Maciejewski</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<addrLine>1 {lecheng4</addrLine>
									<postCode>dongqif2</postCode>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrui</forename><surname>He</surname></persName>
							<email>jingrui@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DEEPER-GXX: DEEPENING ARBITRARY GNNS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, motivated by real applications, a major research direction in graph neural networks (GNNs) is to explore deeper structures. For instance, the graph connectivity is not always consistent with the label distribution (e.g., the closest neighbors of some nodes are not from the same category). In this case, GNNs need to stack more layers, in order to find the same categorical neighbors in a longer path for capturing the class-discriminative information. However, two major problems hinder the deeper GNNs to obtain satisfactory performance, i.e., vanishing gradient and over-smoothing. On one hand, stacking layers makes the neural network hard to train as the gradients of the first few layers vanish. Moreover, when simply addressing vanishing gradient in GNNs, we discover the shading neighbors effect (i.e., stacking layers inappropriately distorts the non-IID information of graphs and degrade the performance of GNNs). On the other hand, deeper GNNs aggregate much more information from common neighbors such that individual node representations share more overlapping features, which makes the final output representations not discriminative (i.e., overly smoothed). In this paper, for the first time, we address both problems to enable deeper GNNs, and propose Deeper-GXX, which consists of the Weight-Decaying Graph Residual Connection module (WDG-ResNet) and Topology-Guided Graph Contrastive Loss (TGCL). Extensive experiments on real-world data sets demonstrate that Deeper-GXX outperforms state-of-the-art deeper baselines. * Both authors contributed equally to this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) have been proven successful at modeling graph data by extracting node hidden representations that are effective for many downstream tasks. In general, they are realized by the message passing schema and aggregate neighbor features to obtain node hidden representations <ref type="bibr" target="#b11">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b5">Hamilton et al., 2017a;</ref><ref type="bibr" target="#b24">Velickovic et al., 2018;</ref><ref type="bibr" target="#b28">Xu et al., 2019)</ref>. Recently, the surge of big data makes graphs' structural and attribute information much more complex and uncertain, which urges the researchers to make GNNs deeper (i.e., stacking more graph neural layers), in order to capture more meaningful information for better performance. For example, in social media, people from different categories (e.g., occupation, interests, etc.) are often connected (e.g., become friends), and one user's immediate neighbor information may not reflect his/her categorical information. Thus, deepening GNNs is necessary to identify the neighbors from the same category in a longer path (e.g., k-hop neighbors), and to aggregate their features to obtain the class-discriminative node representations. To demonstrate the benefit of deeper GNNs, we conduct a case study shown in <ref type="figure">Figure 1</ref> (See the detailed experimental setup in Appendix A.1). In <ref type="figure">Figure 1a</ref>, we observe that the query node (the diamond in the black dashed circle) cannot rely on its closest labeled neighbor (the red star in the circle) to correctly predict its label (the blue). Only by exploring longer paths consisting of more similar neighbors are we able to predict its label as blue. <ref type="figure">Figure 1b</ref> compares the classification accuracy of shallow GNNs and deeper GNNs. We can see that deeper GNNs significantly outperform shallow ones by more than 11%, due to their abilities to explore longer paths on the graph. Similar observations of the benefits of deeper GNNs are also found in the missing feature scenario presented in Section 3.3.</p><p>(a) Two groups of nodes in the semi-supervised setting. Stars are labeled, dots are unlabeled, and the diamond is the query node. Euclidean distance between two nodes indicates the edge connection.</p><p>(b) Comparison of node classification accuracy between shallow and deeper GNN models using data on the left. The deeper GNNs are realized by our Deeper-GXX with corresponding backbones. <ref type="figure">Figure 1</ref>: A toy example to demonstrate the benefit of deeper GNN models.</p><p>However, simply stacking layers of GNNs can be problematic, due to vanishing gradient and oversmoothing issues. On one hand, increasing the number of neural layers can induce the hard-to-train model, where both the training error and test error are higher than shallow ones. This is mainly caused by the vanishing gradient issue <ref type="bibr" target="#b8">(He et al., 2016)</ref>, where the gradient of the first few layers vanish such that the training loss could not be successfully propagated through deeper models. ResNet <ref type="bibr" target="#b8">(He et al., 2016)</ref> has been proposed to address this issue. However, we discover that simply combining ResNet with GNNs still leads to the sub-optimal solution: as ResNet stacks layers, the importance of close neighbors' features gradually decreases during the GNN information aggregation process, and the faraway neighbor information becomes dominant. We call this effect as shading neighbors. On the other hand, GNN utilizes the message passing schema to aggregate neighbor features, in order to get class-discriminative node representations. However, by stacking more layers, each node begins to share more and more overlapping neighbor information during the aggregation process and the node representations gradually become indistinguishable <ref type="bibr" target="#b14">(Li et al., 2018;</ref><ref type="bibr" target="#b19">Oono &amp; Suzuki, 2020)</ref>. This has been referred to as the over-smoothing issue, and it significantly affects the performance of downstream tasks such as node classification and link prediction.</p><p>In this paper, we study how to effectively stack GNN layers by addressing shading neighbors and oversmoothing at the same time. First, to address the shading neighbors caused by the direct application of ResNet on GNNs, we propose Weight-Decaying Graph Residual Connection (WDG-ResNet), which learns the weight of each residual connection layer (instead of setting it as 1 in ResNet), and further introduces a decaying factor to refine the weight of each layer. Interestingly, we find that the hyperparameter ? of the weight decaying factor actually controls the number of effective layers in deeper GNNs based on the input graph inherent property, which is verified in Appendix A.4. Second, for addressing over-smoothing, we propose Topology-Guided Graph Contrastive Loss (TGCL) in the contrastive learning manner <ref type="bibr">(van den Oord et al., 2018)</ref>, where the hidden representations of the positive pairs should be closer, and those of the negative pairs should be pushed apart. Through theoretical and empirical analysis, we find that TGCL can be effectively and efficiently realized by only considering 1-hop neighbors as the positive pair and all the rest as negative pairs. Combining the proposed WDG-ResNet and TGCL, we propose an end-to-end model called Deeper-GXX to help arbitrary GNNs go deeper. Our contributions can be summarized as follows.</p><p>? We propose Weight-Decaying Graph Residual Connection (WDG-ResNet) to address the shading neighbors effect caused by vanilla ResNet in dealing with the vanishing gradient of GNNs. ? We propose Topology-Guided Graph Contrastive Loss (TGCL) to address the over-smoothing problem by encoding the graph topological information to the discriminative node representations. ? We combine the proposed WDG-ResNet and TGCL into an end-to-end model called Deeper-GXX, which is model-agnostic and can help arbitrary GNNs go deeper. ? Extensive experiments show that Deeper-GXX outperforms state-of-the-art deeper baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROPOSED METHOD</head><p>In this section, we begin with the overview of Deeper-GXX. Then, we provide the details of the proposed Weight-Decaying Graph Residual Connection (WDG-ResNet) and Topology-Guided Graph Contrastive Loss (TGCL), which address shading neighbors and over-smoothing problems, respectively. We formalize the graph embedding problem in the context of undirected graph G = (V, E, X), where V consists of n vertices, E consists of m edges, X ? R n?d denotes the feature matrix and d is the feature dimension. We let A ? R n?n denote the adjacency matrix and denote A i ? R n as the adjacency vector for node v i . H i ? R h is the hidden representation vector of v i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">OVERVIEW OF DEEPER-GXX</head><p>The overview of our proposed Deeper-GXX is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The green dash line stands for Weight-Decaying Graph Residual Connection (WDG-ResNet). In WDG-ResNet, after the current hidden representation H (l) is generated by the l-th layer of arbitrary GNNs, H (l) will be adjusted by its second last layer H (l?2) and the first layer H (1) with proper weights. The red dash line stands for Topology-Guided Graph Contrastive Loss (TGCL). In TGCL, we first need to sample positive node pairs and negative node pairs based on the input graph topology such that the hidden representations of positive node pairs get closer and negative ones are pushed farther apart. After introducing the TGCL loss to GNNs, the overall loss function L overall of Deeper-GXX is expressed as follows.</p><formula xml:id="formula_0">L overall = L GN N + ?L TGCL<label>(1)</label></formula><p>where L GN N denotes the loss of the downstream task using an arbitrary GNN model (e.g., node classification in GCN), L TGCL is the TGCL loss, and ? is a constant hyperparameter. Deeper-GXX combines WDG-ResNet and TGCL to address shading neighbors and over-smoothing problems. The design of WDG-ResNet and TGCL are introduced in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">WEIGHT-DECAYING GRAPH RESIDUAL CONNECTION (WDG-RESNET)</head><p>As we increase the number of layers, one unavoidable problem brought by neural networks is the vanishing gradient, which means that the first several layers of the deeper neural network become hard to optimize as their gradients vanish during the training process <ref type="bibr" target="#b8">(He et al., 2016)</ref>. Currently, nascent deeper GNN methods <ref type="bibr" target="#b30">(Zhao &amp; Akoglu, 2020;</ref><ref type="bibr" target="#b21">Rong et al., 2020;</ref><ref type="bibr" target="#b13">Li et al., 2019)</ref> solve this problem by adding ResNet <ref type="bibr" target="#b8">(He et al., 2016)</ref> on graph neural networks. Taking <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref> as an example, the graph residual connection is expressed as follows.</p><formula xml:id="formula_1">H (l) = ?(?H (l?1) W (l?1) ) + H (l?2)<label>(2)</label></formula><p>where l(? 2) denotes the index of layers, H (l?1) and H (l?2) are the hidden representations, ?(?) is the activation function, W (l?1) is the learnable weight matrix, and? is the re-normalized selflooped adjacency matrix with? =D ? 1 2?D ? 1 2 and? = A + I. In ResNet, the residual connection connects the current layer and its second last layer. Without loss of generality, we assume the last layer of GNNs is stacked by ResNet, i.e., l is divisible by 2. Then, by extending H (l?2) iteratively (i.e., substituting it with its previous residual blocks), the above Eq. 2 could be rewritten as follows.</p><formula xml:id="formula_2">H (l) = ?(?H (l?1) W (l?1) ) + H (l?2) H (l) = ?(?H (l?1) W (l?1) ) + ?(?H (l?3) W (l?3) ) + H (l?4) = ?(?H (l?1) W (l?1) ) + ?(?H (l?3) W (l?3) ) + ? ? ?</formula><p>Information aggregated from the faraway neighbors + ?(?H (i) W (i) ) + ? ? ? + ?(?H (1) W (1) ) Information aggregated from the nearest neighbors (3)</p><p>According to the GNN theoretical analysis, stacking l layers and getting H (l) in GNNs can be interpreted as aggregating l-hop neighbors' feature information for the node hidden representations <ref type="bibr" target="#b28">(Xu et al., 2019)</ref>. As shown in Eq. 3, when we stack more layers in GNNs, the information collected from faraway neighbors become dominant (as there are more terms regarding the information from faraway neighbors), compared with the information collected from the nearest neighbors (e.g., 1-hop or 2-hop neighbors). This phenomenon contradicts the general intuition that the close neighbors of a node carry the most important information, and the importance degrades with faraway neighbors. Formally, we describe this phenomenon as shading neighbors effect when stacking graph neural layers, as the importance of the nearest neighbors is diminishing. We show that shading neighbors effect downgrades the GNNs performance in downstream tasks in Section 3.4.</p><p>To address the shading neighbors effect, we propose Weight-Decaying Graph Residual Connection (WDG-ResNet). Here, we first introduce the formulation and then provide the insights regarding why it can address the problem. Specifically, WDG-ResNet introduces the layer similarity and weight decaying factor as follows.</p><p>H (l) = ?(?H (l?1) W (l?1) ), /*l-th layer of an arbitrary GNN*/</p><formula xml:id="formula_3">H (l) = sim(H (1) ,H (l) ) ? e ?l/? ?H (l) + H (l?2) , /*residual connection*/ = e cos(H (1) ,H (l) ) ? l/? ?H (l) + H (l?2) (4) where cos(H (1) ,H (l) ) = 1 n i H (1) i (H (l) i ) H (1) i H (l) i</formula><p>measures the similarity between the l-th layer and the 1-st layer, and we use the exponential function to map the cosine similarity ranging from {?1, 1} to {e ?1 , e 1 }, to avoid the negative similarity weights. The term e ?l/? is the decaying factor to further adjust the similarity weight ofH <ref type="bibr">(l)</ref> , where ? is a constant hyperparameter.</p><p>Different from ResNet <ref type="bibr" target="#b8">(He et al., 2016)</ref>, we add the learnable similarity sim(H (1) ,H (l) ) to essentially expand the hypothesis space of deeper GNNs. Additional to that, simply adding vanilla ResNet on GNNs will cause the shading neighbors effect. The introduced decaying factor e ?l/? can alleviate this negative effect, because it brings the layer-wise dependency to stacking operations, in order to preserve the graph hierarchical information when GNNs go deeper. Since ? is a constant, the value of e ?l/? is decreasing while l increases. Thus, the later stacked layer is always less important than previously stacked ones by the decaying weight, which addresses the shading neighbors effect. Without the decaying factor, the layer-wise weights are independent, and the shading neighbor effect still exists. Moreover, we visualize the layer-wise weight distribution of different residual connection methods (including our WDG-ResNet) and their effectiveness in addressing shading neighbors effect in Appendix A.3.</p><p>From another perspective, the hyperparameter ? of the decaying factor actually controls the number of effective neural layers in deeper GNNs. For example, when ? = 10, the decaying factor for the 10-th layer is 0.3679 (i.e., e ?1 ); but for the 30-th layer, it is 0.0049 (i.e., e ?3 ). This radioactive decay limits the effective information aggregation scope of deeper GNNs, because the later stacked layers will become significantly less important. Based on this controlling property of ?, a natural follow-up question is whether its value depends on the property of input graphs. Interestingly, through our experiments, we find that the optimal ? is very close to the diameter of input graphs (if it is connected) or the largest component (if it does not have many separate components). This observation verifies our conjecture regarding the property of ? (i.e., it controls the number of effective layers or number of hops during the message passing aggregation schema of GNNs). Hence, the value of ? can be searched around the diameter of the input graph, and we discuss the details in the Appendix A.4.</p><p>Simplified WDG-ResNet. In the experiments, we observe that if the number of layers of GNNs is too large (e.g., 50 layers or more), the computational cost of adding the similarity function sim(H (1) ,H (l) ) at each graph residual connection layer can be expensive. To accelerate the computation, we formulate a simpler version of Eq. 4 by fixing the sim(H (1) ,H (l) ) but keeping the decaying factor. The simplified version of WDG-ResNet is expressed as follows.</p><formula xml:id="formula_4">H (l) =e ?l/? ?(AH (l?1) W (l?1) ) + H (l?2) =e ?l/? ?(AH (l?1) W (l?1) ) + e ?(l?2)/? ?(AH (l?3) W (l?3) ) + e ?(l?4)/? ?(AH (l?5) W (l?5) ) + ? ? ? + e ?4/? ?(AH (3) W (3) ) + e ?2/? ?(AH (1) W (1) )</formula><p>(5) Compared with Eq. 4, Eq. 5 gets rid of the exponential cosine similarity measurement, which greatly reduces the computational cost. However, the simplified WDG-ResNet still keeps the decaying factor for the layer-wise dependency, such that the shading neighbors effect can still be alleviated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">TOPOLOGY-GUIDED GRAPH CONTRASTIVE LOSS (TGCL)</head><p>To address over-smoothing, current deeper GNN solutions depend on certain hard-to-acquire prior knowledge (e.g., important hyperparameters or sampling randomness) to get the discriminative node representations <ref type="bibr" target="#b30">(Zhao &amp; Akoglu, 2020;</ref><ref type="bibr" target="#b21">Rong et al., 2020;</ref><ref type="bibr" target="#b33">Zhou et al., 2020)</ref>. Inspired by these solutions, we are seeking for an effective discriminative indicator that can be easily obtained from the input graph without any prior knowledge. Thus, we propose a novel contrastive regularization term, named Topology-Guided Graph Contrastive Loss (TGCL), to transfer this hard-to-acquire knowledge into the topology information of the input graph as follows.</p><formula xml:id="formula_5">L TGCL = ? E vi?V E vj ?Ni [log ? ij f (z i , z j ) ? ij f (z i , z j ) + v k ?Ni ? ik f (z i , z k ) ] ? ij = 1 ? dist(A i , A j )/n, ? ik = 1 + dist(A i , A k )/n (6) where z i = g(H (l) i ), g(?) is an encoder mapping H (l) i to another latent space, f (?) is a similarity function (e.g., f (a, b) = exp( ab ||a||||b|| )), dist(?)</formula><p>is a distance measurement function (e.g., hamming distance <ref type="bibr" target="#b18">(Norouzi et al., 2012)</ref>), N i is the set containing one-hop neighbors of node v i , andN i is the complement of the set N i .</p><p>In Eq. 6, directly connected nodes (v i , v j ) form the positive pair, while not directly connected nodes (v i , v k ) form the negative pair. The intuition of this equation is to maximize the similarity of the representations of the positive pairs, and to minimize the similarity of the representations of the negative pairs, such that the node representations become discriminative. In the graph contrastive learning setting, designing the positive and negative pairs is very important, because it needs to effectively address the over-smoothing issue, and it should also be efficiently obtained given the topology of the input graph. We provide further discussion and theoretical analysis to demonstrate the effectiveness of our positive/negative pair sampling strategy for Eq. 6 in Appendix B.2.</p><p>With Eq. 6, we can analyze the proposed TGCL loss bound in terms of mutual information, as stated in Proposition 1. In particular, Proposition 1 demonstrates that the TGCL loss for the graph is the lower bound of the mutual information between two representations of a neighbor node pair. Therefore, minimizing TGCL is equivalent to maximizing the mutual information of connected nodes by taking graph topology information into consideration.</p><p>Proposition 1 Given a neighbor node pair sampled from the graph G = (V, E, X), i.e., nodes v i and v j , we have</p><formula xml:id="formula_6">I(z i , z j ) ? ?L TGCL + E vi?V log(|N i |), where I(z i , z j )</formula><p>is the mutual information between two representations of the node pair v i and v j , and L TGCL is the topology-guided contrastive loss weighted by hamming distance measurement.</p><p>The proof of this proposition can be found in Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENT</head><p>In this section, we demonstrate the performance of our proposed Deeper-GXX in terms of effectiveness by comparing it with state-of-the-art deeper GNN methods and self-ablations. In addition, we conduct a case study to show how the increasing number of layers influences the performance of deeper GNNs when the input graph has missing features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">EXPERIMENT SETUP</head><p>Data sets: The Cora <ref type="bibr" target="#b16">(Lu &amp; Getoor, 2003)</ref> data set is a citation network consisting of 5,429 edges and 2,708 scientific publications from 7 classes. The edge in the graph represents the citation of one paper by another. CiteSeer <ref type="bibr" target="#b16">(Lu &amp; Getoor, 2003)</ref> data set consists of 3,327 scientific publications which could be categorized into 6 classes, and this citation network has 9,228 edges. PubMed <ref type="bibr" target="#b17">(Namata et al., 2012</ref>) is a citation network consisting of 88,651 edges and 19,717 scientific publications from 3 classes. The Reddit <ref type="bibr" target="#b6">(Hamilton et al., 2017b)</ref> data set is extracted from Reddit posts, which consists of 4,584 nodes and 19,460 edges. OGB-Arxiv ) is a citation network, which consists of 1,166,243 edges and 169,343 nodes from 40 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines:</head><p>We compare the performance of our method with the following baselines including one vanilla GNN model and four state-of-the-art deeper GNN models: (1) <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref>: the vanilla graph convolutional network; (2) GCNII <ref type="bibr" target="#b1">(Chen et al., 2020b)</ref>: an extension of GCN with skip connections and additional identity matrices; (3) DGN <ref type="bibr" target="#b33">(Zhou et al., 2020)</ref>: the differentiable group normalization for GNNs to normalize nodes within the same group and separate nodes among different groups; (4) PairNorm <ref type="bibr" target="#b30">(Zhao &amp; Akoglu, 2020)</ref>: a GNN normalization layer designed to prevent node representations from becoming too similar; (5) DropEdge <ref type="bibr" target="#b21">(Rong et al., 2020)</ref>: a GNNagnostic framework that randomly removes a certain number of edges from the input graph at each training epoch; <ref type="formula">(6)</ref> Deeper-GXX-S: using the simplified WDG-ResNet in Deeper-GXX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Configurations:</head><p>In the experiments, we follow the splitting strategy used in <ref type="bibr" target="#b30">(Zhao &amp; Akoglu, 2020)</ref> by randomly sampling 3% of the nodes as the training samples, 10% of the nodes as the validation samples, and the remaining 87% as the test samples. We set the learning rate to be 0.001 and the optimizer is RMSProp, which is one variant of ADAGRAD <ref type="bibr" target="#b4">(Duchi et al., 2011)</ref>. For fair comparison, we set the feature dimension of the hidden layer to be 50, the dropout rate to be 0.5, the weight decay rate to be 0.0005, and the total number of iterations to be 1500 for all methods. For Deeper-GXX and Deeper-GXX-S, we sample 10 instances and 5 neighbors for each class from the training set, dist(?) is the hamming distance, and f (?) is the cosine similarity measurement. The experiments are repeated 10 times if not otherwise specified. All of the real-world data sets are publicly available. The experiments are performed on a Windows machine with a 16GB RTX 5000 GPU.</p><p>The detailed hyperparameters (e.g., ? and ?) setting for the experimental results in each table could be found in Appendix A.2. Hyperparameter study and efficiency analysis could also be found in Appendix A.4 and A.5, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EXPERIMENTAL ANALYSIS</head><p>In this subsection, we evaluate the effectiveness of the proposed method on four benchmark data sets by comparing it with state-of-the-art methods. The base model for all methods we used in these experiments is graph convolutional neural network (GCN) <ref type="bibr" target="#b11">(Kipf &amp; Welling, 2017)</ref>. For fair comparison, we set the numbers of hidden layers to be 60 for all methods and the dimension of the hidden layer to be 50. The experiments are repeated 5 times and we record the mean accuracy as well as the standard deviation in <ref type="table" target="#tab_0">Table 1</ref>. In the first five rows of <ref type="table" target="#tab_0">Table 1</ref>, we observe that when we set 60 hidden layers as the reference, DropEdge has almost identical performance as the vanilla GCN. PairNorm, GCNII, and DGN increase the performance by more than 30% in four data sets compared with GCN. The latent reason is that the over-smoothing problem is alleviated to some extent, since these three are deliberately proposed to deal with over-smoothing problem in deeper GNNs. Besides, our proposed method (i.e., Deeper-GXX) and its simpler version (i.e., Deeper-GXX-S) outperform all of these baselines over four data sets. Addition to addressing the over-smoothing problem, another part of our outperformance can be credited to our proposed residual connection. To verify this conjecture, we further incorporate ResNet into PairNorm, DropEdge, DGN, GCNII, and DropEdge. Then, in the sixth to the eighth rows of In addition to four small data sets, we also examine the the node classification performance of Deeper-GXX on a large-scale graph called OGB-Arxiv shown in <ref type="figure" target="#fig_1">Figure 3a</ref> and <ref type="figure" target="#fig_1">Figure 3b</ref>. In this experiment, we fix the feature dimension of the hidden layer as 100, the total iteration is set as 3000 and GCN is chosen as the base model. Due to the memory limitation, we set the number of layers as 10 for all baselines methods in <ref type="figure" target="#fig_1">Figure 3b</ref> for fair comparison. By observation, we find that (1) in <ref type="figure" target="#fig_1">Figure 3a</ref>, the performance of Deeper-GXX increases as we increase the number of layers, which verifies our conjuncture that increasing the number of layers indeed leads to better performance in large graphs due to more information aggregated from neighbors; (2) comparing with PairNorm, Deeper-GXX further boosts the performance by more than 5.6% on OGB-Arxiv data set in <ref type="figure" target="#fig_1">Figure 3b</ref>. In <ref type="figure" target="#fig_1">Figure 3c</ref>, we show the performance of our proposed method with different base models (e.g., GAT <ref type="bibr" target="#b24">(Velickovic et al., 2018)</ref> and SGC <ref type="bibr" target="#b27">(Wu et al., 2019)</ref>). In the experiment, we set the numbers of the hidden layers as 60 for all methods and the dimension of the hidden layer as 50. The total number of training iteration is 1500. By observation, we find that both GAT and SGC suffer from vanishing gradient and over-smoothing when the architecture becomes deeper, and our proposed method Deeper-GXX greatly alleviates them and boosts the performance by 40%-60% on average over four data sets. Specifically, compared with the vanilla SGC, our Deeper-GXX boosts its performance by 43% on the CiteSeer data set and more than 67% on the Reddit data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CASE STUDY: MISSING FEATURE SCENARIO</head><p>Why do we need to stack more layers of GNNs? To answer this question, let us first imagine a scenario where some values of attributes are missing in the input graph. In this scenario, the shallow GNNs may not work well because GNNs could not collect useful information from the neighbors due to the massive missing values. However, if we increase the number of layers, GNNs are able to gather more information from the k-hop neighbors and capture latent knowledge to compensate for missing features. To verify this, we conduct the following experiment: we randomly mask p% attributes in Cora and CiteSeer data sets (i.e., setting the masked attributes to be 0), gradually increase the number of layers, and record the accuracy for each setting. In this case study, the number of layers is selected from <ref type="bibr">{2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 40, 50, 60}</ref>, and the base model is GCN. For fair comparison, we add ResNet <ref type="bibr" target="#b8">(He et al., 2016)</ref> if it can boost the baseline model's performance by avoiding the vanishing gradient issue. We repeat the experiments five times and record the mean accuracy and standard deviation. <ref type="table" target="#tab_2">Table 2</ref> shows the performance of Deeper-GXX and various baselines with the optimal number of layers denoted as #L, i.e., when the model achieves the best performance. By observation, we find that when the missing rate is 25%, shallow GCN with ResNet has enough capability to achieve the best performance on both CiteSeer and Cora data sets. Compared with GCN, our proposed method further improves the performance by more than 3.83% on the CiterSeer data set and 4.08% on the Cora data set by stacking more layers. However, when we increase the missing rate to 50% and 75%, we observe that most methods tend to achieve the best performance by stacking more layers. Specifically, PairNorm achieves the best performance at 10 layers when 25% features are missing, while it has the best performance at 40 layers when 75% features are missing. A similar observation could also be found with GCNII on the Cora data set, DropEdge on CiteSeer data set as well as our proposed methods in both data sets. Overall, the experimental results verify that the more features a data set is missing, the more layers GNNs need to be stacked to achieve better performance. One possible explanation is that, when more features are missing, we need more neighbors aggregated for collecting their partial features to compensate for the missing such that stacking GNN layers works. In this subsection, we conduct the ablation study on Cora to show the effectiveness of WDG-ResNet and TGCL in <ref type="table" target="#tab_3">Table 3</ref>. In this experiment, we fix the feature dimension of the hidden layer as 50, the total iteration is set as 3000, the number of layers is set as 60, the sampling batch size for Deeper-GXX is 10, and GCN is chosen as the base model. In <ref type="table" target="#tab_3">Table 3</ref>, Deeper-GXX-T removes TGCL loss, Deeper-GXX-D removes the weight decaying factor in WDG-ResNet and Deeper-GXX-S achieves the simplified WDG-ResNet in Deeper-GXX, which removes the similarity measure in WDG-ResNet. In <ref type="table" target="#tab_3">Table 3</ref>, we have the following observations (1) comparing Deeper-GXX with Deeper-GXX-T, we find that Deeper-GXX boosts the performance by 1.84% after adding TGCL loss, which demonstrates the effectiveness of TGCL to address over-smoothing issue; (2) Deeper-GXX outperforms Deeper-GXX-D by 5.61%, which shows that Deeper-GXX could address the shading neighbors effect by adding the weight decaying factor; (3) comparing Deeper-GXX with Deeper-GXX-S, we verify that adding exponential cosine similarity measure e cos(H (1) ,H (l) ) could further improve the performance by extending the hypothesis space of neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">ABLATION STUDY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Contrastive Learning on Graphs. Recently, contrastive learning attracts researchers' great attention due to its outstanding performance by leveraging the rich unsupervised data. <ref type="bibr">(van den Oord et al., 2018)</ref> is one of the earliest works, which proposes a Contrastive Predictive Coding framework to extract useful information from high dimensional data with a theoretical guarantee. Based on this work, recent studies <ref type="bibr" target="#b22">(Song &amp; Ermon, 2020;</ref><ref type="bibr" target="#b3">Chuang et al., 2020;</ref><ref type="bibr" target="#b10">Khosla et al., 2020;</ref><ref type="bibr" target="#b2">Chen et al., 2020c;</ref><ref type="bibr" target="#b31">Zheng et al., 2022)</ref> reveal a surge of research interest in contrastive learning. <ref type="bibr" target="#b29">(You et al., 2020)</ref> proposes a graph contrastive learning (GraphCL) framework that utilizes different types of augmentation methods to incorporate various priors and to learn unsupervised representations of graph data. <ref type="bibr" target="#b20">(Qiu et al., 2020)</ref> proposes a Graph Contrastive pre-training model named GCC to capture the graph topological properties across multiple networks by utilizing contrastive learning to learn the intrinsic and transferable structural representations. <ref type="bibr" target="#b32">(Zheng et al., 2021)</ref> introduced a weakly supervised contrastive learning framework to tackle the class collision problem by first generating a weak label for similar samples and then pulling the similar samples closer with contrastive regularization. Authors <ref type="bibr" target="#b7">Hassani &amp; Ahmadi (2020)</ref> aims to learn node and graph level representations by contrasting structural views of graphs. In this paper, we leverage the contrastive learning manner and design positive and negative node pairs such that we can discriminate node representations based on the input graph inherent information, which paves the way for us to design effective deeper GNNs.</p><p>Deeper Graph Neural Networks. To effectively make GNNs deeper, many research works focus on addressing the over-smoothing problem. Over-smoothing problem of GNNs is formally described by <ref type="bibr" target="#b14">(Li et al., 2018)</ref> to demonstrate that the final output node representations become indiscriminative after stacking many layers in GNN models. This problem is also analyzed by <ref type="bibr" target="#b19">(Oono &amp; Suzuki, 2020)</ref> showing how over-smoothing hurts the node classification performance. To quantify the degree of over-smoothing, different measurements are proposed <ref type="bibr" target="#b0">(Chen et al., 2020a;</ref><ref type="bibr" target="#b30">Zhao &amp; Akoglu, 2020;</ref><ref type="bibr" target="#b33">Zhou et al., 2020)</ref>. For example, Mean Average Distance <ref type="bibr" target="#b0">(Chen et al., 2020a)</ref> is proposed by calculating the divergences between learned node representations. To make GNNs deeper and maintain performance, some nascent research works are proposed <ref type="bibr" target="#b12">(Klicpera et al., 2019;</ref><ref type="bibr" target="#b0">Chen et al., 2020a;</ref><ref type="bibr" target="#b30">Zhao &amp; Akoglu, 2020;</ref><ref type="bibr" target="#b21">Rong et al., 2020;</ref><ref type="bibr" target="#b1">Chen et al., 2020b;</ref><ref type="bibr" target="#b33">Zhou et al., 2020)</ref>. Among those, some of them share the same logic of keeping the divergence between node representations but differ in specific methodologies. For example, PairNorm <ref type="bibr" target="#b30">(Zhao &amp; Akoglu, 2020)</ref> introduces a normalization layer to keep the divergence of node representation from the original input node feature. In DGN <ref type="bibr" target="#b33">(Zhou et al., 2020)</ref>, node representation by deep GNNs is regularized by group-based mutual information. However, the hyperparameters selection is important and data-driven, which means many efforts need to be paid for the hyperparameter searching. Some methods focus on changing the information aggregation scheme to make GNNs deeper, such as APPNP <ref type="bibr" target="#b12">(Klicpera et al., 2019)</ref>, GCNII <ref type="bibr" target="#b1">(Chen et al., 2020b)</ref>, DropEdge <ref type="bibr" target="#b21">(Rong et al., 2020)</ref> and etc. To the best of our knowledge, in dealing with over-smoothing problem, our Deeper-GXX is the first attempt to transfer hard-to-acquire discriminative knowledge into the topology information of the input graph by comparing adjacency vectors of nodes. Also, we analyze another problem (i.e., vanishing gradient) that hinders GNNs to be effectively deeper and discover the shading neighbors effect caused by simply applying ResNet on GNNs, and propose a GNN-based residual connection to avoid this issue and improve the performance of deeper GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we focus on building deeper graph neural networks to effectively model graph data. To this end, we first provide insights regarding why ResNet is not best suited for many deeper GNN solutions, i.e., the shading neighbors effect. Then we propose a new residual architecture, Weight-Decaying Graph Residual Connection (WDG-ResNet) to address this effect. In addition, we propose a Topology-guided Graph Contrastive Loss (TGCL) to address the problem of over-smoothing, where we utilize graph topological information, pull the representations of connected node pairs closer, and push remote node pairs farther apart via contrastive learning regularization. Combining WDG-ResNet with TGCL, an end-to-end model named Deeper-GXX is proposed towards deeper GNNs. We provide the theoretical analysis of our proposed method, and demonstrate the effectiveness of Deeper-GXX by extensive experiment comparing with state-of-the-art de-oversmoothing algorithms. A case study regarding the missing feature scenario demonstrates the necessity to deepen the GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXPERIMENT A.1 CASE STUDY: A TOY EXAMPLE</head><p>Why do we need to stack more layers of GNNs? To answer this question, we conduct the experiment in a toy example. We first use the existing package (i.e., draw circle function in the Scikit-learn package) to generate a synthetic data set by setting the number of data points to be 1,000 and the noise level to be 0.01. Then, we measure the euclidean distance between each pair of data points, and if the the distance is less than a threshold, then this two data points are connected in a graph. In this way, the adjacency matrix is derived after adding the self loop. Next, we sample 1% data points as the training set, 9% data points as the validation set, and 90% data points as the test set. These data points are visualized in <ref type="figure">Figure 1a</ref> and the experimental results are shown in <ref type="figure">Figure 1b</ref>. In <ref type="figure">Figure 1a</ref>, we observe that the query node (the diamond in the red dashed circle) cannot rely on its closest labeled neighbor (the red star in the circle) to correctly predict its label (red or blue). Only by exploring longer paths consisting of more similar neighbors are we able to predict its label as blue. <ref type="figure">Figure 1b</ref> compares the classification accuracy of shallow GNNs and deeper GNNs. We can see that deeper GNNs significantly outperform shallow ones by more than 11%, due to their abilities to explore longer paths on the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 EXPERIMENTAL SETTING</head><p>In this subsection, we provide the detailed experimental setting for each experiment shown in <ref type="table">Table 4</ref>. <ref type="table">Table 4</ref>: Hyperparameters for Deeper-GXX and Deeper-GXX-S shown in <ref type="table" target="#tab_0">Table 1</ref> Method</p><p>Deeper In this subsection, we visualize the weight of each layer with different weighting functions on the Cora data set. In this experiment, we fix the feature dimension of the hidden layer to be 50; the total iteration is set to be 3000; the number of layers is set to be 60; the sampling batch size for Deeper-GXX is 10; GCN is chosen as the base model; ? is set to be 20. In <ref type="figure" target="#fig_2">Figure 4</ref>, The x-axis is the index of each layer and the y-axis is the weight for each layer. Deeper-GXX-D removes the decaying weight factor and only keeps the exponential cosine similarity e cos(H (1) ,H (l) ) to measure the weight for each layer. Deeper-GXX-S achieves the simplified WDG-ResNet in Deeper-GXX, which removes the exponential cosine similarity e cos(H (1) ,H (l) ) in Deeper-GXX. By observation, we find that (1) ResNet sets the weight of each layer to be 1, which easily leads to shading neighbors effect when stacking more layers, because the faraway neighbor information becomes more dominant in the GCN information aggregation; (2) without weight decaying factor, the weight for each layer in Deeper-GXX-D fluctuates because they are randomly independent. More specially, the weights for the last several layers (e.g., L=58 or L=60) are larger than the weights for the first several layers, which contradicts the intuition that the first several layers should be important than the last several layers; (3) the weights for each layer in both Deeper-GXX and Deeper-GXX-S reduce as the number of layers increase, which suggests that both of them could address the shading neighbors effect to some extents; (4) combining the results from <ref type="table" target="#tab_3">Table 3</ref>, Deeper-GXX achieves better performance than Deeper-GXX-S, as it imposes larger weights to the first several layers, which verifies that the learnable similarity sim(H (1) ,H (l) ) achieves better performance with the enlarged hypothesis space for neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 HYPERARAMETER ANALYSIS</head><p>In this subsection, we conduct the hyperparameter analysis of Deeper-GXX, regarding ? in the weight decaying function of Eq. 4 and ? in the overall loss function of Eq 1. To analyze the hyperparameter ?, we fix the feature dimension of the hidden layer to be 50, the total iteration is set to be 3000, the number of layers is set to be 60, the sampling batch size for Deeper-GXX is 10, and GCN is chosen as the base model. The experiment is repeated five times for each configuration. In each sub-figure of <ref type="figure" target="#fig_3">Figure 5</ref>, the x-axis is the value of ?, and the y-axis is the accuracy of 60-layer GCN in the above setting. First, we can observe that it's not true that Deeper-GXX achieves the best performance with a larger ?. Specifically, we find that the optimal ? = 20 on Cora data set, the optimal ? = 10 on CiteSeer data set, the optimal ? = 18 on PubMed data set, and the optimal ? = 20 on Reddit data set. Then, natural questions to ask are (1) what determines the optimal value of ? in different data sets? (2) can we gather some heuristics to narrow down the hyperparameter search space to efficiently establish effective GNNs? Here, we provide our discovery. In the main text of the paper, we have analyzed that the decaying factor ? in Eq. 4 controls the number of effective layers in deeper GNNs by introducing the layer-wise dependency. It means that larger ? slows down the weight decay and gives considerable large weights to more layers such that they can be effective, and the information aggregation scope of GNN extends as more multi-hop neighbors features are collected and aggregated. In graph theory, diameter represents the scope of the graph, which is the largest value of the shortest path between any node pairs in the graph. Therefore, the optimal ? should be restricted by the input graph, i.e., being close to the input graph diameter. Interestingly, our experiments reflect this observation. Combining the optimal ? in <ref type="figure" target="#fig_3">Figure 5</ref> and the diameter in <ref type="table" target="#tab_4">Table 5</ref>, for connected graphs PubMed and Reddit, the optimal ? is very close to the graph diameter. This also happens to Cora (even though Cora is not connected), because the number of components is not large. As for CiteSeer, the optimal ? is less than the diameter of its largest component. A possible reason is that CiteSeer has many (i.e., 438) small components, which shrinks the information propagation scope, such that we do not need to stack many layers and we do not need to enlarge ? to the largest diameter (i.e., 28). In general, based on the above analysis, we find the optimal value of ? can be searched around the diameter of the input graph. To analyze the hyperparameter ? in Deeper-GXX, we fix the feature dimension of the hidden layer to be 50, the total iteration is set to be 3000, the number of layers is set to be 60, the sampling batch size for Deeper-GXX is 10, GCN is chosen as the base model, and the data set is Cora.</p><p>We gradually increase the value of ? and record the accuracy. The experiment is repeated five times in each setting. In <ref type="figure">Figure 6</ref>, the x-axis is ? and the y-axis is the accuracy score. By observation, when ? = 1, the performance is worst and the performance begins to increase by decreasing the value of ?. It achieves the best accuracy when ? = 0.03. The performance starts to decrease again if we further decrease the value of ?. Our conjecture is that when ? is large, it will dominate the overall objective function, thus jeopardizing the classification performance. Besides, if we set the value of ? to be a small number (i.e., ? = 0.001), the performance also decreases. In addition, comparing with the performance without using TGCL regularization (i.e., ? = 0), our proposed method with ? = 0.03 can boost the performance by more than 1.8%, which demonstrates that our proposed TGCL alleviates the issue of oversmoothing to some extent. In this subsection, we conduct an efficiency analysis regarding our proposed method in the Cora data set. We fix the feature dimension of the hidden layer to be 50, the total iteration is set to be 1500, the sampling batch size for Deeper-GXX and Deeper-GXX-S is 10, and GCN is chosen as the base model. We gradually increase the number of layers and record the running time. In <ref type="figure" target="#fig_4">Figure 7</ref>, the x-axis is the number of layers and the y-axis is the running time in second. We observe that the running time of both Deeper-GXX and Deeper-GXX-S is linearly proportional to the number of layers. Comparing the running time of Deeper-GXX, the running time of Deeper-GXX-S is further reduced after the weighting function in Deeper-GXX (e.g., sim(?)) is replaced by a constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 EFFICIENCY ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ANALYSIS B.1 PROOF OF PROPOSITION 1</head><p>Proof 1 Following the theoretical analysis in (van den Oord et al., 2018) Section 2.3, the optimal value of f (z i , z j ) is given by P (zj |zi) P (zj ) . Thus, the weighted supervised contrastive loss could be rewritten as follows:</p><formula xml:id="formula_7">L TGCL = ? E vi?V E vj ?Ni [log ? ij f (z i , z j ) ? ij f (z i , z j ) + v k ?Ni ? ik f (z i , z k ) ] = E vi?V E vj ?Ni [log ? ij P (zj |zi) P (zj ) + v k ?Ni ? ik P (z k |zi) P (z k ) ? ij P (zj |zi) P (zj ) ] = E vi?V E vj ?Ni [log(1 + P (z j ) ? ij P (z j |z i ) v k ?Ni ? ik P (z k |z i ) P (z k ) )]</formula><p>Since (v i , v k ) is defined as a remote (i.e., negative) node pair, it means that node v i and node v k are not connected in the graph, i.e., A i,k = A k,i = 0. Therefore, we have ? ik ? (1, 2] for all remote nodes v k and ? ij ? (0, 1] for all neighbor nodes v j with hamming distance measurement, which leads to 1 ?ij ? P (zj ) P (zj |zi) ? P (zj ) P (zj |zi) and ? ik P (z k |zi) P (z k ) ? P (z k |zi) P (z k ) . Thus, we have</p><formula xml:id="formula_8">L TGCL ? E vi?V E vj ?Ni [log( P (z j ) P (z j |z i ) v k ?Ni P (z k |z i ) P (z k ) )] ? E vi?V E vj ?Ni [log( P (z j ) P (z j |z i ) (|N i | E v k P (z k |z i ) P (z k ) ))] = E vi?V E vj ?Ni [log( P (z j ) P (z j |z i ) |N i |)]</formula><p>? E vi?V E vj ?Ni [log( P (z j ) P (z j |z i ) ) + log(|N i |)] = ?I(z i , z j ) + E vi?V log(|N i |)</p><p>Finally, we have I(z i , z j ) ? ?L TGCL + E vi?V log(|N i |), which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 SAMPLING METHOD FOR TGCL</head><p>To realize TGCL loss function expressed in Eq. 6, we need to get the positive nodes v j and negative nodes v k towards the selected central node v i . To avoid iterating over all existing nodes or randomly sampling several nodes, we propose to sample positive nodes v j and negative nodes v k from the star subgraph S i of the central node v i . Moreover, to make the sampling be scalable and to reduce the search space of negative nodes, we propose a batch sampling method. <ref type="figure">Figure 8</ref>: Batch Sampling. Each star node in the figure corresponds to node v i in Eq. 6.</p><p>As shown in <ref type="figure">Figure 8</ref>, the batch size is controlled by the number of central nodes (i.e., star nodes in the <ref type="figure">figure)</ref>. For each central node, the positive nodes are those 1-hop neighbors, and the negative nodes consist of unreachable nodes. In our batch sampling, we strictly constrain that the positive nodes are only from the 1-hop neighborhood for the following three reasons: (1) they are efficient to be accessed;</p><p>(2) considering all k-hop neighbors as positive will enlarge the scope of positive nodes and further decrease the intimacy of the directly connected nodes; (3) 1-hop positive nodes in the star subgraph can preserve enough useful information, compared with the positive nodes from the whole graph. For the third point, we prove it through the graph influence loss <ref type="bibr" target="#b9">(Huang &amp; Zitnik, 2020)</ref> in Proposition 2, and the formal definition of graph influence loss is given in the following paragraph after Proposition 2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An arbitrary GNN with the proposed Deeper-GXX.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(a) Accuracy of Deeper-GXX on OBG-Arxiv data set with different number of layers. (b) Performance (i.e., accuracy) comparison on OGB-Arxiv data set. (c) Accuracy of different base models with 60 hidden layers on four data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>OF WEIGHT OF EACH LAYER WITH DIFFERENT WEIGHTING FUNCTIONS Weight visualization. The y-axis represents the weight of each layer, and x-axis represents the index of each layer, in deeper models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Hyperparameter analysis, i.e., ? vs accuracy score on four data sets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>The number of layers vs running time (in seconds) on Cora data set. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy of node classification on four benchmark data sets with 60 hidden layers. GCN is used as the backbone for all methods. ? 0.0084 0.2125 ? 0.0140 0.4172 ? 0.0098 0.1140 ? 0.0136 PairNorm 0.6759 ? 0.0171 0.4817 ? 0.0197 0.7883 ? 0.0101 0.9017 ? 0.0241 DropEdge 0.2911 ? 0.0122 0.2147 ? 0.0184 0.4162 ? 0.0208 0.1019 ? 0.0324 GCNII 0.6076 ? 0.0050 0.5775 ? 0.0027 0.8188 ? 0.0030 0.6969 ? 0.0064 DGN 0.7022 ? 0.0079 0.4398 ? 0.0118 0.7843 ? 0.0032 0.5122 ? 0.0143 PairNorm + ResNet 0.7394 ? 0.0271 0.5544 ? 0.0166 0.7985 ? 0.0068 0.9385 ? 0.0102 DropEdge + ResNet 0.1696 ? 0.0271 0.1951 ? 0.0382 0.5798 ? 0.1501 0.0950 ? 0.0129</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell><cell>Reddit</cell></row><row><cell>GCN</cell><cell>0.2962</cell><cell></cell><cell></cell><cell></cell></row></table><note>GCNII + ResNet 0.7024 ? 0.0075 0.6051 ? 0.0062 0.8093 ? 0.0047 0.7538 ? 0.0095 DGN + ResNet 0.1543 ? 0.0004 0.2104 ? 0.0000 0.2086 ? 0.0000 0.1118 ? 0.0000 Deeper-GXX-S 0.8023 ? 0.0117 0.6544 ? 0.0099 0.8198 ? 0.0012 0.9693 ? 0.0036 Deeper-GXX 0.8059 ? 0.0028 0.6655 ? 0.0117 0.8185 ? 0.0016 0.9721 ? 0.0011</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>, we can observe (1) some baselines like PairNorm and GCNII suffer from the vanishing gradient (e.g., the performance of PairNorm increases by 6% on Cora and 7% on CiteSeer, while the performance of GCNII rises to 75.38% on Reddit and 70.24% on Cora). Also, although GCNII designs a ResNet-like architecture, adding ResNet to GCNII can still boost its performance on three data sets; (2) Compared with their "+ResNet" versions, our Deeper-GXX and Deeper-GXX-S still outperform, which implies that our designed residual connection indeed contributes to the outperformance, and we do the ablation study to quantify each component's contribution of Deeper-GXX in Section 3.4; (3) Not all deeper baselines need ResNet. For example, DropEdge+ResNet almost maintains the performance, and DGN+ResNet drops the performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy of node classification on two data sets by masking p percent of node attributes. #L denotes the number of layers where a model achieves the best performance.</figDesc><table><row><cell cols="2">Node Feature Missing Rate</cell><cell>p = 25%</cell><cell></cell><cell>p = 50%</cell><cell></cell><cell>p = 75%</cell><cell></cell></row><row><cell>data set</cell><cell>Method</cell><cell>Acc</cell><cell>#L</cell><cell>Acc</cell><cell>#L</cell><cell>Acc</cell><cell>#L</cell></row><row><cell></cell><cell>GCN + ResNet</cell><cell>0.7503 ? 0.0101</cell><cell>7</cell><cell cols="4">0.7435 ? 0.0048 10 0.7226 ? 0.0099 10</cell></row><row><cell></cell><cell cols="7">PairNorm + ResNet 0.7529 ? 0.0129 10 0.7482 ? 0.0172 20 0.7262 ? 0.0178 40</cell></row><row><cell>Cora</cell><cell cols="6">DropEdge + ResNet 0.7634 ? 0.0112 15 0.7611 ? 0.0102 20 0.7297 ? 0.0168</cell><cell>8</cell></row><row><cell></cell><cell>GCNII + ResNet</cell><cell cols="6">0.2667 ? 0.0063 25 0.3351 ? 0.0066 25 0.2914 ? 0.0106 40</cell></row><row><cell></cell><cell>DGN w/o ResNet</cell><cell cols="6">0.6850 ? 0.0184 30 0.6846 ? 0.0147 50 0.6717 ? 0.0156 25</cell></row><row><cell></cell><cell>Deeper-GXX-S</cell><cell cols="6">0.7872 ? 0.0128 15 0.7811 ? 0.0147 20 0.7586 ? 0.0121 60</cell></row><row><cell></cell><cell>Deeper-GXX</cell><cell cols="6">0.7915 ? 0.0060 10 0.7848 ? 0.0043 20 0.7598 ? 0.0081 60</cell></row><row><cell></cell><cell>GCN + ResNet</cell><cell>0.6141 ? 0.0080</cell><cell>4</cell><cell cols="3">0.5811 ? 0.0093 10 0.5149 ? 0.0173</cell><cell>9</cell></row><row><cell></cell><cell cols="2">PairNorm + ResNet 0.6184 ? 0.0087</cell><cell>8</cell><cell cols="4">0.5947 ? 0.0083 20 0.5176 ? 0.0075 10</cell></row><row><cell>CiteSeer</cell><cell cols="2">DropEdge + ResNet 0.6348 ? 0.0156</cell><cell>4</cell><cell>0.6083 ? 0.0128</cell><cell>6</cell><cell cols="2">0.5240 ? 0.0128 10</cell></row><row><cell></cell><cell>GCNII + ResNet</cell><cell cols="6">0.2453 ? 0.0045 40 0.2338 ? 0.0028 20 0.2403 ? 0.0046 25</cell></row><row><cell></cell><cell>DGN w/o ResNet</cell><cell cols="6">0.4560 ? 0.0162 20 0.4593 ? 0.0117 15 0.4498 ? 0.0292 15</cell></row><row><cell></cell><cell>Deeper-GXX-S</cell><cell cols="6">0.6508 ? 0.0060 10 0.6132 ? 0.0042 15 0.5544 ? 0.0138 20</cell></row><row><cell></cell><cell>Deeper-GXX</cell><cell cols="6">0.6524 ? 0.0087 20 0.6169 ? 0.0063 60 0.5576 ? 0.0070 50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation Study on Cora Data Set</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>Deeper-GXX</cell><cell>0.8059 ? 0.0028</cell></row><row><cell cols="2">Deeper-GXX-S 0.8023 ? 0.0117</cell></row><row><cell cols="2">Deeper-GXX-D 0.7498 ? 0.0139</cell></row><row><cell cols="2">Deeper-GXX-T 0.7875 ? 0.0092</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Graph Statistics of each Data Set</figDesc><table><row><cell>Cora</cell><cell>Citeseer PubMed Reddit</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proposition 2 (Bounded Graph Influence Loss for Sampling Positive Pairs Locally) Taking GCN as an example of GNN, the graph influence loss R(v c ) on node v c w.r.t positive nodes from the whole graph against positive nodes from the 1-hop neighborhood star subgraph is bounded by R(v c ) ? (n ? d c ) ? (DP * GM ) |P * | , where n is the number of nodes, d c is the degree of node v c including the self-loop, ? is a constant,P * is the path from center node v c to a 1-hop outside node v s which has the maximal node influence I vc,vs , and |P * | denotes the number of nodes in pathP * .</p><p>Proof 2 According to the assumption of <ref type="bibr" target="#b25">(Wang &amp; Leskovec, 2020)</ref>, ?(?) can be identity function and W (?) can be identity matrix. Then, the hidden node representation (of node v c ) in the last layer of GCN can be written as follows.</p><p>Then, based on the above equation, we can iteratively replace h</p><p>The extension procedure is written as follows.</p><p>The above equation suggests that the influence from the positive node v s to the center node v c is</p><p>Following the above path formation and assume the edge weight A(i, j) as the positive constant, according to <ref type="bibr" target="#b9">(Huang &amp; Zitnik, 2020)</ref>, we can obtain the node influence I vc,vs of v s on v c as follows.</p><p>where ? is a constant, DP GM is the geometric mean of degree of nodes sitting in pathP, andP is the path from the positive node v s to the center node v c that could generate the maximal multiplication of normalized edge weight, |P| denotes the number of nodes in pathP.</p><p>The above analysis suggests that the node influence of long-distance positive nodes is decaying.</p><p>Hence, the graph influence loss about learning node v c from the whole graph positive nodes versus from the 1-hop localized positive nodes can be expressed expressed as follows.</p><p>Specifically, the graph influence loss <ref type="bibr" target="#b9">(Huang &amp; Zitnik, 2020)</ref> </p><p>, which is determined by the global graph influence on v c (i.e., I G (v c )) and the star subgraph influence on v c (i.e., I L (v c )). Then, to compute the graph influence I G (v c ), we need to compute node influence of each node v j to node v c , where node v j is reachable from node v c . Based on the final output node representation vectors, the node influence is expressed as I vc,vj = ?h</p><p>, and the norm can be any subordinate norm <ref type="bibr" target="#b25">(Wang &amp; Leskovec, 2020)</ref>. Then, I G (v c ) is computed by the L1-norm of the following vector, i.e., I G (v c ) = [I vc,v1 , I vc,v2 , . . . , I vc,vn ] 1 . Similarly, we can compute the star subgraph influence I L (v c ) on node v c . The only difference is that we collect each reachable node v j in the star subgraph L (i.e., 1-hop neighbours of v c ). Overall, in Proposition 2, we show why positive pairs can be locally sampled with the support from graph influence loss of a node representation vector output by the GCN final layer.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Debiased contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Loyalty in online communities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justine</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Web and Social Media</title>
		<meeting>the Eleventh International Conference on Web and Social Media<address><addrLine>Montr?al, Qu?bec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-05-15" />
			<biblScope unit="page" from="540" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir Hosein Khas</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph meta learning via local subgraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Supervised contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003)</title>
		<editor>Tom Fawcett and Nina Mishra</editor>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Query-driven active surveying for collective classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Edu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Mining and Learning with Graphs</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hamming distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">GCC: graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-label contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A?ron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Contrastive multiview coding</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unifying graph convolutional neural networks and label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft academic graph: When experts are not enough</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? In ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pairnorm: Tackling oversmoothing in gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Contrastive learning with complex heterogeneity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Weakly supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Towards deeper graph neural networks with differentiable group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
