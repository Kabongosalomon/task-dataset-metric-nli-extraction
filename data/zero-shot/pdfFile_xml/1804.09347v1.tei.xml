<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptation and Re-Identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Graduate Institute of Communication Engineering</orgName>
								<orgName type="institution" key="instit2">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-En</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Graduate Institute of Communication Engineering</orgName>
								<orgName type="institution" key="instit2">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ying</forename><surname>Yeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Graduate Institute of Communication Engineering</orgName>
								<orgName type="institution" key="instit2">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Du</surname></persName>
							<email>xiaofei.du@umbocv.com</email>
							<affiliation key="aff2">
								<orgName type="department">Umbo Computer Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang</forename><forename type="middle">Frank</forename><surname>Wang</surname></persName>
							<email>ycwang@ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Graduate Institute of Communication Engineering</orgName>
								<orgName type="institution" key="instit2">National Taiwan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptation and Re-Identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification (Re-ID) aims at recognizing the same person from images taken across different cameras. To address this task, one typically requires a large amount labeled data for training an effective Re-ID model, which might not be practical for real-world applications. To alleviate this limitation, we choose to exploit a sufficient amount of pre-existing labeled data from a different (auxiliary) dataset. By jointly considering such an auxiliary dataset and the dataset of interest (but without label information), our proposed adaptation and re-identification network (ARN) performs unsupervised domain adaptation, which leverages information across datasets and derives domain-invariant features for Re-ID purposes. In our experiments, we verify that our network performs favorably against state-of-the-art unsupervised Re-ID approaches, and even outperforms a number of baseline Re-ID methods which require fully supervised data for training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (Re-ID) <ref type="bibr" target="#b20">[21]</ref> has become popular research topic due to its application to smart city and large-scale surveillance system. Given a person-ofinterest (query) image, Re-ID aims at associating the same pedestrian from multiple cameras, matching people across non-overlapping camera views. Yet, current Re-ID models are still struggling to handle the problems with intensive changes in appearance and environment. With recent advances in deep neural networks, several works have been proposed to tackle the above challenges in supervised <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b13">14]</ref> and unsupervised manners <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>However, the aforementioned methods are not able to achieve satisfactory performances if the appearance or camera settings of query images are very different from the Figure 1: Illustration of cross-dataset person reidentification (Re-ID). While Re-ID of images in the target-domain dataset is of interest, no labeled data is available for training. Our idea is to leverage information from auxiliary labeled images in a distinct and irrelevant source domain (i.e., dataset not of interest). With such an unsupervised domain adaptation setting for learning domain-invariant features, Re-ID in the target domain can be performed accordingly.</p><p>training ones. This is known as the problem of domain shift (or domain/dataset bias) and requires domain adaptation <ref type="bibr" target="#b11">[12]</ref> techniques to address this challenging yet practical problem. Thus, several works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b2">3]</ref> have been proposed to generalize the discriminative ability across different datasets by increasing the cross-domain training samples with style transfer methods. Zhong et al. <ref type="bibr" target="#b22">[23]</ref> smooth style disparities across the cameras with style transfer model and label smooth regularization. Similarly, Deng et al. <ref type="bibr" target="#b2">[3]</ref> further add similarity constraints to enhance the performance on cross-domain Re-ID task. However, the adaptation models based on style transfer are not necessary to preserve the identity during the image translation procedure, and this results in unsatisfactory performance when no corresponding identities appear in both domains/datasets.</p><p>To address the domain shifts between datasets, we propose a deep architecture to perform cross-domain Person Re-identification with the only supervision from a single dataset/domain as shown in <ref type="figure" target="#fig_2">Figure 1</ref>. Toward this end, with the labeled data, our model derives the discriminative property to distinguish the images between different classes. To perform such property on alternative domain without annotation, our model learns to adapt the discriminative property from supervised (i.e., source) to unsupervised (i.e., target) domain. This is achieved by decomposing the cross-domain feature into domain-invariant and domainspecific one. Once the domain-invariant feature is learned, our model can perform cross-domain Re-ID by matching the query image and gallery images in the shared latent space. To further enhance the discriminative property of our proposed model, we aim at increasing the margin between the classes with our proposed contrastive objective, which is later verified in the experiment.</p><p>The contributions of our paper can be summarized as follows:</p><p>? We address unsupervised person Re-ID by exploiting and adapting information learned from an auxiliary labeled dataset, which can be viewed as a unsupervised domain adaptation approach.</p><p>? Our proposed Adaptation and Re-ID Network (ARN) aims at learning domain-invariant features for matching images of the same person, while no label information is required for the data domain of interest.</p><p>? Our ARN not only performs favorably agianst state-ofthe-art Re-ID approaches in the unsupervised setting, it also outperforms baseline supervised Re-ID methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Person Re-Identification (Re-ID)</head><p>Supervised Learning for Re-ID: Most existing Re-ID models are learned in a supervised setting. That is, given a sufficient number of labeled images across cameras, techniques based on metric learning <ref type="bibr" target="#b1">[2]</ref> or representation learning <ref type="bibr" target="#b8">[9]</ref> can be applied to train the associated models. Cheng et al. <ref type="bibr" target="#b1">[2]</ref> propose a multi-channel part-based convolutional network for Re-ID, which is formulated via an improved triplet framework. Lin et al. <ref type="bibr" target="#b8">[9]</ref> present an attribute-person recognition network which performs discriminative embedding for Re-ID and is able to make a prediction for particular attributes. While promising performances have been reported on recent datasets (e.g., Market-1501 <ref type="bibr" target="#b19">[20]</ref>, DukeMTMC-ReID <ref type="bibr" target="#b21">[22]</ref>), it might not be practical since collecting a large amount of annotated training data is typically computationally prohibitive.</p><p>Unsupervised Learning for Re-ID: To alleviate the above limitation, researchers also focus on person Re-ID using unlabeled training data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>. For example, Fan et al. <ref type="bibr" target="#b3">[4]</ref> apply techniques of data clustering, instance selection, and fine-tuning methods to obtain pseudo labels for the unlabeled data; this allows the training of the associated feature extractor with discriminative ability. Wang et al. <ref type="bibr" target="#b16">[17]</ref> propose a kernel-based model to learn cross-view identity discriminative information from unlabeled data. Nevertheless, due to the lack of label information for images across cameras, unsupervised learning based methods typically cannot achieve comparable results as the supervised approaches do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cross-Domain Re-ID</head><p>Recently, some transfer learning algorithms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref> are proposed to leverage the Re-ID models pre-trained in source datasets to improve the performance on target dataset. Geng et al. <ref type="bibr" target="#b4">[5]</ref> transfer representations learned from large image classification datasets to Re-ID datasets using a deep neural network which combines classification loss with verification loss. Peng et al. <ref type="bibr" target="#b12">[13]</ref> propose a multi-task dictionary learning model to transfer a view-invariant representation from a labeled source dataset to an unlabeled target dataset.</p><p>Besides, domain adaption and image-to-image translation approaches have been applied to Re-ID tasks increasingly, Deng et al. <ref type="bibr" target="#b2">[3]</ref> combine CycleGAN <ref type="bibr" target="#b23">[24]</ref> with similarity constraint for domain adaptation which improve performance in cross-dataset setting. Zhong et al. <ref type="bibr" target="#b22">[23]</ref> introduce camera style transfer approach to address image style variation across multiple views and learn a camera-invariant descriptor subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Domain-Invariant Feature Learning</head><p>We deal with the cross-domain Re-ID by learning domain-invariant feature. Here we review the recent works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10]</ref> on learning domain-invariant feature. In order to achieve cross-domain classification tasks, Tzeng et al. <ref type="bibr" target="#b15">[16]</ref> present domain confusion loss to learn domain-invariant representation. Bousmalis et al. <ref type="bibr" target="#b0">[1]</ref> propose to extract the domain-invariant feature to improve the performance of cross-domain classification task. On the other hand, to tackle the problem of image style translation, Coupled GAN <ref type="bibr" target="#b10">[11]</ref> also learn to synthesize cross-domain images from a domain-invariant feature. UNIT <ref type="bibr" target="#b9">[10]</ref> further learn a domain-invariant feature to translate the image across domains. It is worth noting that, inspired by the above methods, we address the cross-domain Re-ID task by learning the domain-invariant feature for describing the human identity across distinct domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Given a set of image-label pairs {I s i , y s i } Ns i=1 and another set of images {I t i } Nt i=1 , where N s and N t denote the total images of source and target dataset respectively, the goal <ref type="figure" target="#fig_1">Figure 2</ref>: The architecture of our Adaptation and Re-Identification Network (ARN). Note that the Encoder contains the two shared modules (E I , E C ), and two private modules (E T , E S ). E I aims to retrieve visual feature maps (X t , X s ), which are fed into E C , E S , and E T for learning domain-invariant (shared) and specific (private) features. With the private (e t p , e s p ) and shared (e t c , e s c ) latent features observed, the Decoder D C performs feature reconstruction for both target and source-domain images. Finally, the classifier C S is designed to perform supervised learning from source-domain data.</p><p>of our model is to perform cross-dataset Re-ID by adapting the discriminative ability learned from source dataset to unlabeled target dataset.</p><p>We present our Person Re-ID model trained in a supervised manner in section 3.1. To address the cross-dataset Person Re-ID, our model leverages information from supervised data and adapts it to unsupervised dataset in section 3.2. Later in section 3.3, we demonstrate the learning and evaluation of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Supervised Learning for Person Re-ID</head><p>To perform person re-identification, our model aims to learn the image feature with discriminative property to distinguish between classes. With labeled data, such feature property can be learned from image classification task. To achieve this, we introduce encoder {E I , E C } and classifier C S to extract the image feature e s c from source dataset image I s and obtain its category prediction? s respectively. Specifically, to reduce the training burden, pretrained model (e.g., ResNet) can be used for feature extractor module E I . Thus, we define the classification loss L class to minimize the negative log-likelihood of the ground truth label y s for source dataset image I s :</p><formula xml:id="formula_0">L class = ? Ns i=1 y s i ? log? s i<label>(1)</label></formula><p>To further enhance the discriminative property of our learned feature, we consider contrastive loss L ctrs <ref type="bibr" target="#b5">[6]</ref> as an additional objective of our model:</p><formula xml:id="formula_1">Lctrs = i,j ?(e s c,i ? e s c,j ) 2 + (1 ? ?)[max(0, m ? (e s c,i ? e s c,j )] 2<label>(2)</label></formula><p>where ? = 1 if {e s c,i , e s c,j } belong to same category, and ? = 0 if {e s c,i , e s c,j } belong to different categories. Note that m &gt; 0 is a margin, which is regarded a radius around E c (x i ). Dissimilar pairs contribute to the loss function only if their distance is within this radius.</p><p>However, the above supervised model cannot be directly applied to alternative dataset without label annotation. Thus, we further consider the adaption technique to generalize the discriminative ability to dataset without any label annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unsupervised Domain Adaptation for Cross-Dataset Re-ID</head><p>Here we regard cross-dataset Re-ID as adaptation of discriminative ability from supervised source dataset to unsupervised target dataset. To this end, our model aims to eliminate dataset shift in the procedure of inferencing discriminative feature. Thus, as depicted in <ref type="figure" target="#fig_1">Figure 2</ref>, our model first introduces E S /E T to decompose visual feature maps X s /X t into dataset-invariant feature e s c /e t c and dataset-specific feature e s p /e t p . Our model acquires discriminative ability by applying the dataset-invariant feature e s c to predict its corresponding category. Once such feature is learned, even without supervision in target dataset, we can transfer discriminative knowledge from supervised to unsupervised dataset. With the goal of reducing information loss in the above procedure for compressing the visual feature maps, here we consider a decoder D C to reconstruct visual feature maps X t /X s from the compact domain-invariant and specific features (e t c , e t p )/(e s c , e s p ). Thus, we define reconstruction loss L rec as:</p><formula xml:id="formula_2">L rec = Ns i=1 X s i ?X s i 2 2 + Nt i=1 X t i ?X t i 2 2<label>(3)</label></formula><p>where X s i /X t i andX s i /X t i denote encoded and reconstructed the visual feature maps for source/target dataset respectively.</p><p>Note that the above learning objectives cannot ensure that the dataset-invariant and specific feature are mutual exclusive and independent, we therefore introduce a difference loss L dif f to encourage the orthogonality between these two features:</p><formula xml:id="formula_3">L dif f = H s c H s p 2 F + H t c H t p 2 F<label>(4)</label></formula><p>where H s c and H t c be matrices whose rows are the latent shared representations e s c = E C (X s ) and e t c = E C (X t ). is the square Frobenius norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning and Performing Re-ID</head><p>In sum, the total training objective L total for our ARN can be written as follows:</p><formula xml:id="formula_4">L total = L class + ? ? Lctrs + ? ? Lrec + ? ? L dif f<label>(5)</label></formula><p>where ?, ?, and ? are hyper-parameters that control the interaction of the total loss. We train our model by the minimizing L total in an end-to-end manner. Once the model is learned, as shown in <ref type="figure" target="#fig_0">Figure 3</ref>, our model performs Re-ID by measuring the cosine similarity of features of query and gallery images. Note that our ARN is able to perform Re-ID task in the unsupervised dataset by adapting discriminative ability from source to target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We now evaluate the performance of our proposed network, which is applied to perform cross-domain Re-ID tasks. To verify the work of each component in ARN, we provide ablation studies in Section 4.3. Furthermore, in Section 4.4, we compare the performance of our ARN with several supervised and unsupervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>To evaluate our proposed method, we conduct experiments on Market-1501 <ref type="bibr" target="#b19">[20]</ref> and DukeMTMC-reID <ref type="bibr" target="#b21">[22]</ref>, because both datasets are large-scale and commonly used. The details of the number of training samples under each camera are shown in <ref type="table">Table.</ref> 1.</p><p>Market-1501 <ref type="bibr" target="#b19">[20]</ref> is composed of 32,668 labeled images of 1,501 identities collected from 6 camera views. The dataset is split into two non-over-lapping fixed parts: 12,936 images from 751 identities for training and 19,732 images from 750 identities for testing. In testing, 3368 query images from 750 identities are used to retrieve the matching persons in the gallery.</p><p>DukeMTMC-reID <ref type="bibr" target="#b21">[22]</ref> is also a large-scale Re-ID dataset. It is collected from 8 cameras and contains 36,411 labeled images belonging to 1,404 identities. It also consists of 16,522 training images from 702 identities, 2,228 query images from the other 702 identities, and 17,661 gallery images.</p><p>We use rank-1 accuracy and mean average precision (mAP) for evaluation on both datasets. In the experiments, there are two source-target settings: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>ARN. Following Section 3, we use ResNet-50 pretrained on ImageNet as our E I model in the encoder. In order to perform the latent embedding easily for the modules E T , E C , and E S , we remove the last few layers including average pooling from the pre-trained ResNet-50 model. The input of the E I will be images with size 224 ? 224 ? 3, denoting width, height, and channel respectively. In this manner, the output of E I is the feature-map X with size 7 ? 7 ? 2048 and will be fed into E T , E C , and E S to obtain the corresponding feature with size 1 ? 1 ? 2048, which is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Market-1501</head><p>DukeMTMC-reID camera # of images camera # of images <ref type="table" target="#tab_0">1  2017  1  2809  2  1709  2  3009  3  2707  3  1088  4  920  4  1395  5  2338  5  1685  6  3245  6  3700  7  1330  8</ref> 1506 then flatten to private e p or sharefji32l4d e c latent feature with size 2048 as the final output of the encoder. Note that E T , E C , and E S are implemented with fully convolution networks (FCNs) which contains three layers. The input of our decoder D c is the concatenated latent feature (e c , e p ) with size 4096. We also implement the latent decoder D c with fully convolution network. The output size of the decoder D c is 7 ? 7 ? 2048, which is identical to the input of E T , E C , and E S modules. Note that the concatenated vectors in both domains, (e t c , e t p ) and (e s c , e s p ), are fed into the latent decoder simultaneously during the training procedure.</p><p>The classifier C S contains only fully connected layers with dropout mechanism. We only feed the shared latent feature e s c into the classifier. The output is the classification result among the identities. That is, the output size would be 702 if the source domain is DukeMTMC-reID <ref type="bibr" target="#b21">[22]</ref> or 751 if the source domain is Market-1501 <ref type="bibr" target="#b19">[20]</ref>.</p><p>Learning procedure. As mentioned in Section 3, we aim to minimize the total loss L total in Equation 5 during the training procedure. The parameters ?, ?, and ? are chosen under the experimental trials. In practice, we set ?, ?, and ? as 0.01, 2.0, and 1500, respectively. We aim to balance the larger value of L ctrs and the smaller one of L dif f . In addition, we need larger weight to enforce the reconstruction.</p><p>While we can directly use the same learning rate for each component to update the whole network, it might result in overfitting issues. We believe that individually setting the customized learning rates for E I , E T , E C , E S , D C , and C S can avoid this problem. For instance, when minimizing L class and L ctrs , the weights of the pre-trained model E I should not be updated faster than other modules because we try to keep much useful pre-trained weights ever trained on ImageNet. Hence, we set the learning rate for E I to a relatively small value, 10 ?7 , and only tune E I in the first few epochs. In addition, we set the learning rate of E T , E C , E S , D C to 10 ?3 , and C S to 2 ? 10 ?3 . We adopt the stochastic gradient descent (SGD) to update the parameters of the network.</p><p>Evaluating procedure. At the end of the learning scenario, we proceed to evaluate the performance of our trained network on Re-ID task. We only use E I and E c in the encoder for generating the latent features in evaluating scenario. For performance evaluation, we sort the cosine distance between the query and all the gallery features to obtain the final ranking result. Note that the cosine distance is equivalent to Euclidean distance when the feature is L2normalized. Moreover, we employ the standard metrics as in most person Re-ID literature, namely the cumulative matching curve (CMC) used for generating ranking accuracy, and the mean Average Precision (mAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>In this subsection, we aim to fully analyze the effectiveness of our ARN via comparing with other baseline settings. As shown in <ref type="table" target="#tab_1">Table 2</ref>, we compare our final version model with the ones removing supervised losses L ctrs , L class in source domain or private components E S , E T . For dataset Market-1501 and DukeMTMC-reID, our full model can achieve 70.3% and 60.2% at Rank-1 accuracy, and 39.4% and 33.4% at mAP respectively.</p><p>Reconstruction loss L rec . For the target dataset on Market-1501 and DukeMTMC-reID, we observe that the Rank-1 accuracy of baseline model without L ctrs , L class , E S , and E T , containing only the reconstruction loss L rec , decrease by 25.8% and by 29% respectively. However, this shows that the reconstruction loss does play a great role in learning basic latent representation, which can still achieve 44.5% and 31.2% at Rank-1 accuracy. We note that without L ctrs , L class , we are not able to fine-tune E I and let E I keep its original pre-trained weights on ImageNet.</p><p>Source supervised losses L ctrs , L class . Refer to Table 2 again, we also observe that without supervised loss L ctrs , L class , the Rank-1 accuracy decrease by 18.1% and by 21.4% on Market-1501 and DukeMTMC-reID respectively. This obvious drop indicates that supervised metrics on source domain has largely improved the performance of our ARN model. We also conclude that the shared latent space does need the losses L ctrs , L class to capture the semantics of person information.</p><p>Private modules E T , E S . In <ref type="table" target="#tab_1">Table 2</ref>, without private modules E T , E S , the Rank-1 accuracy decrease by 9.8% and by 11.8% on Market-1501 and DukeMTMC-reID respectively. We conclude that without partitioning the space to produce a private representation, the feature space may be contaminated with aspects of the noise that are unique for each dataset. Hence, having the private modules E T , E S does help perform representation learning in the shared latent space.   <ref type="table" target="#tab_2">Table 3</ref>, we first compare our model with the unsupervised methods. For the hand-crafted features based models, we compare our model with Bagof-Word (BOW) <ref type="bibr" target="#b19">[20]</ref>. For the cross-domain Re-ID models, there are Unsupervised Multi-task Dictionary Learning (UMDL) <ref type="bibr" target="#b12">[13]</ref>, Progressive Unsupervised Learning (PUL) <ref type="bibr" target="#b3">[4]</ref>, Clustering-based Asymmetric Metric Learning (CAMEL) <ref type="bibr" target="#b17">[18]</ref> and Similarity Preserving Generative Adversarial Network (SPGAN) <ref type="bibr" target="#b2">[3]</ref>. Our model outperforms these models in Rank-1, Rank-5, Rank-10, and mAP on Market-1501. Note that our model outperforms the second best method by 13.6% in Rank-1 accuracy and by 12.7% in mAP.</p><p>In addition, we also compare our model with existing supervised models, observing that our model surpasses BOW <ref type="bibr" target="#b19">[20]</ref>, LDNS <ref type="bibr" target="#b18">[19]</ref> and already boost the performance closely to supervised deep learning based model like SVD-NET <ref type="bibr" target="#b14">[15]</ref>, TriNet <ref type="bibr" target="#b6">[7]</ref>, CamStyle <ref type="bibr" target="#b22">[23]</ref>, or DuATM <ref type="bibr" target="#b13">[14]</ref>.</p><p>DukeMTMC-reID. In <ref type="table" target="#tab_3">Table 4</ref>, our model outperforms unsupervised methods such as BOW <ref type="bibr" target="#b19">[20]</ref>, UMDL <ref type="bibr" target="#b12">[13]</ref>, PUL <ref type="bibr" target="#b3">[4]</ref>, and SPGAN <ref type="bibr" target="#b2">[3]</ref>. Our model achieves Rank-1 ac-curacy=60.2% and mAP=33.4% and outperforms the second best method <ref type="bibr" target="#b2">[3]</ref> roughly by 13.8% in Rank-1 accuracy and by 7.2% in mAP. More importantly, the performance of our model is better than some supervised methods such as BOW <ref type="bibr" target="#b19">[20]</ref> and LOMO <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we presented a deep learning model of Adaptation and Re-Identification Network (ARN) for solving cross-domain Re-ID tasks. Our ARN allows us to jointly exploit a pre-collected supervised source-domain dataset and a target-domain dataset of interest by learning domain invariant and discriminative features. As a result, Re-ID in the target-domain can be performed even without any label information observed during training. With this proposed unsupervised domain adaptation network, we conducted experiments on Market-1501 and DukeMTMC-reID datasets, and confirmed the effectiveness of our model in such a challenging unsupervised learning setting. Moreover, our method also performed favorably against a number of baseline supervised Re-ID approaches, which again supports the use of our ARN for practical Re-ID tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>When performing Re-ID using our ARN, only E I and E c in the latent encoder are required. That is, we match the latent feature e q c of person-of-interest (in the dataset) with the latent feature e t c of the test image by calculating the similarity ranking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>H s p and H t p are obtained in a similar manner. Note that ? 2 F</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 .</head><label>1</label><figDesc>Target: Market-1501 / Source: DukeMTMC-reID. 2. Target: DukeMTMC-reID / Source: Market-1501.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Numbers of training samples and cameras inMarket-1501 and DukeMTMC-reID datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies of Adaptation and Re-Identification Network (ARN) under different experimental settings. L ctrs , L class , E S , E T 44.5 63.2 70.4 78.5 20.3 31.2 42.5 50.1 57.4 18.4 Ours w/o L ctrs , L class</figDesc><table><row><cell></cell><cell cols="3">Target: Market-1501</cell><cell cols="2">Target: DukeMTMC-reID</cell></row><row><cell>Method</cell><cell cols="3">Source: DukeMTMC-reID</cell><cell cols="2">Source: Market-1501</cell></row><row><cell></cell><cell>R1</cell><cell>R5</cell><cell cols="2">R10 R20 mAP R1</cell><cell>R5</cell><cell>R10 R20 mAP</cell></row><row><cell cols="4">Ours w/o 52.2 68.4 75.9 82.1 23.7</cell><cell cols="2">36.7 48.9 58.2 63.4 19.6</cell></row><row><cell>Ours w/o E S , E T</cell><cell cols="3">60.5 74.2 81.9 88.1 28.7</cell><cell cols="2">48.4 62.5 68.8 73.1 26.8</cell></row><row><cell>Ours</cell><cell cols="3">70.3 80.4 86.3 93.6 39.4</cell><cell cols="2">60.2 73.9 79.5 82.5 33.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparisons on Market-1501 with supervised and unsupervised Re-ID methods.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="4">Rank-1 Rank-5 Rank-10 mAP</cell></row><row><cell></cell><cell>BOW [20]</cell><cell>44.4</cell><cell>-</cell><cell>-</cell><cell>20.8</cell></row><row><cell>Supervised</cell><cell>LDNS [19] SVDNET [15] TriNet [7] CamStyle [23]</cell><cell>61.0 82.3 84.9 89.5</cell><cell>----</cell><cell>----</cell><cell>35.7 62.1 69.1 71.6</cell></row><row><cell></cell><cell>DuATM [14]</cell><cell>91.4</cell><cell>-</cell><cell>-</cell><cell>76.6</cell></row><row><cell>Unsupervised</cell><cell>BOW [20] UMDL [13] PUL [4] CAMEL [18] SPGAN [3] Ours</cell><cell>35.8 34.5 45.5 54.5 57.7 70.3</cell><cell>52.4 52.6 60.7 -75.8 80.4</cell><cell>60.3 59.6 66.7 -82.4 86.3</cell><cell>14.8 12.4 20.5 26.3 26.7 39.4</cell></row><row><cell cols="5">4.4. Comparison with State-of-the-art Methods</cell><cell></cell></row></table><note>Market-1501. In</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance comparisons on DukeMTMC-reID with supervised and unsupervised Re-ID methods.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="4">Rank-1 Rank-5 Rank-10 mAP</cell></row><row><cell></cell><cell>BOW [20]</cell><cell>25.1</cell><cell>-</cell><cell>-</cell><cell>12.2</cell></row><row><cell>Supervised</cell><cell>LOMO [8] TriNet [7] SVDNET [15] CamStyle [23]</cell><cell>30.8 72.4 76.7 78.3</cell><cell>----</cell><cell>----</cell><cell>17.0 53.5 56.8 57.6</cell></row><row><cell></cell><cell>DuATM [14]</cell><cell>81.8</cell><cell>-</cell><cell>-</cell><cell>64.6</cell></row><row><cell>Unsupervised</cell><cell>BOW [20] UMDL [13] PUL [4] SPGAN [3] Ours</cell><cell>17.1 18.5 30.0 46.4 60.2</cell><cell>28.8 31.4 43.4 62.3 73.9</cell><cell>34.9 37.6 48.5 68.0 79.5</cell><cell>8.3 7.3 16.4 26.2 33.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised person reidentification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05244</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised image-toimage translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual domain adaptation: A survey of recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1306" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dual attention matching network for context-aware feature sequence based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards unsupervised open-set person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>the IEEE International Conference on Image Processing (ICIP)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-view asymmetric metric learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Person reidentification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Camera style adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networkss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
