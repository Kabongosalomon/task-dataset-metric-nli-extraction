<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning 3D Human Dynamics from Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
							<email>kanazawa@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
							<email>zhang.j@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
							<email>malik@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning 3D Human Dynamics from Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>From an image of a person in action, we can easily guess the 3D motion of the person in the immediate past and future. This is because we have a mental model of 3D human dynamics that we have acquired from observing visual sequences of humans in motion. We present a framework that can similarly learn a representation of 3D dynamics of humans from video via a simple but effective temporal encoding of image features. At test time, from video, the learned temporal representation give rise to smooth 3D mesh predictions. From a single image, our model can recover the current 3D mesh as well as its 3D past and future motion. Our approach is designed so it can learn from videos with 2D pose annotations in a semi-supervised manner. Though annotated data is always limited, there are millions of videos uploaded daily on the Internet. In this work, we harvest this Internet-scale source of unlabeled data by training our model on unlabeled video with pseudo-ground truth 2D pose obtained from an off-the-shelf 2D pose detector. Our experiments show that adding more videos with pseudo-ground truth 2D pose monotonically improves 3D prediction performance. We evaluate our model, Human Mesh and Motion Recovery (HMMR), on the recent challenging dataset of 3D Poses in the Wild and obtain state-of-the-art performance on the 3D prediction task without any fine-tuning. The project website with video, code, and data can be found at https://akanazawa.github.io/ human_dynamics/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Consider the image of the baseball player mid-swing in <ref type="figure">Figure 1</ref>. Even though we only see a flat two-dimensional picture, we can infer the player's 3D pose, as we can easily imagine how his knees bend and arms extend in space. Furthermore, we can also infer his motion in the surrounding moments as he swings the bat through. We can do this because we have a mental model of 3D human dynamics that * equal contribution Input Predictions Different Viewpoint <ref type="figure">Figure 1</ref>: 3D motion prediction from a single image. We propose a method that, given a single image of a person, predicts the 3D mesh of the person's body and also hallucinates the future and past motion. Our method can learn from videos with only 2D pose annotations in a semi-supervised manner. Note our training set does not have any ground truth 3D pose sequences of batting motion. Our model also produces smooth 3D predictions from video input.</p><p>we have acquired from observing many examples of people in motion.</p><p>In this work, we present a computational framework that can similarly learn a model of 3D human dynamics from video. Given a temporal sequence of images, we first extract per-image features, and then train a simple 1D temporal encoder that learns a representation of 3D human dynamics over a temporal context of image features. We force this representation to capture 3D human dynamics by predicting not only the current 3D human pose and shape, but also changes in pose in the nearby past and future frames. We transfer the learned 3D dynamics knowledge to static images by learning a hallucinator that can hallucinate the temporal context representation from a single image feature. The hallucinator is trained in a self-supervised manner using the actual output of the temporal encoder.              . Given a temporal sequence of images, we first extract per-image features ?t. We train a temporal encoder fmovie that learns a representation of 3D human dynamics ?t over the temporal window centered at frame t, illustrated in the blue region. From ?t, we predict the 3D human pose and shape ?t, as well as the change in pose in the nearby ??t frames. The primary loss is 2D reprojection error, with an adversarial prior to make sure that the recovered poses are valid. We incorporate 3D losses when 3D annotations are available. We also train a hallucinator h that takes a single image feature ?t and learns to hallucinate its temporal representation?t. At test time, the hallucinator can be used to predict dynamics from a single image.</p><formula xml:id="formula_0">V f v u l j E x o T B u u B L j t k = " &gt; A A A B 8 H i c b Z C 7 S g N B F I b P x l u M t 6 h g Y z M Y A l Z h 1 0 b L o I 1 l A r l B s s T Z y W w y Z H Z 2 m T k r h C U v Y S U o i K 2 v Y 2 X j s z i 5 F J r 4 w 8 D H f 8 5 h z v m D R A q D r v v l 5 D Y 2 t 7 Z 3 8 r u F v f 2 D w 6 P i 8 U n L x K l m v M l i G e t O Q A 2 X Q v E m C p S 8 k 2 h O o 0 D y d j C + m 9 X b j 1 w b E a s G T h L u R 3 S o R C g Y R W</formula><formula xml:id="formula_1">i Q L I n v Y Z J S N 8 L D m I W M Y F C W Z x w X h d M e M Q / O H W A 8 o H l 5 m 3 p Q F J 5 R t 5 r W T O Y y 2 B X U U a W 2 Z 3 w 5 Q U K y i M Z A O J Z y Y F s p u D k W w A i n U 7 3 h Z J K m m I z x k A 4 U x j i i 0 s 1 n O 0 z N h n I C M 0 y E O j G Y M 1 f / N Z H j S M p J 5 K v O C M N I L t Z K 8 7 / a I I P w y s 1 Z n G Z A Y z J / K M y 4 C Y l Z B m I G T F A C f K I A E 8 H U Z 0 0 y w g I T U L H p K g V 7 c e d l 6 F 4 0 b c V 3 V r 1 1 X e V R Q y f o F J 0 h G 1 2 i F r p F b d R B B D 2 i Z / S K 3 r Q n 7 U V 7 1 z 7 m r S t a N X O E / k j 7 / A F j 3 5 f v &lt; / l a t e x i t &gt; hallucinator &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b n T v l T f N 7 d y J h j a O P U c U Z t s c 3 W Y = " &gt; A A A B / X i c b Z D L S s N A F I Z P 6 q 3 G W 7 V L N 8 F S c F U S N 7 o R i 2 5 c V r A X a E O Z T C f t 0 M k k z J y I J R R 9 E 1 e C g r j 1 E X w A V 7 6 N 0 8 t C W 3 8 Y + P j P O Z w z f 5 A I r t F 1 v 6 3 c y u r a + k Z + 0 9 7 a 3 t n d K + w f N H S c K s r q N B a x a g V E M 8 E l q y N H w V q J Y i Q K B G s G w 6 t J v X n H l O a x v M V R w v y I 9 C U P O S V o r G 6 h 2 E F 2 j 9 m A C J F S L g n G a t w t l N y K O 5 W z D N 4 c S h e f 9 v k j A N S 6 h a 9 O L 6 Z p x C R S Q b R u e 2 6 C f k Y U c i r Y 2 C 5 3 U s 0 S Q o e k z 9 o G J Y m Y 9 r P p 9 W O n b J y e E 8 b K P I n O 1 L V / T W Q k 0 n o U B a Y z I j j Q i 7 W J + V + t n W J 4 5 m d c J i k y S W e L w l Q 4 G D u T K J w e V 4 y i G B k g V H F z r E M H R B G K J j D b p O A t / n k Z G i c V z / C N W 6 p e w k x 5 O I Q j O A Y P T q E K 1 1 C D O l A Y w R O 8 w K v 1 Y D 1 b b 9 b 7 r D V n z W e K 8 E f W x w + 2 z J d 8 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S R S e B m B g D C n B / y j C E L n J G f g 6 p d 8 = " &gt; A A A B / X i c b Z D L S s N A F I Y n X m u 8 R b t 0 M 1 g K r k r i R j d i 0 Y 3 L C v Y C b S i T 6 a Q d O p m E m R M x h O K j u B B B Q d z 6 C D 6 A C / F t n F 4 W 2 v r D w M d / z u G c + Y N E c A 2 u + 2 0 t L a + s r q 0 X N u z N r e 2 d X W d v v 6 H j V F F W p 7 G I V S s g m g k u W R 0 4 C N Z K F C N R I F g z G F 6 O</formula><formula xml:id="formula_2">= " &gt; A A A B / X i c b Z D L S s N A F I Y n X m u 8 R b t 0 M 1 g K r k r i R j d i 0 Y 3 L C v Y C b S i T 6 a Q d O p m E m R M x h O K j u B B B Q d z 6 C D 6 A C / F t n F 4 W 2 v r D w M d / z u G c + Y N E c A 2 u + 2 0 t L a + s r q 0 X N u z N r e 2 d X W d v v 6 H j V F F W p 7 G I V S s g m g k u W R 0 4 C N Z K F C N R I F g z G F 6 O</formula><formula xml:id="formula_3">G d M = " &gt; A A A B / X i c b Z D L S g M x F I Y z 9 V b H 2 2 i X b o K l 4 K r M u N F l 0 Y 3 L C v Y C b S m Z N N O G Z p I h O S M O Q / F R X A k K 4 t Y H c e X b m L a z 0 N Y f A h / / O Y d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q n G G c m i A M i X M s Y T 1 q a I M T V C 2 S c G d / / M i 1 E / L r u E b E 8 c F T J W D Q z i G E r h w B h W 4 h i r U g M E d P M I z v F g P 1 p P 1 a r 1 N W 5 e s 2 c w B / J H 1 / g O x 6 J Q l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + a e t L D M 4 Z Z m D R x I H y Y k o f G y d y f c = " &gt; A A A B 9 X i c b Z C 7 T s M w F I a d c i</head><formula xml:id="formula_4">v h V m B k M V S V y k C V s M B Y A Q N j k e h F a q L K c Z 3 W q u M E + w R U R X 0 G R i Y k k B A r 7 8 A r M P E g 7 L i X A Q q / Z O n T f 8 7 R O f 6 D R H A N j v N p 5 R Y W l 5 Z X 8 q v 2 2 v r G 5 l Z h e 6 e h 4 1 R R V q e x i F U r I J o J L l k d O A j W S h Q j U S B Y M x i c j + v N W 6 Y 0 j + U 1 D B P m</formula><formula xml:id="formula_5">z R n b A Y J d c 1 V u U 0 = " &gt; A A A B 9 X i c b Z B N S 8 N A E I Y 3 f t b 6 V f X o Z b E U v F g S L 3 o s 6 s F j B f s B b S i b 7 a R d u t n E 3 Y l S Q n + H J 0 F B v P p j P P l v 3 L Y 5 a O s L C w / v z D C z b 5 B I Y d B 1 v 5 2 V 1 b X 1 j c 3 C V n F 7 Z 3 d v v 3 R w 2 D R x q j k 0 e C x j 3 Q 6 Y A S k U N F C g h H a i g U W B h F Y w u p 7 W W 4 + g j Y j V P Y 4 T 8 C M 2 U C I U n K G 1 / L C X n X V v Q C K j O O m V</formula><formula xml:id="formula_6">W I V d G y 2 D W l g m Y C 6 Q X c L s Z D Y Z M n t h 5 m w g L H k N K 0 F B b K 1 9 D y s 7 H 8 X J p d D E H w Y + / n M O 5 8 w f p F J o d J w v a 2 1 9 Y 3 N r u 7 B j 7 + 7 t H x w W j 4 6 b O s k U 4 w 2 W y E S 1 A 6 q 5 F D F v o E D J 2 6 n i N A o k b w X D 2 2 m 9 N e J K i y R + w H H K / Y j 2 Y x E K R t F Y X t j N v T s u k R K c d I s l p + L M R F b B X U C p e v Z R / w a A W r f 4 6 f U S l k U 8 R i a p 1 h 3 X S d H P q U L B J J / Y Z S / T P K V s S P u 8 Y z C m E d d + P j t 6 Q s r G 6 Z E w U e b F S G a u / W s i p 5 H W 4 y g w n R H F g V 6 u T c 3 / a p 0 M w 2 s / F 3 G a I Y / Z f F G Y S Y I J m S Z A e k J x h n J s g D I l z L G E D a i i D E 1 O t k n B X f 7 z K j Q v K 6 7 h u o n j B u Y q w C m c w w W 4 c A V V u I</formula><formula xml:id="formula_7">h U j v o d d N t 8 = " &gt; A A A B 9 H i c b Z B N S 8 N A E I Y n f t b 6 V f X o Z b E U P J X E i x 6 L e v B Y w X 5 A E 8 p m u 2 m X b j Z h d 1 I o o X / D k 6 A g X v 0 z n v w 3 b t s c t P W F h Y d 3 Z p j Z N</formula><formula xml:id="formula_8">V 8 T C f O 1 H v m e 6 f Q Z D v R i b W L + V 2 v F 2 L t o J y K I Y o S A z x b 1 Y k k x p J M o a F c o 4 C h H B h h X w h x L + Y A p x t E E l j E p O I t / X o b 6 W c k x f G v i u C Q z p c k h O S Z F 4 p B z U i Y 3 p E J q</formula><formula xml:id="formula_9">V 8 T C f O 1 H v m e 6 f Q Z D v R i b W L + V 2 v F 2 L t o J y K I Y o S A z x b 1 Y k k x p J M o a F c o 4 C h H B h h X w h x L + Y A p x t E E l j E p O I t / X o b 6 W c k x f G v i u C Q z p c k h O S Z F 4 p B z U i Y 3 p E J q</formula><formula xml:id="formula_10">E y V a Y F V E K t I N n x t U M s Q q S V L Y i D X y w F d Y 9 w d X 4 3 r 9 H r W R U V i h Y Y z t g N + F s i c F J 2 t 1 8 o V W p Y / E O y m d t K 5 R E W c 0 6 u S L 7 q k 7 E V s E b w b F 0 s H x 9 w c A l D v 5 z 1 Y 3 E k m A I Q n F j W l 6 b k z t l G u S Q u E o d 9 R K D M Z c D P g d N i 2 G P E D T T i f X j 9 i R d b</formula><formula xml:id="formula_11">V 8 T C f O 1 H v m e 6 f Q Z D v R i b W L + V 2 v F 2 L t o J y K I Y o S A z x b 1 Y k k x p J M o a F c o 4 C h H B h h X w h x L + Y A p x t E E l j E p O I t / X o b 6 W c k x f G v i u C Q z p c k h O S Z F 4 p B z U i Y 3 p E J q h J M R e S</formula><p>illustrates the overview of our training procedure.</p><p>At test time, when the input is a video, the temporal encoder can be used to produce smooth 3D predictions: having a temporal context reduces uncertainty and jitter in the 3D prediction inherent in single-view approaches. The encoder provides the benefit of learned smoothing, which reduces the acceleration error by 56% versus a comparable single-view approach on a recent dataset of 3D humans in the wild. Our approach also obtains state-of-the-art 3D error on this dataset without any fine-tuning. When the input is a single image, the hallucinator can predict the current 3D human mesh as well as the change in 3D pose in nearby future and past frames, as illustrated in <ref type="figure">Figure 1</ref>.</p><p>We design our framework so that it can be trained on various types of supervision. A major challenge in 3D human prediction from a video or an image is that 3D supervision is limited in quantity and challenging to obtain at a large scale. Videos with 3D annotations are often captured in a controlled environment, and models trained on these videos alone do not generalize to the complexity of the real world. When 3D ground truth is not available, our model can be trained with 2D pose annotations via the reprojection loss <ref type="bibr" target="#b58">[59]</ref> and an adversarial prior that constrains the 3D human pose to lie in the manifold of real human poses <ref type="bibr" target="#b30">[31]</ref>. However, the amount of video labeled with ground truth 2D pose is still limited because ground truth annotations are costly to acquire.</p><p>While annotated data is always limited, there are millions of videos uploaded daily on the Internet. In this work we harvest this potentially unlimited source of unlabeled videos. We curate two large-scale video datasets of humans and train on this data using pseudo-ground truth 2D pose obtained from a state-of-the-art 2D pose detector <ref type="bibr" target="#b9">[10]</ref>. Excitingly, our experiments indicate that adding more videos with pseudo-ground truth 2D monotonically improves the model performance both in term of 3D pose and 2D reprojection error: 3D pose error reduces by 9% and 2D pose accuracy increases by 8%. Our approach falls in the category of omni-supervision <ref type="bibr" target="#b44">[45]</ref>, a subset of semi-supervised learning where the learner exploits all data along with Internetscale unlabeled data. We distill the knowledge of an accurate 2D pose detector into our 3D predictors through unlabeled video. While omni-supervision has been shown to improve 2D recognition problems, as far as we know, our experiment is the first to show that training on pseudo-ground truth 2D pose labels improves 3D prediction.</p><p>In summary, we propose a simple but effective temporal encoder that learns to capture 3D human dynamics. The learned representation allows smooth 3D mesh predictions from video in a feed-forward manner. The learned representation can be transferred to a static image, where from a single image, we can predict the current 3D mesh as well as the change in 3D pose in nearby frames. We further show that our model can leverage an Internet-scale source of unlabeled videos using pseudo-ground truth 2D pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D pose and shape from a single image. Estimating 3D body pose and shape from a single image is a fundamentally ambiguous task that most methods deal by using some model of human bodies and priors. Seminal works in this area <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b1">2]</ref> rely on silhouette features or manual interaction from users <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b64">65]</ref> to fit the parameters of a statistical body model. A fully automatic method was proposed by Bogo et al. <ref type="bibr" target="#b7">[8]</ref>, which fits the parametric SMPL <ref type="bibr" target="#b35">[36]</ref> model to 2D joint locations detected by an off-theshelf 2D pose detector <ref type="bibr" target="#b43">[44]</ref> with strong priors. Lassner et al. <ref type="bibr" target="#b31">[32]</ref> extend the approach to fitting predicted silhouettes. <ref type="bibr" target="#b62">[63]</ref> explore the multi-person setting. Very recently, multiple approaches integrate the SMPL body model within a deep learning framework <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40]</ref>, where models are trained to directly infer the SMPL parameters. These methods vary in the cues they use to infer the 3D pose and shape: RGB image <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b30">31]</ref>, RGB image and 2D keypoints <ref type="bibr" target="#b50">[51]</ref>, keypoints and silhouettes <ref type="bibr" target="#b40">[41]</ref>, or keypoints and body part segmentations <ref type="bibr" target="#b39">[40]</ref>. Methods that employ silhouettes obtain more accurate shapes, but require that the person is fully visible and unoccluded in the image. Varol et al. explore predicting a voxel representation of human body <ref type="bibr" target="#b51">[52]</ref>. In this work we go beyond these approaches by proposing a method that can predict shape and pose from a single image, as well as how the body changes locally in time.</p><p>3D pose and shape from video. While there are more papers that utilize video, most rely on a multi-view setup, which requires significant instrumentation. We focus on videos obtained from a monocular camera. Most approaches take a two-stage approach: first obtaining a singleview 3D reconstruction and then post-processing the result to be smooth via solving a constrained optimization problem <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43]</ref>. Recent methods obtain accurate shapes and textures of clothing by pre-capturing the actors and making use of silhouettes <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b3">4]</ref>. While these approaches obtain far more accurate shape, reliance on the pre-scan and silhouettes restricts these approaches to videos obtained in an interactive and controlled environments. Our approach is complementary to these two-stage approaches, since all predictions can be post-processed and refined. There are some recent works that output smooth 3D pose and shape: <ref type="bibr" target="#b50">[51]</ref> predicts SMPL parameters from two video frames by using optical flow, silhouettes, and keypoints in a self-supervised manner. <ref type="bibr" target="#b2">[3]</ref> exploits optical flow to obtain temporally coherent human poses. <ref type="bibr" target="#b29">[30]</ref> fits a body model to a sequence of 3D point clouds and 3D joints obtained from multi-view stereo. Several approaches train LSTM models on various inputs such as image features <ref type="bibr" target="#b34">[35]</ref>, 2D joints <ref type="bibr" target="#b25">[26]</ref>, or 3D joints <ref type="bibr" target="#b12">[13]</ref> to obtain temporally coherent 3D joint outputs. More recently, TP-Net <ref type="bibr" target="#b13">[14]</ref> learns a fully convolutional network that smooths the predicted 3D joints. Concurrently to ours, <ref type="bibr" target="#b41">[42]</ref> use a fully convolutional network to predict 3D joints from 2D joint sequences. We directly predict the 3D mesh outputs from 2D image sequences and can train with images without any ground truth 3D annotation. Furthermore, our temporal encoder predicts the 3D pose changes in nearby frames in addition to the current 3D pose. Our experiments indicate that the prediction losses help the encoder to pay more atten-tion to the dynamics information available in the temporal window. Learning motion dynamics. There are many methods that predict 2D future outputs from video using pixels <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref>, flow <ref type="bibr" target="#b54">[55]</ref>, or 2D pose <ref type="bibr" target="#b56">[57]</ref>. Other methods predict 3D future from 3D inputs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b52">53]</ref>. In contrast, our work predicts future and past 3D pose from 2D inputs. There are several approaches that predict future from a single image <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20]</ref>, but all approaches predict future in 2D domains, while in this work we propose a framework that predicts 3D motions. Closest to our work is that of Chao et al. <ref type="bibr" target="#b11">[12]</ref>, who forecast 2D pose and then estimate the 3D pose from the predicted 2D pose. In this work, we predict dynamics directly in the 3D space and learn the 3D dynamics from video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our goal is to learn a representation of 3D human dynamics from video, from which we can 1) obtain smooth 3D prediction and 2) hallucinate 3D motion from static images. In particular, we develop a framework that can learn 3D human dynamics from unlabeled, everyday videos of people on the Internet. We first define the problem and discuss different tiers of data sources our approach can learn from. We then present our framework that learns to encode 3D human motion dynamics from videos. Finally, we discuss how to transfer this knowledge to static images such that one can hallucinate short-term human dynamics from a static image. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Setup</head><p>Our input is a video V = {I t } T t=1 of length T , where each frame is a bounding-box crop centered around a detected person. We encode the tth image frame I t with a visual feature ? t , obtained from a pretrained feature extractor. We train a function f movie that learns a representation ? t that encodes the 3D dynamics of a human body given a temporal context of image features centered at frame t. Intuitively, ? t is the representation of a "movie strip" of 3D human body in motion at frame t. We also learn a hallucinator h : ? t ? ? t , whose goal is to hallucinate the movie strip representation from a static image feature ? t .</p><p>We ensure that the movie strip representation ? t captures the 3D human body dynamics by predicting the 3D mesh of a human body from ? t at different time steps. The 3D mesh of a human body in an image is represented by 85 parameters, denoted by ? = {?, ?, ?}, which consists of shape, pose, and camera parameters. We use the SMPL body model <ref type="bibr" target="#b35">[36]</ref>, which is a function M(?, ?) ? R N ?3 that outputs the N = 6890 vertices of a triangular mesh given the shape ? and pose ?. Shape parameters ? ? R 10 define the linear coefficients of a low-dimensional statistical shape model, and pose parameters ? ? R 72 define the global rotation of the body and the 3D relative rotations of the kinematic skeleton of 23 joints in axis-angle representation. Please see <ref type="bibr" target="#b35">[36]</ref> for more details. The mesh vertices define 3D locations of k joints X ? R k?3 = W M(?, ?) via a pre-trained linear regressor W ? R k?N . We also solve for the weak-perspective camera ? = [s, t x , t y ] that projects the body into the image plane. We denote x = ?(X(?, ?)) as the projection of the 3D joints.</p><p>While this is a well-formed supervised learning task if the ground truth values were available for every video, such 3D supervision is costly to obtain and not available in general. Acquiring 3D supervision requires extensive instrumentation such as a motion capture (MoCap) rig, and these videos captured in a controlled environment do not reflect the complexity of the real world. While more practical solutions are being introduced <ref type="bibr" target="#b53">[54]</ref>, 3D supervision is not available for millions of videos that are being uploaded daily on the Internet. In this work, we wish to harness this potentially infinite data source of unlabeled video and propose a framework that can learn 3D motion from pseudo-ground truth 2D pose predictions obtained from an off-the-shelf 2D pose detector. Our approach can learn from three tiers of data sources at once: First, we use the MoCap datasets {(V i , ? i , x i )} with full 3D supervision ? i for each video along with ground truth 2D pose annotations for k joints</p><formula xml:id="formula_12">x i = {x t ? R k?2 } T t=1</formula><p>in each frame. Second, we use datasets of videos in the wild obtained from a monocular camera with human-annotated 2D pose: {(V i , x i )}. Third, we also experiment with videos with pseudo-ground truth 2D pose: {(V i ,x i )}. See <ref type="table" target="#tab_1">Table 1</ref> for the list of datasets and their details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning 3D Human Dynamics from Video</head><p>A dynamics model of a 3D human body captures how the body changes in 3D over a small change in time. Therefore, we formulate this problem as learning a temporal representation that can simultaneously predict the current 3D body and pose changes in a short time period. To do this, we learn a temporal encoder f movie and a 3D regressor f 3D that predict the 3D human mesh representation at the current frame, as well as delta 3D regressors f ?t that predict how the 3D pose changes in ??t time steps.</p><p>Temporal Encoder Our temporal encoder consists of several layers of a 1D fully convolutional network f movie that encodes a temporal window of image features centered at t into a representation ? t that encapsulates the 3D dynamics. We use a fully convolutional model for its simplicity. Recent literature also suggests that feed-forward convolutional models empirically out-perform recurrent models while being parallelizable and easier to train with more stable gradients <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39]</ref>. Our temporal convolution network has a ResNet <ref type="bibr" target="#b24">[25]</ref> based architecture similar to <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>The output of the temporal convolution network is sent to a 3D regressor f 3D : ? t ? ? t that predicts the 3D human mesh representation at frame t. We use the same iterative 3D regressor architecture proposed in <ref type="bibr" target="#b30">[31]</ref>. Simply having a temporal context reduces ambiguity in 3D pose, shape, and viewpoint, resulting in a temporally smooth 3D mesh reconstruction. In order to train these modules from 2D pose annotations, we employ the reprojection loss <ref type="bibr" target="#b58">[59]</ref> and the adversarial prior proposed in <ref type="bibr" target="#b30">[31]</ref> to constrain the output pose to lie in the space of possible human poses. The 3D losses are also used when 3D ground truth is available. Specifically, the loss for the current frame consists of the reprojection loss on visible keypoints L 2D = ||v t (x t ?x t )|| 2 2 , where v t ? R k?2 is the visibility indicator over each keypoint, the 3D loss if available, L 3D = ||? t ?? t || 2 2 , and the factorized adversarial prior of <ref type="bibr" target="#b30">[31]</ref>, which trains a discriminator D k for each joint rotation of the body model</p><formula xml:id="formula_13">L adv prior = k (D k (?) ? 1) 2 .</formula><p>In this work, we regularize the shape predictions using a shape prior L ? prior <ref type="bibr" target="#b7">[8]</ref>. Together the loss for frame t consists of L t = L 2D + L 3D + L adv prior + L ? prior . Furthermore, each sequence is of the same person, so while the pose and camera may change every frame, the shape remains constant. We express this constraint as a constant shape loss over each sequence:</p><formula xml:id="formula_14">L const shape = T ?1 t=1 ||? t ? ? t+1 ||.</formula><p>(1)</p><p>Predicting Dynamics We enforce that the learned temporal representation captures the 3D human dynamics by predicting the 3D pose changes in a local time step ??t.</p><p>Since we are training with videos, we readily have the 2D and/or 3D targets at nearby frames of t to train the dynamics predictors. Learning to predict 3D changes encourages the network to pay more attention to the temporal cues, and our experiments show that adding this auxiliary loss improves the 3D prediction results. Specifically, given a movie strip representation of the temporal context at frame ? t , our goal is to learn a dynamics predictor f ?t that predicts the change in 3D parameters of the human body at time t ? ?t.</p><p>In predicting dynamics, we only estimate the change in 3D pose parameters ?, as the shape should remain constant and the weak-perspective camera accounts for where the human is in the detected bounding box. In particular, to improve the robustness of the current pose estimation during training, we augment the image frames with random jitters in scale and translation which emulates the noise in real human detectors. However, such noise should not be modeled by the dynamics predictor.</p><p>For this task, we propose a dynamics predictor f ?t that outputs the 72D change in 3D pose ??. f ?t is a function that maps ? t and the predicted current pose ? t to the pre-dicted change in pose ?? for a specific time step ?t. The delta predictors are trained such that the predicted pose in the new timestep ? t+?t = ? t + ?? minimizes the reprojection, 3D, and the adversarial prior losses at time frame t + ?t. We use the shape predicted in the current time t to obtain the mesh for t ? ?t frames. To compute the reprojection loss without predicted camera, we solve for the optimal scale s and translation t that aligns the orthographically projected 3D joints x orth = X[:, : 2] with the visible ground truth 2D joints x gt : min s, t ||(sx orth + t) ? x gt || 2 . A closed form solution exists for this problem, and we use the optimal camera ? * = [s * , t * ] to compute the reprojection error on poses predicted at times t ? ?t. Our formulation factors away axes of variation, such as shape and camera, so that the delta predictor focuses on learning the temporal evolution of 3D pose. In summary, the overall objective for the temporal encoder is</p><formula xml:id="formula_15">L temporal = t L t + ?t L t+?t + L const shape .<label>(2)</label></formula><p>In this work we experiment with two ?t at {?5, 5} frames, which amounts to ?0.2 seconds for a 25 fps video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hallucinating Motion from Static Images</head><p>Given the framework for learning a representation for 3D human dynamics, we now describe how to transfer this knowledge to static images. The idea is to learn a hallucinator h : ? t ?? t that maps a single-frame representation ? t to its "movie strip" representation? t . One advantage of working with videos is that during training, the target representation ? t is readily available for every frame t from the temporal encoder. Thus, the hallucinator can be trained in a weakly-supervised manner, minimizing the difference between the hallucinated movie strip and the actual movie strip obtained from f movie :</p><formula xml:id="formula_16">L hal = ||? t ?? t || 2 .<label>(3)</label></formula><p>Furthermore, we pass the hallucinated movie strip to the f 3D regressor to minimize the single-view loss as well as the delta predictors f ?t . This ensures that the hallucinated features are not only similar to the actual movie strip but can also predict dynamics. All predictor weights are shared among the actual and hallucinated representations. In summary we jointly train the temporal encoder, hallucinator, and the delta 3D predictors together with overall objective:</p><formula xml:id="formula_17">L = L temporal + L hal + L t (? t ) + ?t L t+?t (? t ). (4)</formula><p>See <ref type="figure" target="#fig_0">Figure 2</ref> for the overview of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning from Unlabeled Video</head><p>Although our approach can be trained on 2D pose annotations, annotated data is always limited -the annotation  effort for labeling keypoints in videos is substantial. However, millions of videos are uploaded to the Internet every day. On YouTube alone, 300 hours of video are uploaded every minute <ref type="bibr" target="#b5">[6]</ref>. Therefore, we curate two Internet-scraped datasets with pseudo-ground truth 2D pose obtained by running Open-Pose <ref type="bibr" target="#b9">[10]</ref>. An added advantage of OpenPose is that it detects toe points, which are not labeled in any of the video datasets with 2D ground truth. Our first dataset is VLOGpeople, a subset of the VLOG lifestyle dataset <ref type="bibr" target="#b17">[18]</ref> on which OpenPose fires consistently. To get a more diverse range of human dynamics, we collect another dataset, InstaVariety, from Instagram using 84 hashtags such as #instruction, #swimming, and #dancing. A large proportion of the videos we collected contain only one or two people moving with much of their bodies visible, so OpenPose produced reasonably good quality 2D annotations. For videos that contain multiple people, we form our pseudo-ground truth by linking the per-frame skeletons from OpenPose using the Hungarian algorithm-based tracker from Detect and Track <ref type="bibr" target="#b20">[21]</ref>. A clear advantage of unlabeled videos is that they can be easily collected at a significantly larger scale than videos with human-annotated 2D pose. Altogether, our pseudo-ground truth data has over 28 hours of 2Dannotated footage, compared to the 79 minutes of footage in the human-labeled datasets. See <ref type="table" target="#tab_1">Table 1</ref> for the full dataset comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setup</head><p>Architecture: We use Resnet-50 <ref type="bibr" target="#b24">[25]</ref> pretrained on singleview 3D human pose and shape prediction <ref type="bibr" target="#b30">[31]</ref> as our feature extractor, where ? i ? R 2048 is the the average pooled features of the last layer. Since training on video requires a large amount of memory, we precompute the image features on each frame similarly to <ref type="bibr" target="#b0">[1]</ref>. This allow us to train on 20 frames of video with mini-batch size of 8 on a single 1080ti GPU. Our temporal encoder consists of 1D temporal convolutional layers, where each layer is a residual block of two 1D convolutional layers of kernel width of 3 with group norm. We use three of these layers, producing an effective receptive field size of 13 frames. The final output of the temporal encoder has the same feature dimension as ?. Our hallucinator contains two fully-connected layers of size 2048 with skip connection. Please see the supplementary material for more details.</p><p>Datasets: Human3.6M <ref type="bibr" target="#b27">[28]</ref> is the only dataset with ground truth 3D annotations that we train on. It consists of motion capture sequences of actors performing tasks in a controlled lab environment. We follow the standard protocol <ref type="bibr" target="#b30">[31]</ref> and train on 4 subjects (S1, S6, S7, S8) and test on 2 subjects (S9, S11) with 1 subject (S5) as the validation set.</p><p>For in-the-wild video datasets with 2D ground truth pose annotations, we use the Penn Action <ref type="bibr" target="#b63">[64]</ref> dataset and our own NBA dataset. Penn Action consists of 15 sports actions, with 1257 training videos and 1068 test. We set aside 10% of the test set as validation. The NBA dataset contains videos of basketball players attempting 3-point shots in 16 basketball games. Each sequence contains one set of 2D annotations for a single player. We split the dataset into 562 training videos, 64 validation, and 151 test. Finally, we also experiment with the new pseudo-ground truth 2D datasets (Section 4). See <ref type="table" target="#tab_1">Table 1</ref> for the summary of each dataset. Unless otherwise indicated, all models are trained with Hu-man3.6M, Penn Action, and NBA.</p><p>We evaluate our approach on the recent 3D Poses in the Wild dataset (3DPW) <ref type="bibr" target="#b53">[54]</ref>, which contains 61 sequences (25 train, 25 test, 12 val) of indoor and outdoor activities. Portable IMUs provide ground truth 3D annotations on challenging in-the-wild videos. To remain comparable to existing methods, we do not train on 3DPW and only used it as a test set. For evaluations on all datasets, we skip frames that have fewer than 6 visible keypoints.</p><p>As our goal is not human detection, we assume a temporal tube of human detections is available. We use ground truth 2D bounding boxes if available, and otherwise use the output of OpenPose to obtain a temporally smooth tube of human detections. All images are scaled to 224x224 where the humans are roughly scaled to be 150px in height.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We first evaluate the efficacy of the learned temporal representation and compare the model to local approaches that only use a single image. We also compare our approaches to state-of-the-art 3D pose methods on 3DPW. We then evaluate the effectiveness of training on pseudoground truth 2D poses. Finally, we quantitatively evaluate the dynamics prediction from a static image on Hu-man3.6M. We show qualitative results on video prediction in <ref type="figure">Figure 3</ref> and static image dynamics prediction in <ref type="figure">Figure 1 and 4</ref>. Please see the supplementary for more ablations, metrics, and discussion of failure modes. In ad-dition, a video with more of our results is available at https://youtu.be/9fNKSZdsAG8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Local vs Temporal Context</head><p>We first evaluate the proposed temporal encoder by comparing with a single-view approach that only sees a local window of one frame. As the baseline for the local window, we use a model similar to <ref type="bibr" target="#b30">[31]</ref>, re-trained on the same training data for a fair comparison. We also run an ablation by training our model with our temporal encoder but without the dynamics predictions f ?t .</p><p>In order to measure smooth predictions, we propose an acceleration error, which measures the average difference between ground truth 3D acceleration and predicted 3D acceleration of each joint in mm/s 2 . This can be computed on 3DPW where ground truth 3D joints are available. On 2D datasets, we simply report the acceleration in mm/s 2 .</p><p>We also report other standard metrics. For 3DPW, we report the mean per joint position error (MPJPE) and the MPJPE after Procrustes Alignment (PA-MPJPE). Both are measured in millimeters. On datasets with only 2D ground truth, we report accuracy in 2D pose via percentage of correct keypoints <ref type="bibr" target="#b61">[62]</ref> with ? = 0.05.</p><p>We report the results on three datasets in <ref type="table" target="#tab_3">Table 2</ref>. Overall, we find that our method produces modest gains in 3D pose estimation, large gains in 2D, and a very significant improvement in acceleration error. The temporal context helps to resolve ambiguities, producing smoother, temporally consistent results. Our ablation study shows that access to temporal context alone is not enough; using the auxiliary dynamics loss is important to force the network to learn the dynamics of the human.</p><p>Comparison to state-of-the-art approaches. In <ref type="table" target="#tab_4">Table 3</ref>, we compare our approach to other state-of-the-art methods. None of the approaches train on 3DPW. Note that Martinez et al. <ref type="bibr" target="#b36">[37]</ref> performs well on the Human3.6M benchmark but achieves the worst performance on 3DPW, showing that methods trained exclusively on Human3.6M do not generalize to in-the-wild images. We also compare our approach to TP-Net, a recently-proposed semisupervised approach that is trained on Human3.6M and MPII 2D pose in-the-wild dataset <ref type="bibr" target="#b4">[5]</ref>. TP-Net also learns a temporal smoothing network supervised on Human3.6M. While this approach is highly competitive on Human3.6M, our approach significantly out-performs TP-Net on in-thewild video. We only compare feed-forward approaches and not methods that smooth the 3D predictions via postoptimization. Such post-processing methods are complementary to feed-forward approaches and would benefit any of the approaches.     <ref type="table">Table 4</ref>: Learning from unlabeled video via pseudo ground truth 2D pose. We collected our own 2D pose datasets by running OpenPose on unlabeled video. Training with these pseudo-ground truth datasets induces significant improvements across the board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Training on pseudo-ground truth 2D pose</head><p>Here we report results of models trained on the two Internet-scale datasets we collected with pseudo-ground truth 2D pose annotations (See <ref type="table">Table 4</ref>). We find that the adding more data monotonically improves the model performance both in terms of 3D pose and 2D pose reprojection error. Using the largest dataset, InstaVariety, 3D pose error reduces by 9% and 2D pose accuracy increases by 8% on 3DPW. We see a small improvement or no change on 2D datasets. It is encouraging to see that not just 2D but also 3D pose improves from pseudo-ground truth 2D pose annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Predicting dynamics</head><p>We quantitatively evaluate our static image to 3D dynamics prediction. Since there are no other methods that predict 3D poses from 2D images, we propose two baselines: a constant baseline that outputs the current frame prediction for both past and future, and an Oracle Nearest Neighbors baseline. We evaluate our method on Human3.6M and compare</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Past</head><p>Current Future PA-MPJPE ? PA-MPJPE ? PA-MPJPE ? <ref type="table">Table 5</ref>: Evaluation of dynamic prediction on Human3.6M. The Nearest Neighbors baseline uses the pose in the training set with the lowest PA-MPJPE with the ground truth current pose to make past and future predictions. The constant baseline uses the current prediction as the future and past predictions. Ours 1 is the prediction model with Eq. 3, Ours 2 is that without Eq. 3.</p><p>with both baselines in <ref type="table">Table 5</ref>. Clearly, predicting dynamics from a static image is a challenging task due to inherent ambiguities in pose and the stochasticity of motion. Our approach works well for ballistic motions in which there is no ambiguity in the direction of the motion. When it's not clear if the person is going up or down our model learns to predict no change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>We propose an end-to-end model that learns a model of 3D human dynamics that can 1) obtain smooth 3D prediction from video and 2) hallucinate 3D dynamics on single images at test time. We train a simple but effective temporal encoder from which the current 3D human body as well as how the 3D pose changes can be estimated. Our approach can be trained on videos with 2D pose annotations in a semi-supervised manner, and we show empirically that our model can improve from training on an Internet-scale dataset with pseudo-ground truth 2D poses. While we show promising results, much more remains to be done in recovering 3D human body from video. Upcoming challenges include dealing with occlusions and interactions between multiple people.  <ref type="figure" target="#fig_15">Figure 5</ref> visualizes the architecture of our temporal encoder f movie . Each 1D convolution has temporal kernel size 3 and filter size 2048. For group norm, we use 32 groups, where each group has 64 channels. We repeat the residual block 3 times, which gives us a field of view of 13 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Encoder</head><p>Hallucinator Our hallucinator consists of two fullyconnected layers of filter size 2048, whose output gets added to the original ? as a skip connection.</p><p>3D regressors Our f 3D regresses the 85D ? t vector in an iterative error feedback (IEF) loop <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31]</ref>, where the current estimates are progressively updated by the regressor. Specifically, the regressor takes in the current image feature ? t and current parameter estimate ? t . This loop is repeated 3 times. We initialize the ? (0) t to be the mean values?, which we also update as a part of the learned parameter.</p><p>The regressor consists of two fully-connected layers, both with 1024 neurons, with a dropout layer in between, followed by a final layer that outputs the 85D outputs. All weights are shared.</p><p>The dynamics predictors f ??t has a similar form, except it only outputs the 72-D changes in pose ?, and the initial estimate is set to the prediction of the current frame t, i.e. ? (0) t+?t = ? t . Each f ??t learns a separate set of weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Additional Ablations and Evaluations</head><p>In <ref type="table">Table 6</ref>, we evaluate our method and comparable methods on 2D/3D pose and 3D shape recovery. We provide another ablation of our approach where the constant shape loss (Eq. 1) is not used (Ours -Const). In addition, we include full results from our ablation studies.</p><p>Shape Evaluation To measure shape predictions, we report Posed Mesh Error (Mesh Pos), which computes the mean Euclidean distance between the predicted and ground truth 3D meshes. Since this metric is affected by the quality of the pose predictions, we also report Unposed Mesh Error (Mesh Unp), which computes the same but with a fixed T-pose to evaluate shape independently of pose accuracy. Both metrics are in units of mm. Note that accurately capturing the shape of the subject is challenging since only 4 ground truth shapes are available in Human3.6M when training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Failure Modes</head><p>While our experiments show promising results, there is still room for improvement.</p><p>Smoothing Overall, our method obtains smooth results, but it can struggle in challenging situations, such as personto-person occlusions or fast motions. Additionally, extreme or rare poses (e.g. stretching, ballet) are difficult to capture. Please refer to our supplementary video for examples. Dynamics Prediction Clearly, predicting the past and future dynamics from a single image is a challenging problem. Even for us humans, from a single image alone, many motions are ambiguous. <ref type="figure">Figure 6</ref> visualizes a canonical example of such ambiguity, where it is unclear from the input, center image, if she is about to raise her arms or lower them. In these cases, our model learns to predict constant pose.</p><p>Furthermore, even the pose in a single image can be ambiguous, for example due to motion blur in videos. <ref type="figure">Figure 7</ref> illustrates a typical example, where the tennis player's arm has disappeared and therefore the model cannot discern whether the person is facing left or right. When the current frame prediction is poor, the resulting dynamics predictions are also not correct, since the dynamics predictions are initialized from the pose of the current frame.</p><p>Note that incorporating temporal context resolves many of these static-image ambiguities. Please see our included supplementary video for examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Effects of pseudo-ground truth dataset on local vs temporal models</head><p>Here we look further into the effects of adding the largescale, unlabeled InstaVariety dataset with pseudo-ground truth 2D annotations. The main paper showed that adding the large scale InstaVariety dataset with pseudo-ground truth 2D labels improved the performance significantly. Here we analyze how much the temporal context helps in learning from noisy pseudo-ground truth annotations, by training the local, single-view model also with the extra In-staVariety dataset. The results are shown in <ref type="table">Table 7</ref>. While both models benefit from pseudo-ground truth datasets, we find the relative improvement is significantly higher for the temporal model. This suggests that temporal context allows the model to be more robust toward bad, outlier pseudoground truth 2D labels.  <ref type="table">Table 6</ref>: Evaluation of baselines, ablations, and our proposed method on 2D and 3D keypoints and 3D mesh. We compare with three other feed-forward methods that predict 3D joints. None of the models are trained on 3DPW, all of the models are trained on H3.6M, and only our models are trained on Penn Action (TP-Net also uses MPII 2D dataset). We show that training with pseudo-ground truth 2D annotations significantly improves 2D and 3D predictions on the in-the-wild video dataset 3DPW. Single-view is retrained on our data. Ours -Dynamics is trained without the past and future regressors f??t.  <ref type="table">Table 7</ref>: Effects of using unlabeled video via pseudo ground truth 2D pose on local (single-view) vs temporal context models. While both local and temporal context models benefit from training with additional pseudo-ground truth datasets in general, the gain is more significant with the model with temporal context. A possible explanation is that the temporal context acts as a regularizer that allows it to be more robust to outlier poor 2D pseudo-ground labels. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>t</head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y x Z t j 7 s z T s j p t p F L n L D S b h v Z R T 0 = " &gt; A A A B 9 n i c b Z D L S s N A F I Z P v N Z 4 q w p u 3 A R L w V V J 3 O i y 6 M Z l C / Y C T a y T y b Q d O p m E m R O x h L 6 H K 0 F B 3 P o u r t z 4 L E 4 v C 2 3 9 Y e D j P + d w z v x h K r h G 1 / 2 y V l b X 1 j c 2 C 1 v 2 9 s 7 u 3 n 7 x 4 L C p k 0 x R 1 q C J S F Q 7 J J o J L l k D O Q r W T h U j c S h Y K x x e T + q t B 6 Y 0 T + Q t j l I W x K Q v e Y 9 T g s a 6 8 5 G L i O V + b c D H X e w W S 2 7 F n c p Z B m 8 O p e p x / f s e A G r d 4 q c f J T S L m U Q q i N Y d z 0 0 x y I l C T g U b 2 2 U / 0 y w l d E j 6 r G N Q k p j p I J + e P X b K x o m c X q L M k + h M X f v X R E 5 i r U d x a D p j g g O 9 W J u Y / 9 U 6 G f Y u g 5 z L N E M m 6 W x R L x M O J s 4 k A y f i i l E U I w O E K m 6 O d e i A K E L R J G W b F L z F P y 9 D 8 7 z i G a 6 b O K 5 g p g K c w C m c g Q c X U I U b q E E D K C h 4 g h d 4 t R 6 t Z + v N e p + 1 r l j z m S P 4 I + v j B 9 e G l M g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c O x f 9 e q m O E G 1 r W N Y U R G i h H b W y g 4 = " &gt; A A A B 9 n i c b Z D L S s N A F I Y n X m u 8 V Q U 3 b g Z L w V V J 3 O i y 1 I 3 L F u w F m l g m k 0 k 7 d D I J M y d i C X 0 P V 4 K C u B J 8 C N / A l R u f x e l l o a 0 / D H z 8 5 x z O m T 9 I B d f g O F / W y u r a + s Z m Y c v e 3 t n d 2 y 8 e H L Z 0 k i n K m j Q R i e o E R D P B J W s C B 8 E 6 q W I k D g R r B 8 O r S b 1 9 x 5 T m i b y B U c r 8 m P Q l j z g l Y K x b D 7 g I W e 7 V B 3 z c g 1 6 x 5 F S c q f A y u H M o V Y 8 b 3 / y t 9 l H v F T + 9 M K F Z z C R Q Q b T u u k 4 K f k 4 U c C r Y 2 C 5 7 m W Y p o U P S Z 1 2 D k s R M + / n 0 7 D E u G y f E U a L M k 4 C n r v 1 r I i e x 1 q M 4 M J 0 x g Y F e r E 3 M / 2 r d D K J L P + c y z Y B J O l s U Z Q J D g i c Z 4 J A r R k G M D B C q u D k W 0 w F R h I J J y j Y p u I t / X o b W e c U 1 3 D B x 1 N B M B X S C T t E Z c t E F q q J r V E d N R J F C D + g J P V v 3 1 q P 1 Y r 3 O W l e s + c w R + i P r / Q c q k p a E &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c O x f 9 e q m O E G 1 r W N Y U R G i h H b W y g 4 = " &gt; A A A B 9 n i c b Z D L S s N A F I Y n X m u 8 V Q U 3 b g Z L w V V J 3 O i y 1 I 3 L F u w F m l g m k 0 k 7 d D I J M y d i C X 0 P V 4 K C u B J 8 C N / A l R u f x e l l o a 0 / D H z 8 5 x z O m T 9 I B d f g O F / W y u r a + s Z m Y c v e 3 t n d 2 y 8 e H L Z 0 k i n K m j Q R i e o E R D P B J W s C B 8 E 6 q W I k D g R r B 8 O r S b 1 9 x 5 T m i b y B U c r 8 m P Q l j z g l Y K x b D 7 g I W e 7 V B 3 z c g 1 6 x 5 F S c q f A y u H M o V Y 8 b 3 / y t 9 l H v F T + 9 M K F Z z C R Q Q b T u u k 4 K f k 4 U c C r Y 2 C 5 7 m W Y p o U P S Z 1 2 D k s R M + / n 0 7 D E u G y f E U a L M k 4 C n r v 1 r I i e x 1 q M 4 M J 0 x g Y F e r E 3 M / 2 r d D K J L P + c y z Y B J O l s U Z Q J D g i c Z 4 J A r R k G M D B C q u D k W 0 w F R h I J J y j Y p u I t / X o b W e c U 1 3 D B x 1 N B M B X S C T t E Z c t E F q q J r V E d N R J F C D + g J P V v 3 1 q P 1 Y r 3 O W l e s + c w R + i P r / Q c q k p a E &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C w r T q Z M G x B W y D A 1 k k u k X Z W S s 6 E U = " &gt; A A A B 9 n i c b Z B N S 8 N A E I Y n f t b 4 V f X o J V g K n k r i R Y 9 F L x 4 r 2 A 9 o Y t l s N u 3 S z S b s T s Q S + j 8 8 C Q r i 1 f / i y X / j t s 1 B W 1 9 Y e H h n h p l 9 w 0 x w j a 7 7 b a 2 t b 2 x u b V d 2 7 N 2 9 / Y P D 6 t F x R 6 e 5 o q x N U 5 G q X k g 0 E 1 y y N n I U r J c p R p J Q s G 4 4 v p n V u 4 9 M a Z 7 K e 5 x k L E j I U P K Y U 4 L G e v C R i 4 g V f m v E p w M c V G t u w 5 3 L W Q W v h B q U a g 2 q X 3 6 U 0 j x h E q k g W v c 9 N 8 O g I A o 5 F W x q 1 / 1 c s 4 z Q M R m y v k F J E q a D Y n 7 2 1 K k b J 3 L i V J k n 0 Z m 7 9 q + J g i R a T 5 L Q d C Y E R 3 q 5 N j P / q / V z j K + C g s s s R y b p Y l G c C w d T Z 5 a B E 3 H F K I q J A U I V N 8 c 6 d E Q U o W i S s k 0 K 3 v K f V 6 F z 0 f A M 3 7 m 1 5 n W Z R w V O 4 Q z O w Y N L a M I t t K A N F B Q 8 w y u 8 W U / W i / V u f S x a 1 6 x y 5 g T + y P r 8 A U c J k u M = &lt; / l a t e x i t &gt; t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z p z S F 7 c T T w C m J 5 I 3 o Z t S A c 9 l j a U = " &gt; A A A B 7 n i c b Z C 7 S g N B F I b P x l t c b 1 F L L Q Z D w C r s 2 m g Z t L F M w F w g W c L s Z D Y Z M 7 s 7 z J w V Q s g 7 W A k K Y u s j + B 5 W d j 6 K k 0 u h i T 8 M f P z n H O a c P 1 R S G P S 8 L y e 3 t r 6 x u Z X f d n d 2 9 / Y P C o d H D Z N m m v E 6 S 2 W q W y E 1 X I q E 1 1 G g 5 C 2 l O Y 1 D y Z v h 8 G Z a b z 5 w b U S a 3 O F I 8 S C m / U R E g l G 0 V q O j B q K L 3 U L R K 3 s z k V X w F 1 C s n H 7 U v g G g 2 i 1 8 d n o p y 2 K e I J P U m L b v K Q z G V K N g k k / c U i c z X F E 2 p H 3 e t p j Q m J t g P F t 3 Q k r W 6 Z E o 1 f Y l S G a u + 2 t i T G N j R n F o O 2 O K A 7 N c m 5 r / 1 d o Z R l f B W C Q q Q 5 6 w + U d R J g m m Z H o 7 6 Q n N G c q R B c q 0 s M s S N q C a M r Q J u T Y F f / n m V W h c l H 3 L N R v H N c y V h x M 4 g 3 P w 4 R I q c A t V q A O D e 3 i E Z 3 h x l P P k v D p v 8 9 a c s 5 g 5 h j 9 y 3 n 8 A G Q m R k g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 n Y B M l G f v e 6 u I 1 h O c Z r 3 w 4 r 4 N w A = " &gt; A A A B 7 n i c b Z D L S g M x F I b P e K 3 j r e p S k W A p u C o z b n R Z d O O y B X u B d i i Z N N P G Z j I h y Q h l 6 N K 9 K 0 F B 3 P o I f Q 9 X P o M v Y X p Z a O s P g Y / / n E P O + U P J m T a e 9 + W s r K 6 t b 2 z m t t z t n d 2 9 / f z B Y V 0 n q S K 0 R h K e q G a I N e V M 0 J p h h t O m V B T H I a e N c H A z q T c e q N I s E X d m K G k Q 4 5 5 g E S P Y W K v e l n 3 W M Z 1 8 w S t 5 U 6 F l 8 O d Q K J + M q 9 + P p + N K J / / Z 7 i Y k j a k w h G O t W 7 4 n T Z B h Z R j h d O Q W 2 6 m m E p M B 7 t G W R Y F j q o N s u u 4 I F a 3 T R V G i 7 B M G T V 3 3 1 0 S G Y 6 2 H c W g 7 Y 2 z 6 e r E 2 M f + r t V I T X Q U Z E z I 1 V J D Z R 1 H K k U n Q 5 H b U Z Y o S w 4 c W M F H M L o t I H y t M j E 3 I t S n 4 i z c v Q / 2 i 5 F u u 2 j i u Y a Y c H M M Z n I M P l 1 C G W 6 h A D Q j c w x O 8 w K s j n W f n z X m f t a 4 4 8 5 k j + C P n 4 w f 4 p Z L 4 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 n Y B M l G f v e 6 u I 1 h O c Z r 3 w 4 r 4 N w A = " &gt; A A A B 7 n i c b Z D L S g M x F I b P e K 3 j r e p S k W A p u C o z b n R Z d O O y B X u B d i i Z N N P G Z j I h y Q h l 6 N K 9 K 0 F B 3 P o I f Q 9 X P o M v Y X p Z a O s P g Y / / n E P O + U P J m T a e 9 + W s r K 6 t b 2 z m t t z t n d 2 9 / f z B Y V 0 n q S K 0 R h K e q G a I N e V M 0 J p h h t O m V B T H I a e N c H A z q T c e q N I s E X d m K G k Q 4 5 5 g E S P Y W K v e l n 3 W M Z 1 8 w S t 5 U 6 F l 8 O d Q K J + M q 9 + P p + N K J / / Z 7 i Y k j a k w h G O t W 7 4 n T Z B h Z R j h d O Q W 2 6 m m E p M B 7 t G W R Y F j q o N s u u 4 I F a 3 T R V G i 7 B M G T V 3 3 1 0 S G Y 6 2 H c W g 7 Y 2 z 6 e r E 2 M f + r t V I T X Q U Z E z I 1 V J D Z R 1 H K k U n Q 5 H b U Z Y o S w 4 c W M F H M L o t I H y t M j E 3 I t S n 4 i z c v Q / 2 i 5 F u u 2 j i u Y a Y c H M M Z n I M P l 1 C G W 6 h A D Q j c w x O 8 w K s j n W f n z X m f t a 4 4 8 5 k j + C P n 4 w f 4 p Z L 4 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A 9 v 7 Y h o 0 J 3 V y c l 7 k 4 q n D p j z a C K s = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v G r 6 p H L 4 u l 4 K k k X v R Y 9 O K x g v 2 A N p T N d t O u 3 W z C 7 k Q o o f / B k 6 A g X v 0 / n v w 3 b t s c t P W F h Y d 3 Z t i Z N 0 y l M O h 5 3 0 5 p Y 3 N r e 6 e 8 6 + 7 t H x w e V Y 5 P 2 i b J N O M t l s h E d 0 N q u B S K t 1 C g 5 N 1 U c x q H k n f C y e 2 8 3 n n i 2 o h E P e A 0 5 U F M R 0 p E g l G 0 V r u f j s U A B 5 W q V / c W I u v g F 1 C F Q s 1 B 5 a s / T F g W c 4 V M U m N 6 v p d i k F O N g k k + c 2 v 9 z P C U s g k d 8 Z 5 F R W N u g n y x 7 o z U r D M k U a L t U 0 g W r v t r I q e x M d M 4 t J 0 x x b F Z r c 3 N / 2 q 9 D K P r I B c q z Z A r t v w o y i T B h M x v J 0 O h O U M 5 t U C Z F n Z Z w s Z U U 4 Y 2 I d e m 4 K / e v A 7 t y 7 p v + d 6 r N m 6 K P M p w B u d w A T 5 c Q Q P u o A k t Y P A I z / A K b 0 7 q v D j v z s e y t e Q U M 6 f w R 8 7 n D w b K j 0 0 = &lt; / l a t e x i t &gt; ? t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 E r 9 x p h 2 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t 1 e o 0 R R 9 r H f r H k V t y 5 y D p 4 S y h V z + r f D w B Q 6 x c / e 4 O Y p R F X y C Q 1 p u u 5 C f o Z 1 S i Y 5 N N C u Z c a n l A 2 p k P e t a h o x I 2 f z R e e k r J 1 B i S M t X 0 K y d w t / J r I a G T M J A p s Z 0 R x Z F Z r M / O / W j f F 8 M b P h E p S 5 I o t P g p T S T A m s + v J Q G j O U E 4 s U K a F X Z a w E d W U o c 2 o Y F P w V m 9 e h 9 Z V x b N c t 3 H c w k J 5 O I c L u A Q P r q E K 9 1 C D J j C Q 8 A Q v 8 O p o 5 9 l 5 c 9 4 X r T l n O X M K f + R 8 / A D 4 3 5 H 7 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T + 4 n Y m w 7 D / g E y k u j f S a l V j c j 4 V 8 = " &gt; A A A B 8 H i c b Z D J S g N B E I Z 7 4 h b H L S p 4 8 d I Y A p 7 C j B c 9 h n j x m E A 2 S I b Q 0 6 l J m v Q s d N c I Y c h L e B I U x K v g W / g G n r z 4 L H a W g y b + 0 P D x V x V d 9 f u J F B o d 5 8 v K b W x u b e / k d + 2 9 / Y P D o 8 L x S U v H q e L Q 5 L G M V c d n G q S I o I k C J X Q S B S z 0 J b T 9 8 e 2 s 3 r 4 H p U U c N X C S g B e y Y S Q C w R k a q 9 N r j A B Z H / u F o l N 2 5 q L r 4 C 6 h W D m r f 4 v 3 6 k e t X / j s D W K e h h A h l 0 z r r u s k 6 G V M o e A S p n a p l 2 p I G B + z I X Q N R i w E 7 W X z h a e 0 Z J w B D W J l X o R 0 7 t q / J j I W a j 0 J f d M Z M h z p 1 d r M / K / W T T G 4 8 T I R J S l C x B c f B a m k G N P Z 9 X Q g F H C U E w O M K 2 G W p X z E F O N o M r J N C u 7 q z e v Q u i q 7 h u s m j i p Z K E / O y Q W 5 J C 6 5 J h V y R 2 q k S T i R 5 I E 8 k W d L W Y / W i / W 6 a M 1 Z y 5 l T 8 k f W 2 w 9 L 6 5 O 3 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T + 4 n Y m w 7 D / g E y k u j f S a l V j c j 4 V 8 = " &gt; A A A B 8 H i c b Z D J S g N B E I Z 7 4 h b H L S p 4 8 d I Y A p 7 C j B c 9 h n j x m E A 2 S I b Q 0 6 l J m v Q s d N c I Y c h L e B I U x K v g W / g G n r z 4 L H a W g y b + 0 P D x V x V d 9 f u J F B o d 5 8 v K b W x u b e / k d + 2 9 / Y P D o 8 L x S U v H q e L Q 5 L G M V c d n G q S I o I k C J X Q S B S z 0 J b T 9 8 e 2 s 3 r 4 H p U U c N X C S g B e y Y S Q C w R k a q 9 N r j A B Z H / u F o l N 2 5 q L r 4 C 6 h W D m r f 4 v 3 6 k e t X / j s D W K e h h A h l 0 z r r u s k 6 G V M o e A S p n a p l 2 p I G B + z I X Q N R i w E 7 W X z h a e 0 Z J w B D W J l X o R 0 7 t q / J j I W a j 0 J f d M Z M h z p 1 d r M / K / W T T G 4 8 T I R J S l C x B c f B a m k G N P Z 9 X Q g F H C U E w O M K 2 G W p X z E F O N o M r J N C u 7 q z e v Q u i q 7 h u s m j i p Z K E / O y Q W 5 J C 6 5 J h V y R 2 q k S T i R 5 I E 8 k W d L W Y / W i / W 6 a M 1 Z y 5 l T 8 k f W 2 w 9 L 6 5 O 3 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m + i j 0 O 8 y L b o S e L Z 1 P + o o z 2 H D u 7 Y = " &gt; A A A B 8 H i c b Z B N S 8 N A E I Y n 9 a v G r 6 p H L 4 u l 4 K k k X v R Y 9 O K x Q r + g D W W z 3 b R L N 5 u w O x F K 6 J / w J C i I V / + O J / + N 2 z Y H b X 1 h 4 e G d G X b m D V M p D H r e t 1 P a 2 t 7 Z 3 S v v u w e H R 8 c n l d O z j k k y z X i b J T L R v Z A a L o X i b R Q o e S / V n M a h 5 N 1 w e r + o d 5 + 4 N i J R L Z y l P I j p W I l I M I r W 6 g 1 a E 4 5 0 i M N K 1 a t 7 S 5 F N 8 A u o Q q H m s P I 1 G C U s i 7 l C J q k x f d 9 L M c i p R s E k n 7 u 1 Q W Z 4 S t m U j n n f o q I x N 0 G + X H h O a t Y Z k S j R 9 i k k S 9 f 9 N Z H T 2 J h Z H N r O m O L E r N c W 5 n + 1 f o b R b Z A L l W b I F V t 9 F G W S Y E I W 1 5 O R 0 J y h n F m g T A u 7 L G E T q i l D m 5 F r U / D X b 9 6 E z n X d t / z o V R t 3 R R 5 l u I B L u A I f b q A B D 9 C E N j C Q 8 A y v 8 O Z o 5 8 V 5 d z 5 W r S W n m D m H P 3 I + f w B o Y p A W &lt; / l a t e x i t &gt; L 2D &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O y r V I T t k E M B 1 R F I 6 B h I 7 8 a e 7 H b o = " &gt; A A A B 9 X i c b Z C 7 S g N B F I b P e o 3 r L S r Y 2 C y G g F X Y T a N l U A s L i w T M B Z I l z k 5 m k y G z F 2 f O q m H Z 5 7 A S F M T W h 7 G y 8 V m c X A p N / G H g 4 z / n c M 7 8 X i y 4 Q t v + M p a W V 1 b X 1 n M b 5 u b W 9 s 5 u f m + / o a J E U l a n k Y h k y y O K C R 6 y O n I U r B V L R g J P s K Y 3 v B j X m / d M K h 6 F N z i K m R u Q f s h 9 T g l q y 7 3 u p h 1 k j 5 i W L 7 O s m y / Y J X s i a x G c G R Q q h 7 X v W w C o d v O f n V 5 E k 4 C F S A V R q u 3 Y M b o p k c i p Y J l Z 7 C S K x Y Q O S Z + 1 N Y Y k Y M p N J 1 d n V l E 7 P c u P p H 4 h W h P X / D W R k k C p U e D p z o D g Q M 3 X x u Z / t X a C / p m b 8 j B O k I V 0 u s h P h I W R N Y 7 A 6 n H J K I q R B k I l 1 8 d a d E A k o a i D M n U K z v y f F 6 F R L j m a a z q O c 5 g q B 0 d w D C f g w C l U 4 A q q U A c K d / A E L / B q P B j P x p v x P m 1 d M m Y z B / B H x s c P 4 8 e U O g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k h x d S C 0 f C W 5 v Z g J R / J Y h + q Z 0 E B k = " &gt; A A A B 9 X i c b Z C 7 S g N B F I Z n v c Z 4 i w o 2 N o M h Y B V 2 0 2 g Z o o W F R Q L m A s k S Z i e z y Z D Z 2 X X m r B q W f Q 4 r Q U H s x J f w D a x s f B Y n l 0 I T f x j 4 + M 8 5 n D O / F w m u w b a / r K X l l d W 1 9 c x G d n N r e 2 c 3 t 7 f f 0 G G s K K v T U I S q 5 R H N B J e s D h w E a 0 W K k c A T r O k N z 8 f 1 5 i 1 T m o f y G k Y R c w P S l 9 z n l I C x 3 K t u 0 g F 2 D 0 n p I k 2 7 u b x d t C f C i + D M I F 8 + r H 3 z t 8 p H t Z v 7 7 P R C G g d M A h V E 6 7 Z j R + A m R A G n g q X Z Q i f W L C J 0 S P q s b V C S g G k 3 m V y d 4 o J x e t g P l X k S 8 M T N / p p I S K D 1 K P B M Z 0 B g o O d r Y / O / W j s G / 8 x N u I x i Y J J O F / m x w B D i c Q S 4 x x W j I E Y G C F X c H I v p g C h C w Q S V N S k 4 8 3 9 e h E a p 6 B i u m T g q a K o M O k L H 6 A Q 5 6 B S V 0 S W q o j q i 6 A Y 9 o C f 0 b N 1 Z j 9 a L 9 T p t X b J m M w f o j 6 z 3 H z b T l f Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k h x d S C 0 f C W 5 v Z g J R / J Y h + q Z 0 E B k = " &gt; A A A B 9 X i c b Z C 7 S g N B F I Z n v c Z 4 i w o 2 N o M h Y B V 2 0 2 g Z o o W F R Q L m A s k S Z i e z y Z D Z 2 X X m r B q W f Q 4 r Q U H s x J f w D a x s f B Y n l 0 I T f x j 4 + M 8 5 n D O / F w m u w b a / r K X l l d W 1 9 c x G d n N r e 2 c 3 t 7 f f 0 G G s K K v T U I S q 5 R H N B J e s D h w E a 0 W K k c A T r O k N z 8 f 1 5 i 1 T m o f y G k Y R c w P S l 9 z n l I C x 3 K t u 0 g F 2 D 0 n p I k 2 7 u b x d t C f C i + D M I F 8 + r H 3 z t 8 p H t Z v 7 7 P R C G g d M A h V E 6 7 Z j R + A m R A G n g q X Z Q i f W L C J 0 S P q s b V C S g G k 3 m V y d 4 o J x e t g P l X k S 8 M T N / p p I S K D 1 K P B M Z 0 B g o O d r Y / O / W j s G / 8 x N u I x i Y J J O F / m x w B D i c Q S 4 x x W j I E Y G C F X c H I v p g C h C w Q S V N S k 4 8 3 9 e h E a p 6 B i u m T g q a K o M O k L H 6 A Q 5 6 B S V 0 S W q o j q i 6 A Y 9 o C f 0 b N 1 Z j 9 a L 9 T p t X b J m M w f o j 6 z 3 H z b T l f Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M f e c K P 0 L 8 9 F T G D w L y z h l J + Z b K + E = " &gt; A A A B 9 X i c b Z B N S 8 N A E I Y 3 f t b 4 V f X o J V g K n k r S i x 6 L e v D g o Y L 9 g D a U z X b a L t 1 s 4 u 5 E L S G / w 5 O g I F 7 9 M Z 7 8 N 2 7 b H L T 1 h Y W H d 2 a Y 2 T e I B d f o u t / W y u r a + s Z m Y c v e 3 t n d 2 y 8 e H D Z 1 l C g G D R a J S L U D q k F w C Q 3 k K K A d K 6 B h I K A V j C + n 9 d Y D K M 0 j e Y e T G P y Q D i U f c E b R W P 5 N L + 0 i P G F a v c q y X r H k V t y Z n G X w c i i R X P V e 8 a v b j 1 g S g k Q m q N Y d z 4 3 R T 6 l C z g R k d r m b a I g p G 9 M h d A x K G o L 2 0 9 n V m V M 2 T t 8 Z R M o 8 i c 7 M t X 9 N p D T U e h I G p j O k O N K L t a n 5 X 6 2 T 4 O D c T 7 m M E w T J 5 o s G i X A w c q Y R O H 2 u g K G Y G K B M c X O s w 0 Z U U Y Y m K N u k 4 C 3 + e R m a 1 Y p n + N Y t 1 S 7 y P A r k m J y Q U + K R M 1 I j 1 6 R O G o S R e / J M X s m b 9 W i 9 W O / W x 7 x 1 x c p n j s g f W Z 8 / U 0 q S V Q = = &lt; / l a t e x i t &gt; L adv prior &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d H v D Z O P u v j k 4 u S D F m J V r r z d s F n s = " &gt; A A A B / n i c b Z C 7 S g N B F I b P e o 3 r L V 4 6 m 8 E Y s A q 7 N l o G b S w s I p g L J G G Z n U y S I b M X Z s 4 G 4 x L w U a w E B b E T S 9 / B y g e x d 3 I p N P G H g Y / / n M M 5 8 / u x F B o d 5 8 t a W F x a X l n N r N n r G 5 t b 2 9 m d 3 Y q O E s V 4 m U U y U j W f a i 5 F y M s o U P J a r D g N f M m r f u 9 i V K / 2 u d I i C m 9 w E P N m Q D u h a A t G 0 V h e d v / K S x v I b z G l r T 6 J l Y j U c O h l c 0 7 B G Y v M g z u F X P H o + + 0 D A E p e 9 r P R i l g S 8 B C Z p F r X X S f G Z k o V C i b 5 0 M 4 3 E s 1 j y n q 0 w + s G Q x p w 3 U z H 5 w 9 J 3 j g t 0 o 6 U e S G S s W v / m k h p o P U g 8 E 1 n Q L G r Z 2 s j 8 7 9 a P c H 2 W T M V Y Z w g D 9 l k U T u R B C M y y o K 0 h O I M 5 c A A Z U q Y Y w n r U k U Z m s R s k 4 I 7 + + d 5 q J w U X M P X J o 5 z m C g D B 3 A I x + D C K R T h E k p Q B g Z 3 8 A B P 8 G z d W 4 / W i / U 6 a V 2 w p j N 7 8 E f W + w 9 y + Z i s &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l e E p f J S i 2 F V E X Z M B X L z v z L e d c A k = " &gt; A A A B / n i c b Z C 7 S g N B F I Z n v c a N l / X S 2 Q z G g F X Y t d E y a G N h E c F c I A n L 7 G Q 2 G T J 7 Y e Z s M C 4 L P o q V o C B 2 Y u 8 j W P k g W j u 5 F J r 4 w 8 D H f 8 7 h n P m 9 W H A F t v 1 p L C w u L a + s 5 t b M / P r G 5 p a 1 v V N T U S I p q 9 J I R L L h E c U E D 1 k V O A j W i C U j g S d Y 3 e u f j + r 1 A Z O K R + E 1 D G P W D k g 3 5 D 6 n B L T l W n u X b t o C d g M p 6 Q x w L H k k s 8 y 1 C n b J H g v P g z O F Q v n w 6 / V 9 k P + u u N Z H q x P R J G A h U E G U a j p 2 D O 2 U S O B U s M w s t h L F Y k L 7 p M u a G k M S M N V O x + d n u K i d D v Y j q V 8 I e O y a v y Z S E i g 1 D D z d G R D o q d n a y P y v 1 k z A P 2 2 n P I w T Y C G d L P I T g S H C o y x w h 0 t G Q Q w 1 E C q 5 P h b T H p G E g k 7 M 1 C k 4 s 3 + e h 9 p x y d F 8 p e M 4 Q x P l 0 D 4 6 Q E f I Q S e o j C 5 Q B V U R R b f o H j 2 i J + P O e D C e j Z d J 6 4 I x n d l F f 2 S 8 / Q B t E 5 o m &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l e E p f J S i 2 F V E X Z M B X L z v z L e d c A k = " &gt; A A A B / n i c b Z C 7 S g N B F I Z n v c a N l / X S 2 Q z G g F X Y t d E y a G N h E c F c I A n L 7 G Q 2 G T J 7 Y e Z s M C 4 L P o q V o C B 2 Y u 8 j W P k g W j u 5 F J r 4 w 8 D H f 8 7 h n P m 9 W H A F t v 1 p L C w u L a + s 5 t b M / P r G 5 p a 1 v V N T U S I p q 9 J I R L L h E c U E D 1 k V O A j W i C U j g S d Y 3 e u f j + r 1 A Z O K R + E 1 D G P W D k g 3 5 D 6 n B L T l W n u X b t o C d g M p 6 Q x w L H k k s 8 y 1 C n b J H g v P g z O F Q v n w 6 / V 9 k P + u u N Z H q x P R J G A h U E G U a j p 2 D O 2 U S O B U s M w s t h L F Y k L 7 p M u a G k M S M N V O x + d n u K i d D v Y j q V 8 I e O y a v y Z S E i g 1 D D z d G R D o q d n a y P y v 1 k z A P 2 2 n P I w T Y C G d L P I T g S H C o y x w h 0 t G Q Q w 1 E C q 5 P h b T H p G E g k 7 M 1 C k 4 s 3 + e h 9 p x y d F 8 p e M 4 Q x P l 0 D 4 6 Q E f I Q S e o j C 5 Q B V U R R b f o H j 2 i J + P O e D C e j Z d J 6 4 I x n d l F f 2 S 8 / Q B t E 5 o m &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q Y w f s N c m U z K S q 5 I b f i l 3 A T + H 8 M c = " &gt; A A A B / n i c b Z D L S s N A F I Y n X m u 8 x c v O z W A p u C q J G 1 0 W 3 b h w U c F e o A 1 h M p m 0 Q y c X Z k 6 K N Q R 8 F F e C g r j 1 P V z 5 N k 7 b L L T 1 h 4 G P / 5 z D O f P 7 q e A K b P v b W F l d W 9 / Y r G y Z 2 z u 7 e / v W w W F b J Z m k r E U T k c i u T x Q T P G Y t 4 C B Y N 5 W M R L 5 g H X 9 0 P a 1 3 x k w q n s T 3 M E m Z G 5 F B z E N O C W j L s 4 5 v v b w P 7 A F y E o x x K n k i i 8 K z q n b d n g k v g 1 N C F Z V q e t Z X P 0 h o F r E Y q C B K 9 R w 7 B T c n E j g V r D B r / U y x l N A R G b C e x p h E T L n 5 7 P w C 1 7 Q T 4 D C R + s W A Z 6 7 5 a y I n k V K T y N e d E Y G h W q x N z f 9 q v Q z C S z f n c Z o B i + l 8 U Z g J D A m e Z o E D L h k F M d F A q O T 6 W E y H R B I K O j F T p + A s / n k Z 2 u d 1 R / O d X W 1 c l X l U 0 A k 6 R W f I Q R e o g W 5 Q E 7 U Q R Y / o G b 2 i N + P J e D H e j Y 9 5 6 4 p R z h y h P z I + f w C l u p X d &lt; / l a t e x i t &gt; L 3D &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a T M 0 z 4 + Y I n 6 8 / d J K G n G u k K t q / 2 8 = " &gt; A A A B 9 X i c b Z C 7 S g N B F I b P e o 3 r L S r Y 2 C y G g F X Y 1 U L L o B Y W F g m Y C y R L n J 3 M J k N m L 8 6 c V c O y z 2 E l K I i t D 2 N l 4 7 M 4 u R S a + M P A x 3 / O 4 Z z 5 v V h w h b b 9 Z S w s L i 2 v r O b W z P W N z a 3 t / M 5 u X U W J p K x G I x H J p k c U E z x k N e Q o W D O W j A S e Y A 1 v c D G q N + 6 Z V D w K b 3 A Y M z c g v Z D 7 n B L U l n v d S d v I H j E 9 u c y y T r 5 g l + y x r H l w p l A o 7 1 e / b w G g 0 s l / t r s R T Q I W I h V E q Z Z j x + i m R C K n g m V m s Z 0 o F h M 6 I D 3 W 0 h i S g C k 3 H V + d W U X t d C 0 / k v q F a I 1 d 8 9 d E S g K l h o G n O w O C f T V b G 5 n / 1 V o J + m d u y s M 4 Q R b S y S I / E R Z G 1 i g C q 8 s l o y i G G g i V X B 9 r 0 T 6 R h K I O y t Q p O L N / n o f 6 c c n R X N V x n M N E O T i A Q z g C B 0 6 h D F d Q g R p Q u I M n e I F X 4 8 F 4 N t 6 M 9 0 n r g j G d 2 Y M / M j 5 + A O V P l D s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P v 2 5 M P a 7 m 1 V c z C q Y m X C U M O g 9 U / M = " &gt; A A A B 9 X i c b Z C 7 S g N B F I Z n v c Z 4 i w o 2 N o M h Y B V 2 t d A y R A s L i w T M B Z I l z E 5 m k y G z s + v M W T U s + x x W g o L Y i S / h G 1 j Z + C x O L o U m / j D w 8 Z 9 z O G d + L x J c g 2 1 / W Q u L S 8 s r q 5 m 1 7 P r G 5 t Z 2 b m e 3 r s N Y U V a j o Q h V 0 y O a C S 5 Z D T g I 1 o w U I 4 E n W M M b n I / q j V u m N A / l N Q w j 5 g a k J 7 n P K Q F j u V e d p A 3 s H p K T i z T t 5 P J 2 0 R 4 L z 4 M z h X x p v / r N 3 8 o f l U 7 u s 9 0 N a R w w C V Q Q r V u O H Y G b E A W c C p Z m C + 1 Y s 4 j Q A e m x l k F J A q b d Z H x 1 i g v G 6 W I / V O Z J w G M 3 + 2 s i I Y H W w 8 A z n Q G B v p 6 t j c z / a q 0 Y / D M 3 4 T K K g U k 6 W e T H A k O I R x H g L l e M g h g a I F R x c y y m f a I I B R N U 1 q T g z P 5 5 H u r H R c d w 1 c R R R h N l 0 A E 6 R E f I Q a e o h C 5 R B d U Q R T f o A T 2 h Z + v O e r R e r N d J 6 4 I 1 n d l D f 2 S 9 / w A 4 W 5 X 3 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P v 2 5 M P a 7 m 1 V c z C q Y m X C U M O g 9 U / M = " &gt; A A A B 9 X i c b Z C 7 S g N B F I Z n v c Z 4 i w o 2 N o M h Y B V 2 t d A y R A s L i w T M B Z I l z E 5 m k y G z s + v M W T U s + x x W g o L Y i S / h G 1 j Z + C x O L o U m / j D w 8 Z 9 z O G d + L x J c g 2 1 / W Q u L S 8 s r q 5 m 1 7 P r G 5 t Z 2 b m e 3 r s N Y U V a j o Q h V 0 y O a C S 5 Z D T g I 1 o w U I 4 E n W M M b n I / q j V u m N A / l N Q w j 5 g a k J 7 n P K Q F j u V e d p A 3 s H p K T i z T t 5 P J 2 0 R 4 L z 4 M z h X x p v / r N 3 8 o f l U 7 u s 9 0 N a R w w C V Q Q r V u O H Y G b E A W c C p Z m C + 1 Y s 4 j Q A e m x l k F J A q b d Z H x 1 i g v G 6 W I / V O Z J w G M 3 + 2 s i I Y H W w 8 A z n Q G B v p 6 t j c z / a q 0 Y / D M 3 4 T K K g U k 6 W e T H A k O I R x H g L l e M g h g a I F R x c y y m f a I I B R N U 1 q T g z P 5 5 H u r H R c d w 1 c R R R h N l 0 A E 6 R E f I Q a e o h C 5 R B d U Q R T f o A T 2 h Z + v O e r R e r N d J 6 4 I 1 n d l D f 2 S 9 / w A 4 W 5 X 3 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 v y Z W U t 9 Q w 8 W F F l K l x n h Z G L k / N 0 = " &gt; A A A B 9 X i c b Z B N S 8 N A E I Y 3 f t b 6 V f X o J V g K n k q i B z 0 W 9 e D B Q w X 7 A W 0 o m + 2 k X b r Z x N 2 J W k J + h y d B Q b z 6 Y z z 5 b 9 y 2 O W j r C w s P 7 8 w w s 6 8 f C 6 7 R c b 6 t p e W V 1 b X 1 w k Z x c 2 t 7 Z 7 e 0 t 9 / U U a I Y N F g k I t X 2 q Q b B J T S Q o 4 B 2 r I C G v o C W P 7 q c 1 F s P o D S P 5 B 2 O Y / B C O p A 8 4 I y i s b y b X t p F e M L 0 9 C r L e q W y U 3 W m s h f B z a F M c t V 7 p a 9 u P 2 J J C B K Z o F p 3 X C d G L 6 U K O R O Q F S v d R E N M 2 Y g O o G N Q 0 h C 0 l 0 6 v z u y K c f p 2 E C n z J N p T t / h r I q W h 1 u P Q N 5 0 h x a G e r 0 3 M / 2 q d B I N z L + U y T h A k m y 0 K E m F j Z E 8 i s P t c A U M x N k C Z 4 u Z Y m w 2 p o g x N U E W T g j v / 5 0 V o n l R d w 7 d O u X a R 5 1 E g h + S I H B O X n J E a u S Z 1 0 i C M 3 J N n 8 k r e r E f r x X q 3 P m a t S 1 Y + c 0 D + y P r 8 A V T S k l Y = &lt; / l a t e x i t &gt; || t ? t || &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E n v / U I 1 H 2 s e b U v C D W l f d o A b U y I 0 = " &gt; A A A C A 3 i c b Z C 7 S g N B F I b P x l u M t 6 i V 2 C w J A U E M u z Z a B m 0 s I 5 g L Z J d l d j J J h s x e m D k r h N 1 g 5 X v Y W A k K Y u t T W P k 2 T i 6 F J v 4 w 8 M 1 / z m H m / H 4 s u E L L + j Z y K 6 t r 6 x v 5 z c L W 9 s 7 u X n H / o K m i R F L W o J G I Z N s n i g k e s g Z y F K w d S 0 Y C X 7 C W P 7 y e 1 F v 3 T C o e h X c 4 i p k b k H 7 I e 5 w S 1 J Z X P M o y p z 7 g H p 4 5 y E W X p Z P b 2 M M s 8 4 p l q 2 p N Z S 6 D P Y d y r e S c P g F A 3 S t + O d 2 I J g E L k Q q i V M e 2 Y n R T I p F T w c a F i p M o F h M 6 J H 3 W 0 R i S g C k 3 n e 4 w N i v a 6 Z q 9 S O o T o j l 1 C 7 8 m U h I o N Q p 8 3 R k Q H K j F 2 s T 8 r 9 Z J s H f p p j y M E 2 Q h n T 3 U S 4 S J k T k J x O x y y S i K k Q Z C J d e f N e m A S E J R x 1 b Q K d i L O y 9 D 8 7 x q a 7 7 V c V z B T H k 4 h h K c g A 0 X U I M b q E M D K D z A M 7 z C m / F o v B j v x s e s N W f M Z w 7 h j 4 z P H 3 f n m X g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r v T f T 7 / k e 4 B i C R g 5 m W b i Q q A A J U c = " &gt; A A A C A 3 i c b Z D L S s N A F I Y n 9 V b r L e p K 3 I S W g i C W x I 0 u i 2 5 c V r A X a E K Y T K b t 0 M k k z J w I I S 2 u f A N f w Z W g I G 5 9 C l d 9 G 6 e X h V Z / G P j m P + c w c / 4 g 4 U y B b U + M w s r q 2 v p G c b O 0 t b 2 z u 2 f u H 7 R U n E p C m y T m s e w E W F H O B G 0 C A 0 4 7 i a Q 4 C j h t B 8 P r a b 1 9 T 6 V i s b i D L K F e h P u C 9 R j B o C 3 f P B q N 3 M a A + X D m A u M h z a e 3 s Q + j k W 9 W 7 J o 9 k / U X n A V U 6 m X 3 9 G l S z x q + + e W G M U k j K o B w r F T X s R P w c i y B E U 7 H p a q b K p p g M s R 9 2 t U o c E S V l 8 9 2 G F t V 7 Y R W L 5 b 6 C L B m b u n H R I 4 j p b I o 0 J 0 R h o F a r k 3 N / 2 r d F H q X X s 5 E k g I V Z P 5 Q L + U W x N Y 0 E C t k k h L g m Q Z M J N O f t c g A S 0 x A x 1 b S K T j L O / + F 1 n n N 0 X y r 4 7 h C c x X R M S q j E + S g C 1 R H N 6 i B m o i g B / S M X t G b 8 W i 8 G O / G x 7 y 1 Y C x m D t E v G Z / f g t e a / g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r v T f T 7 / k e 4 B i C R g 5 m W b i Q q A A J U c = " &gt; A A A C A 3 i c b Z D L S s N A F I Y n 9 V b r L e p K 3 I S W g i C W x I 0 u i 2 5 c V r A X a E K Y T K b t 0 M k k z J w I I S 2 u f A N f w Z W g I G 5 9 C l d 9 G 6 e X h V Z / G P j m P + c w c / 4 g 4 U y B b U + M w s r q 2 v p G c b O 0 t b 2 z u 2 f u H 7 R U n E p C m y T m s e w E W F H O B G 0 C A 0 4 7 i a Q 4 C j h t B 8 P r a b 1 9 T 6 V i s b i D L K F e h P u C 9 R j B o C 3 f P B q N 3 M a A + X D m A u M h z a e 3 s Q + j k W 9 W 7 J o 9 k / U X n A V U 6 m X 3 9 G l S z x q + + e W G M U k j K o B w r F T X s R P w c i y B E U 7 H p a q b K p p g M s R 9 2 t U o c E S V l 8 9 2 G F t V 7 Y R W L 5 b 6 C L B m b u n H R I 4 j p b I o 0 J 0 R h o F a r k 3 N / 2 r d F H q X X s 5 E k g I V Z P 5 Q L + U W x N Y 0 E C t k k h L g m Q Z M J N O f t c g A S 0 x A x 1 b S K T j L O / + F 1 n n N 0 X y r 4 7 h C c x X R M S q j E + S g C 1 R H N 6 i B m o i g B / S M X t G b 8 W i 8 G O / G x 7 y 1 Y C x m D t E v G Z / f g t e a / g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f L b j k Z p E v S O M f K 0 a y n c Z Y D Z z s e k = " &gt; A A A C A 3 i c b Z D L S s N A F I Y n X m u 8 R V 2 J m 2 A p u L E k b n R Z d O O y g r 1 A E 8 J k M m m H T i 7 M n A g l K a 5 8 F F e C g r j 1 K V z 5 N k 7 a L L T 1 h 4 F v / n M O M + f 3 U 8 4 k W N a 3 t r K 6 t r 6 x W d v S t 3 d 2 9 / a N g 8 O u T D J B a I c k P B F 9 H 0 v K W U w 7 w I D T f i o o j n x O e / 7 4 p q z 3 H q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>6 8 1 b p j S P 5 Q 1 k C f M j 0 p c 8 5 J S A s b p O s Q P s D v I B E S K l X B K I 1 a j r l N y K O x F e B G 8 G p f M P + y x 5 / L J r X e e z 0 4 t p G j E J V B C t 2 5 6 b g J 8 T B Z w K N r L L n V S z h N A h 6 b O 2 Q U k i p v 1 8 c v 0 I l 4 3 T w 2 G s z J O A J 6 7 9 a y I n k d Z Z F J j O i M B A z 9 f G 5 n + 1 d g r h q Z 9 z m a T A J J 0 u C l O B I c b j K H C P K 0 Z B Z A Y I V d w c i + m A K E L B B G a b F L z 5 P y 9 C 4 7 j i G b 5 2 S 9 U L N F U B H a B D d I Q 8 d I K q 6 A r V U B 1 R l K E H 9 I x e r H v r y X q 1 3 q a t S 9 Z s p o j + y H r / A a n P m P A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S R S e B m B g D C n B / y j C E L n J G f g 6 p d 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>6 8 1 b p j S P 5 Q 1 k C f M j 0 p c 8 5 J S A s b p O s Q P s D v I B E S K l X B K I 1 a j r l N y K O x F e B G 8 G p f M P + y x 5 / L J r X e e z 0 4 t p G j E J V B C t 2 5 6 b g J 8 T B Z w K N r L L n V S z h N A h 6 b O 2 Q U k i p v 1 8 c v 0 I l 4 3 T w 2 G s z J O A J 6 7 9 a y I n k d Z Z F J j O i M B A z 9 f G 5 n + 1 d g r h q Z 9 z m a T A J J 0 u C l O B I c b j K H C P K 0 Z B Z A Y I V d w c i + m A K E L B B G a b F L z 5 P y 9 C 4 7 j i G b 5 2 S 9 U L N F U B H a B D d I Q 8 d I K q 6 A r V U B 1 R l K E H 9 I x e r H v r y X q 1 3 q a t S 9 Z s p o j + y H r / A a n P m P A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + / g u J a G x E T o H X l T Q C / 8 o 9 4 P l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>z 8 o e J 4 A Z 8 / 9 s p b W x u b e + U d 9 2 9 / Y P D I + / 4 p G 1 U q i l r U S W U 7 o b E M M E l a w E H w b q J Z i Q O B e u E 0 5 t 5 v f P A t O F K 3 k O W s E F M x p J H n B K w 1 t C r 9 I E 9 Q j 4 h Q q S U S w J K z 4 Z e 1 a / 7 C + F 1 C A q o o k L N o f f V H y m a x k w C F c S Y X u A n M M i J B k 4 F m 7 m 1 f m p Y Q u i U j F n P o i Q x M 4 N 8 c f 0 M 1 6 w z w p H S 9 k n A C 9 f 9 N Z G T 2 J g s D m 1 n T G B i V m t z 8 7 9 a L 4 X o a p B z m a T A J F 0 u i l K B Q e F 5 F H j E N a M g M g u E a m 6 P x X R C N K F g A 3 N t C s H q n 9 e h f V E P L N / 5 1 c Z 1 k U c Z n a I z d I 4 C d I k a 6 B Y 1 U Q t R l K F n 9 I r e n C f n x X l 3 P p a t J a e Y q a A / c j 5 / A E X H l a 8 = &lt; / l a t e x i t &gt; f t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 y G k / o 0 B J J C 7 a m 6 j p s 8 + u t p h C P E = " &gt; A A A B 9 X i c b Z D J S g N B E I Z r X O O 4 R T 1 6 a Q 2 B X A w z X v Q Y 1 I P H C G a B Z A g 9 n Z 6 k S c 9 i d 4 0 S h j y H J 0 F B v P o K v o M n H 8 S 7 n e W g i T 8 0 f P x V R V X / f i K F R s f 5 s p a W V 1 b X 1 n M b 9 u b W 9 s 5 u f m + / r u N U M V 5 j s Y x V 0 6 e a S x H x G g q U v J k o T k N f 8 o Y / u B z X G / d c a R F H t z h M u B f S X i Q C w S g a y w s 6 2 U n 7 i k u k B E e d f M E p O x O R R X B n U K g c l b 4 / A K D a y X + 2 u z F L Q x 4 h k 1 T r l u s k 6 G V U o W C S j + x i O 9 U 8 o W x A e 7 x l M K I h 1 1 4 2 u X p E i s b p k i B W 5 k V I J q 7 9 a y K j o d b D 0 D e d I c W + n q + N z f 9 q r R S D c y 8 T U Z I i j 9 h 0 U Z B K g j E Z R 0 C 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>R 6 Q n e c g p A W P 5 Y S c 7 8 i 6 Y A I J h 1 C k U n Y o z E f 4 L 7 g y K 1 f 3 y 1 / u 9 d 1 j r F D 6 8 b k z T i E m g g m j d d p 0 E / I w o 4 F S w k V 3 y U s 0 S Q g e k x 9 o G J Y m Y 9 r P J 1 S N c M k 4 X h 7 E y T w K e u P a P i Y x E W g + j w H R G B P p 6 v j Y 2 / 6 u 1 U w h P / Y z L J A U m 6 X R R m A o M M R 5 H g L t c M Q p i a I B Q x c 2 x m P a J I h R M U L Z J w Z 3 / 8 1 9 o H F d c w 1 c m j j M 0 V R 7 t o Q N U R i 4 6 Q V V 0 i W q o j i i 6 Q Q / o C T 1 b d 9 a j 9 W K 9 T l t z 1 m x m F / 2 S 9 f Y N J u q V O w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + a e t L D M 4 Z Z m D R x I H y Y k o f G y d y f c = " &gt; A A A B 9 X i c b Z C 7 T s M w F I a d c i v h V m B k M V S V y k C V s M B Y A Q N j k e h F a q L K c Z 3 W q u M E + w R U R X 0 G R i Y k k B A r 7 8 A r M P E g 7 L i X A Q q / Z O n T f 8 7 R O f 6 D R H A N j v N p 5 R Y W l 5 Z X 8 q v 2 2 v r G 5 l Z h e 6 e h 4 1 R R V q e x i F U r I J o J L l k d O A j W S h Q j U S B Y M x i c j + v N W 6 Y 0 j + U 1 D B P m R 6 Q n e c g p A W P 5 Y S c 7 8 i 6 Y A I J h 1 C k U n Y o z E f 4 L 7 g y K 1 f 3 y 1 / u 9 d 1 j r F D 6 8 b k z T i E m g g m j d d p 0 E / I w o 4 F S w k V 3 y U s 0 S Q g e k x 9 o G J Y m Y 9 r P J 1 S N c M k 4 X h 7 E y T w K e u P a P i Y x E W g + j w H R G B P p 6 v j Y 2 / 6 u 1 U w h P / Y z L J A U m 6 X R R m A o M M R 5 H g L t c M Q p i a I B Q x c 2 x m P a J I h R M U L Z J w Z 3 / 8 1 9 o H F d c w 1 c m j j M 0 V R 7 t o Q N U R i 4 6 Q V V 0 i W q o j i i 6 Q Q / o C T 1 b d 9 a j 9 W K 9 T l t z 1 m x m F / 2 S 9 f Y N J u q V O w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b V r a S z m K e + E R I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>y m 7 V n Y k u g 5 d D m e S q 9 0 p f 3 X 7 M 0 w g U c s m M 6 X h u g n 7 G N A o u Y V K s d F M D C e M j N o C O R c U i M H 4 2 u 3 p C K 9 b p 0 z D W 9 i m k M 7 f 4 a y J j k T H j K L C d E c O h W a x N z f 9 q n R T D S z 8 T K k k R F J 8 v C l N J M a b T C G h f a O A o x x Y Y 1 8 I e S / m Q a c b R B l W 0 K X i L f 1 6 G 5 n n V s 3 z n l m t X e R 4 F c k x O y C n x y A W p k V t S J w 3 C y Q N 5 J q / k z X l y X p x 3 5 2 P e u u L k M 0 f k j 5 z P H 6 m F k e c = &lt; / l a t e x i t &gt; f t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 1 k O g Y S Q x R 2 6 M a 7 6 U Z H L T Q 6 R x N I = " &gt; A A A B 9 H i c b Z C 7 S g N B F I b P e o 3 r L W q p x W A I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>c a N I B B C o / w D C / W y H q y X q 2 3 e e u a t Z g 5 g T + y 3 n 8 A T 5 O T 9 Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b b w o 9 Q e Y G 3 6 o D 8 z I f Q p c 0 1 9 O 7 x M = " &gt; A A A B 9 H i c b Z D L S s N A F I Y n 9 V b j r e p S k c F S c F U S N 7 o s 6 s J l C / Y C T S i T 6 a Q d O p m E m Z N C C V 3 6 C q 4 E B X H r u u / h y m f w J Z x e F t r 6 w 8 D H f 8 7 h n P m D R H A N j v N l 5 d b W N z a 3 8 t v 2 z u 7 e / k H h 8 K i h 4 1 R R V q e x i F U r I J o J L l k d O A j W S h Q j U S B Y M x j c T u v N I V O a x / I B R g n z I 9 K T P O S U g L G 8 s J N 5 d 0 w A w T D u F I p O 2 Z k J r 4 K 7 g G L l d F L 7 f j y b V D u F T 6 8 b 0 z R i E q g g W r d d J w E / I w o 4 F W x s l 7 x U s 4 T Q A e m x t k F J I q b 9 b H b 0 G J e M 0 8 V h r M y T g G e u / W s i I 5 H W o y g w n R G B v l 6 u T c 3 / a u 0 U w m s / 4 z J J g U k 6 X x S m A k O M p w n g L l e M g h g Z I F R x c y y m f a I I B Z O T b V J w l / + 8 C o 3 L s m u 4 Z u K 4 Q X P l 0 Q k 6 R x f I R V e o g u 5 R F d U R R Q l 6 Q i / o 1 R p a z 9 a b 9 T 5 v z V m L m W P 0 R 9 b H D y 8 + l V s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b b w o 9 Q e Y G 3 6 o D 8 z I f Q p c 0 1 9 O 7 x M = " &gt; A A A B 9 H i c b Z D L S s N A F I Y n 9 V b j r e p S k c F S c F U S N 7 o s 6 s J l C / Y C T S i T 6 a Q d O p m E m Z N C C V 3 6 C q 4 E B X H r u u / h y m f w J Z x e F t r 6 w 8 D H f 8 7 h n P m D R H A N j v N l 5 d b W N z a 3 8 t v 2 z u 7 e / k H h 8 K i h 4 1 R R V q e x i F U r I J o J L l k d O A j W S h Q j U S B Y M x j c T u v N I V O a x / I B R g n z I 9 K T P O S U g L G 8 s J N 5 d 0 w A w T D u F I p O 2 Z k J r 4 K 7 g G L l d F L 7 f j y b V D u F T 6 8 b 0 z R i E q g g W r d d J w E / I w o 4 F W x s l 7 x U s 4 T Q A e m x t k F J I q b 9 b H b 0 G J e M 0 8 V h r M y T g G e u / W s i I 5 H W o y g w n R G B v l 6 u T c 3 / a u 0 U w m s / 4 z J J g U k 6 X x S m A k O M p w n g L l e M g h g Z I F R x c y y m f a I I B Z O T b V J w l / + 8 C o 3 L s m u 4 Z u K 4 Q X P l 0 Q k 6 R x f I R V e o g u 5 R F d U R R Q l 6 Q i / o 1 R p a z 9 a b 9 T 5 v z V m L m W P 0 R 9 b H D y 8 + l V s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N P f Z j m j T j u k c G w O y 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>0 y l M O i 6 3 8 7 G 5 t b 2 z m 5 p r 7 x / c H h 0 X D k 5 b Z s k 0 4 y 3 W C I T 3 Q 2 p 4 V I o 3 k K B k n d T z W k c S t 4 J x 3 f z e m f C t R G J e s J p y o O Y D p W I B K N o L T / q 5 / 4 9 l 0 g J z v q V q l t 3 F y L r 4 B V Q h U L N f u X L H y Q s i 7 l C J q k x P c 9 N M c i p R s E k n 5 V r f m Z 4 S t m Y D n n P o q I x N 0 G + O H p G a t Y Z k C j R 9 i k k C 7 f 8 a y K n s T H T O L S d M c W R W a 3 N z f 9 q v Q y j m y A X K s 2 Q K 7 Z c F G W S Y E L m C Z C B 0 J y h n F q g T A t 7 L G E j q i l D m 1 P Z p u C t / n k d 2 l d 1 z / K j W 2 3 c F n m U 4 B w u 4 B I 8 u I Y G P E A T W s A g h W d 4 h T d n 4 r w 4 7 8 7 H s n X D K W b O 4 I + c z x 8 9 V J G w &lt; / l a t e x i t &gt; ? t t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S D n i 4 M H N t D 3 W W 0 z f r d Y O m J Z q O J A = " &gt; A A A B / X i c b Z C 7 S k N B E I b n x F u M t 2 h K m 9 U g 2 C j n 2 G g Z 1 M I y Q m 6 Q h L B n M z F L 9 l z Y n S O E Q / B R r A Q F s b X 1 H a x 8 E H s 3 l 0 I T f 1 j 4 + G e G m f 3 9 W E l D r v v l Z J a W V 1 b X s u u 5 j c 2 t 7 Z 3 8 7 l 7 N R I k W W B W R i n T D 5 w a V D L F K k h Q 2 Y o 0 8 8 B X W / c H V u F 6 / R 2 1 k F F Z o G G M 7 4 H e h 7 E n B y V q d f K F V 6 S P x T k o n r W t U x B m N O v m i e + p O x B b B m 0 G x d H D 8 / Q E A 5 U 7 + s 9 W N R B J g S E J x Y 5 q e G 1 M 7 5Z q k U D j K H b U S g z E X A 3 6 H T Y s h D 9 C 0 0 8 n 1 I 3 Z k n S 7 r R d q + k N j E z f 2 a S H l g z D D w b W f A q W / m a 2 P z v 1 o z o d 5 F O 5 V h n B C G Y r q o l y h G E R t H w b p S o y A 1 t M C F l v Z Y J v p c c 0 E 2 s J x N w Z v / 8 y L U z k 4 9 y 7 c 2 j k u Y K g v 7 c A j H 4 M E 5 l O A G y l A F A U N 4 h G d 4 c R 6 c J + f V e Z u 2 Z p z Z T A H + y H n / A c j F l v I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s ha 1 _ b a s e 6 4 = " k 8 X u 3 d y 0 w j s V W S G g G 6 6 2 V A I g A i E = " &gt; A A A B / X i c b Z C 7 S g N B F I Z n 4 y 3 G W z S l z W g I x M K w a 6 N l U A v L C L l B N o T Z y U k y Z P b C z F k h L M F H 8 B G s B A W x t f c V r H w Q e y e X Q h N / G P j 4 z z m c M 7 8 X S a H R t r + s 1 M r q 2 v p G e j O z t b 2 z u 5 f d P 6 j r M F Y c a j y U o W p 6 T I M U A d R Q o I R m p I D 5 n o S G N 7 y a 1 B t 3 o L Q I g y q O I m j 7 r B + I n u A M j d X J 5 t z q A J B 1 E j x 1 r 0 E i o z j u Z P N 2 y Z 6 K L o M z h 3 z 5 q P j 9 8 e C e V D r Z T 7 c b 8 t i H A L l k W r c c O 8 J 2 w h Q K L m G c K b i x h o j x I e t D y 2 D A f N D t Z H r 9 m B a M 0 6 W 9 U J k X I J 2 6 m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>h J M R e S T P 5 M W 6 t 5 6 s V + t t 1 p q y 5 j M 5 8 k f W + w 8 9 x 5 g I &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k 8 X u 3 d y 0 w j s V W S G g G 6 6 2 V A I g A i E = " &gt; A A A B / X i c b Z C 7 S g N B F I Z n 4 y 3 G W z S l z W g I x M K w a 6 N l U A v L C L l B N o T Z y U k y Z P b C z F k h L M F H 8 B G s B A W x t f c V r H w Q e y e X Q h N / G P j 4 z z m c M 7 8 X S a H R t r + s 1 M r q 2 v p G e j O z t b 2 z u 5 f d P 6 j r M F Y c a j y U o W p 6 T I M U A d R Q o I R m p I D 5 n o S G N 7 y a 1 B t 3 o L Q I g y q O I m j 7 r B + I n u A M j d X J 5 t z q A J B 1 E j x 1 r 0 E i o z j u Z P N 2 y Z 6 K L o M z h 3 z 5 q P j 9 8 e C e V D r Z T 7 c b 8 t i H A L l k W r c c O 8 J 2 w h Q K L m G c K b i x h o j x I e t D y 2 D A f N D t Z H r 9 m B a M 0 6 W 9 U J k X I J 2 6 m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>h J M R e S T P 5 M W 6 t 5 6 s V + t t 1 p q y 5 j M 5 8 k f W + w 8 9 x 5 g I &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T R D Q O 0 d X H U c 4 B V 2 b J G e o 2 e 2 W Q S E = " &gt; A A A B / X i c b Z D J S g N B E I Z 7 4 h b H b T R H L 4 0 h 4 M U w 4 0 W P Q T 1 4 j J A N k h B 6 O j V J k 5 6 F 7 h p h G I K P 4 k l Q E K 8 + i C f f x s 5 y 0 M Q f G j 7 + q q K q f z + R Q q P r f l u F j c 2 t 7 Z 3 i r r 2 3 f 3 B 4 5 B y f t H S c K g 5 N H s t Y d X y m Q Y o I m i h Q Q i d R w E J f Q t u f 3 M 7 q 7 U d Q W s R R A 7 M E + i E b R S I Q n K G x B k 6 p 1 x g D s k G O F 7 0 7 k M g o T g d O 2 a 2 6 c 9 F 1 8 J Z Q J k v V B 8 5 X b x j z N I Q I u W R a d z 0 3 w X 7 O F A o u Y W p X e q m G h P E J G 0 H X Y M R C 0 P 1 8 f v 2 U V o w z p E G s z I u Q z l 3 7 1 0 T O Q q 2 z 0 D e d I c O x X q 3 N z P 9 q 3 R S D 6 3 4 u o i R F i P h i U Z B K i j G d R U G H Q g F H m R l g X A l z L O V j p h h H E 5 h t U v B W / 7 w O r c u q Z / j B L d d u l n k U y S k 5 I + f E I 1 e k R u 5 J n T Q J J x l 5 J q / k z X q y X q x 3 6 2 P R W r C W M y X y R 9 b n D 8 B i l L Q = &lt; / l a t e x i t &gt; ? t+ t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x 5 O H U 2 e p c d k I h T k O q W d f H + + T 6 6 4 = " &gt; A A A B / X i c b Z D J S g N B E I Z r 4 h b j F s 3 R S 2 s Q B E F m v O g x q A e P E b J B E k J P p 2 K a 9 C x 0 1 w h h C D 6 K J 0 F B v H r 1 H T z 5 I N 7 t L A d N / K H h 4 6 8 q q v r 3 Y y U N u e 6 X k 1 l a X l l d y 6 7 n N j a 3 t n f y u 3 s 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>q s F 2 n 7 Q m I T N / d r I u W B M c P A t 5 0 B p 7 6 Z r 4 3 N / 2 r N h H o X 7 V S G c U I Y i u m i X q I Y R W w c B e t K j Y L U 0 A I X W t p j m e h z z Q X Z w H I 2 B W / + z 4 t Q O z v 1 L N / a O C 5 h q i z s w y E c g w f n U I I b K E M V B A z h E Z 7 h x X l w n p x X 5 2 3 a m n F m M w X 4 I + f 9 B 8 W p l v A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D 4 T + 7 w P e z J d b V M x b z + z a Z m m p y A 0 = " &gt; A A A B / X i c b Z D J S g N B E I Z 7 4 h b j F s 3 R S 2 s I R I Q w 4 0 W P Q T 1 4 j J A N M i H 0 d C p J k 5 6 F 7 h o h D M F H 8 B E 8 C Q r i 1 b u v 4 M k H 8 W 5 n O W j i D w 0 f f 1 V R 1 b 8 X S a H R t r + s 1 M r q 2 v p G e j O z t b 2 z u 5 f d P 6 j r M F Y c a j y U o W p 6 T I M U A d R Q o I R m p I D 5 n o S G N 7 y a 1 B t 3 o L Q I g y q O I m j 7 r B + I n u A M j d X J 5 t z q A J B 1 E j x 1 r 0 E i o z j u Z P N 2 y Z 6 K L o M z h 3 z 5 q P j 9 8 e C e V D r Z T 7 c b 8 t i H A L l k W r c c O 8 J 2 w h Q K L m G c K b i x h o j x I e t D y 2 D A f N D t Z H r 9 m B a M 0 6 W 9 U J k X I J 2 6 m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 2 :</head><label>2</label><figDesc>T P 5 M W 6 t 5 6 s V + t t 1 p q y 5 j M 5 8 k f W + w 8 6 q 5 g G &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D 4 T + 7 w P e z J d b V M x b z + z a Z m m p y A 0 = " &gt; A A A B / X i c b Z D J S g N B E I Z 7 4 h b j F s 3 R S 2 s I R I Q w 4 0 W P Q T 1 4 j J A N M i H 0 d C p J k 5 6 F 7 h o h D M F H 8 B E 8 C Q r i 1 b u v 4 M k H 8 W 5 n O W j i D w 0 f f 1 V R 1 b 8 X S a H R t r + s 1 M r q 2 v p G e j O z t b 2 z u 5 f d P 6 j r M F Y c a j y U o W p 6 T I M U A d R Q o I R m p I D 5 n o S G N 7 y a 1 B t 3 o L Q I g y q O I m j 7 r B + I n u A M j d X J 5 t z q A J B 1 E j x 1 r 0 E i o z j u Z P N 2 y Z 6 K L o M z h 3 z 5 q P j 9 8 e C e V D r Z T 7 c b 8 t i H A L l k W r c c O 8 J 2 w h Q K L m G c K b i x h o j x I e t D y 2 D A f N D t Z H r 9 m B a M 0 6 W 9 U J k X I J 2 6 mV 8 T C f O 1 H v m e 6 f Q Z D v R i b W L + V 2 v F 2 L t o J y K I Y o S A z x b 1 Y k k x p J M o a F c o 4 C h H B h h X w h x L + Y A p x t E E l j E p O I t / X o b 6 W c k x f G v i u C Q z p c k h O S Z F 4 p B z U i Y 3 p E J qh J M R e S T P 5 M W 6 t 5 6 s V + t t 1 p q y 5 j M 5 8 k f W + w 8 6 q 5 g G &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U W / V 0 S 7 W R N c B K Q t P U P F c G 3 M D f q E = " &gt; A A A B / X i c b Z D J S g N B E I Z 7 4 h b H b T R H L 4 0 h I A h h x o s e g 3 r w G C E b J C H 0 d G q S J j 0 L 3 T X C M A Q f x Z O g I F 5 9 E E + + j Z 3 l o I k / N H z 8 V U V V / 3 4 i h U b X / b Y K G 5 t b 2 z v F X X t v / + D w y D k + a e k 4 V R y a P J a x 6 v h M g x Q R N F G g h E 6 i g I W + h L Y / u Z 3 V 2 4 + g t I i j B m Y J 9 E M 2 i k Q g O E N j D Z x S r z E G Z I M c L 3 p 3 I J F R n A 6 c s l t 1 5 6 L r 4 C 2 h T J a q D 5 y v 3 j D m a Q g R c s m 0 7 n p u g v 2 c K R R c w t S u 9 F I N C e M T N o K u w Y i F o P v 5 / P o p r R h n S I N Y m R c h n b v 2 r 4 m c h V p n o W 8 6Q 4 Z j v V q b m f / V u i k G 1 / 1 c R E m K E P H F o i C V F G M 6 i 4 I O h Q K O M j P A u B L m W M r H T D G O J j D b p O C t / n k d W p d V z / C D W6 7 d L P M o k l N y R s 6 J R 6 5 I j d y T O m k S T j L y T F 7 J m / V k v V j v 1 s e i t W A t Z 0 r k j 6 z P H 7 1 G l L I = &lt; / l a t e x i t &gt; Losses &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a W f J z Y a D r Q 1 w R E G Q b P s r B U S M 6 t 4 = " &gt; A A A B 9 X i c b Z A 9 S w N B E I b n / I z n V 9 T S 5 j A E r M K d j T Z i 0 M b C I o J R I T n C 3 m Y u W b L 3 4 e 6 c G o 6 A / 8 J K U B B b f 4 Y / w M p / 4 y a x 0 M Q X F h 7 e m W F m 3 y C V Q p P r f l k z s 3 P z C 4 u F J X t 5 Z X V t v b i x e a m T T H G s 8 0 Q m 6 j p g G q W I s U 6 C J F 6 n C l k U S L w K e i f D + t U t K i 2 S + I L 6 K f o R 6 8 Q i F J y R s f w m 4 T 3 l Z 4 n W q A e t Y s m t u C M 5 0 + D 9 Q O n o w z 5 8 A I B a q / j Z b C c 8 i z A m L p n W D c 9 N y c + Z I s E l D u x y M 9 O Y M t 5 j H W w Y j F m E 2 s 9 H V w + c s n H a T p g o 8 2 J y R q 7 9 a y J n k d b 9 K D C d E a O u n q w N z f 9 q j Y z C A z 8 X c Z o R x n y 8 K M y k Q 4 k z j M B p C 4 W c Z N 8 A 4 0 q Y Y x 3 e Z Y p x M k H Z J g V v 8 s / T c L l X 8 Q y f u 6 X q M Y x V g G 3 Y g V 3 w Y B + q c A o 1 q A O H G 3 i E Z 3 i x 7 q w n 6 9 V 6 G 7 f O W D 8 z W / B H 1 v s 3 W F m U g g = = &lt; / l a t ex i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i + 3 t B Y 5 1 V I q K p 2 d E m l U B q V T M b n w = " &gt; A A A B 9 X i c b Z D J S g N B E I Z 7 4 h b H L e r R S 2 M I e A o z X v Q i B r 1 4 8 B D B L J A M o a d T S Z r 0 L H b X q G H I c 3 g S F M W r j + E D e B D f x s 5 y 0 M Q f G j 7 + q q K q f z + W Q q P j f F u Z h c W l 5 Z X s q r 2 2 v r G 5 l d v e q e o o U R w q P J K R q v t M g x Q h V F C g h H q s g A W + h J r f P x / V a 7 e g t I j C a x z E 4 A W s G 4 q O 4 A y N 5 T U R 7 j G 9 j L Q G P W z l 8 k 7 R G Y v O g z u F / O m H f R K / f N n l V u 6 z 2 Y 5 4 E k C I X D K t G 6 4 T o 5 c y h Y J L G N q F Z q I h Z r z P u t A w G L I A t J e O r x 7 S g n H a t B M p 8 0 K k Y 9 f + N Z G y Q O t B 4 J v O g G F P z 9 Z G 5 n + 1 R o K d Y y 8 V Y Z w g h H y y q J N I i h E d R U D b Q g F H O T D A u B L m W M p 7 T D G O J i j b p O D O / n k e q o d F 1 / C V k y + d k Y m y Z I / s k w P i k i N S I h e k T C q E k x v y Q J 7 I s 3 V n P V q v 1 t u k N W N N Z 3 b J H 1 n v P 0 t c l f Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i + 3 t B Y 5 1 V I q K p 2 d E m l U B q V T M b n w = " &gt; A AA B 9 X i c b Z D J S g N B E I Z 7 4 h b H L e r R S 2 M I e A o z X v Q i B r 1 4 8 B D B L J A M o a d T S Z r 0 L H b X q G H I c 3 g S F M W r j + E D e B D f x s 5 y 0 M Q f G j 7 + q q K q f z + W Q q P j f F u Z h c W l 5 Z X s q r 2 2 v r G 5 l d v e q e o o U R w q P J K R q v t M g x Q h V F C g h H q s g A W + h J r f P x / V a 7 e g t I j C a x z E 4 A W s G 4 q O 4 A y N 5 T U R 7 j G 9 j L Q G P W z l 8 k 7 R G Y v O g z u F / O m H f R K / f N n l V u 6 z 2 Y 5 4 E k C I X D K t G 6 4 T o 5 c y h Y J L G N q F Z q I h Z r z P u t A w G L I A t J e O r x 7 S g n H a t B M p 8 0 K k Y 9 f + N Z G y Q O t B 4 J v O g G F P z 9 Z G 5 n + 1 R o K d Y y 8 V Y Z w g h H y y q J N I i h E d R U D b Q g F H O T D A u B L m W M p 7 T D G O J i j b p O D O / n k e q o d F 1 / C V k y + d k Y m y Z I / s k w P i k i N S I h e k T C q E k x v y Q J 7 I s 3 V n P V q v 1 t u k N W N N Z 3 b J H 1 n v P 0 t c l f Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T I 8 l P g x S T o i 5 t V 1 2 5 P Z 0 j b W A Y L c = " &gt; A A A B 9 X i c b Z A 9 S w N B E I b 3 / I z n V 9 T S 5 j A E r M K d j Z Z B G w u L C O Y D k i P s b S b J k r 3 d c 3 d O D U d + h 5 W g I L b + G C v / j Z v k C k 1 8 Y e H h n R l m 9 o 0 S w Q 3 6 / r e z s r q 2 v r F Z 2 H K 3 d 3 b 3 9 o s H h w 2 j U s 2 g z p R Q u h V R A 4 J L q C N H A a 1 E A 4 0 j A c 1 o d D W t N x 9 A G 6 7 k H Y 4 T C G M 6 k L z P G U V r h R 2 E J 8 x u l D F g J t 1 i y a / 4 M 3 n L E O R Q I r l q 3 e J X p 6 d Y G o N E J q g x 7 c B P M M y o R s 4 E T N x y J z W Q U D a i A 2 h b l D Q G E 2 a z q y d e 2 T o 9 r 6 + 0 f R K 9 m e v + m s h o b M w 4 j m x n T H F o F m t T 8 7 9 a O 8 X + R Z h x m a Q I k s 0 X 9 V P h o f K m E X g 9 r o G h G F u g T H N 7 r M e G V F O G N i j X p h A s / n k Z G m e V w P K t X 6 p e 5 n k U y D E 5 I a c k I O e k S q 5 J j d Q J I / f k m b y S N + f R e X H e n Y 9 5 6 4 q T z x y R P 3 I + f w D n R Z K 1 &lt; / l a t e x i t &gt; Overview of the proposed framework: Human Mesh and Motion Recovery (HMMR)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Qualitative results of our approach on sequences from Penn Action, NBA, and VLOG. For each sequence, the top row shows the cropped input images, the middle row shows the predicted mesh, and the bottom row shows a different angle of the predicted mesh. Our method produces smooth, temporally consistent predictions.Input &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y r W O l U Z a s F V y R t A h f v r h w M 3 z O b Q = " &gt; A A A B 9 H i c b Z C 7 S g N B F I b P x l t c b 1 F L L R Z D w C r s 2 m g Z t N E u A X O B 7 B J m J 7 P J k N k L M 2 e D Y c l r W A k K Y m v t e 1 j Z + S j O J i k 0 8 Y e B j / + c w 5 z z + 4 n g C m 3 7 y y i s r W 9 s b h W 3 z Z 3 d v f 2 D 0 u F R S 8 W p p K x J Y x H L j k 8 U E z x i T e Q o W C e R j I S + Y G 1 / d J P X 2 2 M m F Y + j e 5 w k z A v J I O I B p w S 1 5 b r I H j C 7 i 5 I U p 7 1 S 2 a 7 a M 1 m r 4 C y g X D v 9 a H w D Q L 1 X + n T 7 M U 1 D F i E V R K m u Y y f o Z U Q i p 4 J N z Y q b K p Y Q O i I D 1 t U Y k Z A p L 5 s t P b U q 2 u l b Q S z 1 i 9 C a u e a v i Y y E S k 1 C X 3 e G B I d q u Z a b / 9 W 6 K Q Z X X s b z m 1 h E 5 x 8 F q b A w t v I E r D 6 X j K K Y a C B U c r 2 s R Y d E E o o 6 J 1 O n 4 C z f v A q t i 6 q j u a H j u I a 5 i n A C Z 3 A O D l x C D W 6 h D k 2 g k M A j P M O L M T a e j F f j b d 5 a M B Y z x / B H x v s P L 9 6 U h w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y y e 6 1 + Z k D r E Q D N j 5 o P g o s t h + S U A = " &gt; A A A B 9 H i c b Z D L S s N A F I Y n X m u 8 V V 0 q M l g K r k r i R p d F N 7 p r w V 6 g C W U y n b R D J 5 M w c 1 I s o U t f w Z W g I G 5 d 9 z 1 c + Q y + h N P L Q l t / G P j 4 z z n M O X + Q C K 7 B c b 6 s l d W 1 9 Y 3 N 3 J a 9 v b O 7 t 5 8 / O K z r O F W U 1 W g s Y t U M i G a C S 1 Y D D o I 1 E 8 V I F A j W C P o 3 k 3 p j w J T m s b y H Y c L 8 i H Q l D z k l Y C z P A / Y A 2 Z 1 M U h i 1 8 w W n 5 E y F l 8 G d Q 6 F 8 M q 5 + P 5 6 O K + 3 8 p 9 e J a R o x C V Q Q r V u u k 4 C f E Q W c C j a y i 1 6 q W U J o n 3 R Z y 6 A k E d N + N l 1 6 h I v G 6 e A w V u Z J w F P X / j W R k U j r Y R S Y z o h A T y / W J u Z / t V Y K 4 Z W f 8 c l N T N L Z R 2 E q M M R 4 k g D u c M U o i K E B Q h U 3 y 2 L a I 4 p Q M D n Z J g V 3 8 e Z l q F + U X M N V E 8 c 1 m i m H j t E Z O k c u u k R l d I s q q I Y o S t A T e k G v 1 s B 6 t t 6 s 9 1 n r i j W f O U J / Z H 3 8 A A + J l e 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y y e 6 1 + Z k D r E Q D N j 5 o P g o s t h + S U A = " &gt; A A A B 9 H i c b Z D L S s N A F I Y n X m u 8 V V 0 q M l g K r k r i R p d F N 7 p r w V 6 g C W U y n b R D J 5 M w c 1 I s o U t f w Z W g I G 5 d 9 z 1 c + Q y + h N P L Q l t / G P j 4 z z n M O X + Q C K 7 B c b 6 s l d W 1 9 Y 3 N 3 J a 9 v b O 7 t 5 8 / O K z r O F W U 1 W g s Y t U M i G a C S 1 Y D D o I 1 E 8 V I F A j W C P o 3 k 3 p j w J T m s b y H Y c L 8 i H Q l D z k l Y C z P A / Y A 2 Z 1 M U h i 1 8 w W n 5 E y F l 8 G d Q 6 F 8 M q 5 + P 5 6 O K + 3 8 p 9 e J a R o x C V Q Q r V u u k 4 C f E Q W c C j a y i 1 6 q W U J o n 3 R Z y 6 A k E d N + N l 1 6 h I v G 6 e A w V u Z J w F P X / j W R k U j r Y R S Y z o h A T y / W J u Z / t V Y K 4 Z W f 8 c l N T N L Z R 2 E q M M R 4 k g D u c M U o i K E B Q h U 3 y 2 L a I 4 p Q M D n Z J g V 3 8 e Z l q F + U X M N V E 8 c 1 m i m H j t E Z O k c u u k R l d I s q q I Y o S t A T e k G v 1 s B 6 t t 6 s 9 1 n r i j W f O U J / Z H 3 8 A A + J l e 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o g i / d z B g n Y D H o p 3 X e u 9 j 4 C j D E U 8 = " &gt; A A A B 9 H i c b Z D L S s N A F I Y n 9 V b j r e r S z W A p u C q J G 1 0 W 3 e i u g r 1 A G 8 p k e t I O n V y Y O S m W 0 N d w J S i I W 1 / G l W / j p M 1 C W 3 8 Y + P j P O c w 5 v 5 9 I o d F x v q 3 S x u b W 9 k 5 5 1 9 7 b P z g 8 q h y f t H W c K g 4 t H s t Y d X 2 m Q Y o I W i h Q Q j d R w E J f Q s e f 3 O b 1 z h S U F n H 0 i L M E v J C N I h E I z t B Y / T 7 C E 2 b 3 U Z L i f F C p O n V n I b o O b g F V U q g 5 q H z 1 h z F P Q 4 i Q S 6 Z 1 z 3 U S 9 D K m U H A J c 7 v W T z U k j E / Y C H o G I x a C 9 r L F 0 n N a M 8 6 Q B r E y L 0 K 6 c O 1 f E x k L t Z 6 F v u k M G Y 7 1 a i 0 3 / 6 v 1 U g y u v U z k N 0 H E l x 8 F q a Q Y 0 z w B O h Q K O M q Z A c a V M M t S P m a K c T Q 5 2 S Y F d / X m d W h f 1 l 3 D D 0 6 1 c V P k U S Z n 5 J x c E J d c k Q a 5 I 0 3 S I p w k 5 J m 8 k j d r a r 1 Y 7 9 b H s r V k F T O n 5 I + s z x 8 d n 5 J C &lt; / l a t e x i t &gt; Predictions &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m N v P D R Y r i l d a s / 7 X 2 N Z w x 6 9 j C c 8 = " &gt; A A A B / H i c b Z D J S g N B E I Z r X O O 4 x X j 0 M h g C n s K M F 7 2 I Q S 8 e I 5 g F k h B 6 O p W k S c 9 C d 4 0 k D B G f x J O g I F 5 9 B R / A k 2 9 j Z z l o 4 g 8 N H 3 9 V U d W / H 0 u h y X W / r Z X V t f W N z c y W v b 2 z u 7 e f P c h V d Z Q o j h U e y U j V f a Z R i h A r J E h i P V b I A l 9 i z R 9 c T + q 1 e 1 R a R O E d j W J s B a w X i q 7 g j I z V z u a a h E N K y w o 7 g k 8 s P W 5 n 8 2 7 R n c p Z B m 8 O + c t P + + I R A M r t 7 F e z E / E k w J C 4 Z F o 3 P D e m V s o U C S 5 x b B e a i c a Y 8 Q H r Y c N g y A L U r X R 6 / N g p G K f j d C N l X k j O 1 L V / T a Q s 0 H o U + K Y z Y N T X i 7 W J + V + t k V D 3 v J W K M E 4 I Q z 5 b 1 E 2 k Q 5 E z S c L p C I W c 5 M g A 4 0 q Y Y x 3 e Z 4 p x M n n Z J g V v 8 c / L U D 0 t e o Z v 3 X z p C m b K w B E c w w l 4 c A Y l u I E y V I D D E J 7 g B V 6 t B + v Z e r P e Z 6 0 r 1 n z m E P 7 I + v g B w 3 + W 8 A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a y d a u 2 L Q P t r 3 R D 6 p e x 6 1 S Q m X 8 1 g = " &gt; A A A B / H i c b Z D L S s N A F I Y n 9 V b j L d a l m 2 A p u C q J G 9 2 I R T c u K 9 g L t K V M J q f t 0 M m F m R N p C f V R 3 C g o i F t f w Q d w I b 6 N k 7 Y L b f 1 h 4 O M / 5 3 D O / F 4 s u E L H + T Z y K 6 t r 6 x v 5 T X N r e 2 d 3 z 9 o v 1 F W U S A Y 1 F o l I N j 2 q Q P A Q a s h R Q D O W Q A N P Q M M b X m X 1 x h 1 I x a P w F s c x d A L a D 3 m P M 4 r a 6 l q F N s I I 0 6 o E n 7 P M U p O u V X T K z l T 2 M r h z K F 5 8 m O f x 4 5 d Z 7 V q f b T 9 i S Q A h M k G V a r l O j J 2 U S u R M w M Q s t R M F M W V D 2 o e W x p A G o D r p 9 P i J X d K O b / c i q V + I 9 t Q 1 f 0 2 k N F B q H H i 6 M 6 A 4 U I u 1 z P y v 1 k q w d 9 Z J e R g n C C G b L e o l w s b I z p K w f S 6 B o R h r o E x y f a z N B l R S h j o v U 6 f g L v 5 5 G e o n Z V f z j V O s X J K Z 8 u S Q H J F j 4 p J T U i H X p E p q h J E R e S D P 5 M W 4 N 5 6 M V + N t 1 p o z 5 j M H 5 I + M 9 x + 2 g p h k &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a y d a u 2 L Q P t r 3 R D 6 p e x 6 1 S Q m X 8 1g = " &gt; A A A B / H i c b Z D L S s N A F I Y n 9 V b j L d a l m 2 A p u C q J G 9 2 I R T c u K 9 g L t K V M J q f t 0 M m F m R N p C f V R 3 C g o i F t f w Q d w I b 6 N k 7 Y L b f 1 h 4 O M / 5 3 D O / F 4 s u E L H + T Z y K 6 t r 6 x v 5 T X N r e 2 d 3 z 9 o v 1 F W U S A Y 1 F o l I N j 2 q Q P A Q a s h R Q D O W Q A N P Q M M b X m X 1 x h 1 I x a P w F s c x d A L a D 3 m P M 4 r a 6 l q F N s I I 0 6 o E n 7 P M U p O u V X T K z l T 2 M r h z K F 5 8 m O f x 4 5 d Z 7 V q f b T 9 i S Q A h M k G V a r l O j J 2 U S u R M w M Q s t R M F M W V D 2 o e W x p A G o D r p 9 P i J X d K O b / c i q V + I 9 t Q 1 f 0 2 k N F B q H H i 6 M 6 A 4 U I u 1 z P y v 1 k q w d 9 Z J e R g n C C G b L e o l w s b I z p K w f S 6 B o R h r o E x y f a z N B l R S h j o v U 6 f g L v 5 5 G e o n Z V f z j V O s X J K Z 8 u S Q H J F j 4 p J T U i H X p E p q h J E R e SD P 5 M W 4 N 5 6 M V + N t 1 p o z 5 j M H 5 I + M 9 x + 2 g p h k &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x M C + j N F z p 9 p 0 M TC I t / c k Y k 1 X T w Q = " &gt; A A A B / H i c b Z D L S s N A F I Y n X m u 8 x b p 0 E y w F V y V x o 8 u i G 5 c V 7 A X a U C a T k 3 b o 5 M L M i b S E + i i u B A V x 6 4 u 4 8 m 2 c t F l o 6 w 8 D H / 8 5 h 3 P m 9 1 P B F T r O t 7 G x u b W 9 s 1 v Z M / c P D o + O r Z N q R y W Z Z N B m i U h k z 6 c K B I + h j R w F 9 F I J N P I F d P 3 J b V H v P o J U P I k f c J a C F 9 F R z E P O K G p r a F U H C F P M W x I C z g p L z Y d W z W k 4 C 9 n r 4 J Z Q I 6 V a Q + t r E C Q s i y B G J q h S f d d J 0 c u p R M 4 E z M 3 6 I F O Q U j a h I + h r j G k E y s s X x 8 / t u n Y C O 0 y k f j H a C 9 f 8 N Z H T S K l Z 5 O v O i O J Y r d Y K 8 7 9 a P 8 P w 2 s t 5 n G Y I M V s u C j N h Y 2 I X S d g B l 8 B Q z D R Q J r k + 1 m Z j K i l D nZ e p U 3 B X / 7 w O n c u G q / n e q T V v y j w q 5 I y c k w v i k i v S J H e k R d q E k S l 5 J q / k z X g y X o x 3 4 2 P Z u m G U M 6 f k j 4 z P H 1 J 6 l S M = &lt; / l a t e x i t &gt; Di?erent &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L Y + L t F P 9 + h d B G t E 6 m T f H p S n t X D 0 = " &gt; A A A B + n i c b V C 7 S g N B F L 3 r M 6 6 v V U s t B k P A K u z a a B n U w j I B o 0 I S w u z k r g 6 Z f T B z N x j W / I m V o C C 2 F v 6 H l Z 2 f 4 i S x 8 H V g 4 H D O P d w 7 J 8 y U N O T 7 7 8 7 M 7 N z 8 w m J p y V 1 e W V 1 b 9 z Y 2 z 0 2 a a 4 F N k a p U X 4 b c o J I J N k m S w s t M I 4 9 D h R d h / 3 j s X w x Q G 5 k m Z z T M s B P z q 0 R G U n C y U t f z 2 o Q 3 V J z I K E K N C Y 2 6 X t m v + h O w v y T 4 I u X a z m v j A w D q X e + t 3 U t F H t u w U N y Y V u B n 1 C m 4 J i k U j t x K O z e Y c d H n V 9 i y N O E x m k 4 x O X 3 E K l b p s S j V 9 i X E J q r 7 L V H w 2 J h h H N r J m N O 1 + e 2 N x f + 8 V k 7 R Y a e Q S Z Y T J m K 6 K M o V o 5 S N e 2 A 9 q V G Q G l r C h Z b 2 W C a u u e a C b F u u b S H 4 / e e / 5 H y / G l j e s H U c w R Q l 2 I Z d 2 I M A D q A G p 1 C H J g g Y w B 0 8 w K N z 6 9 w 7 T 8 7 z d H T G + c p s w Q 8 4 L 5 + j 0 J Z n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 T D j o x C S 6 9 a q 3 5 5 E z J 3 J w l W k Q u Q = " &gt; A A A B + n i c b V C 7 S g N B F J 2 N r 7 i +V i 0 V W Q w B q 7 B r o 2 V Q C 8 s E z A O S E G Y n d 5 M h s w 9 m 7 g b D m t K / s B I U x N Y i / 2 H l N / g T T h 6 F J h 4 Y O J x z D / f O 8 W L B F T r O l 5 F Z W V 1 b 3 8 h u m l v b O 7 t 7 1 v 5 B V U W J Z F B h k Y h k 3 a M K B A + h g h w F 1 G M J N P A E 1 L z + 9 c S v D U A q H o V 3 O I y h F d B u y H 3 O K G q p b V l N hH t M b 7 j v g 4 Q Q R 2 0 r 5 x S c K e x l 4 s 5 J r n g 8 L n 8 / n o x L b e u z 2 Y l Y E u g w E 1 S p h u v E 2 E q p R M 4 E j M x 8 M 1 E Q U 9 a n X W h o G t I A V C u d n j 6 y 8 1 r p 2 H 4 k 9 Q v R n q r m r 0 R K A 6 W G g a c n A 4 o 9 t e h N x P + 8 R o L + Z S v l Y Z w g h G y 2 y E + E j Z E 9 6 c H u c A k M x V A T y i T X x 9 q s R y V l q N s y d Q v u 4 p + X S f W 8 4 G p e 1 n V c k R m y 5 I i c k j P i k g t S J L e k R C q E k Q F 5 I i / k 1 X g w n o 0 3 4 3 0 2 m j H m m U P y B 8 b H D 4 N 7 l 8 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 T D j o x C S 6 9 a q 3 5 5 E z J 3 J w l W k Q u Q = " &gt; A A A B + n i c b V C 7 S g N B F J 2 N r 7 i + V i 0 V W Q w B q 7 B r o 2 V Q C 8 s E z A O S E G Y n d 5 M h s w 9 m 7 g b D m t K / s B I U x N Y i / 2 H l N / g T T h 6 F J h 4 Y O J x z D / f O 8 W L B F T r O l 5 F Z W V 1 b 3 8 h u m l v b O 7 t 7 1 v 5 B V U W J Z F B h k Y h k 3 a M K B A + h g h w F 1 G M J N P A E 1 L z + 9 c S v D U A q H o V 3 O I y h F d B u y H 3 O K G q p b V l N h H t M b 7 j v g 4 Q Q R 2 0 r 5 x S c K e x l 4 s 5 J r n g 8 L n 8 / n o x L b e u z 2 Y l Y E u g w E 1 S p h u v E 2 E q p R M 4 E j M x 8 M 1 E Q U 9 a n X W h o G t I A V C u d n j 6 y 8 1 r p 2 H 4 k 9 Q v R n q r m r 0 R K A 6 W G g a c n A 4 o 9 t e h N x P + 8 R o L + Z S v l Y Z w g h G y 2 y E + E j Z E 9 6 c H u c A k M x V A T y i T X x 9 q s R y V l q N s y d Q v u 4 p + X S f W 8 4 G p e 1 n V c k R m y 5 I i c k j P i k g t S J L e k R C q E k Q F 5 I i / k 1 X g w n o 0 3 4 3 0 2 m j H m m U P y B 8 b H D 4 N 7 l 8 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m 7 I 3 7 p 8 W U N Q R 6 P H H Z K 8 l j j e X b o E = " &gt; A A A B + n i c b V D L S s N A F J 3 4 r P E V d e k m W A q u S u J G l 0 V d u K x g H 9 C G M p n e t E M n k z B z U y y x f + J K U B C 3 / o k r / 8 Z p m 4 W 2 H h g 4 n H M P 9 8 4 J U 8 E 1 e t 6 3 t b a + s b m 1 X d q x d / f 2 D w 6 d o + O m T j L F o M E S k a h 2 S D U I L q G B H A W 0 U w U 0 D g W 0 w t H N z G + N Q W m e y A e c p B D E d C B 5 x B l F I / U c p 4 v w i P k t j y J Q I H H a c 8 p e 1 Z v D X S V + Q c q k Q L 3 n f H X 7 C c t i E 2 a C a t 3 x v R S D n C r k T M D U r n Q z D S l l I z q A j q G S x q C D f H 7 6 1 K 0 Y p e 9 G i T J P o j t X 7 V + J n M Z a T + L Q T M Y U h 3 r Z m 4 n / e Z 0 M o 6 s g 5 z L N E C R b L I o y 4 W L i z n p w + 1 w B Q z E x h D L F z b E u G 1 J F G Z q 2 b N O C v / z n V d K 8 q P q G 3 3 v l 2 n X R R 4 m c k j N y T n x y S W r k j t R J g z A y J s / k l b x Z T 9 a L 9 W 5 9 L E b X r C J z Q v 7 A + v w B k Z G U I g = = &lt; / l a t e x i t &gt; Viewpoint &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 6 I / 4 T Q s y Q C d s h b h y s H a 5 5 V k F 8 Y = " &gt; A A A B + n i c b Z A 9 S w N B E I b n / I z x 6 9 R S i 8 M Q s J I 7 G y 1 F G 8 s E z A c k I e x t J s n i 3 t 6 x O 6 e G M / / E S l A Q W w v / h 5 W d P 8 X N R 6 G J L y w 8 v D P D z L 5 h I o U h 3 / 9 y F h a X l l d W c 2 v 5 9 Y 3 N r W 1 3 Z 7 d q 4 l R z r P B Y x r o e M o N S K K y Q I I n 1 R C O L Q o m 1 8 O Z y V K / d o j Y i V t c 0 S L A V s Z 4 S X c E Z W a v t u k 3 C e 8 q q A u + S W C g a t t 2 C f + y P 5 c 1 D M I X C + c F H + R s A S m 3 3 s 9 m J e R q h I i 6 Z M Y 3 A T 6 i V M U 2 C S x z m i 8 3 U Y M L 4 D e t h w 6 J i E Z p W N j 5 9 6 B W t 0 / G 6 s b Z P k T d 2 8 7 8 m M h Y Z M 4 h C 2 x k x 6 p v Z 2 s j 8 r 9 Z I q X v W y o R K U k L F J 4 u 6 q f Q o 9 k Y 5 e B 2 h k Z M c W G B c C 3 u s x / t M M 0 4 2 r b x N I Z j 9 8 z x U T 4 4 D y 2 U b x w V M l I N 9 O I Q j C O A U z u E K S l A B D r f w C M / w 4 j w 4 T 8 6 r 8 z Z p X X C m M 3 v w R 8 7 7 D + r u l p U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i b O W x L / K G g d A N E t M Q h / A 4 a 0 5 w I U = " &gt; A A A B + n i c b Z C 7 S g N B F I Z n 4 y 3 G 2 6 q l I o s h Y B V 2 b b Q M 2 l g m Y C 6 Q h D A 7 O U m G z M 4 u M 2 e j Y U 3 p W 1 g J C m J r k f e w 8 h l 8 C S e X Q h N / G P j 4 z z m c M 7 8 f C a 7 R d b + s 1 M r q 2 v p G e j O z t b 2 z u 2 f v H 1 R 0 G C s G Z R a K U N V 8 q k F w C W X k K K A W K a C B L 6 D q 9 6 8 n 9e o A l O a h v M V h B M 2 A d i X v c E b R W C 3 b b i D c Y 1 L h c B e F X O K o Z W f d v D u V s w z e H L K F 4 3 H p + / F k X G z Z n 4 1 2 y O I A J D J B t a 5 7 b o T N h C r k T M A o k 2 v E G i L K + r Q L d Y O S B q C b y f T 0 k Z M z T t v p h M o 8 i c 7 U z f y a S G i g 9 T D w T W d A s a c X a x P z v 1 o 9 x s 5 l M + E y i h E k m y 3 q x M L B 0 J n k 4 L S 5 A o Z i a I A y x c 2 x D u t R R R m a t D I m B W / x z 8 t Q O c 9 7 h k s m j i s y U 5 o c k V N y R j x y Q Q r k h h R J m T A y I E / k h b x a D 9 a z 9 W a 9 z 1 p T 1 n z m k P y R 9 f E D y p m X + w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i b O W x L / K G g d A N E t M Q h / A 4 a 0 5 w I U = " &gt; A AA B + n i c b Z C 7 S g N B F I Z n 4 y 3 G 2 6 q l I o s h Y B V 2 b b Q M 2 l g m Y C 6 Q h D A 7 O U m G z M 4 u M 2 e j Y U 3 p W 1 g J C m J r k f e w 8 h l 8 C S e X Q h N / G P j 4 z z m c M 7 8 f C a 7 R d b + s 1 M r q 2 v p G e j O z t b 2 z u 2 f v H 1 R 0 G C s G Z R a K U N V 8 q k F w C W X k K K A W K a C B L 6 D q 9 6 8 n 9e o A l O a h v M V h B M 2 A d i X v c E b R W C 3 b b i D c Y 1 L h c B e F X O K o Z W f d v D u V s w z e H L K F 4 3 H p + / F k X G z Z n 4 1 2 y O I A J D J B t a 5 7 b o T N h C r k T M A o k 2 v E G i L K + r Q L d Y O S B q C b y f T 0 k Z M z T t v p h M o 8 i c 7 U z f y a S G i g 9 T D w T W d A s a c X a x P z v 1 o 9 x s 5 l M + E y i h E k m y 3 q x M L B 0 J n k 4 L S 5 A o Z i a I A y x c 2 x D u t R R R m a t D I m B W / x z 8 t Q O c 9 7 h k s m j i s y U 5 o c k V N y R j x y Q Q r k h h R J m T A y I E/ k h b x a D 9 a z 9 W a 9 z 1 p T 1 n z m k P y R 9 f E D y p m X + w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k j 6 l J f e 9 i b / T + I K q I F p l p o z u 6 n Y = " &gt; A A A B + n i c b Z B N S 8 N A E I Y 3 f t b 4 F f X o J V g K n k r i R Y 9 F L x 4 r 2 A 9 o Q 9 l s J + 3 S z S b s T q o l 9 p 9 4 E h T E q / / E k / / G b Z u D t r 6 w 8 P D O D D P 7 h q n g G j 3 v 2 1 p b 3 9 j c 2 i 7 t 2 L t 7 + w e H z t F x U y e Z Y t B g i U h U O 6 Q a B J f Q Q I 4 C 2 q k C G o c C W u H o Z l Z v j U F p n s h 7 n K Q Q x H Q g e c Q Z R W P 1 H K e L 8 I h 5 k 8 N D m n C J 0 5 5 T 9 q r e X O 4 q + A W U S a F 6 z / n q 9 h O W x S C R C a p 1 x / d S D H K q k D M B U 7 v S z T S k l I 3 o A D o G J Y 1 B B / n 8 9 K l b M U 7 f j R J l n k R 3 7 t q / J n I a a z 2 J Q 9 M Z U x z q 5 d r M / K / W y T C 6 C n I u 0 w x B s s W i K B M u J u 4 s B 7 f P F T A U E w O U K W 6 O d d m Q K s r Q p G W b F P z l P 6 9 C 8 6 L q G 7 7 z y r X r I o 8 S O S V n 5 J z 4 5 J L U y C 2 p k w Z h Z E y e y S t 5 s 5 6 s F + v d + l i 0 r l n F z A n 5 I + v z B 9 i v l F A = &lt; / l a t e x i t &gt; Input &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y r W O l U Z a s F V y R t A h f v r h w M 3 z O b Q = " &gt; A A A B 9 H i c b Z C 7 S g N B F I b P x l t c b 1 F L L R Z D w C r s 2 m g Z t N E u A X O B 7 B J m J 7 P J k N k L M 2 e D Y c l r W A k K Y m v t e 1 j Z + S j O J i k 0 8 Y e B j / + c w 5 z z + 4 n g C m 3 7 y y i s r W 9 s b h W 3 z Z 3 d v f 2 D 0 u F R S 8 W p p K x J Y x H L j k 8 U E z x i T e Q o W C e R j I S + Y G 1 / d J P X 2 2 M m F Y + j e 5 w k z A v J I O I B p w S 1 5 b r I H j C 7 i 5 I U p 7 1 S 2 a 7 a M 1 m r 4 C y g X D v 9 a H w D Q L 1 X + n T 7 M U 1 D F i E V R K m u Y y f o Z U Q i p 4 J N z Y q b K p Y Q O i I D 1 t U Y k Z A p L 5 s t P b U q 2 u l b Q S z 1 i 9 C a u e a v i Y y E S k 1 C X 3 e G B I d q u Z a b / 9 W 6 K Q Z X X s b z m 1 h E 5 x 8 F q b A w t v I E r D 6 X j K K Y a C B U c r 2 s R Y d E E o o 6 J 1 O n 4 C z f v A q t i 6 q j u a H j u I a 5 i n A C Z 3 A O D l x C D W 6 h D k 2 g k M A j P M O L M T a e j F f j b d 5 a M B Y z x / B H x v s P L 9 6 U h w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y y e 6 1 + Z k D r E Q D N j 5 o P g o s t h + S U A = " &gt; A A A B 9 H i c b Z D L S s N A F I Y n X m u 8 V V 0 q M l g K r k r i R p d F N 7 p r w V 6 g C W U y n b R D J 5 M w c 1 I s o U t f w Z W g I G 5 d 9 z 1 c + Q y + h N P L Q l t / G P j 4 z z n M O X + Q C K 7 B c b 6 s l d W 1 9 Y 3 N 3 J a 9 v b O 7 t 5 8 / O K z r O F W U 1 W g s Y t U M i G a C S 1 Y D D o I 1 E 8 V I F A j W C P o 3 k 3 p j w J T m s b y H Y c L 8 i H Q l D z k l Y C z P A / Y A 2 Z 1 M U h i 1 8 w W n 5 E y F l 8 G d Q 6 F 8 M q 5 + P 5 6 O K + 3 8 p 9 e J a R o x C V Q Q r V u u k 4 C f E Q W c C j a y i 1 6 q W U J o n 3 R Z y 6 A k E d N + N l 1 6 h I v G 6 e A w V u Z J w F P X / j W R k U j r Y R S Y z o h A T y / W J u Z / t V Y K 4 Z W f 8 c l N T N L Z R 2 E q M M R 4 k g D u c M U o i K E B Q h U 3 y 2 L a I 4 p Q M D n Z J g V 3 8 e Z l q F + U X M N V E 8 c 1 m i m H j t E Z O k c u u k R l d I s q q I Y o S t A T e k G v 1 s B 6 t t 6 s 9 1 n r i j W f O U J / Z H 3 8 A A + J l e 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y y e 6 1 + Z k D r E Q D N j 5 o P g o s t h + S U A = " &gt; A A A B 9 H i c b Z D L S s N A F I Y n X m u 8 V V 0 q M l g K r k r i R p d F N 7 p r w V 6 g C W U y n b R D J 5 M w c 1 I s o U t f w Z W g I G 5 d 9 z 1 c + Q y + h N P L Q l t / G P j 4 z z n M O X + Q C K 7 B c b 6 s l d W 1 9 Y 3 N 3 J a 9 v b O 7 t 5 8 / O K z r O F W U 1 W g s Y t U M i G a C S 1 Y D D o I 1 E 8 V I F A j W C P o 3 k 3 p j w J T m s b y H Y c L 8 i H Q l D z k l Y C z P A / Y A 2 Z 1 M U h i 1 8 w W n 5 E y F l 8 G d Q 6 F 8 M q 5 + P 5 6 O K + 3 8 p 9 e J a R o x C V Q Q r V u u k 4 C f E Q W c C j a y i 1 6 q W U J o n 3 R Z y 6 A k E d N + N l 1 6 h I v G 6 e A w V u Z J w F P X / j W R k U j r Y R S Y z o h A T y / W J u Z / t V Y K 4 Z W f 8 c l N T N L Z R 2 E q M M R 4 k g D u c M U o i K E B Q h U 3 y 2 L a I 4 p Q M D n Z J g V 3 8 e Z l q F + U X M N V E 8 c 1 m i m H j t E Z O k c u u k R l d I s q q I Y o S t A T e k G v 1 s B 6 t t 6 s 9 1 n r i j W f O U J / Z H 3 8 A A + J l e 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o g i / d z B g n Y D H o p 3 X e u 9 j 4 C j D E U 8 = " &gt; A A A B 9 H i c b Z D L S s N A F I Y n 9 V b j r e r S z W A p u C q J G 1 0 W 3 e i u g r 1 A G 8 p k e t I O n V y Y O S m W 0 N d w J S i I W 1 / G l W / j p M 1 C W 3 8 Y + P j P O c w 5 v 5 9 I o d F x v q 3 S x u b W 9 k 5 5 1 9 7 b P z g 8 q h y f t H W c K g 4 t H s t Y d X 2 m Q Y o I W i h Q Q j d R w E J f Q s e f 3 O b 1 z h S U F n H 0 i L M E v J C N I h E I z t B Y / T 7 C E 2 b 3 U Z L i f F C p O n V n I b o O b g F V U q g 5 q H z 1 h z F P Q 4 i Q S 6 Z 1 z 3 U S 9 D K m U H A J c 7 v W T z U k j E / Y C H o G I x a C 9 r L F 0 n N a M 8 6 Q B r E y L 0 K 6 c O 1 f E x k L t Z 6 F v u k M G Y 7 1 a i 0 3 / 6 v 1 U g y u v U z k N 0 H E l x 8 F q a Q Y 0 z w B O h Q K O M q Z A c a V M M t S P m a K c T Q 5 2 S Y F d / X m d W h f 1 l 3 D D 0 6 1 c V P k U S Z n 5 J x c E J d c k Q a 5 I 0 3 S I p w k 5 J m 8 k j d r a r 1 Y 7 9 b H s r V k F T O n 5 I + s z x 8 d n 5 J C &lt; / l a t e x i t &gt; Input &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y r W O l U Z a s F V y R t A h f v r h w M 3 z O b Q = " &gt; A A A B 9 H i c b Z C 7 S g N B F I b P x l t c b 1 F L L R Z D w C r s 2 m g Z t N E u A X O B 7 B J m J 7 P J k N k L M 2 e D Y c l r W A k K Y m v t e 1 j Z + S j O J i k 0 8 Y e B j / + c w 5 z z + 4 n g C m 3 7 y y i s r W 9 s b h W 3 z Z 3 d v f 2 D 0 u F R S 8 W p p K x J Y x H L j k 8 U E z x i T e Q o W C e R j I S + Y G 1 / d J P X 2 2 M m F Y + j e 5 w k z A v J I O I B p w S 1 5 b r I H j C 7 i 5 I U p 7 1 S 2 a 7 a M 1 m r 4 C y g X D v 9 a H w D Q L 1 X + n T 7 M U 1 D F i E V R K m u Y y f o Z U Q i p 4 J N z Y q b K p Y Q O i I D 1 t U Y k Z A p L 5 s t P b U q 2 u l b Q S z 1 i 9 C a u e a v i Y y E S k 1 C X 3 e G B I d q u Z a b / 9 W 6 K Q Z X X s b z m 1 h E 5 x 8 F q b A w t v I E r D 6 X j K K Y a C B U c r 2 s R Y d E E o o 6 J 1 O n 4 C z f v A q t i 6 q j u a H j u I a 5 i n A C Z 3 A O D l x C D W 6 h D k 2 g k M A j P M O L M T a e j F f j b d 5 a M B Y z x / B H x v s P L 9 6 U h w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y y e 6 1 + Z k D r E Q D N j 5 o P g o s t h + S U A = " &gt; A A A B 9 H i c b Z D L S s N A F I Y n X m u 8 V V 0 q M l g K r k r i R p d F N 7 p r w V 6 g C W U y n b R D J 5 M w c 1 I s o U t f w Z W g I G 5 d 9 z 1 c + Q y + h N P L Q l t / G P j 4 z z n M O X + Q C K 7 B c b 6 s l d W 1 9 Y 3 N 3 J a 9 v b O 7 t 5 8 / O K z r O F W U 1 W g s Y t U M i G a C S 1 Y D D o I 1 E 8 V I F A j W C P o 3 k 3 p j w J T m s b y H Y c L 8 i H Q l D z k l Y C z P A / Y A 2 Z 1 M U h i 1 8 w W n 5 E y F l 8 G d Q 6 F 8 M q 5 + P 5 6 O K + 3 8 p 9 e J a R o x C V Q Q r V u u k 4 C f E Q W c C j a y i 1 6 q W U J o n 3 R Z y 6 A k E d N + N l 1 6 h I v G 6 e A w V u Z J w F P X / j W R k U j r Y R S Y z o h A T y / W J u Z / t V Y K 4 Z W f 8 c l N T N L Z R 2 E q M M R 4 k g D u c M U o i K E B Q h U 3 y 2 L a I 4 p Q M D n Z J g V 3 8 e Z l q F + U X M N V E 8 c 1 m i m H j t E Z O k c u u k R l d I s q q I Y o S t A T e k G v 1 s B 6 t t 6 s 9 1 n r i j W f O U J / Z H 3 8 A A + J l e 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y y e 6 1 + Z k D r E Q D N j 5 o P g o s t h + S U A = " &gt; A A A B 9 H i c b Z D L S s N A F I Y n X m u 8 V V 0 q M l g K r k r i R p d F N 7 p r w V 6 g C W U y n b R D J 5 M w c 1 I s o U t f w Z W g I G 5 d 9 z 1 c + Q y + h N P L Q l t / G P j 4 z z n M O X + Q C K 7 B c b 6 s l d W 1 9 Y 3 N 3 J a 9 v b O 7 t 5 8 / O K z r O F W U 1 W g s Y t U M i G a C S 1 Y D D o I 1 E 8 V I F A j W C P o 3 k 3 p j w J T m s b y H Y c L 8 i H Q l D z k l Y C z P A / Y A 2 Z 1 M U h i 1 8 w W n 5 E y F l 8 G d Q 6 F 8 M q 5 + P 5 6 O K + 3 8 p 9 e J a R o x C V Q Q r V u u k 4 C f E Q W c C j a y i 1 6 q W U J o n 3 R Z y 6 A k E d N + N l 1 6 h I v G 6 e A w V u Z J w F P X / j W R k U j r Y R S Y z o h A T y / W J u Z / t V Y K 4 Z W f 8 c l N T N L Z R 2 E q M M R 4 k g D u c M U o i K E B Q h U 3 y 2 L a I 4 p Q M D n Z J g V 3 8 e Z l q F + U X M N V E 8 c 1 m i m H j t E Z O k c u u k R l d I s q q I Y o S t A T e k G v 1 s B 6 t t 6 s 9 1 n r i j W f O U J / Z H 3 8 A A + J l e 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o g i / d z B g n Y D H o p 3 X e u 9 j 4 C j D E U 8 = " &gt; A A A B 9 H i c b Z D L S s N A F I Y n 9 V b j r e r S z W A p u C q J G 1 0 W 3 e i u g r 1 A G 8 p k e t I O n V y Y O S m W 0 N d w J S i I W 1 / G l W / j p M 1 C W 3 8 Y + P j P O c w 5 v 5 9 I o d F x v q 3 S x u b W 9 k 5 5 1 9 7 b P z g 8 q h y f t H W c K g 4 t H s t Y d X 2 m Q Y o I W i h Q Q j d R w E J f Q s e f 3 O b 1 z h S U F n H 0 i L M E v J C N I h E I z t B Y / T 7 C E 2 b 3 U Z L i f F C p O n V n I b o O b g F V U q g 5 q H z 1 h z F P Q 4 i Q S 6 Z 1 z 3 U S 9 D K m U H A J c 7 v W T z U k j E / Y C H o G I x a C 9 r L F 0 n N a M 8 6 Q B r E y L 0 K 6 c O 1 f E x k L t Z 6 F v u k M G Y 7 1 a i 0 3 / 6 v 1 U g y u v U z k N 0 H E l x 8 F q a Q Y 0 z w B O h Q K O M q Z A c a V M M t S P m a K c T Q 5 2 S Y F d / X m d W h f 1 l 3 D D 0 6 1 c V P k U S Z n 5 J x c E J d c k Q a 5 I 0 3 S I p w k 5 J m 8 k j d r a r 1 Y 7 9 b H s r V k F T O n 5 I + s z x 8 d n 5 J C &lt; / l a t e x i t &gt; Predicting 3D dynamics.In the top row, the boxed image is the single-frame input to the hallucinator while the left and right images are the ground truth past and future respectively. The second and third rows show two views of the predicted meshes for the past, present, and future given the input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 5 :</head><label>5</label><figDesc>Architecture of f movie .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Ambiguous motion. Dynamic prediction is difficult from the center image alone, where her arms may reasonably lift or lower in the future. Ambiguous pose. The tennis player's pose in the input, center image is difficult to disambiguate between hunched forward verses arched backward due to the motion blur. This makes it challenging for our model to recover accurate dynamics predictions from the single image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Three tiers of video datasets. We jointly train on videos with: full ground truth 2D and 3D pose supervision, only ground truth 2D supervision, and pseudo-ground truth 2D supervision. Note the difference in scale for pseudo-ground truth datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>MPJPE ? PA-MPJPE ? Accel Error ? PCK ? Accel PCK ? Accel</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>3DPW</cell><cell></cell><cell cols="2">NBA</cell><cell cols="2">Penn Action</cell></row><row><cell cols="2">PCK ? Single-view retrained [31] 84.1</cell><cell>130.0</cell><cell>76.7</cell><cell>37.4</cell><cell>55.9</cell><cell>163.6</cell><cell>73.2</cell><cell>79.9</cell></row><row><cell>Context. no dynamics</cell><cell>82.6</cell><cell>139.2</cell><cell>78.4</cell><cell>15.2</cell><cell>64.2</cell><cell>46.6</cell><cell>71.2</cell><cell>29.3</cell></row><row><cell>Contextual</cell><cell>86.4</cell><cell>127.1</cell><cell>80.1</cell><cell>16.4</cell><cell>68.4</cell><cell>44.1</cell><cell>77.9</cell><cell>29.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Local vs temporal context. Our temporal encoder produces smoother predictions, significantly lowering the acceleration error. We also find that training for dynamic prediction considerably improves 2D keypoint estimation.</figDesc><table><row><cell></cell><cell></cell><cell>3DPW</cell><cell>H36M</cell></row><row><cell></cell><cell cols="3">MPJPE ? PA-MPJPE ? PA-MPJPE ?</cell></row><row><cell>Martinez et al. [37]</cell><cell>-</cell><cell>157.0</cell><cell>47.7</cell></row><row><cell>SMPLify [8]</cell><cell>199.2</cell><cell>106.1</cell><cell>82.3</cell></row><row><cell>TP-Net [15]</cell><cell>163.7</cell><cell>92.3</cell><cell>36.3</cell></row><row><cell>Ours</cell><cell>127.1</cell><cell>80.1</cell><cell>58.1</cell></row><row><cell>Ours + InstaVariety</cell><cell>116.5</cell><cell>72.6</cell><cell>56.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison to state-of-the-art 3D pose reconstruction approaches. Our approach achieves state-of-the-art performance on 3DPW. Good performance on Human3.6M does not always translate to good 3D pose prediction on in-the-wild videos. MPJPE ? PA-MPJPE ? PCK ? PCK ?</figDesc><table><row><cell></cell><cell></cell><cell>3DPW</cell><cell></cell><cell>NBA</cell><cell>Penn</cell></row><row><cell cols="2">PCK ? Ours 86.4</cell><cell>127.1</cell><cell>80.1</cell><cell>68.4</cell><cell>77.9</cell></row><row><cell>Ours + VLOG</cell><cell>91.7</cell><cell>126.7</cell><cell>77.7</cell><cell>68.2</cell><cell>78.6</cell></row><row><cell>Ours + InstaVariety</cell><cell>92.9</cell><cell>116.5</cell><cell>72.6</cell><cell>68.1</cell><cell>78.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>MPJPE ? PA-MPJPE ? Accel Err ? Mesh Pos ? Mesh Unp ? MPJPE ? PA-MPJPE ? Accel Err ? PCK ?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>3DPW</cell><cell></cell><cell></cell><cell></cell><cell>H3.6M</cell><cell></cell><cell>Penn Action</cell></row><row><cell cols="2">PCK ? Martinez et al. [37] -</cell><cell>-</cell><cell>157.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>62.9</cell><cell>47.7</cell><cell>-</cell><cell>-</cell></row><row><cell>SMPLify [8]</cell><cell>-</cell><cell>199.2</cell><cell>106.1</cell><cell>-</cell><cell>211.2</cell><cell>61.2</cell><cell>-</cell><cell>82.3</cell><cell>-</cell><cell>-</cell></row><row><cell>TP-Net [14]</cell><cell>-</cell><cell>163.7</cell><cell>92.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>52.1</cell><cell>36.3</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>86.4</cell><cell>127.1</cell><cell>80.1</cell><cell>16.4</cell><cell>144.4</cell><cell>25.8</cell><cell>87.0</cell><cell>58.1</cell><cell>9.3</cell><cell>77.9</cell></row><row><cell>Ours + VLOG</cell><cell>91.7</cell><cell>126.7</cell><cell>77.7</cell><cell>15.7</cell><cell>147.4</cell><cell>29.7</cell><cell>85.9</cell><cell>58.3</cell><cell>9.3</cell><cell>78.6</cell></row><row><cell>Ours + InstaVariety</cell><cell>92.9</cell><cell>116.5</cell><cell>72.6</cell><cell>14.3</cell><cell>138.6</cell><cell>26.7</cell><cell>83.7</cell><cell>56.9</cell><cell>9.3</cell><cell>78.7</cell></row><row><cell>Single-view retrained [31]</cell><cell>84.1</cell><cell>130.0</cell><cell>76.7</cell><cell>37.4</cell><cell>144.9</cell><cell>24.4</cell><cell>94.0</cell><cell>59.3</cell><cell>23.9</cell><cell>73.2</cell></row><row><cell>Ours -Dynamics</cell><cell>82.6</cell><cell>139.2</cell><cell>78.4</cell><cell>15.2</cell><cell>155.2</cell><cell>24.8</cell><cell>88.6</cell><cell>58.3</cell><cell>9.1</cell><cell>71.2</cell></row><row><cell>Ours -Const</cell><cell>86.5</cell><cell>128.3</cell><cell>78.2</cell><cell>16.6</cell><cell>145.9</cell><cell>27.5</cell><cell>83.5</cell><cell>57.8</cell><cell>9.3</cell><cell>78.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Ours -Const is trained without Lconst shape. MPJPE ? PA-MPJPE ? MPJPE ? PA-MPJPE ? PCK ?</figDesc><table><row><cell></cell><cell></cell><cell>3DPW</cell><cell></cell><cell></cell><cell>H3.6M</cell><cell>Penn</cell></row><row><cell cols="2">PCK ? Local 84.1</cell><cell>130.0</cell><cell>76.7</cell><cell>94.0</cell><cell>59.3</cell><cell>73.2</cell></row><row><cell>Local + Insta.</cell><cell>88.6</cell><cell>126.5</cell><cell>73.0</cell><cell>93.5</cell><cell>59.5</cell><cell>73.2</cell></row><row><cell>Improvement</cell><cell>5.4%</cell><cell>2.7%</cell><cell>4.8%</cell><cell>0.6%</cell><cell>-0.3%</cell><cell>0%</cell></row><row><cell>Temporal</cell><cell>86.4</cell><cell>127.1</cell><cell>80.1</cell><cell>87.0</cell><cell>58.1</cell><cell>77.9</cell></row><row><cell>Temp. + Insta.</cell><cell>92.9</cell><cell>116.5</cell><cell>72.6</cell><cell>83.7</cell><cell>56.9</cell><cell>78.7</cell></row><row><cell>Improvement</cell><cell>7.6%</cell><cell>8.4%</cell><cell>9.3%</cell><cell>3.7%</cell><cell>2.0%</cell><cell>1.0%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank David Fouhey for providing us with the people subset of VLOG, Rishabh Dabral for providing the source code for TP-Net, Timo von Marcard and Gerard Pons-Moll for help with 3DPW, and Heather Lockwood for her help and support. This work was supported in part by Intel/NSF VEC award IIS-1539099 and BAIR sponsors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep lip reading: A comparison of models and an online application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recovering 3d human pose from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optical flow-based 3d human motion estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kassubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="347" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video based reconstruction of 3d people model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aslam</surname></persName>
		</author>
		<idno>2018-05-15. 5</idno>
		<ptr target="https://www.omnicoreagency.com/youtube-statistics/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep representation learning for human motion prediction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>B?tepage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kjellstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Forecasting human dynamics from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3643" to="3651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory kalman filters: Recurrent neural estimators for pose regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Coskun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Achilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Dipietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structure-aware and temporally coherent 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4414" to="4423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From lifestyle vlogs to everyday interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Im2flow: Motion hallucination from static images for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detect-and-Track: Efficient Pose Estimation in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inferring 3d structure with a statistical image-based shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">641</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1381" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Reticam: Real-time human performance capture from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="69" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards accurate marker-less human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structuralrnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8320" to="8329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Endto-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Flow-grounded spatial-temporal video prediction from still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Autoconditioned recurrent networks for extended complex human motion synthesis. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recurrent 3d pose sequence machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5543" to="5552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">SMPL: A skinned multi-person linear model. SIG-GRAPH Asia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Stable recurrent models. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model-based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sfv: Reinforcement learning of physical skills from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Asia</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">DeepCut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Data distillation: Towards omni-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Nrsfm using local rigidity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rehan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Usmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">General automatic human shape and motion capture using volumetric contour cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Robertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="509" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Combined discriminative and generative articulated pose and non-rigid shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1337" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Indirect deep structured learning for 3d human shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K V</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep autoencoder for combined human pose estimation and body model upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5242" to="5252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">BodyNet: Volumetric inference of 3D human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Neural kinematic networks for unsupervised motion retargetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dense optical flow prediction from a static image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2443" to="2451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The pose knows: Video forecasting by generating pose futures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">3d reconstruction of human motion from monocular image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1505" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Monoperfcap: Human performance capture from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>27:1-27:15</idno>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2248" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Parametric reshaping of human bodies in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">126</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4966" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
