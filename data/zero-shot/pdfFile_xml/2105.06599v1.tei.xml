<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TriPose: A Weakly-Supervised 3D Human Pose Estimation via Triangulation from Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Gholami</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Rezaei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><forename type="middle">Rhodin</forename><surname>Rabab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ward</forename><forename type="middle">Z Jane</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TriPose: A Weakly-Supervised 3D Human Pose Estimation via Triangulation from Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating 3D human poses from video is a challenging problem. The lack of 3D human pose annotations is a major obstacle for supervised training and for generalization to unseen datasets. In this work, we address this problem by proposing a weakly-supervised training scheme that does not require 3D annotations or calibrated cameras. The proposed method relies on temporal information and triangulation. Using 2D poses from multiple views as the input, we first estimate the relative camera orientations and then generate 3D poses via triangulation. The triangulation is only applied to the views with high 2D human joint confidence. The generated 3D poses are then used to train a recurrent lifting network (RLN) that estimates 3D poses from 2D poses. We further apply a multi-view re-projection loss to the estimated 3D poses and enforce the 3D poses estimated from multi-views to be consistent. Therefore, our method relaxes the constraints in practice, only multi-view videos are required for training, and is thus convenient for in-thewild settings. At inference, RLN merely requires single-view videos. The proposed method outperforms previous works on two challenging datasets, Human3.6M and MPI-INF-3DHP. Codes and pretrained models will be publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Monocular 3D human pose estimation has gained much attention due to its various applications. It is however an ill-posed problem and most of the proposed solutions rely on supervised training that requires 3D annotations <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28]</ref>. Obtaining 3D human annotations is not trivial and the available datasets are mostly specific to the lab settings. Therefore, generalization to in-thewild applications remains challenging. Weakly-supervised methods were proposed to address this problem using unpaired 2D and 3D annotations <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b19">20]</ref>, limited available 1 2 3 <ref type="figure">Figure 1</ref>. Sample predictions of the proposed weakly-supervised method for in-the-wild videos from 3DPW dataset. We note that human joints that are occluded in some frames can be estimated correctly.</p><p>3D annotations <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b30">30]</ref>, or calibrated multi-view recordings <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33]</ref>. However, obtaining such information for the unsupervised learning task is still an obstacle. To the best of our knowledge, there are only a few works that propose weaklysupervised training schemes without using any 3D annotation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40]</ref>. <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b39">[39]</ref> propose multi-view consistency as a supervision while <ref type="bibr" target="#b17">[18]</ref> generates pseudo ground-truth 3D poses using epipolar geometry. Although multi-view consistency is a promising self-supervision element, it is relatively a weak cue. 3D poses from different views that are enforced to be consistent may still be erroneous. Generating pseudo ground truth <ref type="bibr" target="#b17">[18]</ref> by triangulation is another approach that has shown promising performance. However, the final error is limited to the error of the generated pseudo ground truth.</p><p>In this work, we propose a method that combines multiview cameras with triangulation supervision while leveraging temporal dependency in video using a temporal lifting network. This approach needs neither 3D annotations nor calibrated cameras for training. <ref type="figure">Fig. 1</ref> illustrates the performance of our method in unseen videos, and <ref type="figure">Fig. 2</ref> shows the overview of the proposed method. The relaxations made in this work avail the training with in-the-wild multi-view images. Inspired by EpipolarPose <ref type="bibr" target="#b17">[18]</ref>, we generate 3D poses using triangulation. However, we integrate camera joint confidence scores into the triangulation to choose sufficiently good views. This step helps diminish wrong triangulation for challenging datasets such as MPI-INF-3DHP <ref type="bibr" target="#b22">[23]</ref>. In fact, instead of triangulating complete but poor pseudo ground truths for the lifting network, we choose to generate pseudo ground truths that are sufficiently good.</p><p>The generated pseudo ground truth is only taken as a guidance. We further enforce estimated 3D poses to be consistent from different views using multi-view re-projection, which essentially addresses the wrong depth estimation and tackles possible errors in triangulation. Note that even though this re-projection loss is equivalent to the preprocessing triangulation objective, it behaves differently since it is used to train the neural network parameters that have to fit consistently to all training images as well as being temporally consistent, as we explain below.</p><p>Despite previous weakly-supervised works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">39]</ref>, we propose learning from video instead of a single frame. Pavllo et al. <ref type="bibr" target="#b30">[30]</ref> use a temporal convolution network for semi-supervised training using re-projection loss. However, their re-projection scheme requires the root trajectory to be estimated. Estimating root trajectory from video is a difficult task. Therefore they require a small set of 3D annotations to initialize the training. Kocabas et al. <ref type="bibr" target="#b16">[17]</ref> use a temporal motion discriminator in an adversarial training scheme, which uses an archive of ground truth 3D poses. Despite the limitations, both <ref type="bibr" target="#b30">[30]</ref> and <ref type="bibr" target="#b16">[17]</ref> demonstrate the advantage of exploiting the temporal information in the video. The temporal structure helps with the problem of occluded body parts in a single frame and reduces jittery motions, as the network can benefit from the pose information of previous frames. Similarly, we employ a recurrentbased temporal lifting network to map 2D poses to 3D ones. By contrast to previous approaches, we do not require any 3D annotation for training.</p><p>Single-view re-projection from 3D to 2D has been used in previous weakly-supervised works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b38">38]</ref>. Due to perspective ambiguity, a single 2D pose corresponds to many different 3D poses. Therefore, re-projection alone is insufficient for training and should be accompanied by other supervision elements such as unpaired 3D annotations <ref type="bibr" target="#b38">[38]</ref> or limited 3D annotations <ref type="bibr" target="#b30">[30]</ref>. In this work we use multiview re-projection, which is often more convenient to obtain than unpaired/limited 3D annotations. Multi-view reprojection does not suffer from perspective ambiguity since there is only one 3D pose that can be accurately re-projected to multi-view 2D planes.</p><p>We evaluate our method on two public datasets, namely Human3.6M <ref type="bibr" target="#b11">[12]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b22">[23]</ref>, and achieve state-of-the-art results. We also qualitatively evaluate the performance of our method on in-the-wild videos. The contributions of our work are as follows:</p><p>? Proposing a weakly supervised training scheme that requires neither camera extrinsic nor any 3D annotations.</p><p>? Proposing a temporal lifting network that is tailored for and supports self-supervised training.</p><p>? Proposing a multi-view re-projection method that enforces view-consistency.</p><p>? Providing in-depth comparisons with existing supervision strategies and state-of-the-art results on two most widely used 3D pose estimation benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Full-3D Supervision. Fully supervised 3D human pose estimation methods can be broadly divided into two categories: i) end-to-end training that accepts image or video as input and directly estimates 3D poses <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b20">21]</ref>; ii) two-step methods that firstly estimate 2D skeleton and then lift this 2D pose to 3D space <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b35">35]</ref>. The second approach has the advantage of intermediate supervision from large in-the-wild 2D annotations. Recent works have shown promising results using lifting methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">30]</ref> and our approach falls in this category.</p><p>Temporal Supervision. Temporal information provides further knowledge of previous and future human motions, which can help with occlusion and jittery estimations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16]</ref>. A long short-term memory (LSTM) sequence to sequence method was proposed to exploit temporal information in video and predict a sequence of 3D poses from a sequence of 2D poses <ref type="bibr" target="#b10">[11]</ref>. Sequence to sequence pose models were shown to be prone to drift <ref type="bibr" target="#b30">[30]</ref>. Pavllo et al. <ref type="bibr" target="#b30">[30]</ref> propose a dilated convolutional network to explore long history information, while being robust to drift in a long sequences. <ref type="bibr" target="#b16">[17]</ref> also uses a gated recurrent unit (GRU) encoder to encode the sequence of pose information generated by convolutional layers. Our approach is close to <ref type="bibr" target="#b16">[17]</ref>, however we utilize a residual network <ref type="bibr" target="#b21">[22]</ref> to decode pose information. We show that a many-input to singleoutput recurrent lifting network achieves better results than that of the sequence to sequence model <ref type="bibr" target="#b10">[11]</ref>.</p><p>Unpaired/Limited 3D supervision. Obtaining limited or unpaired 3D annotations is more convenient than full 3D annotations. Employing generative adversarial networks have been extensively investigated to use unpaired 2D and 3D data <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b5">6]</ref>. In these works, the generator is expected to estimate 3D fake poses that are as plausible as the real poses so that the discriminator can not distinguish between real and fake 3D poses. The re-projection error has been commonly used along with adversarial losses to further supervise the training <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b14">15]</ref>. Partial 3D annotation  <ref type="figure">Figure 2</ref>. Overview of the proposed weakly supervised pose estimation approach. Video frames from two views are firstly used to estimate the relative camera parameters and also to triangulate a 3D pose in the cameras coordinate system. The triangulated 3d poses are used for training the lifting network. The 3D predictions are re-projected to multi-view 2D to further supervise the lifting network.</p><p>has also been showed to be effective for semi-supervised training along with re-projection <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b6">7]</ref> or multi-view consistency loss <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b25">25]</ref>. The idea of multi-view consistency is that the estimated 3D poses from different views should be the same. The multi-view constraint has also been used to learn geometry of human poses in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">32]</ref>, which lessens the need for 3D annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervision without 3D.</head><p>Recently, there is more interest in training schemes that are fully independent of 3D annotations. The required additional supervision can be provided by cycle consistency loss <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6]</ref> or multi-view data <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b17">18]</ref>. This relaxation facilitates training for many in-the-wild applications like field sports (hockey, football) <ref type="bibr" target="#b3">[4]</ref> that multi-view data is already available. To our knowledge, there are only three similar works in the literature that merely need multi-view data as input <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">39]</ref>. Kocabas et al. <ref type="bibr" target="#b17">[18]</ref> generate pseudo 3D ground truth by triangulation from multi-view data. This simple approach was shown to be effective if 2D keypoints are sufficiently accurate. The main limitation of this approach is the lack of robustness to error in the input 2D keypoints. Iqbal et al. <ref type="bibr" target="#b12">[13]</ref> propose a 2.5 training method that disentangles depth from x and y coordinates. They use an independent 2D annotation for estimating x and y. For depth estimation, assuming that the estimated 3D pose from all views should be the same, they use a multi-view consistency loss between estimated 3D poses from different views. Although multi-view consistency has been shown to be strong, the indirect supervision remains vague when two wrong 3D poses are consistent. <ref type="bibr" target="#b39">[39]</ref> proposes a multi-view re-projection of estimated 3D poses back to 2D. Instead of simply using a loss between multi-view estimated 3D poses, <ref type="bibr" target="#b39">[39]</ref> defines the loss between re-projected 2D poses. Comparing with multi-view consistency in 3D, multi-view re-projection to 2D has been shown to be more promising for view-consistency enforcing.</p><p>In the absence of 3D annotations, accurate 2D poses are important for accurate multi-view re-projection. Occluded body parts can lean to degenerated solutions. Therefore, we encode information from a sequence of 2D keypoints using a temporal lifting network. Similar to EpipolarPose <ref type="bibr" target="#b17">[18]</ref>, we generate pseudo ground truth 3D annotations to train the lifting network. However, to ensure triangulation is only done from appropriate views, we also incorporate confidence of 2D joint. Furthermore, we employ a multi-view re-projection loss so that the training is not fully dependent on pseudo ground truth annotations. The overall method is shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our framework requires un-calibrated multi-view videos for training and can be applied to a single video at inference time. During training, we use triangulation to produce a rough pseudo ground truth 3D annotation and selfcalibrated camera orientation. We then train a recurrent lifting network to estimate the single 3D pose from a sequence of input 2D poses. The training criteria are the least-square loss to the pseudo 3D annotations and the multi-view reprojection loss. With this design, inference only requires single-view videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Triangulation</head><p>We consider a set of 2D keypoints from two views as inputs, X 1,j , X 2,j , where j is in range of 2D keypoints. We also accept joint confidences [c 1,1 , ..., c 1,J ], [c 2,1 , ..., c 2,J ] from two views where J is the number of human joints. If the average confidence of any view is less than 0.8 or if the confidence of any of joint is less than 0.7, we exclude the view from triangulation. The numbers were selected empirically. The epipolar constrain is then applied to X 1,j , X 2,j through</p><formula xml:id="formula_0">X 1,j FX 2,j = 0,<label>(1)</label></formula><p>where F is fundamental matrix. The above constrain can be reformulated to Af = 0, where f is a vector form of F. It is solved by singular value decomposition (SVD) to find the fundamental matrix and remove outliers with RANSAC over a set of two views. The threshold for consensus set of RANSAC was set to 3/(f 1 + f 2 ) where f 1 and f 2 are focal length of two cameras. The level of confidence for estimated matrix to be correct was set to 0.999. From the fundamental matrix, we compute the relative camera orientation using essential matrix decomposition, E = K T FK We consider the first camera as the center of coordinate frame. Therefore, rotation matrix, R, and transition vector, t, of the first camera are an identical matrix and zero, respectively. The essential matrix decomposition has four solutions with different camera rotations. We use charity check to remove those camera parameters that after triangulation return a negative depth in the camera coordinate frame.</p><p>The estimated R and t are used to reconstruct 3D poses by polynominal triangulation <ref type="bibr" target="#b9">[10]</ref>. The triangulated 3D poses are in the camera coordinate system of the first view. They can be transferred to the second view using R. The output of triangulation is not scaled to actual human skeleton. Therefore, we use the average body bone length of the training set to adjust the scale of 3D poses. The triangulation loss is defined as mean per joint position error between the triangulated 3D pose? and estimated 3D pose Y</p><formula xml:id="formula_1">L T = ? ? Y 2 .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">View-Switching re-projection</head><p>Re-projecting the estimated 3D pose to image plane should generate a 2D pose identical to the input 2D in an ideal case. However, a 2D pose corresponds to many 3D poses due to perspective ambiguity and single-view reprojection can not disambiguate wrong depth estimates. On the other hand, re-projection to multi-view enforces correct depth estimation since there is only one point in 3D space that can be exactly projected to corresponding points in multiple views. In this work we use multi-view reprojection on an estimate of the relative camera orientation between multiple views. Therefore, we do not need camera extrinsic parameters for same-view projection. Furthermore, we use a weak perspective re-projection,</p><formula xml:id="formula_2">X (i,j) rep = 1 0 0 0 1 0 R ij X 3D ,<label>(3)</label></formula><p>R ij is the relative rotation matrix between view i and j. For re-projection to the same camera, R ij is equal to the identity. The relative camera parameter is initialized using the RANSAC algorithm in the previous step. We explain in Section 3.4 how this estimate is further refined with a lifting network. Since we are not estimating the root joint trajectory, the re-projected 2D poses are all in the same scale and there is an offset between root joints. Therefore, we perform a root centering and then normalization to the reconstructed and input 2D. The re-projection loss over all camera pairs i, j is as follows</p><formula xml:id="formula_3">L R = n i=1 n j=1 X 2D X 2D ? X (ij) rep X (ij) rep ,<label>(4)</label></formula><p>where X 2D and X (ij)</p><p>rep are actual and re-projected 2D skeletons and n is number of views. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates structure of our lifting network. We encode information from previous and future time frames using two layers of GRUs with size of 1024. Considering an input sequence of [X t?T , ..., X t+T ], GRU layers return the hidden state of sequence as [h t?T , ..., h t+T ]. Instead of using the final hidden state, we perform a maxpooling and average pooling over the sequence of hidden states and concatenate the two representations. The learnt information from GRU layers are then fed to blocks of residual neural network. The second part of the network has 2 fully connected residual blocks, each block with two fully connected layers with 1000 neurons. Kocabas et al. <ref type="bibr" target="#b16">[17]</ref> use two layers of GRU with self-attention mechanism as a motion discriminator. We use GRU layers with static pooling layers that aggregates information from all hidden layers. <ref type="bibr" target="#b16">[17]</ref> shows that using self-attention mechanism after GRU layers helped to dynamically learn important time frames. However, we did not observe a difference between static feature aggregation and self-attention mechanism in our experiments. <ref type="bibr" target="#b10">[11]</ref> uses the information from the last hidden layer of an LSTM-based encoder and passes it to a decoder. We aggregate sequence of hidden layers and then use two fully connected residual blocks to return 3D poses following the work in <ref type="bibr" target="#b21">[22]</ref>. We do not use batch normalization and dropout of <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Lifting Network</head><p>For training the lifting network, two loss functions are used. This includes 2D multi-view re-projection loss (L R ) and triangulation loss (L T ). The total lifting network loss is as follows</p><formula xml:id="formula_4">L = L R + L T .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Camera Correction Network</head><p>We use the same structure of two GRU layers and two residual blocks for a camera network to estimate the relative rotation matrix R ? R 3?3 between the two input views. Despite the lifting network that has only one sequence of 2D poses as input, the input to the camera network consists of two sequences of 2D poses from both views. This input allows the network to learn the relative rotation matrix of two input views. Despite previous works that estimate a rotation matrix for each camera <ref type="bibr" target="#b39">[39]</ref>, estimating the relative rotation matrix has the benefit of using multiple views as input to the camera model. Moreover, since we do not need the camera network during inference, our framework can work with single-view input in the inference time. Considering two input views, the first and second view rotation matrices are R i and R j . The relative rotation matrix of these two views is</p><formula xml:id="formula_5">R ij = R j ? R T i . Camera correction network estimatesR ij which is defined byR ij = R ij ? R ij (triang)</formula><p>where R ij is the actual rotation matrix and R ij (triang) is the rotation matrix computed in triangulation. This means that our camera network corrects the previous rotation matrix. We optimize the parameters of camera correction network to minimize the objective function in equation 4. The computed loss in the equation 4 back-propagates through the camera correction network as well as the lifting network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Adversarial Training</head><p>We further analyse the performance of our lifting network for unpaired 2D and 3D annotation instead of multiview data, as in <ref type="bibr" target="#b38">[38]</ref>. In these experiments, we aim to merely evaluate performance of our temporal lifting network for common weakly supervised training schemes. The input for these experiments is a single-view video, however we have a collection of unpaired 3D human poses. This enables combining outdoor images with 2D labels and 3D poses captured in an indoor motion capture studio. We implement a Wasserstein GAN <ref type="bibr" target="#b1">[2]</ref> that includes a generator and a critic network. The introduced lifting network in the previous section is used as a generator. The critic network tries to maximize the distance between the output of the lifting network and batches of real 3D human pose. We use a motion critic network that accepts single 3D pose as well as a sequence of 3D poses. This motion critic is compromised of 2 layers of GRU units followed by a linear layer. The parameters of the critic are clipped between [?0.01, 0.01]. We also performed experiments on different critic models introduced in <ref type="bibr" target="#b38">[38]</ref> and a convolutional network with four convolutional layers and a maxpooling layer in the middle. Our experiment showed that the GRU critic outperformed the others even when we do not have a sequence of 3D poses as output. We use a single-view re-projection loss, L R , and an adversarial loss, L advG , from unpaired 3D annotations for training the lifting network as follows</p><formula xml:id="formula_6">L = L R + L advG ,<label>(6)</label></formula><p>where the loss functions used in adversarial training of the critic and the generator networks are defined as follows</p><formula xml:id="formula_7">L advC = E W ?P R [(C(W )] ? E W ?P G [(C(W )], (7) L advG = E W ?P G [(C(W )],<label>(8)</label></formula><p>where G and C are generator and critic networks, respectively and L advC is the adversarial loss for the critic network. P R and P G are distribution of real and generated 3D poses, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we introduce the datasets, metrics, and the training procedure. We then compare our results with state-of-the-art self-supervision methods and perform ablation studies on different parts of our framework. We use an AlphaPose network <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b8">9]</ref> pretrained on MPII dataset <ref type="bibr" target="#b0">[1]</ref> as the 2D backbone. The input 2D poses are normalized using frame height (h) and width (w) so that [0, w] is mapped to [?1, 1] while preserving the aspect ratio. Following previous works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b10">11]</ref>, we estimate the 3D poses in camera coordinate systems and all 3D joints are relative to the root joint (pelvis). We follow <ref type="bibr" target="#b17">[18]</ref> and freeze the triangulation part during training. The learning rate was 0.001 and Adam optimizer was used in all of the experiments except for the critic network. A stochastic gradient descent optimizer was used for the critic network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We use three different datasets to perform analysis on our framework. We also use in-the-wild videos to further evaluate plausibility of the predictions.</p><p>Human3.6M <ref type="bibr" target="#b11">[12]</ref> is the most popular 3D human dataset that includes videos from 4 views recorded in an indoor setting. The dataset is from 7 subjects that perform 15 challenging actions. We use the standard splitting of training and test sets, and use subjects 1,5,6,7,8 for training and subjects 9 and 11 for testing. We use the original 50Hz frame rate without down-sampling the video. We do not perform data augmentation on the dataset.</p><p>MPI-INF-3DHP <ref type="bibr" target="#b22">[23]</ref> has been recorded in a lab setting, same as Human3.6m dataset. However, the test set has outdoor recorded videos. The dataset includes 14 camera views. We follow the standard protocol and use 5 chest camera views for training. We use the original frame rate of the dataset for training and testing.</p><p>3DPW <ref type="bibr" target="#b37">[37]</ref> has been recorded fully in-the-wild. The dataset includes videos from a single view. The camera is moving while the subject performs different daily activities in street, park, or indoors. We use this dataset for qualitative analysis. We also use a sample ski video from YouTube for further evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics</head><p>We use three variants of the mean per joint position error (MPJPE). It computes the average euclidean distance between true and estimated 3D poses. Since our method is self-supervised, the scale of estimated 3D poses can be arbitrary. We therefore normalize triangulated poses by the average bone lengths in the training set as explained in Section 3.1. Following previous work, we also report the NM-PJPE, the MPJPE after normalizing scale, which does not require using the training set lengths. In addition, we perform Procrustes alignment on the estimated 3D poses and compute the MPJPE on the aligned pose (PMPJPE). PM-PJPE thereby excludes rotation, scale, and transition errors. <ref type="table">Table 1</ref> shows comparison results on the Human3.6M dataset. We consider three different levels of supervision. First, 3D, the fully supervised baseline that uses 2D and 3D annotations of training. In this scenario, the 2D estimator has been fine-tuned on 2D annotation of the training set. Second, UP-MV, in this scenario multi-view data is available for self-supervision and 2D annotations of training set are also provided for fine tuning. We obtain an error of 56.7 mm in UP-MV which is lower than fully supervised training of recurrent temporal models in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b16">[17]</ref>. This shows the superiority of our lifting network comparing with the previous recurrent temporal models. Our fully-supervised training error compares well with state-of-the art methods ae well as with fully-supervised training of <ref type="bibr" target="#b12">[13]</ref>. In the third scenario, MV, we use only multi-view data without 2D annotation. We obtain an error of 64.4 mm using temporal GRU lifting network (GRUA) and an error of 62.9 <ref type="table">Table 1</ref>. Estimation results of different networks for the Hu-man3.6M dataset. All values are in millimeters and the lower the better. UP-3D means unpaired 3D and MV means multi-view. The check-mark in the 2D column indicates that the ground truth 2D of training set is used. The best results are marked bold. mm using temporal lifting network introduced by Pavllo et al. <ref type="bibr" target="#b30">[30]</ref>, which is 6 mm (8%) better than the previous stateof-the-art method <ref type="bibr" target="#b12">[13]</ref>. <ref type="bibr" target="#b39">[39]</ref>. The gap between our weaklysupervised and fully-supervised results is only 8 mm which is smaller than the same gap from Iqbal et al. <ref type="bibr" target="#b12">[13]</ref>. There is a great margin between our results and the results from Kocabas et al <ref type="bibr" target="#b17">[18]</ref> who also use triangulation. In the next section we validate our results without temporal model and multi-view re-projection by results from <ref type="bibr" target="#b17">[18]</ref> and show that the improvements are gained from these two supervision elements. We further evaluate the performance of our framework on 3DHP dataset with two different levels of supervision. The evaluation set of 3DHP is more challenging than Hu-man3.6M and contains outdoor videos. <ref type="table" target="#tab_1">Table 2</ref> shows our results on this dataset. In fully supervised training, our error is 9 mm less than the previous works. Our weaklysupervised result has a great margin from previous works. When compared with other weakly-supervised methods that rely on unpaired 3D annotations, we obtain comparable results with these works <ref type="bibr" target="#b19">[20]</ref>, however our training scheme is more convenient since it is independent of 3D annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head><p>We also perform qualitative analysis on 3DPW, which is an in-the-wild dataset. This dataset is not multi-view, therefore we do not fine-tune the model on this dataset. <ref type="figure" target="#fig_3">Fig. 5</ref> shows performances of our method on challenging tasks such as ski and running outdoors. Here we use an pretrained AlphaPose for 2D joint detection and bounding boxes are detected using YOLOv3 <ref type="bibr" target="#b31">[31]</ref>. <ref type="figure" target="#fig_3">Fig. 5</ref> shows that temporal information enables the network to track the motion and correctly predict body parts in some of the frames that human joints are occluded. <ref type="figure" target="#fig_2">Fig. 4</ref> compares performances of EpipolarPose and TriPose on the test set of Human3.6M and in-the-wild videos from 3DPW. Although EpipolarPose shows promising estimation using Hu-man3.6M images, its predictions for in-the-wild frames are poor. EpipolarPose uses the mean and standard deviation of Human3.6M dataset to normalize the input frames. Therefore, the trained models are not generalizable to unseen datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Temporal Networks and Self-Supervision</head><p>We evaluate performance of our temporal lifting network on three training schemes including unpaired data annotations using adversarial training, UP-3D, multi-view supervision from re-projection only, MV-Reproj, and multi-view supervision from triangulation, MV-Triang. Particularly, we compare performance of our lifting network with dilated convolutional neural network (dilated-CNN) that was introduced in <ref type="bibr" target="#b30">[30]</ref>. Dilated-CNN showed promising performance when strong supervision is provided. <ref type="table" target="#tab_2">Table 3</ref> shows that our model outperforms in UP-3D and MV-Reproj. In MV-Triang, the generated pseudo ground truth annotations make the training close to supervised training. Therefore, dilated-CNN outperforms our lifting network by 1.5 mm. We hypothesize that dilated-CNN needs stronger supervision than our lifting network. Our recurrent lifting network outperforms the recurrent lifting network proposed in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr" target="#b10">[11]</ref> uses an LSTM-based sequence to sequence model to exploit temporal information. For fully supervised training, <ref type="table">Table 1</ref> shows that our model obtained an error of 55.4 mm while Hossain et al. <ref type="bibr" target="#b10">[11]</ref> obtain 58.3 mm error. We observed that using sequence to one output is better than sequence to sequence model. Weakly-supervised adversarial models have been widely used in literature for  single frames <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b14">15]</ref>. The proposed temporal network can therefore further improve previous weakly-supervised training frameworks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Studies</head><p>We perform ablation studies on supervision elements of our framework and on temporal information.  dicates that using temporal information significantly improves our MPJPE results. We use 27 frames as the input. Increasing the number of frames to 81 did not change our results. The idea of using multi-view re-projection along with triangulation could improve the results by 2 mm both in single-frame input and video-input. Triangulation alone with single-frame input obtains an error of 74.4 mm. Kocabas et al. <ref type="bibr" target="#b17">[18]</ref> reported the error of 77.0 mm with only triangulation. This validates that the method with only triangulation is a baseline for our work. Our error reduces from 74.4 to 64.4 when we use temporal information and multi-view re-projection. We also perform an ablation study on the error of 2D keypoints. <ref type="table" target="#tab_5">Table 5</ref> shows the affect of the accuracy of 2D keypoints on the final estimation. The accuracy of 2D keypoints is critical for the lifting network as well as triangulation. Our experiments show that estimation of the rotation matrix by SVD particularly is affected by 2D errors. Using the fine-tuned model or a pretrained model with 8.9 pixel error <ref type="bibr" target="#b12">[13]</ref> could obtain an error of less than 40 mm for triangulation. This error level is comparable to that of state-of-the-art fully supervised training models <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Conclusions and Future Work</head><p>Weakly-supervised training for 3D pose estimation has been attracting increasing attention <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref>. How-ever, previous works did not exploit temporal information for weakly-supervised training. In this work, we showed that designing a temporal encoder for weakly-supervised training is challenging, as temporal models introduced for fully supervised training are not necessarily suitable for weakly-supervised setting. Furthermore, we showed that triangulation from multi-views together with multiview re-projection outperforms previous multi-view selfsupervision methods. We achieved state-of-the-art performance on benchmark datasets. In the future works, we plan to improve the proposed method from several aspects. Currently we freeze the triangulation part and the error of the multi-view re-projection does not back-propagate through SVD. Therefore, each view contributes equally to the triangulation. Future work should address this by implementing a learnable triangulation <ref type="bibr" target="#b13">[14]</ref>. Our experiments showed that the error of 2D poses is critical in triangulation. However, our networks does not back-propagate the view-consistency losses to improve 2D poses. Future works should imple- ment an end-to-end framework that accepts sequence of frames as its input.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Overview of our temporal lifting network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative comparisons of EpipolarPose<ref type="bibr" target="#b17">[18]</ref> and the proposed TriPose using in-the-wild frames from 3DPW dataset and in-door videos from Human3.6M. TriPose and Epipolar-Pose show promising results in Human3.6 frames; while TriPose outperforms EpipolarPose when testing on unseen frames from 3DPW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative analysis examples on 3DPW dataset videos (first and second row) and challenging videos from YouTube (third row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Estimation results of different networks for the MPI-INF-3DHP dataset. All values are in millimeters and the lower the better. (*) indicates the use of extra in-the-wild data for training and (**) indicates the use of extra camera views.</figDesc><table><row><cell>Method</cell><cell cols="4">2D Supervision MPJPE NMPJPE PMPJPE</cell></row><row><cell>Rhodin [33]</cell><cell>3D</cell><cell>-</cell><cell>101.5</cell><cell>-</cell></row><row><cell>Kokabas [18]</cell><cell>3D</cell><cell>109.0</cell><cell>106.4</cell><cell>-</cell></row><row><cell>Iqbal [13]</cell><cell>3D</cell><cell>110.8</cell><cell>98.9</cell><cell>-</cell></row><row><cell>ours</cell><cell>3D</cell><cell>101.5</cell><cell>97.5</cell><cell>76.5</cell></row><row><cell>Kanzawa [15]</cell><cell>UP-3D</cell><cell>169.5</cell><cell>-</cell><cell>113.2</cell></row><row><cell>Kolotouros [19]</cell><cell>UP-3D</cell><cell>124.8</cell><cell>-</cell><cell>80.4</cell></row><row><cell>Kundu (*) [20]</cell><cell>UP-3D</cell><cell>103.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Kocabas [18]</cell><cell>MV+R</cell><cell>126.79</cell><cell>125.65</cell><cell>-</cell></row><row><cell>Iqbal(*) [13]</cell><cell>MV</cell><cell>122.4</cell><cell>110.1</cell><cell></cell></row><row><cell>Wandt (**) [39]</cell><cell>MV</cell><cell>-</cell><cell>104.0</cell><cell>70.3</cell></row><row><cell>Ours</cell><cell>MV</cell><cell>105.6</cell><cell>101.1</cell><cell>78.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Human3.6M dataset: Evaluation of temporal lifting networks for different weakly-supervised training methods.</figDesc><table><row><cell>Lifting Net</cell><cell cols="2">Supervision MPJPE</cell></row><row><cell>Pavllo (27 frames) in [30]</cell><cell>UP-3D</cell><cell>85</cell></row><row><cell>GRUA (ours)</cell><cell>UP-3D</cell><cell>79</cell></row><row><cell>Pavllo (27 frames) in [30]</cell><cell>MV-Reproj</cell><cell>219.8</cell></row><row><cell>GRUA (ours)</cell><cell>MV-Reproj</cell><cell>144.6</cell></row><row><cell>Pavllo (27 frames) in [30]</cell><cell>MV-Triang</cell><cell>62.9</cell></row><row><cell>GRUA (ours)</cell><cell>MV-Triang</cell><cell>64.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Human3.6M dataset: Ablation study on supervision elements of the proposed model.</figDesc><table><row><cell></cell><cell>Supervision</cell><cell></cell></row><row><cell>Temp</cell><cell>Triang Reproj</cell><cell cols="2">MPJPE PMPJPE</cell></row><row><cell></cell><cell></cell><cell>74.4</cell><cell>56.1</cell></row><row><cell></cell><cell></cell><cell>72.5</cell><cell>54.7</cell></row><row><cell></cell><cell></cell><cell>66.0</cell><cell>48.8</cell></row><row><cell></cell><cell></cell><cell>64.4</cell><cell>48.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>in-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Human3.6M dataset: Ablation study on 2D errors. The unit of 2D error is pixel (px) and units of 3D errors are mm. 3D errors have been reported after Procrustes alignments (P).</figDesc><table><row><cell>Method</cell><cell cols="3">2D (px) Triang (P) 3D (P)</cell></row><row><cell>Pretrained (from [22])</cell><cell>12.5</cell><cell>53.6</cell><cell>58.5</cell></row><row><cell>Pretrained (ours)</cell><cell>8.9</cell><cell>30.0</cell><cell>48.8</cell></row><row><cell>Fine-tuned (from [30])</cell><cell>7.3</cell><cell>26.69</cell><cell>43.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal hockey action recognition via pose and optical flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Neher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanav</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zelek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d human pose estimation = 2d pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wending</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Can 3d pose be learned from 2d projections alone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Phuoc</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Triangulation. Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="157" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weaklysupervised 3d human pose learning via multi-view images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-supervised 3d human pose estimation via part guided novel image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mugalodi</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cascaded deep monocular 3d human pose estimation with evolutionary training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<editor>Mohamed Elgharib, Pascal Fua, Hans-Peter Seidel, Helge Rhodin, Gerard Pons-Moll, and</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Xnect: Real-time multi-person 3d motion capture with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="82" to="83" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiview-consistent semi-supervised learning for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d human pose estimation using convolutional neural networks with 2d pose information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungheon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihye</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 Workshops</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="156" to="169" />
		</imprint>
	</monogr>
	<note>Gang Hua and Herv? J?gou</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
	<note>Derpanis, and Kostas Daniilidis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="750" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Marquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Canonpose: Self-supervised monocular 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petrissa</forename><surname>Zell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Distill knowledge from nrsfm for weakly supervised 3d pose learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pose Flow: Efficient online pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
