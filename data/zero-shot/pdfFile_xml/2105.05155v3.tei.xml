<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TAG: TASK-BASED ACCUMULATED GRADIENTS FOR LIFELONG LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranshu</forename><surname>Malviya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer</orgName>
								<orgName type="institution">Software Engineering Mila -Quebec AI Institut? Ecole Polytechnique de Montr?al Montr?al</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Robert Bosch Center for Data Science and Artificial Intelligence Department of Computer Science and Engineering IIT Madras Chennai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer and Software Engineering Mila -Quebec AI Institut? Ecole Polytechnique de Montr?al Canada CIFAR AI Chair Montr?al</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TAG: TASK-BASED ACCUMULATED GRADIENTS FOR LIFELONG LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When an agent encounters a continual stream of new tasks in the lifelong learning setting, it leverages the knowledge it gained from the earlier tasks to help learn the new tasks better. In such a scenario, identifying an efficient knowledge representation becomes a challenging problem. Most research works propose to either store a subset of examples from the past tasks in a replay buffer, dedicate a separate set of parameters to each task or penalize excessive updates over parameters by introducing a regularization term. While existing methods employ the general task-agnostic stochastic gradient descent update rule, we propose a task-aware optimizer that adapts the learning rate based on the relatedness among tasks. We utilize the directions taken by the parameters during the updates by additively accumulating the gradients specific to each task. These task-based accumulated gradients act as a knowledge base that is maintained and updated throughout the stream. We empirically show that our proposed adaptive learning rate not only accounts for catastrophic forgetting but also exhibits knowledge transfer. We also show that our method performs better than several state-of-the-art methods in lifelong learning on complex datasets. Moreover, our method can also be combined with the existing methods and achieve substantial improvement in performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Lifelong learning (LLL), also known as continual learning, is a setting where an agent continuously learns from data belonging to different tasks <ref type="bibr" target="#b42">(Parisi et al., 2019)</ref>. Here, the goal is to maximize performance on all the tasks arriving in a stream without replaying the entire datasets from past tasks <ref type="bibr" target="#b45">(Riemer et al., 2018)</ref>. Approaches proposed in this setting involve investigating the stability-plasticity dilemma <ref type="bibr" target="#b38">(Mermillod et al., 2013)</ref> in different ways where stability refers to preventing the forgetting of past knowledge and plasticity refers to accumulating new knowledge by learning new tasks <ref type="bibr" target="#b38">(Mermillod et al., 2013;</ref><ref type="bibr" target="#b13">Delange et al., 2021)</ref>.</p><p>forgetting. These methods further are classified as data-focused <ref type="bibr" target="#b33">(Li &amp; Hoiem, 2017;</ref><ref type="bibr" target="#b55">Triki et al., 2017;</ref><ref type="bibr" target="#b31">Li et al., 2021)</ref> and prior-focused methods <ref type="bibr" target="#b41">(Nguyen et al., 2018;</ref><ref type="bibr" target="#b16">Ebrahimi et al., 2019)</ref>. In particular, Elastic Weight Consolidation (EWC) <ref type="bibr" target="#b28">(Kirkpatrick et al., 2017)</ref>, a prior-focused method, regularizes the loss function to minimize changes in the parameters important for previous tasks. Yet, when the model needs to adapt to a large number of tasks, the interference between task-based knowledge is inevitable with fixed model capacity. Parameter Isolation methods <ref type="bibr" target="#b48">(Rusu et al., 2016;</ref><ref type="bibr" target="#b61">Xu &amp; Zhu, 2018;</ref><ref type="bibr" target="#b51">Serra et al., 2018)</ref> such as  assign a model copy to every new task that arrives. These methods alleviate catastrophic forgetting in general, but they rely on a strong base network and work on a small number of tasks. Another closely related methods, called Expansion-based methods, handle the LLL problem by expanding the model capacity in order to adapt to new tasks <ref type="bibr" target="#b53">(Sodhani et al., 2018;</ref><ref type="bibr" target="#b43">Rao et al., 2019)</ref>. Replay-based methods maintain an 'episodic memory', containing a few examples from past tasks, that is revisited while learning a new task <ref type="bibr" target="#b45">(Riemer et al., 2018;</ref><ref type="bibr" target="#b25">Jin et al., 2020)</ref>. For instance, Averaged Gradient Episodic Memory (A-GEM) <ref type="bibr" target="#b9">(Chaudhry et al., 2018b)</ref>, alleviating computational inefficiency of GEM <ref type="bibr" target="#b34">(Lopez-Paz &amp; Ranzato, 2017)</ref>, uses the episodic memory to project the gradients based on hard constraints defined on the episodic memory and the current mini-batch. Experience Replay (ER) <ref type="bibr" target="#b10">(Chaudhry et al., 2019)</ref> uses both replay memory and input mini-batches in the optimization step by averaging their gradients to mitigate forgetting. On the other hand, we propose a replay-free method that maintains a fixed-capacity model during training and test time.</p><p>Task-relatedness <ref type="bibr" target="#b33">(Li &amp; Hoiem, 2017;</ref><ref type="bibr" target="#b24">Jerfel et al., 2019;</ref><ref type="bibr" target="#b52">Shaker et al., 2020)</ref> or explicitly learning task representations <ref type="bibr" target="#b62">(Yoon et al., 2017)</ref> is also an alternative approach studied in LLL. Efficient Lifelong Learning Algorithm (ELLA) <ref type="bibr" target="#b49">(Ruvolo &amp; Eaton, 2013)</ref> maintains sparsely shared basis vectors for all the tasks and refines them whenever the model sees a new task. <ref type="bibr" target="#b43">Rao et al. (2019)</ref> perform dynamic expansion of the model while learning task-specific representation and task inference within the model. Orthogonal Gradient Descent (OGD) ) maintains a space based on a subset of gradients from each task. As a result, OGD often faces memory issues during run-time depending upon the size of the model and the subset <ref type="bibr" target="#b5">(Bennani &amp; Sugiyama, 2020)</ref>. Unlike OGD, we accumulate the gradients and hence alleviate the memory requirements by orders for each task.</p><p>A recent work <ref type="bibr" target="#b39">(Mirzadeh et al., 2020)</ref> argues that tuning the hyper-parameters gives a better result than several state-of-the-art methods including A-GEM and ER. They introduce Stable SGD that involves an adjustment in the hyper-parameters like initial learning rate, learning rate decay, dropout, and batch size. They present this gain in performance on simplistic benchmarks like Permuted <ref type="bibr">MNIST (Goodfellow et al., 2013)</ref>, Rotated MNIST and Split-CIFAR100 <ref type="bibr" target="#b39">(Mirzadeh et al., 2020)</ref>. Another related work <ref type="bibr" target="#b21">(Gupta et al., 2020</ref>) discusses a similar technique of using learning rate with episodic memory to reflect the similarities between the old and new tasks. On the other hand, our method explicitly computes the similarities between the tasks to regulate the learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LIFELONG LEARNING SETUP</head><p>In this section, we introduce the notations and the LLL setup used in the paper. We focus on the standard task-incremental learning scenario which is adopted in the numerous state-of-the-art LLL methods. It involves solving new tasks using an artificial neural network with a multi-head output where each head is associated with a unique task and the task identity is known beforehand <ref type="bibr" target="#b34">(Lopez-Paz &amp; Ranzato, 2017;</ref><ref type="bibr" target="#b56">van de Ven &amp; Tolias, 2019;</ref><ref type="bibr" target="#b13">Delange et al., 2021)</ref>. We denote the current task as t and any of the previous tasks by ? . The model receives new data of the form {X (t) , D (t) , Y (t) } where X (t) are the input features, D (t) is the task descriptor (that is a natural number in this work) and Y (t) is the target vector specific to the task t.</p><p>We consider the 'single-pass per task' setting in this work following <ref type="bibr" target="#b34">(Lopez-Paz &amp; Ranzato, 2017;</ref><ref type="bibr" target="#b45">Riemer et al., 2018;</ref><ref type="bibr" target="#b10">Chaudhry et al., 2019)</ref> where the model is trained only for one epoch on the dataset. It is more challenging than the multiple pass setting used in numerous research works <ref type="bibr" target="#b28">(Kirkpatrick et al., 2017;</ref><ref type="bibr" target="#b44">Rebuffi et al., 2017)</ref>. The goal is to learn a classification model f (X (t) ; ?), parameterized by ? ? R P to minimize the loss L(f (X (t) ; ?), Y (t) ) for the current task t while preventing the loss on the past tasks from increasing. We evaluate the model on a held-out set of examples of all the tasks (? t) seen in the stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TASK-BASED ACCUMULATED GRADIENTS</head><p>The specific form of our proposed method depends on the underlying adaptive optimizer. For ease of exposition, we describe it as a modification of RMSProp <ref type="bibr" target="#b54">(Tieleman &amp; Hinton, 2012)</ref> here and call it TAG-RMSProp. The TAG versions of other methods such as the Adagrad <ref type="bibr" target="#b15">(Duchi et al., 2011)</ref> and Adam are available in Appendix A.1. A Naive RMSProp update, for a given learning rate ?, looks like the following:</p><formula xml:id="formula_0">V n = ?V n?1 + (1 ? ?)g 2 n ; 1 ? n ? N ? n+1 = ? n ? ? ? V n + g n<label>(1)</label></formula><p>where ? n is the parameter vector at step n in the epoch, g n is the gradient of the loss, N is the total number of steps in one epoch, V n is the moving average of the square of gradients (or the second moment), and ? is the decay rate. We will use TAG-optimizers as a generic terminology for the rest of the paper.</p><p>We maintain the second moment V (t) n for each task t in the stream and store it as the knowledge base. When the model shifts from one task to another, the new loss surface may look significantly different. We argue that by using the task-based second moment to regulate the new task updates, we can reduce the interference with the previously optimized parameters in the model. We define the second moment V (t) n for task t for TAG-RMSProp as:</p><formula xml:id="formula_1">V (t) n = ? 2 V (t) n?1 + (1 ? ? 2 )g 2</formula><p>n where ? 2 is constant throughout the stream. We use V (t) n to denote a matrix that stores the second moments from all previous tasks, i.e.,</p><formula xml:id="formula_2">V (t) n = {V (1) N , ..., V (t?1) N , V (t)</formula><p>n } of size (t ? P ). Hence, the memory required to store these task-specific accumulated gradients increases linearly as the number of tasks in the setting.</p><p>Note that each V (? ) N (where ? &lt; t) vector captures the gradient information when the model receives data from a task ? and does not change after the task ? is learned. It helps in regulating the magnitude of the update while learning the current task t. To alleviate the catastrophic forgetting problem occurring in the Naive RMSProp, we replace V n (in Eq. 1) to a weighted sum of V (t) n . We propose a way to regulate the weights corresponding to V (t) n for each task in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ADAPTIVE LEARNING RATE</head><p>Next, we describe our proposed learning rate that adapts based on the relatedness among tasks. We discuss how task-based accumulated gradients can help regulate the parameter updates to minimize catastrophic forgetting and transfer knowledge.</p><p>We first define a representation for each task to enable computing correlation between different tasks. We take inspiration from a recent work <ref type="bibr" target="#b20">(Guiroy et al., 2019)</ref> which is based on a popular meta-learning approach called Model-Agnostic Meta-Learning (MAML) <ref type="bibr" target="#b18">(Finn et al., 2017)</ref>. <ref type="bibr" target="#b20">Guiroy et al. (2019)</ref> suggest that with reference to given parameters ? s (where s denotes shared parameters), the similarity between the adaptation trajectories (and also meta-test gradients) among the tasks can act as an indicator of good generalization. This similarity is defined by computing the inner dot product between adaptation trajectories. In the experiments, <ref type="bibr" target="#b20">Guiroy et al. (2019)</ref> show an improvement in the overall target accuracy by adding a regularization term in the outer loop update to enhance the similarity.</p><p>In the case of LLL, instead of a fixed point of reference ? s , the parameters continue to update as the model adapts to a new task. Analogous to the adaptation trajectories, we essentially want to capture those taskspecific gradient directions in the LLL setting. Momentum serves as a less noisy estimate for the overall gradient direction and hence approximating the adaptation trajectories. The role of momentum has been crucial in the optimization literature for gradient descent updates <ref type="bibr" target="#b47">(Ruder, 2016;</ref>. Therefore, we introduce the task-based first moment M (t) n in order to approximate the adaptation trajectories of each task t. It is essentially the momentum maintained while learning each task t and would act as the task representation for computing the correlation.</p><formula xml:id="formula_3">The M (t) n is defined as: M (t) n = ? 1 M (t)</formula><p>n?1 + (1 ? ? 1 )g n where ? 1 is the constant decay rate. Intuitively, if the current task t is correlated with a previous task ? , the learning rate in the parameter update step should be higher to encourage the transfer of knowledge between task t and ? . In other words, it should allow knowledge transfer. Whereas if the current task t is uncorrelated or negatively correlated to a previous task ? , the new updates over parameters may cause catastrophic forgetting because these updates for task t may point in the opposite direction of the previous task ? 's updates. In such a case, the learning rate should adapt to lessen the effects of the new updates. We introduce a scalar quantity ? n (t, ? ) to capture the correlation that is computed using M </p><formula xml:id="formula_4">? n (t, ? ) = exp ? ? ?b M (t) n T M (? ) N |M (t) n ||M (? ) N | ? ? (2)</formula><p>where |.| is the Euclidean norm. Existing adaptive optimizers, such as Adam, tend to overfit on the most recent dataset from a task, which results in catastrophic forgetting in LLL. By using the exponential term, the resulting ? n (t, ? ) will attain a higher value for uncorrelated tasks and will minimize the new updates (hence prevent forgetting). Here, b is a hyperparameter that tunes the magnitude of ? n (t, ? ). The higher the b is, the greater is the focus on preventing catastrophic forgetting. Its value can vary for different datasets. For the current task t at step n (with ?</p><formula xml:id="formula_5">(t) 1 = ? (t?1)</formula><p>N +1 ), we define the TAG-RMSProp update as:</p><formula xml:id="formula_6">? (t) n+1 = ? (t) n ? ? ? n (t, t) V (t) n + t?1 ? =1 ? n (t, ? ) V (? ) N + g n<label>(3)</label></formula><p>Hence, the role of each ? n (t, ? ) is to regulate the influence of corresponding task-based accumulated gradient V (? ) N of the previous task ? . Since we propose a new way of looking at the gradients, our update rule (Eq. 3) can be applied with any kind of task-incremental learning setup. In this way, the overall structure of the algorithm for this setup remains the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We describe the experiments performed to evaluate our proposed method. 1 In the first experiment, we show the gain in performance by introducing TAG update instead of naive optimizers update. We analyse how our proposed learning rate adapts and achieves a higher accuracy over the tasks in the stream. Next, we compare our proposed replay-free method with other state-of-the-art baselines and also show that TAG update (in Eq. 3) can be used along with other state-of-the-art methods to improve their results.</p><p>The experiments are performed on four benchmark datasets: Split-CIFAR100, Split-miniImageNet, Split-CUB and 5-dataset. Split-CIFAR100 and Split-miniImageNet splits the CIFAR-100 <ref type="bibr" target="#b29">(Krizhevsky et al., 2009;</ref><ref type="bibr" target="#b39">Mirzadeh et al., 2020)</ref> and Mini-imagenet <ref type="bibr" target="#b57">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b10">Chaudhry et al., 2019)</ref> datasets into 20 disjoint 5-way classification tasks. Split-CUB splits the CUB <ref type="bibr" target="#b58">(Wah et al., 2011)</ref> dataset into 20 disjoint tasks with 10 classes per task. 5-dataset is a sequence of five different datasets as five 10-way classification tasks. These datasets are: CIFAR-10 <ref type="bibr" target="#b29">(Krizhevsky et al., 2009</ref><ref type="bibr">), MNIST (LeCun, 1998</ref>, SVHN <ref type="bibr" target="#b40">(Netzer et al., 2011)</ref>, notMNIST <ref type="bibr" target="#b7">(Bulatov, 2011)</ref> and Fashion-MNIST <ref type="bibr" target="#b59">(Xiao et al., 2017)</ref>. More details about the datasets are given in Appendix A.2. For experiments with Split-CIFAR100 and Split-miniImageNet, we use a reduced ResNet18 architecture following <ref type="bibr" target="#b34">(Lopez-Paz &amp; Ranzato, 2017;</ref><ref type="bibr" target="#b10">Chaudhry et al., 2019)</ref>. We use the same reduced ResNet18 architecture for 5-dataset. For Split-CUB, we use a ResNet18 model which is pretrained on Imagenet dataset <ref type="bibr" target="#b14">(Deng et al., 2009</ref>) as used in <ref type="bibr" target="#b10">(Chaudhry et al., 2019)</ref>.</p><p>We report the following metrics by evaluating the model on the held-out test set: (i) Accuracy <ref type="bibr" target="#b34">(Lopez-Paz &amp; Ranzato, 2017)</ref> i.e., average test accuracy when the model has been trained sequentially up to the latest task, (ii) Forgetting <ref type="bibr" target="#b8">(Chaudhry et al., 2018a)</ref> i.e., decrease in performance of each task from their peak accuracy to their accuracy after training on the latest task and (iii) Learning Accuracy (LA) <ref type="bibr" target="#b45">(Riemer et al., 2018)</ref> i.e., average accuracy for each task immediately after it is learned. The overall goal is to maximise the average test Accuracy. Further, a LLL algorithm should also achieve high LA while maintaining a low value of Forgetting because it should learn the new task better without compromising its performance on the previous tasks (see Appendix A.2).</p><p>We report the above metrics on the best hyper-parameter combination obtained from a grid-search. The overall implementation of the above setting is based on the code provided by <ref type="bibr" target="#b39">(Mirzadeh et al., 2020)</ref>. The details of the grid-search and other implementation details corresponding to all experiments described in our paper are given in Appendix A.2.1. For all the experiments described in this section, we train the model for a single epoch per task. Results for multiple epochs per task are given in Appendix A.3.4. All the performance results reported are averaged over five runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NAIVE OPTIMIZERS</head><p>We validate the improvement by our proposed setting over the gradient descent based methods and demonstrate the impact of using correlation among tasks in the TAG-optimizers. Firstly, we train the model on a stream of tasks using Naive SGD update without applying any specific LLL method. Similarly, we replace the SGD update with Naive Adagrad, Naive RMSProp, Naive Adam and their respective TAG-optimizers to compare their performances. We show the resulting Accuracy (%) (in <ref type="figure">Fig. 1a</ref>) and Forgetting (in <ref type="figure">Fig. 1b</ref>) when the model is trained in with the above-mentioned optimizers.</p><p>It is clear that TAG-optimizers outperform their naive counterparts as well as Naive SGD for all four datasets by a significant amount. There is a notable decrease in Forgetting by TAG-optimizers (in <ref type="figure">Fig. 1b</ref>) in general that eventually reflects on the gain in final test Accuracy as seen in <ref type="figure">Fig. 1a</ref>. In Split-CUB, TAG-Adam (57%) shows a remarkable improvement in accuracy when compared to Naive Adam (45%) such that it even surpasses Naive SGD (55%). Interestingly, TAG-Adam results in slightly lower accuracy as compared to TAG-Adagrad except in Split-miniImageNet. Moreover, Naive Adagrad results in a better performance than Naive RMSProp and Naive Adam for all the datasets. This observation aligns with the results by <ref type="bibr" target="#b23">Hsu et al. (2018)</ref>. Naive SGD performs almost equivalent to Naive Adagrad except in 5-dataset where it is outperformed.</p><p>Next, we analyse ?(t, ? ) which is the average of ? n (t, ? ) across all steps n for all t and ? when stream is finished i.e., ?(t, ? ) = 1 N N n=1 ? n (t, ? ). We show how ?(t, ? ) values play role in the gain in TAGoptimizers accuracies in case of Split-CUB dataset. Each plot in <ref type="figure">Fig. 2</ref> corresponds to test accuracies a t,? (with shaded areas indicating their standard deviations) for Naive RMSProp (blue) and TAG-RMSProp (red) for a particular ? for all tasks t in the stream (x-axis). Along with that, the grey-coloured curves are (a) Accuracy (%) (b) Forgetting <ref type="figure">Figure 1</ref>: Final average test Accuracy (%) (higher is better) and Forgetting (lower is better) obtained after the stream is finished for all four datasets. The vertical bars with hatches are the performance by TAG-optimizers while others are Naive optimizers. TAG-optimizers outperforms the naive optimizers in all datasets in terms of accuracy and also results in a lower forgetting.</p><formula xml:id="formula_7">(a) ? = 2 (b) ? = 5 (c) ? = 6</formula><p>Figure 2: Evolution of (a) ?(t, 2) and test accuracy a t,2 (left), (b) ?(t, 5) and test accuracy a t,5 (middle) and  The accuracies of TAG-RMSProp and Naive RMSProp appear to correlate for most of the stream. We mark the regions (black dashed ellipse) of subtle improvements in the accuracy by TAG-RMSProp that is later maintained throughout the stream. While observing ?(t, ? ) in those regions particularly, we note that the rank of ?(t, ? ) affects the accuracy in TAG-RMSProp as following: (i) Lower (or decrease in) rank of ?(t, ? ) means that there exists some correlation between the tasks t and ? . So, the model should take advantage of the current task updates and seek (or even amplify) backward transfer. Such observations can be made for the following (t, ? ): (4, 2), (5, 2), (14, 2), (9, 5), (14, 6), (19, 6) etc. Our method also prevents drastic forgetting as well in few cases. For example: (7, 2), (12, 2), (10, 6). (ii) Higher (or increase in) rank of ?(t, ? ) results in prevention of forgetting as observed in <ref type="figure">Fig. 2</ref>. Such (t, ? ) pairs are (11, 2), (7, 5), (12, 5), (12, 6), etc. It also results in backward transfer as observed in (11, 5) and (15, 5). We report the same analysis for the other three datasets and show the results in Appendix A.3.3.  <ref type="bibr">, 2020)</ref>. Additional details about our implementation are given in Appendix A.2. Apart from these baselines, we report the performance of Naive SGD and Naive RMSProp from the previous section. We also report results on multi-task learning (MTL) settings on all four datasets where the dataset from all the tasks is always available throughout the stream. Hence, the resulting accuracies of the MTL setting serve as the upper bounds for the test accuracies in LLL. Following <ref type="bibr" target="#b39">Mirzadeh et al. (2020)</ref>, the size of the episodic memory for both A-GEM and ER is set to store 1 example per class. Since we want to evaluate TAG with all other baselines on the original non-i.i.d. problem, we keep the episodic memory size in the replay-based methods small for the comparison. We still report A-GEM and ER results with bigger memory sizes in Appendix A.3.5. The size of the mini-batch sampled from the episodic memory is set equal to the batch-size to avoid data imbalance while training.</p><p>Other baselines such as OGD  requires storing N (=200 in their experiments) number of gradients per task and it is evaluated only on variants of the MNIST dataset by training a small feed-forward network. On the other hand, TAG additively accumulates the gradients and hence requires memory equal to two copies of the model as the knowledge base. This enabled us to train a reduced ResNet18 on complex datasets. Due to greater memory requirements, OGD faced memory errors in our setting. We would also like to highlight that OGD use Naive-SGD, whereas TAG, being an adaptive learning rate based method, is complementary to this approach. Although we utilize a similar amount of memory as Progressive Neural in the number of parameters as the number of tasks increases. Hence, we do not compare our approach with the expansion-based method in our experiments.</p><p>From the results reported in <ref type="table" target="#tab_0">Table 1</ref>, we observe that TAG-RMSProp achieves the best performance in terms of test Accuracy as compared to other baselines for all datasets. The overall improvement is decent in Split-CIFAR100, Split-miniImageNet and Split-CUB which are 3.65%, 2.5% and 2.3% with regard to the next-best baseline. On the other hand, the improvement by TAG-RMSProp is relatively minor in 5-dataset i.e., 1% as compared to ER with the similar amount of Forgetting (0.29 and 0.28) occurring in the stream.</p><p>In terms of LA, TAG-RMSProp achieves almost similar performance as Naive RMSProp in Split-CUB and 5-dataset. We also note that the LA of TAG-RMSProp is higher in Split-CIFAR100, Split-CUB and 5-dataset than ER and A-GEM. The higher LA with similar Forgetting as compared to other baselines shows that while TAG exploits the adaptive nature of existing optimizers, it also ensures minimal forgetting of the gained knowledge. The existing optimizers tend to aggressively fit the model on the most recent task at an immense cost of forgetting the earlier tasks. Hence, even if a similar (or lower) Forgetting occurs in TAG, the higher test Accuracy (with high LA) shows that TAG is capable of retaining the gained knowledge from each task. Although LA is lower in Split-miniImageNet, TAG-RMSProp manages to prevent catastrophic forgetting better than these methods and hence results in a higher test Accuracy.</p><p>Figure 3 provides a detailed view of the test Accuracy of individual baseline as the model encounters the new tasks throughout the LLL stream in Split-CIFAR100, Split-miniImageNet and Split-CUB. At the starting task t = 1, TAG-RMSProp beats other baselines because of lower initial learning rates (see Appendix A.2.1) and reflects the performance gain by the RMSProp over SGD optimizer. In all three datasets, the performance of TAG-RMSProp is very similar to ER specially from task t = 5 to task t = 15, but ultimately improves as observed at t = 20. These results show a decent gain in the final test Accuracy by TAG-RMSProp as compared to other baselines. To analyze how the presence of similar tasks help the model to perform better, we also perform the experiment with Rotated MNIST (Lopez-Paz &amp; Ranzato, 2017) dataset, details of which are given in Appendix A.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">COMBINED WITH OTHER BASELINES</head><p>Lastly, we show that the existing baselines can also benefit from our proposed method TAG-RMSProp. We replace the conventional SGD update from EWC, A-GEM and ER, and apply RMSProp update (Eq. 1) and</p><p>TAG-RMSProp update (Eq. 3) respectively. We use the same task-incremental learning setup as used in the previous sections in terms of architecture and hyper-parameters. We compare the resulting accuracies of the baselines with their RMSProp and TAGed versions in <ref type="figure" target="#fig_4">Fig. 4</ref>. For a given dataset, we see that gain in the final accuracy in the TAGed versions is similar for the baselines described in Section 4.2. That is, TAG improves these baselines with SGD update on Split-CIFAR100, Split-miniImageNet, Split-CUB and 5-dataset by at least 8%, 4%, 4% and 9% respectively. On the other hand, TAG improves the baselines with RMSProp update on the datasets by at least 12%, 12%, 7% and 9% respectively. The improvement is also significant in A-GEM with bigger episodic memory (i.e., 10 samples per class or M = 10) but we observe relatively smaller improvement (2%) by TAGed ER (M = 10) as compared to ER (M = 10). These results show that apart from outperforming the baselines independently (with smaller episodic memory in replay-based methods), TAG can also be used as an update rule in the existing research works for improving their performances.</p><p>While A-GEM and ER are strong baselines for LLL, we would like to highlight that these replay-based methods are not applicable in settings where storing examples is not an option due to privacy concerns. TAG-RMSProp would be a more appropriate solution in such settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We propose a new task-aware optimizer for the LLL setting that adapts the learning rate based on the relatedness among tasks. We introduce the task-based accumulated gradients that act as the representation for individual tasks for the same. We conduct experiments on complex datasets to compare TAG-RMSProp with several state-of-the-art methods. Results show that TAG-RMSProp outperforms the existing methods in terms of final accuracy with a commendable margin without storing past examples or using dynamic architectures. We also show that it results in a significant gain in performance when combined with other baselines. To the best of our knowledge, ours is the first work in the LLL literature showing that we can use an adaptive gradient method for LLL and prevent forgetting better than Naive SGD. For future work, as the memory required to store the task-specific accumulated gradients increases linearly with the tasks, reducing memory complexity without compromising the performance can be an interesting direction. This can be achieved by (i) computing correlation using a smaller quantity than the task-based first moments, and (ii) clustering the similar tasks together to reduce the number of task-based second moments (in settings with a soft margin between the tasks). Another possible direction from here can be shifting to a class-incremental scenario where the task identity is not known beforehand and is required to be inferred along the stream.</p><p>A.1 TAG-OPTIMIZERS Similar to RMSProp (in Section 3), with the task-based first moments M (t) n , let W (t) n = {? n (t, ? ); ? ? [1, t]} (from Eq. 2) and,</p><formula xml:id="formula_8">W (t) n V (t) n = ? ? ? V (1) n ; t = 1 ? n (t, t) V (t) n + t?1 ? =1 ? n (t, ? ) V (? ) N ; t &gt; 1<label>(4)</label></formula><p>We define the TAG versions of Adagrad and Adam as following:</p><p>? TAG-Adagrad:</p><formula xml:id="formula_9">V (t) n = V (t) n?1 + g 2 n ? (t) n+1 = ? (t) n ? ? W (t) n V n + g n<label>(5)</label></formula><p>? TAG-Adam:</p><formula xml:id="formula_10">V (t) n = ? 2 V (t) n?1 + (1 ? ? 2 )g 2 n ? (t) n+1 = ? (t) n ? ? 1 ? ? n 2 (1 ? ? n 1 ) W (t) n V n + M (t) n<label>(6)</label></formula><p>Both TAG-Adagrad (Eq. 5) and TAG-Adam (Eq. 6) result in a significant gain in Accuracy and prevent Forgetting as observed in <ref type="figure">Fig. 1a</ref> and <ref type="figure">Fig. 1b</ref> respectively in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 IMPLEMENTATION DETAILS</head><p>The summary of the datasets used in the experiments is shown in <ref type="table" target="#tab_1">Table 2 and Table 3</ref>. In 5-dataset, we convert all the monochromatic images to RGB format depending on the task dataset. All images are then resized to 3 ? 32 ? 32. The overall training and test data statistics of 5-dataset are described in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>Details about the metrics used for evaluating the model:</p><p>? Accuracy <ref type="bibr" target="#b34">(Lopez-Paz &amp; Ranzato, 2017)</ref>: If a t,? is the accuracy on the test set of task ? when the current task is t, it is defined as,</p><formula xml:id="formula_11">A t = 1 t t ? =1</formula><p>a t,? .</p><p>? Forgetting <ref type="bibr" target="#b8">(Chaudhry et al., 2018a)</ref>: It is the average forgetting that occurs after the model is trained on all tasks. If the latest task is t and is defined as, ? Learning Accuracy (LA) <ref type="bibr" target="#b45">(Riemer et al., 2018)</ref>: It is the measure of learning capability when the model sees a new task. For the current task t, it is defined as,</p><formula xml:id="formula_12">F t = 1 t?1 t?1 ? =1 max t ?{1,...,t?1} (a t ,? ? a t,? ).</formula><formula xml:id="formula_13">L t = 1 t t ? =1 a ?,? .</formula><p>We implement the following baselines to compare with our proposed method:</p><p>? EWC: Our implementation of EWC is based on the original paper <ref type="bibr" target="#b28">(Kirkpatrick et al., 2017)</ref>.</p><p>? A-GEM: We implemented A-GEM based on the the official implementation provided by <ref type="bibr" target="#b9">(Chaudhry et al., 2018b</ref>). ? ER <ref type="bibr" target="#b10">(Chaudhry et al., 2019)</ref>: Our implementation is based on the one provided by <ref type="bibr" target="#b3">(Aljundi et al., 2019)</ref> with reservoir sampling except that the sampled batch does not contain examples from the current task. ? Stable SGD <ref type="bibr" target="#b39">(Mirzadeh et al., 2020)</ref>: We obtain the best hyper-parameter set by performing gridsearch over different combinations of the learning rate, learning rate decay, and dropout (see Appendix A.2.1).</p><p>We provide our code as supplementary material that contains the scripts for reproducing the results from experiments described in this paper. In the CODE folder, we include README.MD file that contains the overall code structure, procedure for installing the required packages, links to download the datasets and steps to execute the scripts. All experiments were executed on an NVIDIA GTX 1080Ti machine with 11 GB GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 HYPER-PARAMETER DETAILS</head><p>In this section, we report the grid search details for finding the best set of hyper-parameters for all datasets and baselines. We train the model with 90% of the training set and choose the best hyper-parameters based on the highest accuracy on the validation set which consists of remaining 10% for the training set. For existing baselines, we perform the grid search either suggested by the original papers or by . For all TAG-optimizers, ? 1 is set to 0.9. For TAG-RMSProp and TAG-Adagrad, ? 2 is set to 0.99 and for TAG-Adam it is 0.999. In all the experiments, the mini-batch size is fixed to 10 for Split-CIFAR100, Split-miniImageNet, Split-CUB similar to <ref type="bibr" target="#b10">(Chaudhry et al., 2019;</ref><ref type="bibr" target="#b39">Mirzadeh et al., 2020)</ref>. We set mini-batch size to 64 for 5-dataset following <ref type="bibr" target="#b51">(Serra et al., 2018)</ref>. This is because we wanted to highlight the role of learning rate and to show how TAG-RMSProp improves the performance while the other hyper-parameters (including batch-size) were fixed.</p><p>? Naive SGD -Learning rate: [0.1 (Split-CIFAR100, 5-dataset), 0.05 (Split-miniImageNet), 0.01(Split-CUB), 0.001]</p><p>? Naive Adagrad -Learning rate: [0.01, 0.005 (Split-CIFAR100, Split-miniImageNet, 5-dataset), 0.001, 0.0005 (Split-CUB), 0.0001]</p><p>? Naive RMSProp -Learning rate: [0.01, 0.005 (Split-CIFAR100), 0.001 (Split-miniImageNet, 5-dataset), 0.0005, 0.0001 (Split-CUB), 0.00005, 0.00001]</p><p>? Naive Adam -Learning rate: [0.01, 0.005 (Split-CIFAR100), 0.001 (Split-miniImageNet, 5-dataset), 0.0005, 0.0001 (Split-CUB)]</p><p>? TAG-Adagrad -Learning rate: [0.005 (Split-CIFAR100, 5-dataset), 0.001 (Split-miniImageNet), 0.0005 (Split-CUB), 0.00025, 0.0001] b: [1, 3, 5 (Split-CIFAR100, Split-miniImageNet, Split-CUB), 7 (5-dataset)]</p><p>? TAG-RMSProp -Learning rate: [0.005, 0.001, 0.0005 (5-dataset), 0.00025 (Split-CIFAR100), 0.0001 (Split-miniImageNet), 0.00005, 0.000025 (Split-CUB), 0.00001] b: [1, 3, 5 (Split-CIFAR100, Split-miniImageNet, Split-CUB), 7 (5-dataset)]</p><p>? TAG-Adam -Learning rate: [0.005, 0.001 (5-dataset), 0.0005 (Split-CIFAR100), 0.00025 (Split-miniImageNet), 0.0001 (Split-CUB)] b: [1, 3, 5 (Split-CIFAR100, Split-miniImageNet, Split-CUB), 7 (5-dataset)]</p><p>? EWC -Learning rate: [0.1 (Split-CIFAR100, 5-dataset), 0.05 (Split-miniImageNet), 0.01(Split-CUB), 0.001] ? (regularization): [1 (Split-CIFAR100, Split-miniImageNet, Split-CUB), 10, 100 (5dataset)]</p><p>? A-GEM -Learning rate: [0.1 (Split-CIFAR100, Split-miniImageNet, 5-dataset), 0.05, 0.01(Split-CUB), 0.001]</p><p>? ER -Learning rate: [0.1 (Split-CIFAR100, 5-dataset), 0.05 (Split-miniImageNet), 0.01(Split-CUB), 0.001]</p><p>? Stable SGD -Initial learning rate: [0.1 (Split-CIFAR100, Split-miniImageNet, 5-dataset), 0.05 (Split-CUB), 0.01] -Learning rate decay: [0.9 (Split-CIFAR100, Split-miniImageNet, Split-CUB), 0.8, 0.7 (5dataset)] -Dropout: [0.0 (Split-miniImageNet, Split-CUB, 5-dataset), 0.1 (Split-CIFAR100), 0.25, 0.5]</p><p>In case of TAG-RMSProp, we empirically found that the best performance of the all three benchmarks with 20 tasks occurred when hyper-parameter b = 5 and for 5-dataset, b = 7. We also found that a lower value of Learning rate in TAG-RMSProp results in a better performance. These empirical observations can reduce the search space for hyperparameter setup by a huge amount when applying TAG-RMSProp on a LLL setup.</p><p>For the experiments in Section 4.3 that require a hybrid version of these methods, we use the same hyperparameters from above except for TAGed ER in Split-CIFAR100 (Learning rate = 0.0005) and Split-CUB (Learning rate = 0.0001). We choose the learning rates of TAG-RMSProp and Naive-RMSProp over EWC, A-GEM and ER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 ADDITIONAL EXPERIMENTS</head><p>In this section, we describe the additional experiments and analysis done in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 BACKWARD TRANSFER METRIC</head><p>While we show the occurrence of knowledge transfer in <ref type="figure">Fig. 2</ref>, we can quantify the Backward Transfer (BWT) <ref type="bibr" target="#b10">(Chaudhry et al., 2019)</ref> by computing the difference between the final Accuracy and LA. i.e., BWT = 1 t?1 t?1 ? =1 a t,? ? a ?,? . We report the BWT results for all datasets and baselines in <ref type="table" target="#tab_3">Table 4</ref>. While TAG-RMSProp outperforms the other baselines in terms of BWT for Split-miniImageNet, it is overall the second-best method for Split-CIFAR100 and 5-dataset. In case of Split-CUB, even if TAG-RMSProp achieves the highest Accuracy, it results in a lower BWT because of a significantly higher LA as compared to the other baselines (see <ref type="table" target="#tab_0">Table 1</ref>). Here, Rotated MNIST <ref type="bibr" target="#b34">(Lopez-Paz &amp; Ranzato, 2017</ref>) is a version of MNIST dataset <ref type="bibr" target="#b30">(LeCun, 1998)</ref> where the images in a task are basically MNIST images with some fixed degrees of rotation. In our experiment, we consider 10 tasks where we incrementally rotate the images by 30 ? for each task.</p><p>The goal of this experiment to answer two questions: (i) Is the proposed way to find similar tasks by computing correlation between task-based first moments valid? (ii) Can the presence of similar tasks in the stream help model to perform better?</p><p>We plot the Accuracy for Naive SGD, ER and TAG-RMSProp in <ref type="figure" target="#fig_5">Figure 5</ref>. Best performance of Naive SGD and ER was observed with learning rates 0.1. For TAG-RMSProp, we found the setting with learningrate = 0.00025 and b = 5 results in the best final Accuracy. Details of 5-dataset is given in Appendix A.2.</p><p>In <ref type="figure" target="#fig_5">Figure 5a</ref>, we observe that TAG-RMSProp not only outperforms ER but there is a significant gain in Accuracy at t = 7 (i.e., images with 180 ? rotation). Since there are several digits that could look exactly same even with 180 ? rotation, TAG recognizes the similarity between t = 7 and t = 1 and is able to transfer the gained knowledge to result it increase in the Accuracy. This observation answers the above two questions since similar tasks in Rotated MNIST are recognized by TAG and also help in improving the performance. As observed in <ref type="figure" target="#fig_5">Figure 5b</ref>, although TAG-RMSProp is outperformed by ER from task t = 2 (MNIST) to t = 4 (notMNIST) for 5-dataset, it results in the highest final Accuracy as compared to other methods. Moreover, the increase in Accuracy from t = 2 (MNIST) to t = 3 (SVHN) also shows the positive transfer exhibited by TAG as compared to ER. Next, we continue the analysis done in Section 4.1 for Split-CIFAR100 (in <ref type="figure">Fig. 6</ref>), Split-miniImageNet (in <ref type="figure" target="#fig_6">Fig. 7)</ref>, Split-CUB (in <ref type="figure" target="#fig_7">Fig. 8</ref>) for the first 9 tasks and 5-dataset (in <ref type="figure" target="#fig_8">Fig. 9</ref>) for first 3 tasks. In Split-CIFAR100 and Split-miniImageNet, the model with Naive RMSProp tends to forget the task t by significant amount as soon as it receives the new tasks. On the other hand, TAG-RMSProp prevents catastrophic forgetting and hence results in a higher accuracy throughout the stream. We can observe that for Split-CIFAR100 and Split-miniImageNet, ?(? + 1, ? ) (where ? ? [1, 9]) generally have a higher rank in the set</p><formula xml:id="formula_14">(a) ? = 1 (b) ? = 2 (c) ? = 3 (d) ? = 4 (e) ? = 5 (f) ? = 6 (g) ? = 7 (h) ? = 8 (i) ? = 9</formula><p>Figure 6: Evolution of ?(t, ? ) and test accuracy a t,? where ? ? <ref type="bibr">[1,</ref><ref type="bibr">9]</ref> along the stream of 20 tasks in the Split-CIFAR100 dataset. The grey-coloured lines are max ? ? n (t, ? ) (top, dashed line), E ? [?(t, ? )] (middle, solid line) and min ? ?(t, ? ) (bottom, dashed line) that indicate the range of ?(t, ? ).</p><p>{?(t, ? ); ? ? [1, t]}. This is because TAG-RMSProp also recognizes an immediate change in the directions when the model receives a new task (from M</p><formula xml:id="formula_15">(t?1) N to M (t)</formula><p>n ). A similar observation is made in case of Split-CUB but the visible gain in the accuracy by TAG-RMSProp does not occur instantly. Apart from that, we observe that the lower and higher rank of ?(t, ? ) results in backward transfer and prevents catastrophic forgetting respectively in the stream. Overall, in all datasets, we arrive at the same conclusion obtained in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.4 MULTIPLE-PASS PER TASK</head><p>In this section, we report the performance of TAG-RMSProp and all other baselines discussed in Section 4.2 for 5 epochs per task in <ref type="table" target="#tab_4">Table 5</ref>. Hyper-parameters for this experiment are kept the same as the single-pass per task setting. TAG-RMSProp results in high average Accuracy in all the datasets. We also observe less amount of Forgetting in TAG-RMSProp as compared to other baselines. In terms of Learning Accuracy, TAG-RMSProp is outperformed by the other baselines in Split-CIFAR100, Split-miniImageNet and 5-dataset but performs better in Split-CUB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.5 BIGGER MEMORY SIZE IN REPLAY-BASED METHODS</head><p>We also compare the performance of A-GEM and ER with a larger number of samples per class (M) in the episodic memory for all four datasets in <ref type="table" target="#tab_5">Table 6</ref>. With M = 10, total episodic memory size for Split-CIFAR100, Split-miniImageNet, Split-CUB and 5-dataset becomes 1000, 1000, 2000 and 500 respectively. We observe ER results in a significant gain in the performance as the episodic memory size increases. But TAG-RMSProp is able to outperform A-GEM in Split-CIFAR100, Split-miniImageNet and Split-CUB with a large margin even when M is set to 10.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.6 ABLATION STUDIES</head><p>In the following experiments, we perform ablation studies over TAG-RMSProp.</p><p>? Instead of Eq. 2 in TAG-RMSProp, we define TAG-1 where ? n (t, ? ) = 1 i.e., equal importance is given to all task based second moments. ? Similarly, we define TAG-age by replacing Eq. 2 with ? n (t, ? ) = t ? ? + 1 i.e., giving importance to the past tasks based on their age.</p><p>? There can be different ways to approximate the gradient directions such as averaging the gradients or storing the gradients for each task etc. Therefore, we define a variant TAG-G, where we store first B gradients obtained for each task instead of maintaining task-based first moments. When the model sees a new task t, we then obtain the ? n (t, ? ) by computing cosine similarity between these stored gradients.  catastrophically than TAG-RMSProp on all datasets. But, TAG-RMSProp outperforms TAG-G in terms of LA on Split-CUB and 5-dataset. Interestingly, lower value of B for TAG-G results in better Accuracy on 5-dataset whereas a higher B is better choice for other three datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(c) ?(t, 6) and test accuracy a t,6 (right) along the stream of 20 tasks in the Split-CUB dataset. The greycoloured lines are max ? ? n (t, ? ) (top, dashed line), E ? [?(t, ? )] (middle, solid line) and min ? ?(t, ? ) (bottom, dashed line) that indicate the range of ?(t, ? ). Elliptical regions (black dashed) highlight subtle gain in the accuracy by TAG-RMSProp that are maintained throughout the stream. Observing corresponding ?(t, ? ) in those regions validates our hypothesis discussed from Section 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>max ? ? n (t, ? ) (top, dashed line), E ? [?(t, ? )] (middle, solid line) and min ? ?(t, ? ) (bottom, dashed line) respectively. These curves are shown along with the corresponding ?(t, ? ) to indicate the rank of ?(t, ? ) in the set {?(t, ? ); ? ? [1, t]}. This set is computed when the model encounters the task t in the stream.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Networks Rusu et al. (2016), an expansion-based method, we do not make any changes to the size of the model during the training and testing process. On the other hand, Rusu et al. (2016) require quadratic growth (a) Split-CIFAR100 (b) Split-miniImageNet (c) Split-CUB Evolution of average test Accuracy (%) A t for different existing methods and TAG-RMSProp throughout the stream in Split-CIFAR100, Split-miniImageNet and Split-CUB. All results are averaged across 5 runs and the shaded area represent standard deviation. Performing similar as ER for major part of the stream, TAG-RMSProp always results in the highest final accuracy as compared to other methods with a low standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Comparing performance for different existing methods with their RMSProp and TAGed versions on all four datasets in terms of final average test Accuracy (%) along with A-GEM and ER for different samples per class (M) in the episodic memory. The vertical bars with hatches are the performance by TAGed versions of the baselines. All results are averaged across 5 runs. All TAGed versions results in a similar gain in the accuracy over baselines with both SGD and RMSProp update.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Evolution of average test Accuracy (%) A t for different existing methods and TAG-RMSProp throughout the stream in Rotated MNIST and 5-dataset. All results are averaged across 5 runs and the shaded area represent standard deviation.A.3.3 EVOLUTION OF ?(t, ? ) AND TEST ACCURACY a t,?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Evolution of ?(t, ? ) and test accuracy a t,? where ? ?[1, 9]  along the stream of 20 tasks in the Split-miniImageNet dataset. The grey-coloured lines are max ? ? n (t, ? ) (top, dashed line), E ? [?(t, ? )] (middle, solid line) and min ? ?(t, ? ) (bottom, dashed line) that indicate the range of ?(t, ? ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Evolution of ?(t, ? ) and test accuracy a t,? where ? ? {1, 3, 4, 7, 8, 9} along the stream of 20 tasks in the Split-CUB dataset. The grey-coloured lines are max ? ? n (t, ? ) (top, dashed line), E ? [?(t, ? )] (middle, solid line) and min ? ?(t, ? ) (bottom, dashed line) that indicate the range of ?(t, ? ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Evolution of ?(t, ? ) and test accuracy a t,? where ? ? {1, 2, 3} along the stream of 5 tasks in the 5-dataset dataset. The grey-coloured lines are max ? ? n (t, ? ) (top, dashed line), E ? [?(t, ? )] (middle, solid line) and min ? ?(t, ? ) (bottom, dashed line) that indicate the range of ?(t, ? ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparing performance in terms of final average test Accuracy (%) (higher is better), Forgetting (lower is better) and Learning Accuracy (LA (%)) (higher is better) of the existing baselines with TAG-RMSProp on all four datasets. All metrics are averaged across 5 runs. Overall, TAG-RMSProp outperforms all other methods in terms of Accuracy. *MTL assumes that the whole dataset from all tasks is always available during training, hence it is a different setting and its accuracy acts as an upper bound.?2.14) 0.11 (?0.01) 62.15 (?1.12) 46.51 (?2.75) 0.46 (?0.03) 83.3 (?1.44) TAG-RMSProp (Ours) 61.58 (?1.24) 0.11 (?0.01) 71.56 (?0.74) 62.59 (?1.82) 0.29 (?0.02) 86.08 (?0.55) COMPARED WITH OTHER BASELINESIn the next experiment, we show that the TAG-RMSProp results in a strong performance as compared to other LLL algorithms. InTable 1,we report the performance of TAG-RMSProp and the following state-of-the-art baselines: EWC (Kirkpatrick et al., 2017), A-GEM (Chaudhry et al., 2018b), ER (Aljundi et al., 2019) with reservoir sampling and Stable SGD (Mirzadeh et al.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>Split-CIFAR100</cell><cell></cell><cell cols="2">Split-miniImageNet</cell><cell></cell></row><row><cell></cell><cell>Accuracy (%)</cell><cell>Forgetting</cell><cell>LA (%)</cell><cell>Accuracy (%)</cell><cell>Forgetting</cell><cell>LA (%)</cell></row><row><cell>Naive SGD</cell><cell cols="6">51.36 (?3.21) 0.18 (?0.03) 68.46 (?1.93) 48.19 (?0.79) 0.13 (?0.01) 60.6 (?0.95)</cell></row><row><cell>Naive RMSProp</cell><cell>48.91 (?2.88)</cell><cell cols="2">0.2 (?0.03) 67.28 (?0.43)</cell><cell>45.06 (?0.6)</cell><cell cols="2">0.21 (?0.01) 64.39 (?1.02)</cell></row><row><cell>EWC</cell><cell cols="6">49.06 (?3.44) 0.19 (?0.04) 66.82 (?1.41) 47.87 (?2.08) 0.15 (?0.02) 61.66 (?1.06)</cell></row><row><cell>A-GEM</cell><cell>54.25 (?2.0)</cell><cell cols="5">0.16 (?0.03) 68.98 (?1.19) 50.32 (?1.29) 0.11 (?0.02) 61.02 (?0.64)</cell></row><row><cell>ER</cell><cell cols="4">59.14 (?1.77) 0.12 (?0.02) 70.36 (?1.23) 54.67 (?0.71)</cell><cell cols="2">0.1 (?0.01) 64.06 (?0.41)</cell></row><row><cell>Stable SGD</cell><cell>57.04 (?1.07)</cell><cell cols="5">0.09 (?0.0) 64.62 (?0.91) 51.81 (?1.66) 0.09 (?0.01) 59.99 (?0.94)</cell></row><row><cell cols="2">TAG-RMSProp (Ours) 62.79 (?0.29)</cell><cell cols="2">0.1 (?0.01) 72.06 (?1.01)</cell><cell>57.2 (?1.37)</cell><cell cols="2">0.06 (?0.02) 62.73 (?0.61)</cell></row><row><cell>MTL*</cell><cell>67.7 (?0.58)</cell><cell>-</cell><cell>-</cell><cell>66.14 (?1.0)</cell><cell>-</cell><cell>-</cell></row><row><cell>Methods</cell><cell></cell><cell>Split-CUB</cell><cell></cell><cell></cell><cell>5-dataset</cell><cell></cell></row><row><cell></cell><cell>Accuracy (%)</cell><cell>Forgetting</cell><cell>LA (%)</cell><cell>Accuracy (%)</cell><cell>Forgetting</cell><cell>LA (%)</cell></row><row><cell>Naive SGD</cell><cell cols="6">54.88 (?1.83) 0.12 (?0.01) 65.97 (?0.59) 46.48 (?3.62) 0.48 (?0.05) 84.55 (?1.06)</cell></row><row><cell>Naive RMSProp</cell><cell>49.4 (?1.77)</cell><cell cols="3">0.24 (?0.01) 71.76 (?0.94) 45.49 (?1.89)</cell><cell cols="2">0.5 (?0.03) 85.58 (?1.21)</cell></row><row><cell>EWC</cell><cell cols="4">55.66 (?0.97) 0.12 (?0.01) 66.36 (?0.71) 48.58 (?1.47)</cell><cell cols="2">0.4 (?0.03) 79.56 (?3.18)</cell></row><row><cell>A-GEM</cell><cell>56.91 (?1.37)</cell><cell>0.1 (?0.01)</cell><cell>65.6 (?0.73)</cell><cell>55.9 (?2.58)</cell><cell cols="2">0.34 (?0.04) 82.61 (?2.13)</cell></row><row><cell>ER</cell><cell>59.25 (?0.82)</cell><cell cols="5">0.1 (?0.01) 66.17 (?0.42) 61.58 (?2.65) 0.28 (?0.04) 84.3 (?1.08)</cell></row><row><cell cols="2">Stable SGD 53.76 (MTL* 71.65 (?0.76)</cell><cell>-</cell><cell>-</cell><cell>70.0 (?4.44)</cell><cell>-</cell><cell>-</cell></row><row><cell>4.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Dataset Statistics</figDesc><table><row><cell></cell><cell>Input size</cell><cell cols="2">Training samples per task Test samples per task</cell></row><row><cell>Split-CIFAR100</cell><cell>3 ? 32 ? 32</cell><cell>2500</cell><cell>500</cell></row><row><cell>Split-miniImageNet</cell><cell>3 ? 84 ? 84</cell><cell>2400</cell><cell>600</cell></row><row><cell>Split-CUB</cell><cell>3 ? 224 ? 224</cell><cell>300</cell><cell>290</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>5-dataset statistics.</figDesc><table><row><cell></cell><cell cols="2">Training samples Test samples</cell></row><row><cell>CIFAR-10</cell><cell>50000</cell><cell>10000</cell></row><row><cell>MNIST</cell><cell>60000</cell><cell>10000</cell></row><row><cell>SVHN</cell><cell>73257</cell><cell>26032</cell></row><row><cell>notMNIST</cell><cell>16853</cell><cell>1873</cell></row><row><cell>Fashion-MNIST</cell><cell>60000</cell><cell>10000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparing performance in terms of final average test Accuracy (%) (higher is better) and BWT (higher is better) with the standard deviation values for different existing methods with TAG-RMSProp for all four datasets. All metrics are averaged across 5 runs. COMPARING WITH OTHER BASELINES ON ROTATED-MNIST AND 5-DATASET We evaluate the performance of TAG-RMSProp on Rotated MNIST and 5-dataset and in terms of average test Accuracy on each task in the stream.</figDesc><table><row><cell>Methods</cell><cell cols="2">Split-CIFAR100</cell><cell cols="2">Split-miniImageNet</cell></row><row><cell></cell><cell>Accuracy (%)</cell><cell>BWT (%)</cell><cell>Accuracy (%)</cell><cell>BWT (%)</cell></row><row><cell>Naive SGD</cell><cell>51.36 (?3.21)</cell><cell>?17.1 (?2.64)</cell><cell cols="2">48.19 (?0.79) ?13.83 (?1.97)</cell></row><row><cell>Naive RMSProp</cell><cell cols="2">48.91 (?2.88) ?18.37 (?2.71)</cell><cell>45.06 (?0.6)</cell><cell>?19.32 (?1.39)</cell></row><row><cell>EWC</cell><cell cols="4">49.06 (?3.44) ?17.76 (?3.35) 47.87 (?2.08) ?13.79 (?2.26)</cell></row><row><cell>A-GEM</cell><cell>54.25 (?2.0)</cell><cell cols="3">?14.73 (?2.48) 50.32 (?1.29) ?10.69 (?1.57)</cell></row><row><cell>ER</cell><cell cols="3">59.14 (?1.77) ?11.22 (?2.19) 54.67 (?0.71)</cell><cell>?9.39 (?0.64)</cell></row><row><cell>Stable SGD</cell><cell>57.04 (?1.07)</cell><cell>?7.59 (?0.36)</cell><cell>51.81 (?1.66)</cell><cell>?8.18 (?1.18)</cell></row><row><cell cols="2">TAG-RMSProp (Ours) 62.79 (?0.29)</cell><cell>?9.27 (?1.16)</cell><cell>57.2 (?1.37)</cell><cell>?5.52 (?1.71)</cell></row><row><cell>Methods</cell><cell cols="2">Split-CUB</cell><cell cols="2">5-dataset</cell></row><row><cell></cell><cell>Accuracy (%)</cell><cell>BWT (%)</cell><cell>Accuracy (%)</cell><cell>BWT (%)</cell></row><row><cell>Naive SGD</cell><cell cols="4">54.88 (?1.83) ?11.09 (?1.43) 46.48 (?3.62) ?38.06 (?3.69)</cell></row><row><cell>Naive RMSProp</cell><cell>49.4 (?1.77)</cell><cell cols="2">?22.36 (?0.95) 45.49 (?1.89)</cell><cell>?40.09 (?2.6)</cell></row><row><cell>EWC</cell><cell>55.66 (?0.97)</cell><cell>?10.7 (?0.39)</cell><cell cols="2">48.58 (?1.47) ?30.98 (?3.34)</cell></row><row><cell>A-GEM</cell><cell>56.91 (?1.37)</cell><cell>?8.69 (?0.93)</cell><cell>55.9 (?2.58)</cell><cell>?26.71 (?3.6)</cell></row><row><cell>ER</cell><cell>59.25 (?0.82)</cell><cell>?6.93 (?0.92)</cell><cell cols="2">61.58 (?2.65) ?22.72 (?3.08)</cell></row><row><cell>Stable SGD</cell><cell>53.76 (?2.14)</cell><cell>?8.39 (?1.26)</cell><cell cols="2">46.51 (?2.75) ?36.79 (?2.19)</cell></row><row><cell cols="2">TAG-RMSProp (Ours) 61.58 (?1.24)</cell><cell>?9.99 (?1.62)</cell><cell cols="2">62.59 (?1.82) ?23.49 (?1.73)</cell></row><row><cell>A.3.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparing performance in terms of Final average test Accuracy (%) (higher is better), Forgetting (lower is better) and Learning Accuracy (LA (%)) (higher is better) with the standard deviation values for different existing methods with TAG-RMSProp running for 5 epochs per task for all four datasets (see Section 4). All metrics are averaged across 5 runs. *MTL assumes that the dataset from all tasks are always available during training, hence it is a different setting and its accuracy acts as an upper bound.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>Split-CIFAR100</cell><cell></cell><cell cols="2">Split-miniImageNet</cell><cell></cell></row><row><cell></cell><cell>Accuracy (%)</cell><cell>Forgetting</cell><cell>LA (%)</cell><cell>Accuracy (%)</cell><cell>Forgetting</cell><cell>LA (%)</cell></row><row><cell>Naive SGD</cell><cell cols="3">52.26 (?0.65) 0.28 (?0.01) 78.45 (?0.41)</cell><cell>46.9 (?1.18)</cell><cell cols="2">0.25 (?0.02) 70.82 (?0.6)</cell></row><row><cell>Naive RMSProp</cell><cell cols="6">46.12 (?2.33) 0.32 (?0.02) 76.29 (?0.53) 41.07 (?0.66) 0.32 (?0.01) 71.43 (?0.42)</cell></row><row><cell>EWC</cell><cell>51.7 (?1.71)</cell><cell cols="5">0.27 (?0.02) 77.72 (?0.84) 48.17 (?0.81) 0.25 (?0.01) 71.87 (?0.26)</cell></row><row><cell>A-GEM</cell><cell cols="6">54.24 (?1.14) 0.25 (?0.01) 78.38 (?0.39) 49.08 (?0.52) 0.23 (?0.01) 70.49 (?0.4)</cell></row><row><cell>ER</cell><cell cols="3">60.03 (?0.96) 0.19 (?0.01) 78.15 (?0.7)</cell><cell cols="3">54.01 (?0.56) 0.19 (?0.01) 71.77 (?0.58)</cell></row><row><cell>Stable SGD</cell><cell cols="6">58.92 (?0.73) 0.19 (?0.01) 76.91 (?0.72) 51.23 (?0.88) 0.22 (?0.01) 71.77 (?0.56)</cell></row><row><cell cols="4">TAG-RMSProp (Ours) 60.64 (?1.38) 0.17 (?0.01) 77.12 (?0.76)</cell><cell>58.0 (?1.11)</cell><cell cols="2">0.11 (?0.02) 68.14 (?0.38)</cell></row><row><cell>MTL*</cell><cell>67.7 (?0.58)</cell><cell>-</cell><cell>-</cell><cell>66.14 (?1.0)</cell><cell>-</cell><cell>-</cell></row><row><cell>Methods</cell><cell></cell><cell>Split-CUB</cell><cell></cell><cell></cell><cell>5-dataset</cell><cell></cell></row><row><cell></cell><cell>Accuracy (%)</cell><cell>Forgetting</cell><cell>LA (%)</cell><cell>Accuracy (%)</cell><cell>Forgetting</cell><cell>LA (%)</cell></row><row><cell>Naive SGD</cell><cell cols="6">59.87 (?1.48) 0.21 (?0.02) 79.77 (?0.44) 49.95 (?2.42) 0.51 (?0.04) 90.86 (?0.63)</cell></row><row><cell>Naive RMSProp</cell><cell cols="3">35.87 (?1.14) 0.46 (?0.01) 79.59 (?0.3)</cell><cell cols="3">50.47 (?0.99) 0.51 (?0.01) 90.89 (?0.44)</cell></row><row><cell>EWC</cell><cell>59.73 (?2.4)</cell><cell cols="2">0.21 (?0.02) 79.8 (?0.58)</cell><cell cols="3">52.51 (?7.34) 0.43 (?0.09) 86.8 (?2.52)</cell></row><row><cell>A-GEM</cell><cell cols="2">62.65 (?1.61) 0.17 (?0.02)</cell><cell>79.1 (?0.4)</cell><cell cols="3">62.48 (?3.16) 0.35 (?0.04) 90.53 (?0.73)</cell></row><row><cell>ER</cell><cell cols="6">66.06 (?1.28) 0.14 (?0.02) 78.79 (?0.55) 62.84 (?1.58) 0.35 (?0.02) 90.52 (?0.69)</cell></row><row><cell>Stable SGD</cell><cell cols="3">58.75 (?0.96) 0.19 (?0.01) 76.6 (?0.64)</cell><cell cols="3">51.95 (?3.83) 0.48 (?0.05) 90.41 (?0.29)</cell></row><row><cell>TAG-RMSProp (Ours)</cell><cell>68.0 (?1.01)</cell><cell cols="5">0.13 (?0.01) 80.15 (?0.22) 61.13 (?3.05) 0.36 (?0.04) 89.9 (?0.33)</cell></row><row><cell>MTL*</cell><cell>71.65 (?0.76)</cell><cell>-</cell><cell>-</cell><cell>70.0 (?4.44)</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparing performance in terms of Final average test Accuracy (%) (higher is better), Forgetting (lower is better) and Learning Accuracy (LA (%)) (higher is better) with the standard deviation values for A-GEM and ER for different number of samples per class (M) in the episodic memory with TAG-RMSProp for all four datasets (see Section 4). All metrics are averaged across 5 runs. Overall, ER with bigger memory outperforms all other methods in terms of Accuracy.?0.0) 66.76 (?0.73) 71.56 (?1.54) 0.16 (?0.02) 84.34 (?1.46) ER (M = 10) 70.73 (?0.23) 0.03 (?0.01) 66.83 (?0.86) 75.44 (?1.07) 0.12 (?0.02) 84.62 (?0.89) TAG-RMSProp (Ours) 61.58 (?1.24) 0.11 (?0.01) 71.56 (?0.74) 62.59 (?1.82) 0.3 (?0.02) 86.08 (?0.55)</figDesc><table><row><cell>Methods</cell><cell></cell><cell>Split-CIFAR100</cell><cell></cell><cell cols="3">Split-miniImageNet</cell></row><row><cell></cell><cell>Accuracy (%)</cell><cell>Forgetting</cell><cell>LA (%)</cell><cell>Accuracy (%)</cell><cell>Forgetting</cell><cell>LA (%)</cell></row><row><cell>A-GEM (M = 1)</cell><cell>54.25 (?2.0)</cell><cell cols="5">0.16 (?0.03) 68.98 (?1.19) 50.32 (?1.29) 0.11 (?0.02) 61.02 (?0.64)</cell></row><row><cell>A-GEM (M = 5)</cell><cell cols="6">55.74 (?1.14) 0.14 (?0.01) 68.97 (?0.56) 49.52 (?2.02) 0.12 (?0.02) 60.49 (?0.78)</cell></row><row><cell>A-GEM (M = 10)</cell><cell cols="6">56.68 (?1.92) 0.13 (?0.02) 68.72 (?0.96) 49.77 (?2.41) 0.12 (?0.02) 60.6 (?0.66)</cell></row><row><cell>ER (M = 1)</cell><cell cols="4">59.14 (?1.77) 0.12 (?0.02) 70.36 (?1.23) 52.76 (?1.53)</cell><cell>0.1 (?0.01)</cell><cell>61.7 (?0.74)</cell></row><row><cell>ER (M = 5)</cell><cell cols="6">65.74 (?1.47) 0.07 (?0.01) 70.91 (?1.13) 58.49 (?1.21) 0.05 (?0.01) 62.24 (?0.85)</cell></row><row><cell>ER (M = 10)</cell><cell cols="6">68.94 (?0.93) 0.05 (?0.01) 71.26 (?1.01) 60.06 (?0.63) 0.04 (?0.01) 62.21 (?1.24)</cell></row><row><cell cols="2">TAG-RMSProp (Ours) 62.79 (?0.29)</cell><cell cols="2">0.1 (?0.01) 72.06 (?1.01)</cell><cell>57.2 (?1.37)</cell><cell cols="2">0.06 (?0.02) 62.73 (?0.61)</cell></row><row><cell>Methods</cell><cell></cell><cell>Split-CUB</cell><cell></cell><cell></cell><cell>5-dataset</cell></row><row><cell></cell><cell>Accuracy (%)</cell><cell>Forgetting</cell><cell>LA (%)</cell><cell>Accuracy (%)</cell><cell>Forgetting</cell><cell>LA (%)</cell></row><row><cell>A-GEM (M = 1)</cell><cell>56.91 (?1.37)</cell><cell>0.1 (?0.01)</cell><cell>65.6 (?0.73)</cell><cell>55.9 (?2.58)</cell><cell cols="2">0.34 (?0.04) 82.61 (?2.13)</cell></row><row><cell>A-GEM (M = 5)</cell><cell>56.4 (?1.5)</cell><cell cols="2">0.1 (?0.01) 65.63 (?0.64)</cell><cell>61.39 (?1.0)</cell><cell cols="2">0.28 (?0.01) 83.48 (?1.05)</cell></row><row><cell>A-GEM (M = 10)</cell><cell>56.71 (?1.6)</cell><cell>0.1 (?0.01)</cell><cell>65.73 (?0.9)</cell><cell cols="3">62.43 (?1.38) 0.26 (?0.03) 83.38 (?1.79)</cell></row><row><cell>ER (M = 1)</cell><cell>59.25 (?0.82)</cell><cell cols="5">0.1 (?0.01) 66.17 (?0.42) 61.58 (?2.65) 0.28 (?0.04) 84.31 (?1.08)</cell></row><row><cell>ER (M = 5)</cell><cell>68.89 (?0.31)</cell><cell>0.04 (</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparing performance in terms of Final average test Accuracy (%) (higher is better), Forgetting (lower is better) and Learning Accuracy (LA (%)) (higher is better) with the standard deviation values for different variants of TAG-RMSProp (see Appendix A.3.6) for all four datasets. All metrics are averaged across 5 runs. ?0.01) 72.62 (?1.35) 58.47 (?0.82) 0.05 (?0.01) 62.99 (?0.66) TAG-G (B = 20) 62.97 (?1.39) 0.1 (?0.01) 72.55 (?1.06) 58.62 (?0.83) 0.05 (?0.01) 63.12 (?0.81) ?0.01) 66.96 (?0.48) 63.57 (?1.24) 0.27 (?0.02) 85.37 (?1.34) TAG-G (B = 5) 58.73 (?1.09) 0.09 (?0.01) 67.01 (?0.54) 62.84 (?1.18) 0.28 (?0.01) 84.99 (?1.18) TAG-G (B = 20) 58.73 (?1.07) 0.09 (?0.01) 66.95 (?0.58) 62.38 (?2.58) 0.29 (?0.03) 85.24 (?2.72) TAG-RMSProp 61.58 (?1.24) 0.11 (?0.01) 71.56 (?0.74) 62.59 (?1.82) 0.3 (?0.02) 86.08 (?0.55)</figDesc><table><row><cell>Methods</cell><cell></cell><cell>Split-CIFAR100</cell><cell></cell><cell cols="3">Split-miniImageNet</cell></row><row><cell></cell><cell>Accuracy (%)</cell><cell>Forgetting</cell><cell>LA (%)</cell><cell>Accuracy (%)</cell><cell>Forgetting</cell><cell>LA (%)</cell></row><row><cell>TAG-1</cell><cell>48.91 (?0.32)</cell><cell cols="5">0.22 (?0.0) 70.19 (?0.47) 48.21 (?1.25) 0.11 (?0.02) 58.79 (?1.41)</cell></row><row><cell>TAG-age</cell><cell cols="4">54.55 (?1.37) 0.13 (?0.02) 66.59 (?2.07) 49.42 (?0.78)</cell><cell>0.06 (?0.0)</cell><cell>55.0 (?0.48)</cell></row><row><cell>TAG-G (B = 1)</cell><cell>62.6 (?0.38)</cell><cell cols="5">0.1 (?0.01) 72.37 (?0.71) 58.58 (?0.66) 0.05 (?0.01) 63.24 (?0.72)</cell></row><row><cell cols="4">TAG-G (B = 5) 0.1 (TAG-RMSProp 62.79 (?0.29) 62.95 (?1.24) 0.1 (?0.01) 72.06 (?1.01)</cell><cell>57.2 (?1.37)</cell><cell cols="2">0.06 (?0.02) 62.73 (?0.61)</cell></row><row><cell>Methods</cell><cell></cell><cell>Split-CUB</cell><cell></cell><cell></cell><cell>5-dataset</cell></row><row><cell></cell><cell>Accuracy (%)</cell><cell>Forgetting</cell><cell>LA (%)</cell><cell>Accuracy (%)</cell><cell>Forgetting</cell><cell>LA (%)</cell></row><row><cell>TAG-1</cell><cell>43.53 (?0.61)</cell><cell cols="5">0.06 (?0.0) 47.86 (?0.49) 50.49 (?0.95) 0.44 (?0.01) 85.58 (?1.16)</cell></row><row><cell>TAG-age</cell><cell>30.8 (?1.07)</cell><cell cols="2">0.04 (?0.01) 33.05 (?0.8)</cell><cell>53.28 (?1.46)</cell><cell>0.4 (?0.02)</cell><cell>85.43 (?1.8)</cell></row><row><cell>TAG-G (B = 1)</cell><cell>58.7 (?1.04)</cell><cell>0.09 (</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code for the experiments is submitted as supplementary material and will be released publicly upon acceptance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We implement TAG-1, TAG-age and TAG-G in our code and run experiments on Split-CIFAR100, Split-miniImageNet, Split-CUB and 5-dataset. For TAG-G, we run it for different values of B. All results shown is <ref type="table">Table 7</ref> are averaged across 5 runs.</p><p>We observe that that TAG-1 and TAG-age perform better than Naive RMSProp and EWC (refer <ref type="table">Table 1</ref>) on all datasets except on Split-CUB. But, they are outperformed by TAG-RMSProp. The significantly worse performance on Split-CUB suggests that the model relies heavily on similarity among tasks and even maintaining the importance based on age is not sufficient for the learning process.</p><p>On the other hand, TAG-RMSProp and TAG-G (B = 1) store the same amount of memory, but TAG-RMSProp only performs better on two (Split-CIFAR100 and Split-CUB) out of four datasets. In particular, even of B = 20, TAG-G is outperformed by TAG-RMSProp on Split-CUB. This could be due to complexity of Split-CUB and the model (ResNet18), more number of iterations were required to solve individual tasks. Hence, storing the first few gradients is not sufficient to represent a task to compute the similarities. On the other hand, momentum is a better choice for computing tasks similarity since it is able to approximate the overall gradient directions when the model converges. We also observe that TAG-G tends to forget less</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tameem</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09514</idno>
		<title level="m">Continual learning with adaptive weights (claw)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Uncertainty-based continual learning with adaptive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4392" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Expert gate: Lifelong learning with a network of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3366" to="3375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Online continual learning with maximal interfered retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Belilovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Page-Caccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11849" to="11860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rainbow memory: Continual learning with a memory of diverse samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihwan</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heesu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8218" to="8227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generalisation guarantees for continual learning with orthogonal gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbana</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Bennani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11942</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05424</idno>
		<title level="m">Weight uncertainty in neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Google (Books/OCR)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Bulatov</surname></persName>
		</author>
		<ptr target="http://yaroslavvb.blogspot.it/2011/09/notmnist-dataset.html" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Riemannian walk for incremental learning: Understanding forgetting and intransigence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Puneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="532" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficient lifelong learning with a-gem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00420</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Puneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10486</idno>
		<title level="m">On tiny episodic memories in continual learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Closing the generalization gap of adaptive gradient methods in training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongruo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06763</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lifelong machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="9" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ales Leonardis, Greg Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Delange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Uncertainty-guided continual learning with bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02425</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Orthogonal gradient descent for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navid</forename><surname>Azizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3762" to="3773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An empirical investigation of catastrophic forgetting in gradient-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6211</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Towards understanding generalization in gradient-based meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Guiroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07287</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">La-maml: Look-ahead meta learning for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunshi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karmesh</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Paull</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13904</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Embracing change: Continual learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2020.09.004.URLhttp:/www.sciencedirect.com/science/article/pii/S1364661320302199</idno>
		<idno>1364-6613</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2020.09.004.URLhttp://www.sciencedirect.com/science/article/pii/S1364661320302199" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1028" to="1040" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Re-evaluating continual learning scenarios: A categorization and case for strong baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anita</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12488</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reconciling meta-learning and continual learning with online mixtures of tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassen</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">A</forename><surname>Heller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9122" to="9133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gradient based memory editing for task-free continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15294</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improving generalization performance by switching from adam to sgd. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirish</forename><surname>Nitish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno>abs/1712.07628</idno>
		<ptr target="http://arxiv.org/abs/1712.07628" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lifelong learning with sketched structural regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Praveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Pilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Braverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="985" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Convergence analysis of proximal gradient with momentum for nonconvex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qunwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramod K</forename><surname>Varshney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04925</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6467" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Online continual learning in image classification: An empirical survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheda</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihwan</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Quispe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">469</biblScope>
			<biblScope unit="page" from="28" to="51" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Class-incremental learning: survey and performance evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartlomiej</forename><surname>Twardowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Menta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Weijer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15277</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0079-7421(08)60536-8.URLhttp:/www.sciencedirect.com/science/article/pii/S0079742108605368</idno>
		<ptr target="https://doi.org/10.1016/S0079-7421(08)60536-8.URLhttp://www.sciencedirect.com/science/article/pii/S0079742108605368" />
	</analytic>
	<monogr>
		<title level="j">Psychology of Learning and Motivation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
			<date type="published" when="1989" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Mermillod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aur?lia</forename><surname>Bugaiska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Bonin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">504</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Understanding the role of training regimes in continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Seyed Iman Mirzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghasemzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06958</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Variational continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Continual lifelong learning with neural networks: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>German</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Part</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="54" to="71" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Continual unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7647" to="7657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning to learn without forgetting by maximizing transfer and minimizing interference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ajemian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhai</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11910</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in neural networks: the role of rehearsal mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robins</surname></persName>
		</author>
		<idno type="DOI">10.1109/ANNES.1993.323080</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings 1993 The First New Zealand International Two-Stream Conference on Artificial Neural Networks and Expert Systems</title>
		<meeting>1993 The First New Zealand International Two-Stream Conference on Artificial Neural Networks and Expert Systems</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">An overview of gradient descent optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04747</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<title level="m">Razvan Pascanu, and Raia Hadsell. Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ella: An efficient lifelong learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ruvolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Eaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="507" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Gradient projection memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gobinda</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isha</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09762</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Overcoming catastrophic forgetting with agemention to the task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didac</forename><surname>Suris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01423</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Modular-relatedness for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ammar</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Alesiani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.01272</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">On training recurrent neural networks for lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shagun</forename><surname>Sodhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1811.07017</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Encoder based lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Amal Rannen Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Gido M Van De Ven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tolias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07734</idno>
		<title level="m">Three scenarios for continual learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Artificial neural variability for deep learning: on overfitting, noise memorization, and catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeke</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengxiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Issei</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2163" to="2192" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Reinforced continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="899" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongtae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01547</idno>
		<title level="m">Lifelong learning with dynamically expandable networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Continual learning through synaptic intelligence. Proceedings of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">3987</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">A APPENDIX In this document, we provide the details and results excluded from the main paper. In A.1, we describe the TAG versions of Adagrad and Adam. The implementation details are described in Section A.2. We also report results obtained by performing additional experiments</title>
		<imprint/>
	</monogr>
	<note>in Section A.3</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
