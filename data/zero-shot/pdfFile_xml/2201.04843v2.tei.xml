<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-task Pre-training Language Model for Semantic Network Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Beijing Institute of Microbiology and Epidemiology</orgName>
								<address>
									<postCode>100071</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Laboratory of Pathogen and Biosecurity</orgName>
								<address>
									<postCode>100071</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kele</forename><surname>Xu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">National Key Lab of Parallel and Distributed Processing</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukai</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaimin</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">National Key Lab of Parallel and Distributed Processing</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tencent</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-task Pre-training Language Model for Semantic Network Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Knowledge Graph</term>
					<term>Link Prediction</term>
					<term>Semantic Matching</term>
					<term>Translational Distance</term>
					<term>Multi-Task Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic networks, such as the knowledge graph, can represent the knowledge leveraging the graph structure. Although the knowledge graph shows promising values in natural language processing, it suffers from incompleteness. This paper focuses on knowledge graph completion by predicting linkage between entities, which is a fundamental yet critical task. Semantic matching is a potential solution for link prediction as it can deal with unseen entities, while the translational distance based methods struggle with the unseen entities. However, to achieve competitive performance as translational distance based methods, semantic matching based methods require large-scale datasets for the training purpose, which are typically unavailable in practical settings. Therefore, we employ the language model and introduce a novel knowledge graph architecture named LP-BERT, which contains two main stages: multi-task pre-training and knowledge graph fine-tuning. In the pre-training phase, three tasks are taken to drive the model to learn the relationship information from triples by predicting either entities or relations. While in the fine-tuning phase, inspired by contrastive learning, we design a triple-style negative sampling in a batch, which greatly increases the proportion of negative sampling while keeping the training time almost unchanged. Furthermore, we propose a new data augmentation method utilizing the inverse relationship of triples to improve the performance and robustness of the model. To demonstrate the effectiveness of our proposed framework, we conduct extensive experiments on three widely-used knowledge graph datasets, WN18RR, FB15k-237, and UMLS. The experimental results demonstrate the superiority of our methods, and our approach achieves state-of-the-art results on the WN18RR and FB15k-237 datasets. Significantly, the Hits@10 indicator is improved by 5% from the previous state-of-the-art result on the WN18RR dataset while reaching 100% on the UMLS dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The applications of knowledge graphs (KG) seems to be evident in both the industrial and in academic fields <ref type="bibr" target="#b0">[1]</ref>, including the question answering, recommendation systems, natural language processing <ref type="bibr" target="#b1">[2]</ref>, etc. These evident applications in turn have attracted considerable interest for the construction of large-scale KG. Despite the sustainable efforts that have been made, many previous knowledge graphs suffered from the incompleteness <ref type="bibr" target="#b2">[3]</ref>, as it is difficult to store all these facts in one time. To address this incompleteness issue, many link prediction approaches have been explored, with the aim to discover unannotated relations between entities to complete knowledge graphs which is challenging but critical due to its potential to boost downstream applications.</p><p>The link prediction for KG is also known as KG completion. Previous link prediction methods can be classified into two main categories: translational distance based approach and semantic matching based approach <ref type="bibr" target="#b3">[4]</ref>. Translational distance based models typically embed both entities and relations into vector space and exploit scoring functions to measure the distance between them. Although the distance representation of entity relations can be very diverse, it is difficult to predict the entity information which did not appear in the training phase. As a promising alternative, semantic matching based approaches utilize semantic information of entities and relationships, being capable of embedding those unseen entities based on their text description. Furthermore, due to the high-complex structure of the model and the slow training speed, the proportion of negative sampling is much lower for the training purpose, which leads to insufficient learning of negative sample entity information in the entity library and severely constrains the performance of the model.</p><p>To address the aforementioned issues, especially, with the goal to alleviate poor prediction performance of the unseen node of the translation distance model and insufficient training of the text matching model, in this paper, we propose a novel pre-training framework for knowledge graph, namely LP-BERT. Specifically, LP-BERT employs the semantic matching representation, which leverages the multi-task pre-training strategy, including masked language model task (MLM) for context learning, masked entity model task (MEM) for entity semantic learning, and mask relation model task (MRM) for relational semantic learning. With the pre-training tasks, LP-BERT could learn relational information and unstructured semantic knowledge of structured knowledge graphs. Moreover, to solve the problem of insufficient training induced by the low negative sampling ratio, we propose a negative sampling in a batch inspired by contrastive learning, which significantly increases the proportion of negative sampling while ensuring that the training time remains unchanged. At the same time, we propose a data augmentation method based on the inverse relationship of triples to increase the sample diversity, which can boost the performance further.</p><p>To demonstrate the effectiveness of robustness of the proposed solution, we comprehensively evaluate the performance of LP-BERT on WN18RR, FB15k-237, and UMLS datasets. Without bells and whistles, LP-BERT outperforms a group of competitive methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> and achieves state-ofthe-art results on WN18RR and UMLS datasets. The Hits@10 indicator is improved by 5% from the previous state-of-the-art result on the WN18RR dataset while reaching 100% on the UMLS dataset.</p><p>The structure of the remainder paper is as follows. Section II discusses the relationship between the proposed method and prior works. In Section III, we provide the details of our methodology, while Section IV describes comprehensive experimental results. Section V provides the conclusion of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Knowledge Graph Embedding</head><p>KG embedding is a well-studied topic, and comprehensive surveys can be found in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Traditional methods could only utilize the structural information which is observed in the triple to complete the knowledge graph. For example, TransE <ref type="bibr" target="#b10">[11]</ref> and TransH <ref type="bibr" target="#b11">[12]</ref> were two representative works that iteratively updated the vector representation of entities by calculating the distance between entities. Convolutional Neural Network (CNN) also obtained satisfying performance in knowledge graph embedding <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. In addition, different types of external information, such as entity types, logical rules and text descriptions, were introduced to enhance results <ref type="bibr" target="#b3">[4]</ref>. For text descriptions, <ref type="bibr" target="#b15">[16]</ref> firstly represented entities by averaging word embeddings contained in their names, where the word embeddings are learned from external corpora. <ref type="bibr" target="#b3">[4]</ref> embedded entities and words into the same vector space by aligning the Wikipedia anchor points and entity names. CNN also has been utilized to encode word sequences in entity description <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr" target="#b17">[18]</ref> proposed Semantic Space Projection (SSP) to learn topics and knowledge graph embedding together by depicting the strong correlation between fact triples and text descriptions.</p><p>Despite the satisfying performance of these models, they only learned the same text representation of entities and relationships, while the words in entity/relationship descriptions could have different meanings or importance weights in different triples. To address these problems, <ref type="bibr" target="#b18">[19]</ref> proposed a textenhanced KG embedding model TEKE, which could assign different embeddings to relationships in different triples and use the co-occurrence of entities and words in entity-labeled text corpus. <ref type="bibr" target="#b19">[20]</ref> used Long Short-Term Memory (LSTM) encoder with attention mechanism to construct context text representation with given different relationships. <ref type="bibr" target="#b20">[21]</ref> proposed an accurate text-enhanced KG embedding method by using the mutual attention mechanism between triple specific relation mention and relation mention and entity description.</p><p>Although these methods could deal with the semantic changes of entities and relations in different triples, they could not make full use of syntactic and semantic information in large-scale free text data because they only use entity description, relation mention and word co-occurrence with entities. KG-BERT <ref type="bibr" target="#b21">[22]</ref>, MLMLM <ref type="bibr" target="#b22">[23]</ref>, StAR <ref type="bibr" target="#b8">[9]</ref> and other works have introduced a pre-training paradigm. As aforementioned, due to the high complexities of the model and the slow training speed, the proportion of negative sampling is far lower than that of previous work, which leads to insufficient learning of negative sample entity information in the entity library.</p><p>Compared with aforementioned methods, our method introduces the contrastive learning strategy. In the training process, a novel negative sampling approach is carried out in the batch, thus the proportion of negative sampling can be exponentially increased, which can alleviate the insufficient learning issue. Moreover, we also optimize the pre-training strategy, so that the model can not only learn context knowledge, but also learn the element information of left entity, relationship and right entity, which greatly improves the performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Link Prediction</head><p>Link prediction is an active research area in KG embedding and has received lots of attention in recent years. KBGAT <ref type="bibr" target="#b23">[24]</ref>, a novel attention based feature embedding, was proposed to capture both entity and relation features in any given entity's neighbourhood. AutoSF <ref type="bibr" target="#b24">[25]</ref> endeavored to automatically design SFs for distinct KGs by the AutoML techniques. CompGCN <ref type="bibr" target="#b25">[26]</ref> aimed to build a novel Graph Convolutional framework that jointly embeds both nodes and relations in a relational graph, which leverages a variety of entity-relation composition operations from Knowledge Graph Embedding techniques and scales with the number of relations. Meta-KGR <ref type="bibr" target="#b26">[27]</ref> was a meta-based multi-hop reasoning method adopting meta-learning to learn effective meta parameters from high-frequency relations that could quickly adapt to fewshot relations. ComplEx-N3-RP <ref type="bibr" target="#b27">[28]</ref> designed a new selfsupervised training objective for multi-relational graph representation learning via simply incorporating relation prediction into the commonly used 1vsAll objective, which contains not only terms for predicting the subject and object of a given triple, but also a term for predicting the relation type. AttH <ref type="bibr" target="#b28">[29]</ref> was a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns, which combines hyperbolic reflections and rotations with attention to model complex relational patterns.</p><p>RotatE <ref type="bibr" target="#b29">[30]</ref> defined each relation as a rotation from the source entity to the target entity in the complex vector space, being able to model and infer various relation patterns, including symmetry/antisymmetry, inversion, and composition. HAKE <ref type="bibr" target="#b7">[8]</ref> combined TransE and RotatE to model entities at different levels of a hierarchy while distinguishing entities at the same level. GAAT <ref type="bibr" target="#b30">[31]</ref> integrated an attenuated attention mechanism and assigns diverse weights to relation paths to acquire the information from the neighborhoods. StAR <ref type="bibr" target="#b8">[9]</ref> partitioned a triple into two asymmetric parts as in translation distance based graph embedding approach and encodes both parts into contextualized representations by a Siamese-style textual encoder. However, the pre-training stage of textual models could only learn the context knowledge of the text, while ignoring the graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pre-training of Language Model</head><p>The pre-training language model method could be divided into two categories: feature-based method and fine-tuning based method <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Traditional word embedding methods, such as Word2Vec <ref type="bibr" target="#b33">[34]</ref> and Glove <ref type="bibr" target="#b34">[35]</ref>, aimed to use feature-based methods to learn context semantic information and express words in the form of feature vectors. ELMo <ref type="bibr" target="#b35">[36]</ref> extends traditional word embedding to context-aware word embedding where polysemy can be properly handled. Different from feature-based approaches, MLM-based pretraining method used in BERT <ref type="bibr" target="#b36">[37]</ref> opened up a pre-training paradigm of language model based on transformer structure. RoBERTa <ref type="bibr" target="#b37">[38]</ref> carefully measures the impact of key hyper-parameters and training data size and further enhances the effect. SpanBERT <ref type="bibr" target="#b38">[39]</ref> extended the BERT by masking contiguous random spans rather than random tokens, and training the span boundary representations to predict the entire content of the masked span without relying on the individual token representations within it. MacBERT <ref type="bibr" target="#b39">[40]</ref> improves upon RoBERTa in several ways, especially the masking strategy that adopts MLM as a correction (Mac).</p><p>Recently, the language model of pre-training has also been explored in the context of KG. <ref type="bibr" target="#b40">[41]</ref> learned context embeddings on entity-relational chains (sentences) generated by random walk in knowledge graph and initialized them as knowledge graph embedding models such as TransE <ref type="bibr" target="#b10">[11]</ref>. ERNIE <ref type="bibr" target="#b41">[42]</ref> utilized knowledge graphs, which can enhance language representation with external knowledge to train an enhanced language representation model. COMET <ref type="bibr" target="#b42">[43]</ref> used GPT to generate a given head phrase and a tail phrase tag of relation type in the knowledge base, which is not completely suitable for comparing the patterns of two entities with known relations. KG-BERT <ref type="bibr" target="#b21">[22]</ref> used the MLM method for pretraining on the triple data to learn the corpus information of the knowledge graph scene.</p><p>Unlike these studies, we use a multi-task pre-training strategy based on MLM, Mask Entity Model (MEM) and Mask Relation Model (MRM) so that the model can learn not only context corpus information but also learn the association information of triples at the semantic level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Task Formulation</head><p>A knowledge graph G can be represented as a set of triples</p><formula xml:id="formula_0">G = {E h , R, E t } i , i ? [1, N ],</formula><p>where N is number of triples in G, E h and E t denote head entity and tail entity, and from E h to E t there existing a directed edge with attributes (i.e. relation) R. All entities and relations are typically short text containing several tokens. Each entity also has its description, which is a long text describing the entity in detail. We use D h and D t to denote descriptions of E h and E t , respectively. The task of link prediction is to predict whether there exists a specific relation between two entities, namely, given E h and E t (with their descriptions D h and D t ), prediction whether <ref type="figure">Figure 1</ref> shows the overall framework of our proposed LP-BERT. It displays two procedures for link prediction: multitask pre-training and knowledge fine-tuning. In the pre-training stage, in addition to the prevalent Mask Language Model task (MLM) <ref type="bibr" target="#b36">[37]</ref>, we also propose two novel pre-training tasks: Mask Entity Model task (MEM) and Mask Relation Model task (MRM). Leveraging these three pre-training tasks in parallel, LP-BERT can learn both the context information of corpus and the semantic information of head-relationtail triples. In the fine-tuning stage, inspired by contrastive learning, we design a triple-style negative sampling in a batch, which significantly increasing the proportion of negative sampling while keeping the training time almost unchanged. Furthermore, we propose a data augmentation method based on the inverse relationship of triples to increase sample diversity. <ref type="figure">Fig. 1</ref>. An overview of the overall framework. LP-BERT consists of two phases: pre-training and fine-tuning. In the pre-training stage, except for the Mask Language Model task (MLM), we also propose two novel pre-training tasks, which are the Mask Entity Model task (MEM) and the Mask Relation Model task (MRM). Three tasks are trained in parallel, using the multi-task manner. In the fine-tuning stage, we design a triple-style negative sampling in a batch of data, which can significantly increase the proportion of negative sampling while retaining the training time almost unchanged.</p><formula xml:id="formula_1">{E h , R, E t } holds in G.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overall Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-task Pre-training</head><p>We propose to pre-train the LP-BERT model with three tasks. Although they require different masking strategy, they also share the same input, which concatenates entities, relation, as well as entity descriptions in a triple as a whole sequence:</p><formula xml:id="formula_2">X = [CLS] E h D h [SEP ] R [SEP ] Et Dt [SEP ]<label>(1)</label></formula><p>Here, symbol " " represents sentence concatenation, "[CLS]" and "[SEP]" are two reserved tokens in BERT <ref type="bibr" target="#b36">[37]</ref>, denoting the beginning and separation of the sequence. More details of the three pre-training tasks are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Mask Entity Modeling (MEM):</head><p>In the Mask Entity Modeling task, the entity sequence of the input is masked, and the model is required to recover the masked entity based on another entity and relation. The corresponding entity description is also masked to avoid information leaking. Since each triple includes two entities, MEM can mask either head entity or tail entity, but cannot mask both at the same time. Taking MEM on the tail entity as an instance, the input is as follows:  <ref type="figure">Fig. 2</ref>. An instance demonstrating the pre-training tasks. Entities, relations, and entity descriptions are concatenated as a whole sequence. For MLM, random tokens are masked; For MEM, either head entity or tail is masked, so we qualify MEM with subscript "h" or "t"; While For MRM, the relation is masked. It is worthwhile noticing that, these pre-training tasks can be combined using the multi-task learning paradigm during the pre-training procedure.</p><formula xml:id="formula_3">X = [CLS] E h D h [SEP ] R [M ASK] [P AD] [SEP ] (2) half mile E</formula><p>Here, we use "[MASK]" to represent those masked tokens. Since the tail entity is masked, its description will be replaced with reserved token "[PAD]" to avoid the model inferring entity just from its description. Note that both "[MASK]" and "[PAD]" could contain multiple tokens because they share the same length with the tail entity and tail description. The prediction objective is to recover the tail entity, not including its description. Similarly, for the prediction of head entities, LP-BERT just masks the head entity and predicts corresponding tokens. The head and tail entities are masked randomly with the same probabilities in the pre-training procedure.</p><p>To predict tokens in the entity, an classifier combining a multi-layer perceptron and Batch-Norm layer is built on the top of LP-BERT encoder to output the probability matrix of the prediction results:</p><formula xml:id="formula_4">p = BN ? GeLU ? MLP ? LP-BERT(X)<label>(3)</label></formula><p>in which "?" denotes function composition. Each token has a probability vector of the word vocabulary size, but the prediction results are not involved in the loss calculation except tokens of the masked entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Mask Relation Modeling (MRM):</head><p>A similar sample construction strategy as MEM is conducted for the Mask Relation Modeling task (MRM) (as shown in Algorithm 1). Instead of masking one of two entities in triple, MRM replaces tokens in the relation with "[MASK]", while preserving the head and tail entities and descriptions. Then MRM drives the model to predict the corresponding relation between two entities. The masked sample can be represented as follows:</p><formula xml:id="formula_5">X = [CLS] E h D h [SEP ] [M ASK] E t D t [SEP ] (4)</formula><p>3) Mask Language Modeling (MLM): In order to coexist with MEM and MRM, unlike BERT which employs <ref type="bibr" target="#b36">[37]</ref> random masking prediction of all tokens in the sequence, the proposed MLM method only makes local random masking for the specific text range of the sample. The random masking strategy is as follows:</p><p>? For head entity prediction task in MEM, random mask only in token sequences of E t and D t ; ? For tail entity prediction task in MEM, random mask only in token sequences of E h and D h ; </p><p>where L is the final loss, y and y are the prediction objectives and the gold label, respectively, ? is the random number uniformly distributed in the interval [0, 1]. Details of L M IM are shown as follows: </p><formula xml:id="formula_7">L M IM (y , y|?) = ? ? ? L M EM h (y , y) 0.0 ? ? &lt; 0.4 L M EMt (y , y) 0.4 ? ? &lt; 0.8 L M RM (y , y) 0.8 ? ? &lt; 1.0 (6) Algorithm</formula><formula xml:id="formula_8">y 1 [E h ] ? x[E h ] 22: x[E h ] ? [MASK] * len(E h ) 23: x[D h ] ? [PAD] * len(E h ) 24: x, y 2 ? MLM(x, y 2 , R E t D t ) 25: end if 26: if 0.4?random state&lt;0.8 then 27: y 1 [E t ] ? x[E t ] 28: x[E t ] ? [MASK] * len(E t ) 29: x[D t ] ? [PAD] * len(E t )</formula><p>30:</p><p>x, y 2 ? MLM(x, y 2 , E h D h R) 31: end if 32: if random state?0.8 then 33:</p><formula xml:id="formula_9">y 1 [R] ? x[R]</formula><p>34:</p><p>x[R] ? [MASK] * len(R) <ref type="bibr">35:</ref> x, y 2 ? MLM(x, y 2 , regions of x other than R) 36: end if 37: return x, y 1 , y 2 D. Knowledge Graph Fine-tuning 1) Knowledge Representation: The sample construction method in the fine-tuning stage is different from that in the pre-training stage. Inspired by the StAR <ref type="bibr" target="#b8">[9]</ref>, for each triple, we concatenate E h and R texts. Then the pre-trained LP-BERT model uses the structure of Siamese <ref type="bibr" target="#b43">[44]</ref> to encode E h R and E t , respectively. The fine-tuning objective of the model is to make the two representations of the positive sample closer and the negative further. Here, the positive sample means the ( E h , R, E t ) exists in the knowledge base, while the negative sample does not.</p><p>Since a triple (E h , R, E t ) is splitted into E h R and E t , the knowledge graph can only supply positive samples. Hence, to conduct binary classification during fine-tuning, we propose a simple but effective method to generate negative samples. As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>, for a mini batch of size n, by interleaving E hi R i and E tj (1 ? i, j ? n), we take E hi R i , E tj (i = j) as negative samples. Therefore, for a mini-batch, LP-BERT only forwards twice for n 2 sample distance, which greatly increases the proportion of negative sampling and reduces the training time. The detailed procedure of constructing negative samples for fine-tunning is shown in Algorithm 2.  end for 13: end for 14: return p, y 2) Triple Augmentation: The above pair-based knowledge graph representation method has limitations because it cannot represent (E h , RE t ) pair directly in the head entity prediction task. Specifically, for the construction of the negative samples, we can only make negative sampling for E t but cannot for E h , limiting the diversity of negative samples, especially when there are mo relationships in the dataset. Therefore, we propose a dual relationship data enhancement method. For each relationship R, we define a corresponding inverse relationship R rev . For the head entity prediction sample in the form of (?, R, E r ), we rewrite it into the form of (E r , R rev , ?)</p><p>to enhance the data. In this way, we can use the vector representation of (E t R rev , E h ) to replace (E h , RE t ), improving the diversity of sampling and the robustness of the model. Moreover, we use the mixed precision strategy of fp16 and fp32 to reduce the GPU memory usage of gradient calculation to improve the size of n and increase the negative sampling ratio.</p><p>3) Fine-tuning Loss Designing: We designed two distance calculation methods to jointly calculate the loss function,</p><formula xml:id="formula_10">L = L 1 (V E h R , V Et ) + L 2 (V E h R , V Et )<label>(7)</label></formula><p>where V E h R and V Et are encoded vectors of E h R and E t , respectively.</p><formula xml:id="formula_11">L1(d1) = ??t(1 ? d1) ? log(d1) y = 1 ??t(d1) ? log(1 ? d1) y = 1 (8)</formula><p>L2(d2) = Sigmoid(sum(d2)) y = 1 1 ? Sigmoid(sum(d2)) y = 1 <ref type="bibr" target="#b8">(9)</ref> where</p><formula xml:id="formula_12">d 1 = V E h RVE t V E h R V E t d 2 = V E h R ? V Et<label>(10)</label></formula><p>where ? is used to adjust the weights of positive and negative samples, ? is used to adjust the weights of samples that are difficult to distinguish. Two different dimensional distance calculation methods are used to calculate the distance relationship between multi-task learning vector pairs.</p><p>IV. EXPERIMENTS In this part, we firstly detail the experimental settings. Then, we demonstrate the effectiveness of the proposed model on multiple widely-used datasets, including WN18RR <ref type="bibr" target="#b12">[13]</ref> FB15k-237 <ref type="bibr" target="#b44">[45]</ref> and UMLS <ref type="bibr" target="#b12">[13]</ref> benchmark datasets. Secondly, ablation studies are conducted and we tease apart them from our standard model and keep other structures as they are, with the goal to justify the contribution of each improvement. Finally, we conduct extensive analysis of the prediction performance for the unseen entities and a case study is also provided to show the effectiveness of proposed LP-BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment Settings and Datasets</head><p>We evaluate the proposed models on WN18RR <ref type="bibr" target="#b12">[13]</ref>, FB15k-237 <ref type="bibr" target="#b44">[45]</ref> and UMLS <ref type="bibr" target="#b12">[13]</ref> datasets. For WN18RR, the dataset was adopted from WordNet, for the link prediction task <ref type="bibr" target="#b45">[46]</ref> and it consists of English phrases and the corresponding semantic relations. FB15k-237 <ref type="bibr" target="#b44">[45]</ref> is a subset of Freebase <ref type="bibr" target="#b46">[47]</ref>, which consists of real-world named entities and their relations. Both WN18RR and FB15k-237 are updated from WN18 and FB15k <ref type="bibr" target="#b10">[11]</ref> respectively by removing inverse relations and data leakage, which is one of the most popular benchmark. UMLS <ref type="bibr" target="#b12">[13]</ref> is a small KG containing medical semantic entities and their relations. The summary statistics of the datasets are shown in <ref type="table" target="#tab_3">Table I</ref>.</p><p>We implement the LP-BERT using PyTorch 1 framework, on a workstation with an Intel Xeon processor with a 64GB In the inference phase, all other entities in the knowledge graph are regarded as the wrong candidate which can damages their head or tail entities. The trained model aims to use the "filtered" settings to correct triple ranking of the corrupt. The evaluation metrics has two aspects : (1) Hits@N represents the ratio of test instances in the top N of correct candidates;</p><p>(2) The average rank (MR) and the average reciprocal rank (MRR) reflect the absolute ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>We benchmark the link prediction tasks using proposed methods and competitive approaches, including both the translational distance-based approaches and semantic matchingbased methods. For the translational distance models, 18 widely-used solutions are tested in our experiments, including TransE <ref type="bibr" target="#b10">[11]</ref>, DistMult <ref type="bibr" target="#b47">[48]</ref>, ComplEx <ref type="bibr" target="#b48">[49]</ref>, R-GCN <ref type="bibr" target="#b14">[15]</ref>, ConvE <ref type="bibr" target="#b12">[13]</ref>, KBAT <ref type="bibr" target="#b23">[24]</ref>, QuatE <ref type="bibr" target="#b49">[50]</ref>, RotatE <ref type="bibr" target="#b29">[30]</ref>, TuckER <ref type="bibr" target="#b50">[51]</ref>, AttH <ref type="bibr" target="#b28">[29]</ref>, DensE <ref type="bibr" target="#b52">[53]</ref>, Rot-Pro <ref type="bibr" target="#b4">[5]</ref>, QuatDE <ref type="bibr" target="#b5">[6]</ref>, Lin-eaRE <ref type="bibr" target="#b6">[7]</ref>, CapsE <ref type="bibr" target="#b53">[54]</ref>, RESCAL-DURA <ref type="bibr" target="#b54">[55]</ref> and HAKE <ref type="bibr" target="#b7">[8]</ref>. As expected, only a few previous attempts employed semantic-matching-based methods, due to the difficulties of the training. Here, KG-BERT <ref type="bibr" target="#b21">[22]</ref> and StAR <ref type="bibr" target="#b8">[9]</ref> are used for the quantitative comparison.</p><p>The detailed results are shown in <ref type="table" target="#tab_3">Table II</ref>. As can be observed from the Table, the LB-BERT is able to achieve stateof-the-art or competitive performance on all three widely used datasets, including WN18RR, FB15k-237 and UMLS dataset. The improvement is especially significant in terms of Hits@10 and Hits@3 due to the superior generalization performance of multi-task pre-training textual encoding approach, which will be further analyzed in the section below. Furthermore, LP-BERT surpasses all other methods by a large margin in terms of Hits@3, Hits@10 on WN18RR and Hits@10 on UMLS. Although it only achieves inferior <ref type="table" target="#tab_2">performance  TABLE II  EXPERIMENTAL RESULTS ON WN18RR, FB15K-237 AND UMLS DATASETS. THE BOLD NUMBERS DENOTE THE BEST RESULTS IN EACH GENRE WHILE  THE UNDERLINED ONES ARE STATE-OF-THE-ART PERFORMANCE. WE CAN SEE THAT LP-BERT ACHIEVES STATE-OF-THE-ART PERFORMANCE IN  MULTIPLE EVALUATION RESULTS ON WN18RR AND UMLS DATASETS, AND OUTPERFORMS OTHER SEMANTIC MATCHING MODELS ON THE FB15K-</ref> on FB15k-237 dataset and Hits@1 and MRR of WN18RR dataset compared to translational distance models, it still remarkably outperforms other semantic matching models such as KG-BERT and StAR from the same genre by introducing structured knowledge. In particular, LB-BERT outperforms StAR <ref type="bibr" target="#b8">[9]</ref>, which is the previous state-of-the-art model, on all three datasets with only one-third parameters numbers of it. For the WN18RR datasets, the Hits@1 is increased from 0.243 to 0.343, Hits@3 is increased to 0.563 from 0.491. The experimental results show that the semantic matching models perform well in the topK recall evaluation methods, but the Hits@1 result is significantly inferior to the translation distance models. This is because that the features of the semantic matching models are based on text, which leads to the vector representation of similar entities in the text being close and difficult to distinguish. Although translation distance models perform well on the Hits@1, they are incapable of understanding the text semantics. For new entities not seen in the training set, the prediction results of translation distance models are random, while the semantic matching models are reliable, which is why Hits@3 and Hits@10 LP-BERT can far exceed the translation distance models to achieve state-of-art performance.</p><p>Similar to KG-BERT and StAR, our model is relied BERT and we compare LP-BERT with KG-BERT and StAR on WN18RR in detail, including different initialization manners. As shown in <ref type="table" target="#tab_3">Table III</ref>, LP-BERT consistently achieves superior performance over most metrics. The evaluation effect of the LB-BERT model based on RoBERTa-base has exceeded the evaluation effect of KG-BERT and StAR based on RoBERTa-large. As for empirical efficiency, due to the small number of model parameters and the strategy of negative sampling based on the batch in the training process, our model is faster than KG-BERT and StAR for both the training and inference phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>In this part, we tease apart each module from our standard model and keep other structures as they are, with the goal to justify the contribution of each improvement. The ablation experiments are conducted on the WN18RR dataset, and we find similar performance on all three datasets. <ref type="table" target="#tab_3">Table IV</ref> shows the results. After adding a batch-based triple-style negative sampling strategy combined with focal-loss, the appropriate negative sampling ratio dramatically improves the model evaluation effect, as shown in the second line. The original pretraining weights (BERT or RoBERTa pre-trained weights) are not familiar with the corpus information of the link prediction task. After adding the pre-training strategy based on MLM, the evaluation effect is improved. However, the pre-training method based on MLM strategy does not fully excavate the relationship information of triplets, and the multi-task pretraining strategy combined with MLM, MEM, and MRM makes the model evaluation result optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Unseen Entities</head><p>To verify the prediction performance of LP-BERT on unseen entities, we re-split the dataset. Specifically, we randomly select 10% of the entity triples as the validation set and test set, respectively, to ensure that the training set, validation set, and test set don't overlap on any entity. We then re-train and evaluate LP-BERT as well as other baselines, of which the results are shown in <ref type="table" target="#tab_6">Table V</ref>.</p><p>As can be seen from the table, all models experienced dramatic performance drop across the five metrics. In particular, the distance-based methods are inferior in coping with unseen entities. As mentioned above, such methods only encode the relationship and distance between entities without including semantic information, thus incapable of encoding entities not seen in the training set. Contrastively, pre-training based models, including MLMLM, StAR, and our LP-BERT, have displayed their ability to cope with unseen entities. Furthermore, our LP-BERT surpasses MLMLM and StAR on almost all metrics, proving its superiority for processing unseen entities. Especially, LP-BERT outperforms StAR, the previous state-of-the-art, with over 6-9 points on Hits@3 and Hits@10. However, the score of LP-BERT on the mean rank metric is not as well as other metrics, indicating LP-BERT performs worse on those failed entities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Case Study</head><p>To further demonstrate the performance of LP-BERT, additional case studies are conducted on the WN18RR datasets and the visualization of the results are provided in <ref type="table" target="#tab_3">Table  VI</ref>. In the table, each row denotes a real sample which is randomly selected from the test set. The first column is a triple which is formatted as (left entity, relation, ?) ? right entity. The prediction models employ the left entity and the relationship to predict the right entity. From the second column to the fourth column, we present the Top-5 ranked entities with highest predicted probability obtained using different pre-training approaches. The entities are ordered using the predicted probability and the correct answers are highlighted using the bold font. Column 2 presents the predicted results of the proposed pre-training strategy of our proposed LP-BERT. Column 3 provides the results obtained by only using the MLM-based pre-training, while the last column presents the results obtained without pre-training.</p><p>For different approaches, the order of the correctly predicted results is given in the table (the number in each element). For LP-BERT, the orders of the correct predicted results are [1,2,1,2,2], while the orders are <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3]</ref> for MLMbased pre-training and the orders are <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4]</ref> without pre-training. The results suggest that LP-BERT can provide superior performance, with comparison to the MLM-based pre-training the model without pre-training. Noting that, all the presented results are typical and not cherry-picked for presentation, with the goal to avoid misrepresenting the actual performance of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>This paper focuses on a fundamental yet critical task in the natural language processing field, i.e., semantic network completion. More specifically, we manage to predict the linkage  <ref type="table" target="#tab_3">FROM THE SECOND COLUMN TO THE FOURTH COLUMN, WE PRESENT THE TOP-5 RANKED ENTITIES WITH HIGHEST PREDICTED PROBABILITY  WHICH WERE OBTAINED FROM DIFFERENT PRE-TRAINING APPROACHES (INCLUDING THE PROPOSED LP-BERT-BASED PRE-TRAINING, MLM-BASED  PRE-TRAINING AND WITHOUT PRE-TRAINING). THE ENTITIES ARE ORDERED USING THE PREDICTED PROBABILITY. THE CORRECT ANSWERS ARE  HIGHLIGHTED USING THE BOLD FONT. THE NUMBER IN EACH ELEMENT IS THE ORDER OF THE CORRECTLY PREDICTED</ref>  between entities in the semantic network of the knowledge graph. We employ the language model and introduce the LP-BERT, which contains multi-task pre-training and knowledge graph fine-tuning phases. In the pre-training phase, we propose two novel pre-training tasks, MEM and MRM, to encourage the model to learn the knowledge of context and the structure information of the knowledge graph. While in the finetuning phase, we design a triple-style negative sampling in a batch, which greatly increases the proportion of negative sampling while keeping the training time almost unchanged. Extensive experimental results on three datasets demonstrated the efficiency and effectiveness of our proposed LP-BERT. In future work, we will explore more diverse pre-training tasks and increase the model parameter size to enable LP-BERT to store larger graph knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGEMENTS</head><p>This work is partially supported by the National Key Research and Development Program of China (2021ZD0112901).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>For MRM task, random mask only in token sequences of E h , D h , E t and D t . In this way, MLM drives the model to learn the corpus's context information. More important, though MRM and MEM are exclusive, they are both compatible with MLM. Therefore, MLM is conducted together with either MEM or MRM during the pre-training procedure for a minibatch. At the same time, doing masking is equivalent to doing dropout-like regularization for the text features of MEM and MRM tasks, and it can improve the performance of MEM and MRM, as shown in the experiments. Algorithm 1 shows more details about construction samples for pre-training LP-BERT. Specifically, line 20-25 and line 26-31 shows the procedure of MEM (for head entity and tail entity, respectively) with MLM, and line 32-36 shows the procedure of MRM with MLM. Line 1-15 of Algorithm 1 displays in detail the strategy for masking tokens, i.e., MLM. 4) Pre-traning Loss Designing: Since the strategies of constructing samples in MEM and MRM tasks are mutually exclusive, the prediction of head entity and tail entity cannot be simultaneously predicted for the triple samples trained by the same input model. To ensure the generalization ability of the model, we use Mask Item Model (MIM) task as a unified expression of MEM and MRM tasks and define loss function as follows: L = L M LM (y , y) + L M IM (y , y|?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Label matrix for a batch of size n. For E h R i and E ti in the ith(1 ? i ? n) element of batch, they can only generate 1 positive sample and n ? 1 negative samples. Therefore, there are n 2 samples in the batch, including n positive samples and n(n ? 1) negative samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>1</head><label></label><figDesc>Sample construction in the pre-training phase. Require: Tokens: Token list of Triples Ensure: x: masked input, y 1 : MRM or MEM target, y 2 : MLM target 1: function MLM(x, y 2 , region) 2:for i in region do3:    if random[0, 1]&lt;0.15 then</figDesc><table><row><cell>4:</cell><cell>y 2 [i] = x[i]</cell></row><row><cell>5:</cell><cell>if random[0, 1]&lt;0.8 then</cell></row><row><cell>6:</cell><cell>x[i] = [MASK]</cell></row><row><cell>7:</cell><cell>else</cell></row><row><cell>8:</cell><cell>if random[0, 1]&gt;0.5 then</cell></row><row><cell>9:</cell><cell>x[i] = sample(Vocab)</cell></row><row><cell>10:</cell><cell>end if</cell></row><row><cell>11:</cell><cell>end if</cell></row><row><cell>12:</cell><cell>end if</cell></row><row><cell>13:</cell><cell>end for</cell></row><row><cell>14:</cell><cell>return x, y 2</cell></row><row><cell cols="2">15: end function</cell></row><row><cell>16:</cell><cell></cell></row><row><cell cols="2">17: x ?X in Equation 1</cell></row><row><cell cols="2">18: y 1 , y 2 ? [PAD] * len(X)</cell></row><row><cell cols="2">19: random state? random([0, 1])</cell></row><row><cell cols="2">20: if random state&lt;0.4 then</cell></row><row><cell>21:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Algorithm 2</head><label>2</label><figDesc>Batch sample construction in the fine-tuning phase. Require: x 1 : E h R batch tokens, x 2 : E t batch tokens, dict: a dict can get positive Entities for each E h R Ensure: p: model predict results, y: ground truth</figDesc><table><row><cell>8:</cell><cell>y.append(1)</cell></row><row><cell>9:</cell><cell>else</cell></row><row><cell>10:</cell><cell>y.append(0)</cell></row><row><cell>11:</cell><cell>end if</cell></row><row><cell>12:</cell><cell></cell></row></table><note>1: Emb 1 ? Encoding of x 1 2: Emb 2 ? Encoding of x 2 3: p ? cosine similarity of Emb 1 and Emb 2 4: y = []5: for E h R ? x 1 do6: for E t ? x 2 do7: if E t ? dict[E h R] then</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I THE</head><label>I</label><figDesc>SUMMARY STATISTICS OF THE USED DATASETS, WHICH INCLUDING WN18RR, FB15K-237 AND UMLS. Nvidia P40 GPU for the training purpose. The AdamW optimizer is used with 5% steps of warmup. For the hyper-parameters in LP-BERT, we set the epochs to 50, the batch size as 32, and the learning rate=10 ?4 /5 ? 10 ?5 respectively for the linear and attention parts initialized. The earlystop epoch number is set as 3. In the knowledge graph finetuning phase, we set the batch size as 64 on WN18RR, 120 for FB15k-237, 128 on UMLS based on the best Hits@10 on development dataset. The learning rate is set as 10 ?3 /5?10 ?5 respectively for the linear and attention parts initialized with LB-BERT. The number of training epochs is 7 on WN18RR and FB15k-237, while 30 for the UMLS datasets, ? = 0.8 on WN18RR and UMLS, ? = 0.5 on FB15k-237, and ? = 2 in Eq 8.</figDesc><table><row><cell></cell><cell cols="3">WN18RR FB15k-237 UMLS</cell></row><row><cell>Entities</cell><cell>40943</cell><cell>14541</cell><cell>135</cell></row><row><cell>Relations</cell><cell>11</cell><cell>237</cell><cell>46</cell></row><row><cell>Train samples</cell><cell>86835</cell><cell>272115</cell><cell>5216</cell></row><row><cell>Valid samples</cell><cell>3034</cell><cell>17535</cell><cell>652</cell></row><row><cell>Test samples</cell><cell>3034</cell><cell>20466</cell><cell>661</cell></row><row><cell>of RAM and a</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>237 DATASET. ? MEANS THAT HIGHER VALUES PROVIDE BETTER PERFORMANCE, WHILE ? MEANS THAT LOWER VALUES PROVIDE BETTER PERFORMANCE.</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="2">WN18RR</cell><cell></cell><cell></cell><cell></cell><cell cols="2">FB15k-237</cell><cell></cell><cell></cell><cell cols="2">UMLS</cell></row><row><cell></cell><cell cols="12">Hits@1? Hits@3? Hits@10? MR? MRR? Hits@1? Hits@3? Hits@10? MR? MRR? Hits@10? MR?</cell></row><row><cell cols="2">Translational distance models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TransE[11]</cell><cell>0.043</cell><cell>0.441</cell><cell cols="4">0.532 2300 0.243 0.198</cell><cell>0.376</cell><cell>0.441</cell><cell cols="2">323 0.279</cell><cell>0.989</cell><cell>1.84</cell></row><row><cell>DistMult[48]</cell><cell>0.412</cell><cell>0.470</cell><cell cols="4">0.504 7000 0.444 0.199</cell><cell>0.301</cell><cell>0.446</cell><cell cols="2">512 0.281</cell><cell>0.846</cell><cell>5.52</cell></row><row><cell>ComplEx[49]</cell><cell>0.409</cell><cell>0.469</cell><cell cols="4">0.530 7882 0.449 0.194</cell><cell>0.297</cell><cell>0.450</cell><cell cols="2">546 0.278</cell><cell>0.967</cell><cell>2.59</cell></row><row><cell>R-GCN [15]</cell><cell>0.080</cell><cell>0.137</cell><cell cols="4">0.207 6700 0.123 0.100</cell><cell>0.181</cell><cell>0.300</cell><cell cols="2">600 0.164</cell><cell>-</cell><cell>-</cell></row><row><cell>ConvE[13]</cell><cell>0.419</cell><cell>0.470</cell><cell cols="4">0.531 4464 0.456 0.225</cell><cell>0.341</cell><cell>0.497</cell><cell cols="2">245 0.312</cell><cell>0.990</cell><cell>1.51</cell></row><row><cell>KBAT[24]</cell><cell>-</cell><cell>-</cell><cell cols="3">0.554 1921 0.412</cell><cell>-</cell><cell>-</cell><cell>0.331</cell><cell cols="2">270 0.157</cell><cell>-</cell><cell>-</cell></row><row><cell>QuatE[50]</cell><cell>0.436</cell><cell>0.500</cell><cell cols="4">0.564 3472 0.481 0.221</cell><cell>0.342</cell><cell>0.495</cell><cell cols="2">176 0.311</cell><cell>-</cell><cell>-</cell></row><row><cell>RotatE[30]</cell><cell>0.428</cell><cell>0.492</cell><cell cols="4">0.571 3340 0.476 0.241</cell><cell>0.375</cell><cell>0.533</cell><cell cols="2">177 0.338</cell><cell>-</cell><cell>-</cell></row><row><cell>TuckER[51]</cell><cell>0.443</cell><cell>0.482</cell><cell>0.526</cell><cell>-</cell><cell cols="2">0.470 0.266</cell><cell>0.394</cell><cell>0.544</cell><cell>-</cell><cell>0.358</cell><cell>-</cell><cell>-</cell></row><row><cell>AttH[29]</cell><cell>0.443</cell><cell>0.499</cell><cell>0.573</cell><cell>-</cell><cell cols="2">0.486 0.252</cell><cell>0.384</cell><cell>0.540</cell><cell>-</cell><cell>0.348</cell><cell>-</cell><cell>-</cell></row><row><cell>ConE[52]</cell><cell>0.453</cell><cell>0.515</cell><cell>0.579</cell><cell>-</cell><cell cols="2">0.496 0.247</cell><cell>0.381</cell><cell>0.54</cell><cell>-</cell><cell>0.345</cell><cell>-</cell><cell>-</cell></row><row><cell>DensE[53]</cell><cell>0.443</cell><cell>0.508</cell><cell cols="4">0.579 3052 0.491 0.256</cell><cell>0.384</cell><cell>0.535</cell><cell cols="2">169 0.349</cell><cell>-</cell><cell>-</cell></row><row><cell>Rot-Pro[5]</cell><cell>0.397</cell><cell>0.482</cell><cell>0.577</cell><cell>-</cell><cell cols="2">0.457 0.246</cell><cell>0.383</cell><cell>0.540</cell><cell>-</cell><cell>0.344</cell><cell>-</cell><cell>-</cell></row><row><cell>QuatDE[6]</cell><cell>0.438</cell><cell>0.509</cell><cell cols="4">0.586 1977 0.489 0.268</cell><cell>0.400</cell><cell>0.563</cell><cell cols="2">90 0.365</cell><cell>-</cell><cell>-</cell></row><row><cell>LineaRE [7]</cell><cell>0.453</cell><cell>0.509</cell><cell cols="4">0.578 1644 0.495 0.264</cell><cell>0.391</cell><cell>0.545</cell><cell cols="2">155 0.357</cell><cell>-</cell><cell>-</cell></row><row><cell>CapsE[54]</cell><cell>-</cell><cell>-</cell><cell>0.559</cell><cell cols="2">718 0.415</cell><cell>-</cell><cell>-</cell><cell>0.356</cell><cell cols="2">403 0.150</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">RESCAL-DURA [55] 0.455</cell><cell>-</cell><cell>0.577</cell><cell>-</cell><cell cols="2">0.498 0.276</cell><cell>-</cell><cell>0.550</cell><cell>-</cell><cell>0.368</cell><cell>-</cell><cell>-</cell></row><row><cell>HAKE[8]</cell><cell>0.452</cell><cell>0.516</cell><cell>0.582</cell><cell>-</cell><cell cols="2">0.497 0.250</cell><cell>0.381</cell><cell>-</cell><cell>-</cell><cell>0.346</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Semantic matching models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Parameters</cell></row><row><cell>KG-BERT[22]</cell><cell>0.041</cell><cell>0.302</cell><cell>0.524</cell><cell cols="2">97 0.216</cell><cell>-</cell><cell>-</cell><cell>0.420</cell><cell>153</cell><cell>-</cell><cell>0.990</cell><cell>1.47</cell><cell>102M</cell></row><row><cell>StAR[9]</cell><cell>0.243</cell><cell>0.491</cell><cell>0.709</cell><cell cols="3">51 0.401 0.205</cell><cell>0.322</cell><cell>0.482</cell><cell cols="2">117 0.296</cell><cell>0.991</cell><cell>1.49</cell><cell>335M</cell></row><row><cell>LP-BERT</cell><cell>0.343</cell><cell>0.563</cell><cell>0.752</cell><cell cols="3">92 0.482 0.223</cell><cell>0.336</cell><cell>0.490</cell><cell cols="2">154 0.310</cell><cell>1.000</cell><cell>1.18</cell><cell>102M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III QUANTITATIVE</head><label>III</label><figDesc>COMPARISONS WITH KG-BERT AND STAR ON WN18RR DATASETS. "TRAIN" DENOTE THE TIME FOR PER TRAINING EPOCH, AND AND "INFERENCE" DENOTES TOTAL INFERENCE TIME ON TEST SET. THE VALUES WERE COLLECTED USING TESLA P40 WITHOUT MIXED PRECISION.</figDesc><table><row><cell></cell><cell cols="7">Weight Initialization Hits@1? Hits@3? Hits@10? MR? MRR? Train? Inference?</cell></row><row><cell cols="2">KG-BERT RoBERTa-base</cell><cell>0.130</cell><cell>0.320</cell><cell>0.636</cell><cell cols="2">84 0.278 4h</cell><cell>32h</cell></row><row><cell>StAR</cell><cell>RoBERTa-base</cell><cell>0.202</cell><cell>0.410</cell><cell>0.621</cell><cell cols="2">71 0.343 2h</cell><cell>0.9h</cell></row><row><cell cols="2">LP-BERT RoBERTa-base</cell><cell>0.278</cell><cell>0.502</cell><cell>0.708</cell><cell cols="2">79 0.424 0.8h</cell><cell>0.8h</cell></row><row><cell cols="2">KG-BERT RoBERTa-large</cell><cell>0.119</cell><cell>0.387</cell><cell>0.698</cell><cell cols="2">95 0.297 7.9h</cell><cell>92h</cell></row><row><cell>StAR</cell><cell>RoBERTa-large</cell><cell>0.243</cell><cell>0.491</cell><cell>0.709</cell><cell cols="2">51 0.401 5.5h</cell><cell>1h</cell></row><row><cell cols="2">LP-BERT RoBERTa-large</cell><cell>0.306</cell><cell>0.517</cell><cell>0.718</cell><cell cols="2">69 0.444 2.2h</cell><cell>1h</cell></row><row><cell cols="2">LP-BERT LP-BERT-base</cell><cell>0.343</cell><cell>0.563</cell><cell>0.752</cell><cell cols="2">92 0.482 0.8h</cell><cell>0.8h</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE IV</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">ABLATION STUDY FOR LP-BERT ON THE WN18RR DATASET.</cell></row><row><cell cols="8">MLM MEM MRM Batch-Sampling Hits@1? Hits@3? Hits@10? MR? MRR?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.136</cell><cell>0.306</cell><cell>0.499</cell><cell cols="2">143 0.257</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.278</cell><cell>0.502</cell><cell>0.708</cell><cell cols="2">79 0.424</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.307</cell><cell>0.530</cell><cell>0.721</cell><cell cols="2">101 0.449</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.300</cell><cell>0.540</cell><cell>0.718</cell><cell cols="2">103 0.447</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.329</cell><cell>0.555</cell><cell>0.733</cell><cell cols="2">109 0.469</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.343</cell><cell>0.563</cell><cell>0.752</cell><cell cols="2">92 0.482</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>OF LP-BERT ON UNSEEN ENTITIES.</figDesc><table><row><cell cols="5">Models Hits@1? Hits@3? Hits@10? MRR? MR?</cell></row><row><cell>TransE</cell><cell cols="2">0.0010 0.0010</cell><cell cols="2">0.0010 0.0010 21708</cell></row><row><cell>DistMult</cell><cell>0.000</cell><cell>0.000</cell><cell>0.000</cell><cell>0.000 33955</cell></row><row><cell cols="2">ComplEx 0.000</cell><cell>0.000</cell><cell>0.000</cell><cell>0.000 24678</cell></row><row><cell>RotatE</cell><cell cols="2">0.0010 0.0010</cell><cell cols="2">0.0010 0.0010 21023</cell></row><row><cell cols="3">LinearRE 0.0010 0.0010</cell><cell cols="2">0.0010 0.0010 21502</cell></row><row><cell>QuatDE</cell><cell cols="2">0.0010 0.0010</cell><cell cols="2">0.0010 0.0010 21301</cell></row><row><cell cols="3">MLMLM 0.0490 0.0932</cell><cell cols="2">0.1413 0.0812 6324</cell></row><row><cell>StAR</cell><cell cols="2">0.1108 0.2355</cell><cell cols="2">0.4053 0.2072 535</cell></row><row><cell cols="3">LP-BERT 0.1204 0.2978</cell><cell cols="2">0.4919 0.2434 1998</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI CASE</head><label>VI</label><figDesc>STUDY USING THE WN18RR DATASETS. THE FIRST COLUMN IS A TRIPLE WHICH IS FORMATTED AS (LEFT ENTITY, RELATION, ?) ? RIGHT ENTITY.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>RESULTS.</figDesc><table><row><cell>Incomplete Triple</cell><cell cols="3">Positive entity ranking position &amp; Top-5 ranked candidate entities</cell></row><row><cell></cell><cell>LP-BERT Pre-training</cell><cell>MLM Pre-training</cell><cell>without Pre-training</cell></row><row><cell>(wheeled vehicle, has part, ?)</cell><cell>1, (axle, handlebar, wheel spoke,</cell><cell>3, (hub, axletree, axle,geared</cell><cell>6, (car wheel, bicycle wheel, wagon</cell></row><row><cell>? axle</cell><cell>hub, splash guard)</cell><cell>wheel, wheel spoke)</cell><cell>wheel, tyre, axletree)</cell></row><row><cell>(heterokontae, hypernym, ?)</cell><cell>2, (division a , class, kingdom,</cell><cell>3, (hub, axletree, axle,geared</cell><cell>21, (protista, algae, euglenophyta,</cell></row><row><cell>? class</cell><cell>subphylum, division b )</cell><cell>wheel, wheel spoke)</cell><cell>pyrrophyta, protoctista)</cell></row><row><cell>(chorus line, member meronym, ?)</cell><cell>1, (showgirl, chorister, chorus line,</cell><cell>5, (chorus line, chorus, greek</cell><cell>12, (chorus line, chorus, choir,</cell></row><row><cell>? showgirl</cell><cell>chorus, choir)</cell><cell>chorus, choir, showgirl)</cell><cell>greek chorus, chorus)</cell></row><row><cell>(intense, also see, ?)</cell><cell>2, (intense, profound, impressive,</cell><cell>3, (intense, extraordinary,</cell><cell>6, (intense, strong, hot,</cell></row><row><cell>? profound</cell><cell>significant, strong)</cell><cell>profound, wide, large)</cell><cell>powerful, violent)</cell></row><row><cell>(white, synset domain topic of, ?)</cell><cell>2, (board game, chess game,</cell><cell>3, ([board game, table game,</cell><cell>4, (board game, cards, bridge,</cell></row><row><cell>? chess game</cell><cell>gameboard, table game, cards)</cell><cell>chess game, cards, game)</cell><cell>chess game, game)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://pytorch.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">From feedforward to recurrent lstm neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="517" to="529" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding for link prediction: A comparative analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Firmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matinata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rot-pro: Modeling transitivity by projection in knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Quatde: Dynamic quaternion embedding for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Zakari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Owusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.09002</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lineare: Simple but powerful knowledge graph embedding for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="422" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning hierarchy-aware knowledge graph embeddings for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structureaugmented text representation learning for efficient knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1737" to="1748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph embedding techniques, applications, and performance: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="78" to="94" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A convolutional neural network-based model for knowledge base completion and its application to search personalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="947" to="960" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ssp: semantic space projection for knowledge graph embedding with text descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Text-enhanced representation learning for knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligent (IJCAI)</title>
		<meeting>International Joint Conference on Artificial Intelligent (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Knowledge graph representation with jointly structural and textual encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08661</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accurate text-enhanced knowledge graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="745" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Kg-bert: Bert for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03193</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Mlmlm: Link prediction with mean likelihood masked language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Clouatre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Trempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zouaq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07058</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning attentionbased embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaul</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01195</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Autosf: Searching scoring functions for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Engineering</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Compositionbased multi-relational graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adapting meta knowledge graph information for multi-hop reasoning over few-shot relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3376" to="3381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Relation prediction as an auxiliary training objective for improving multi-relational graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AKBC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lowdimensional hyperbolic knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via graph attenuated attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="5212" to="5224" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pre-training with whole word masking for chinese bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3504" to="3514" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lsbert: Lexical simplification based on bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3064" to="3076" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detecting formal thought disorder by deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sarzynska-Wawer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wawer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pawlak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Szymanowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stefaniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jarkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Okruszek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychiatry Research</title>
		<imprint>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="page">114135</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Revisiting pretrained models for chinese natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Dolores: deep contextualized knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00147</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ernie: Enhanced language representation with informative entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07129</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Comet: Commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05317</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Siamese recurrent architectures for learning sentence similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thyagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">WordNet: An electronic lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Quaternion knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Tucker: Tensor factorization for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bala?evi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5185" to="5194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Modeling heterogeneous hierarchies with relation-specific hyperbolic cones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Dense: An enhanced non-abelian group representation for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04548</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A capsule networkbased embedding model for knowledge graph completion and search personalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2180" to="2189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Duality-induced regularizer for tensor factorization based knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
