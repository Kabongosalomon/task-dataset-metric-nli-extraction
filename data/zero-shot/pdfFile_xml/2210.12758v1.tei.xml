<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beta R-CNN: Looking into Pedestrian Detection from Another Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Xu</surname></persName>
							<email>zixuanxu@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Banghuai</forename><surname>Li</surname></persName>
							<email>libanghuai@megvii.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
							<email>yuanye@megvii.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megvii</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhong</forename><surname>Dang</surname></persName>
							<email>ahdang@pku.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Megvii Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beta R-CNN: Looking into Pedestrian Detection from Another Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently significant progress has been made in pedestrian detection, but it remains challenging to achieve high performance in occluded and crowded scenes. It could be attributed mostly to the widely used representation of pedestrians, i.e., 2D axis-aligned bounding box, which just describes the approximate location and size of the object. Bounding box models the object as a uniform distribution within the boundary, making pedestrians indistinguishable in occluded and crowded scenes due to much noise. To eliminate the problem, we propose a novel representation based on 2D beta distribution, named Beta Representation. It pictures a pedestrian by explicitly constructing the relationship between full-body and visible boxes, and emphasizes the center of visual mass by assigning different probability values to pixels. As a result, Beta Representation is much better for distinguishing highlyoverlapped instances in crowded scenes with a new NMS strategy named BetaNMS. What's more, to fully exploit Beta Representation, a novel pipeline Beta R-CNN equipped with BetaHead and BetaMask is proposed, leading to high detection performance in occluded and crowded scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pedestrian detection is a critical research topic in computer vision field with various real-world applications such as autonomous vehicles, intelligent video surveillance, robotics, and so on. During the last decade, with the rise of deep convolutional neural networks (CNNs), great progress has been achieved in pedestrian detection. However, it remains challenging to accurately distinguish pedestrians in occluded and crowded scenes.</p><p>Although extensive methods have been attempted for occlusion and crowd issues, the performance is still limited by pedestrian representation, i.e., 2D bounding box representation. The axis-aligned minimum bounding box is widely utilized to explicitly define a distinct object, with its approximate location and size. Although box representation has advantages such as parameterization-and annotation-friendly as the identity of an object, some nonnegligible drawbacks are limiting the performance of pedestrian detection especially in occluded and crowded scenes. Firstly, the bounding box can be regarded as modeling the object as a uniform distribution in the box, but it actually goes against our intuitive perception. Given an occluded pedestrian, what attracts our attention should be the visible part rather than the occluded noise. Secondly, based on box representation, intersection over union (IoU) serves as the metric to measure the difference between objects, which results in difficulty to distinguish highly-overlapped instances in crowded scenes. As shown in <ref type="figure">Fig. 2</ref>, even if the detectors succeed to identify different human instances in a crowded scene, the highly-overlapped detections may also be suppressed by the post-processing of non-maximum suppression (NMS). Last, the full-body and visible boxes treat a distinct person as two separate parts, which omit their inner relationship as a whole and lead to difficulty for model optimization.</p><p>To eliminate the weaknesses of box representation and preserve its advantages in the meanwhile, we propose a novel representation for pedestrians based on 2D beta distribution, named Beta Representation. In probability theory, the beta distribution is a family of continuous probability distribution defined in the interval [0, 1], as depicted in <ref type="figure">Fig. 1</ref>. By assigning different values to ?, ?, we could control the shape of the beta distribution, especially the peak and the full width at half maximum (FWHM), which is naturally suitable for pedestrian representation with unpredictable visible patterns. We take each pedestrian as a 2D beta distribution on the image and generate eight new parameters as the Beta Representation. As illustrated in <ref type="figure">Fig. 2</ref>, the boundary of 2D beta distribution is consistent with the full-body box, while the peak along with FWHM depends on the relation between the visible part and full-body box. Compared with paired boxes, i.e., full-body and visible boxes, 2D beta distribution treats each pedestrian more like an integrated whole and emphasizes the object center of visual mass meanwhile.</p><p>Besides, instead of IoU, Kullback-Leibler (KL) divergence is adopted as a new metric to measure the distance of two objects and the beta-distribution-based NMS strategy is named BetaNMS. <ref type="figure">Fig. 2</ref> illustrates that while the bounding boxes are too close to distinguish (fIoU &gt; 0.5, vIoU &gt; 0.3 2 ), the 2D beta distributions still maintain high discrimination (KL &gt; 7) between each other, thereby leading to better performance in distinguishing highly-overlapped instances.</p><p>Moreover, to fully exploit Beta Representation in pedestrian detection, we design a novel pedestrian detector named Beta R-CNN, equipped with two different key modules, i.e., BetaHead and BetaMask. BetaHead is utilized to regress the eight beta parameters and the class score, while BetaMask serves as an attention mechanism to modulate the extracted feature with beta-distribution-based masks. Experiments on the extremely crowded benchmark CrowdHuman <ref type="bibr" target="#b0">[1]</ref> and CityPersons <ref type="bibr" target="#b1">[2]</ref> show that our proposed approach can outperform the state-of-the-art results, which strongly validate the superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Pedestrian Detection. Pedestrian detection can be viewed as object detection for the specific category. With the development of deep learning, CNN-based detectors can be roughly divided into two categories: the two-stage approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> comprise separate proposal generation followed by classification and regression module to refine the proposals; and the one-stage approaches <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> perform localization and classification simultaneously on the feature maps without the separate proposal generation module. Most existing pedestrian detection methods employ either the singlestage or two-stage strategy as their model architectures.</p><p>Occlusion Handling. In pedestrian detection, occlusion leads to misclassifying pedestrians. A common strategy is the part-based approaches <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, which ensemble a series of body-part detectors to localize partially occluded pedestrians. Also some methods train different models for most frequent occlusion patterns <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> or model different occlusion patterns in a joint framework <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, but they are all just designed for some specific occlusion patterns and not able to generalize well in other occluded scenes. Besides, attention mechanism has been applied to handle different occlusion patterns <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>. MGAN <ref type="bibr" target="#b15">[16]</ref> introduces a novel mask guided attention network, which emphasizes visible pedestrian regions while suppressing the occluded parts by modulating extracted features. Moreover, a few recent works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> have exploited to utilize annotations of the visible box as extra supervisions to improve pedestrian detection performance.</p><p>Crowdness Handling. As for crowded scenes, except for the misclassifying issues, crowdedness makes it difficult to distinguish highly-overlapped pedestrians. A few previous works propose new loss functions to address the problem of crowded detections. For example, OR-CNN <ref type="bibr" target="#b7">[8]</ref> proposes aggregation loss to enforce proposals to be close to the corresponding objects and minimize the internal region distances of proposals associated with the same objects. RepLoss <ref type="bibr" target="#b18">[19]</ref> proposes Repulsion Loss, which introduces extra penalty to proposals intertwined with multiple ground truths. Moreover, some advanced NMS strategies <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b17">18]</ref> are proposed to alleviate the crowded issues to some extent, but they still take IoU as the metric to measure the difference between detected objects, which limits the performance on identifying highly-overlapped instances from crowded boxes.</p><p>Object Representation. In computer vision, object representation is one primary topic, and there are many representations for objects in 2D images, such as 2D bounding boxes <ref type="bibr" target="#b3">[4]</ref>, polygons <ref type="bibr" target="#b23">[24]</ref>, splines <ref type="bibr" target="#b24">[25]</ref>, and pixels <ref type="bibr" target="#b25">[26]</ref>. Each has strengths and weaknesses from a specific application's practical perspective, providing annotation cost, information density, and variable levels of fidelity. Distribution-based representation has also been tried in <ref type="bibr" target="#b26">[27]</ref> which utilizes the bivariate normal distribution as the representation of objects. However, when transformed from bounding boxes rather than segmentation, the mean and variance of bivarite normal distribution are still consistent with the center and scale. Besides, its performance is considerably poor compared to other methods.</p><p>In this paper, Beta Representation provides a more detailed representation for occluded pedestrians, along with a new metric to substitute for IoU and a new detector Beta R-CNN, thereby alleviating the occlusion and crowd issues to a great extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first introduce the parameterized Beta Representation for pedestrians. Then to fully exploit the Beta Representation, a novel pipeline Beta R-CNN is proposed. Moreover, a specific NMS strategy based on beta distribution and KL divergence, i.e., BetaNMS, is analyzed in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Beta Representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Beta Distribution</head><p>In probability theory and mathematical statistics, the beta distribution is a family of one-dimensional continuous probability distribution defined in the interval [0, 1], parameterized by two positive shape parameters ? and ?. For 0 ? x ? 1 and shape parameters ?, ? &gt; 0, the probability density function (PDF) of beta distribution is a exponential function of the variable x and its reflection (1 ? x) as follows:</p><formula xml:id="formula_0">Be(x; ?, ?) = ?(? + ?) ?(?)?(?) ? x (??1) (1 ? x) (??1) = 1 B(?, ?) ? x (??1) (1 ? x) (??1) ,<label>(1)</label></formula><p>where ?(z) is the gamma function and B(?, ?) is a normalization factor to ensure the total probability is 1. Some beta distribution samples are shown in <ref type="figure">Fig. 1</ref>. According to the above definition, the mean ?, variance ? 2 and shape parameter ? can be formulated as follows:</p><formula xml:id="formula_1">? = E(x) = ? ? + ? , ? 2 = E(x ? ?) 2 = ?? (? + ?) 2 (? + ? + 1) , ? = ? + ?.</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Beta Representation for Pedestrian</head><p>As introduced in Sec. 3.1.1, the beta distribution has two key characteristics: 1) Boundedness, the beta distribution is defined in the interval [0, 1]; 2) Asymmetry, the peak and FWHM can be controlled by parameters ? and ?. These two characteristics make beta distribution suitable to describe the location, shape and visible pattern of occluded pedestrians. Parameterized Beta Representation is generated from the two annotated boxes, i.e., full-body and visible boxes. Considering bounding box is a 2D representation and it is always axis-aligned, we utilize two independent beta distributions on the x-axis and y-axis respectively.</p><p>As mentioned before, we take the full-body box as the boundary of 2D beta distribution, while the peak along with FWHM depends on the relation between the visible part and full-body box. However, the transition relation between the peak, FWHM and the parameters ?, ? is hard to formulate. Instead, we calculate the mean and variance of the beta distribution with different weights assigned to the visible part and non-visible part, formulated as follows:</p><formula xml:id="formula_2">? x = r f l f xf (x)dx r f l f f (x)dx , ? x 2 = r f l f (x ? ? x ) 2 f (x)dx r f l f f (x)dx , ? y = b f t f yf (y)dy b f t f f (y)dy , ? y 2 = b f t f (y ? ? y ) 2 f (y)dy b f t f f (y)dy ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">[l f , t f , r f , b f ], [l v , t v , r v , b v ]</formula><p>denote the full-body box and visible box respectively, and f (x) is defined as the weight of each pixel based on the visibility:</p><formula xml:id="formula_4">f (x) = W v , l v ? x ? r v W f , others , f (y) = W v , t v ? y ? b v W f , others ,<label>(4)</label></formula><p>where W f = 0.04, W v = 1 in our experiments and the size of visible box can be approximated as <ref type="bibr" target="#b11">12)</ref>. Finally, we can calculate the parameters ?, ? according to the normalized mean and variance, while ? (set to ?/4) is a constant to keep ?, ? &gt; 1: </p><formula xml:id="formula_5">w v = ?? x , h v = ?? y (? =<label>?</label></formula><formula xml:id="formula_6">? x = ? x ? l r ? l , ? y = ? y ? t b ? t , ? x = ? ? ? x r ? l , ? y = ? ? ? y b ? t , ? x = ? x + ? x = ? x (1 + ? x ) ? 2 x ? 1, ? y = ? y + ? y = ? y (1 + ? y ) ? 2 y ? 1, ? x = ? x ? x = ? x ( ? x (1 + ? x ) ? 2 x ? 1), ? y = ? y ? y = ? y ( ? y (1 + ? y ) ? 2 y ? 1), ? x = (1 ? ? x )? x , ? y = (1 ? ? y )? y .<label>(5)</label></formula><formula xml:id="formula_7">P (x, y) = C ? Be(x; ? x , ? x ) ? Be(?; ? y , ? y ), l ? x ? r, t ? y ? b, 0, others,<label>(6)</label></formula><formula xml:id="formula_8">wherex = (x ? l)/(r ? l),? = (y ? t)/(b ? t)</formula><p>, and C is a normalization factor to keep the sum of PDF to 1. For pixels inside the beta boundary, the probability values are consistent with the product of two one-dimensional beta distribution, otherwise the probability values are set to zeros.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Advantages</head><p>Our proposed Beta Representation shows several impressive advantages. Firstly, it is more precise in terms of the shape and visibility of pedestrians compared with box representation. While the bounding box models the object as a uniform distribution inside the box, 2D beta distribution concentrates more on the center of visual mass. Secondly, compared with the paired boxes, i.e., full-body box along with visible box, 2D beta distribution treats the pedestrian more like an integrated whole rather than two individual parts. Last, it can handle a few problematic situations such as identifying highly-occluded and highly-overlapped objects, which will be discussed in detail. Moreover, it is worth mentioning that pixel-wise annotations in segmentation can also be transformed to the parameterized Beta Representation based on the above equations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Beta R-CNN</head><p>To better implement the Beta Representation, we introduce a new detector named Beta R-CNN inspired by Faster R-CNN <ref type="bibr" target="#b3">[4]</ref> and Cascade R-CNN <ref type="bibr" target="#b27">[28]</ref>. The architecture is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. BetaHead and BetaMask are two core modules in Beta R-CNN. In the following section, we will discuss them respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">BetaHead</head><p>Since we adopt Beta Representation to describe a pedestrian, BetaHead is designed to regress the eight beta parameters, i.e., [l, t, r, b, ? x , ? x , ? y , ? y ], which is analogous to the regression head in vanilla Faster R-CNN. Specifically, as ?, ? are too abstractive to learn, we adopt the mean and variance as regression targets, i.e., [l, t, r, b, ? x , ? y , ? x , ? y ]. The four boundary parameters, i.e., [l, t, r, b], utilize the same normalization strategy introduced in <ref type="bibr" target="#b3">[4]</ref>. And for the other four shape parameters, i.e., [? x , ? y , ? x , ? y ], we adopt the normalization as follows:</p><formula xml:id="formula_9">t ?x = (? x ? x a )/w a , t ?y = (? y ? y a )/h a , t ?x = log(? x /w a ), t ?y = log(? y /h a ), t * ?x = (? * x ? x a )/w a , t * ?y = (? * y ? y a )/h a , t * ?x = log(? * x /w a ), t * ?y = log(? * y /h a ),<label>(7)</label></formula><p>where x, y, w, h denote the center coordinates and size of the boundary; ? x , ? x , ? y , ? y denote the mean and variance of the object; ? and ? * stand for the predicted and ground-truth beta respectively, while subscript a denotes the anchor box. SmoothL1 loss is adopted to optimize the BetaHead. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">BetaMask</head><p>BetaMask is another novel module introduced in Beta R-CNN. Most pedestrian detectors treat the whole extracted features of a person equally important, which will result in poor performance for high-occluded scenes due to the obvious noise. As we introduced in Sec. 3.1, Beta Representation itself has different focuses to picture a person, which emphasizes the visible part in occluded scenes. It is very intuitive to adopt attention mechanism with 2D beta distribution to highlight the features of visible parts and suppress other noise simultaneously, which could induce the network to pay more attention to the discriminative features and achieve better localization accuracy and higher confidence.</p><p>Different from the common attention mechanism, our proposed BetaMask is based on 2D beta distribution, which is more targeted. In this paper, we directly generate the mask based on prediction results of the previous BetaHead instead of a CNN module like <ref type="bibr" target="#b15">[16]</ref>, as the beta mask is more like a parameterized probability distribution and it is difficult to keep the consistency of the distribution with convolutional kernels. Referring to equation <ref type="formula" target="#formula_6">(5)</ref>, we get [? x , ? x , ? y , ? y ] from the predicted [l, t, r, b, ? x , ? y , ? x , ? y ], and the mask values are sampled from the 2D beta distribution Be(x, y; ? x , ? x , ? y , ? y ) = C ? Be(x; ? x , ? x ) ? Be(y; ? y , ? y ). Then we utilize the element-wise product to modulate the pooled feature with sampled beta masks. Finally, we use KL divergence as the loss function to supervise the BetaMask module:</p><formula xml:id="formula_10">L mask = ?Be * (x, y)(logBe * (x, y) ? logBe(x, y)),<label>(8)</label></formula><p>where Be * (x, y) refers to the distribution generated from the ground truth, while Be(x, y) is generated from the predicted beta parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">BetaNMS</head><p>When it comes to NMS, instead of taking IoU as the metric to measure the difference between detected objects, we follow <ref type="bibr" target="#b26">[27]</ref> to utilize KL divergence as an alternative, but based on 2D beta distribution rather than bivariate normal distribution in <ref type="bibr" target="#b26">[27]</ref>. KL divergence is defined as follows:</p><formula xml:id="formula_11">D KL (p||q) =</formula><p>x,y p(x, y)(log(p(x, y)) ? log(q(x, y))),</p><p>where p and q refer to two parameterized distributions. In practice, to keep the symmetry of the distance metric, we adopt the symmetrified KL divergenceD KL (p||q) as:</p><formula xml:id="formula_13">D KL (p||q) = (D KL (p||q) + D KL (q||p))/2.<label>(10)</label></formula><p>Figure. 4 shows significant differences between symmetrized KL divergence metric and IoU metric on the CrowdHuman validation set. Each dot stands for a pair of two overlapped (fIoU &gt; 0) pedestrians in the same scene, while there are 206088 dots in each graph. When we adopt KL divergence and IoU to perform non-maximum suppression between the above paired boxes respectively, we find only 2844 failed cases based on KL divergence while there are more than 10000 failed cases based on IoU neither fIoU nor vIoU. The comparisons actively demonstrate the superiority of our proposed Beta Representation and the BetaNMS strategy. More details will be shown in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>CityPersons Dataset. The CityPersons dataset <ref type="bibr" target="#b1">[2]</ref> is a subset of Cityscapes which only consists of person annotations. There are 2975 images for training, 500 and 1575 images for validation and testing. The average number of pedestrians in an image is 7. We evaluate our proposed method under the full-body setting, following the evaluation protocol in <ref type="bibr" target="#b1">[2]</ref>, and the partition of validation set follows the standard setting in <ref type="bibr" target="#b18">[19]</ref>  CrowdHuman Dataset. The CrowdHuman dataset <ref type="bibr" target="#b0">[1]</ref>, has been recently released to specifically target the crowd issue in the human detection task. There are 15000, 4370, and 5000 images in the training, validation, and testing set respectively. The average number of persons in an image is 22.6, which is much more crowded than other pedestrian datasets. All the experiments are trained on the CrowdHuman training set and evaluated on the validation set.</p><p>Evaluation Metric. AP (Averaged Precision), which is the most popular metric for detection. AP reflects both the precision and recall ratios of the detection results. Larger AP indicates better performance. M R ?2 , which is short for log-average Miss Rate on False Positive Per Image (FPPI) in <ref type="bibr" target="#b28">[29]</ref>, is commonly used in pedestrian detection. Smaller MR ?2 indicates better performance. MR ?2 emphasizes FP and FN more than AP, which are critical in pedestrian detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>In this paper, we adopt Feature Pyramid Network (FPN) <ref type="bibr" target="#b29">[30]</ref> with ResNet-50 <ref type="bibr" target="#b30">[31]</ref> as the backbone for all the experiments. The two-stage Cascade R-CNN <ref type="bibr" target="#b27">[28]</ref> is taken as our baseline detection framework to perform coarse-to-fine mechanism for more accurate beta prediction. As for anchor settings, we follow the same anchor scales in <ref type="bibr" target="#b29">[30]</ref>, while the aspect ratios are set to H : W = {1 : 1, 2 : 1, 3 : 1}.</p><p>For training, the batch size is 16, split to 8 GPUs. Each training round includes 16000 iterations on CityPersons and 40000 iterations on CrowdHuman. The learning rate is initialized to 0.02 and divided by 10 at half and three-quarter of total iterations respectively. During training, the sampling ratio of positive to negative proposals for RoI branch is 1 : 1 for CrowdHuman and 1 : 4 for CityPersons. On CityPersons, the input size for both training and testing is 1024 ? 2048. On CrowdHuman, the short edge of each image is resized to 800 pixels for both training and testing. It is worth mentioning that the proposed components like BetaHead in Beta R-CNN are all optimization-friendly, thus there is no essential difference between Beta R-CNN and Faster R-CNN <ref type="bibr" target="#b3">[4]</ref> or Cascade R-CNN <ref type="bibr" target="#b27">[28]</ref> for model training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study on CrowdHuman</head><p>Ablation study and main results.  <ref type="table" target="#tab_4">Table 2</ref> and all reported experiments here are based on Beta R-CNN. BetaNMS outperforms all other NMS methods with a large margin. Compared with fIoU-, vIoU-based NMS tends to recall more overlapped instances but bring in more false positives meanwhile, reflecting in the higher MR ?2 and AP. In addition, although we integrate fIoU and vIoU in NMS, we can find BetaNMS still outperforms by at least 0.4% on MR ?2 and 1.5% on AP, which means BetaNMS surely better distinguishes highly overlapped instances than IoU-based NMS, whether it is based on the full-body box or visible box or both. Speed/accuracy trade off. Each proposed module in Beta R-CNN is light-weight with little computation cost. We take CrowdHuman validation set with 800x1400 input size to conduct speed experiments on NVIDIA 2080Ti GPU with 8 GPUs, and the average speeds are 0.483s/image ( Cascade R-CNN baseline) and 0.487s/image (Beta R-CNN) respectively. The difference can be negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">State-of-the-art (SOTA) Comparison on CrowdHuman</head><p>Comparisons with some recent methods on the CrowdHuman validation set are shown in <ref type="table" target="#tab_5">Table 3</ref>. It clearly shows that our Beta R-CNN outperforms others with a large margin, especially on the metric MR ?2 . Such a large gap demonstrates the superiority of our Beta R-CNN. It is worth noting that CrowdDet <ref type="bibr" target="#b31">[32]</ref> achieves a little higher AP than ours, which attributes to its motivation, i.e., laying emphasis on larger recall at the expense of more false positives, reflecting in higher MR ?2 than ours.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiments on CityPersons</head><p>To further verify the generalization ability of our method, we also conduct experiments on CityPersons. <ref type="table" target="#tab_6">Table 4</ref> compares Beta R-CNN with some state-of-the-art methods. For a fair comparison, we only list those methods that follow the standard settings, i.e., adopting subset partition criterion in <ref type="bibr" target="#b18">[19]</ref> and feeding images with original size as inputs when performing evaluation. Because of the space limit, we will report the results with 1.3x enlarged input images in our supplementary materials. From the table, we can see that our Beta R-CNN outperforms all published methods on all four subsets, especially with a large margin on the Heavy subset, which verifies that our method is effective in occluded and crowded scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a statistic representation for occluded pedestrians based on 2D beta distributions, which takes the paired boxes as an integrated whole and emphasize the object center of visual mass. Besides, Beta R-CNN, equipped with BetaHead and BetaMask, aims to alleviate the pedestrian detection in occluded and crowded scenes. BetaNMS could effectively distinguish highly-overlapped instances based on Beta Representation and KL divergence. The quantitative and qualitative experiments powerfully demonstrate the superiority of our methods. Beta Representation, as well as BetaHead, BetaMask, BetaNMS are all flexible enough to be integrated into other two-stage or single-shot detectors and are also compatible with existing optimization methods to further boost their performance. Moreover, our method could be extended to more general scenes and other detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Our contributions focus on the novel representation and pipeline for pedestrian detection, which can be extended to other computer vision tasks. Also, it may provide new ideas for follow-up research. It therefore has the potential to advance both the beneficial and harmful applications of object detectors, such as autonomous vehicles, intelligent video surveillance, robotics and so on. As for ethical aspects and future societal consequences, this technology can bring harmful or beneficial effects to the society, which depends on the citizens who have evil or pure motivation and who can make good use of this technological progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparisons on CityPersons</head><p>As some SOTA methods also report results with 1.3? enlarged input size on CityPersons, here we evaluate our Beta R-CNN with both original and enlarged input size. The results are shown in <ref type="table">Table  1</ref>. Due to the space limit in the paper, some baseline methods are also reported in the table for a thorough comparison. By the way, we have to emphasize again that we only list the methods which follow the standard settings in RepLoss <ref type="bibr" target="#b18">[19]</ref> for a fair comparison. From the table, we can see that whether with 1.3? enlarged or original input size, our method both outperforms other state-of-the-art methods with a large gap, especially on Heavy subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Visualization</head><p>To show the performance of our proposed Beta R-CNN more intuitively, we visualize the results of several images with high-occluded scenes from the CrowdHuman, which are shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. We can find that although people in these images overlap with each other very seriously, Beta R-CNN  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Beta distributions have flexible shapes with different peaks and FWHMs. Beta Representation samples and comparisons between IoU and KL divergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Beta R-CNN equipped with BetaHead and BetaMask. BetaHead regresses the class label and eight new parameters of Beta Representation, while BetaMask modulates the pooled features with beta-distribution-based masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Comparisons between symmetrized KL divergence and IoU on CrowdHuman validation set. Each dot is a pair of two overlapped pedestrians in the same scene, measured by symmetrized KL divergence between their Beta Representation and IoU-based on full-body/visible box. The number of total dots is 206088. (The thresholds are tested in experiments.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualization results of Beta R-CNN on CrowdHuman.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Generally speaking, for each pedestrian, Beta Representation is parameterized by eight parameters, i.e., [l, t, r, b, ? x , ? x , ? y , ? y ], where[l, t, r, b] are the boundaries indicating the location on the image, and [? x , ? x , ? y , ? y ] are the shape parameters of the 2D beta distribution describing the visibility of pedestrians. The probability density function of the 2D beta distribution over the whole image is formulated as follows:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>B Box Be Beta BH BetaHead BM BetaMask</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">RoI</cell><cell>Mask</cell><cell>RoI</cell></row><row><cell></cell><cell></cell><cell cols="2">Pooling</cell><cell>Pooling</cell></row><row><cell cols="2">Backbone</cell><cell></cell><cell></cell></row><row><cell></cell><cell>C1</cell><cell>Be1</cell><cell>C2</cell><cell>Be2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">BH2</cell></row><row><cell>C0</cell><cell>B0</cell><cell>BH1</cell><cell></cell></row><row><cell></cell><cell></cell><cell>BM</cell><cell></cell></row><row><cell>H0</cell><cell></cell><cell>Pool</cell><cell>Pool</cell></row><row><cell cols="2">Backbone</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 Table 1 :</head><label>11</label><figDesc>shows the ablation experiments of the proposed Beta R-CNN in Sec. 3, including BetaHead, BetaMask, Mask Loss, and BetaNMS. The baseline is a two-stage Cascade R-CNN with default settings introduced in Sec. 4.2. As we claimed in Sec. 3, it is clear that our method consistently improves the performance in all criteria. BetaHead and BetaMask are proposed to implement Beta Representation and alleviate the occluded issue with new regression targets and attention mechanism, which surely reduce the MR ?2 from 43.8% to 41.3% and improve AP from 85.2% to 87.1%. And the Mask Loss, i.e., equation 8, helps model get a more accurate mask. Moreover, the improvement of BetaNMS well demonstrates the superiority over the IoU-based NMS. We further analyze the role of each module. Beta Representation could picture more details of the shape and visibility of pedestrians especially in occluded and crowded scenes, and BetaMask adopts attention mechanism by utilizing 2D beta distribution to modulate more discriminative features, which enhances Beta R-CNN further. At last, BetaNMS eliminates the inherent drawback of IoU-based NMS when it meets highly-overlapped instances under crowded scenes. More details can be found in Sec.<ref type="bibr" target="#b2">3</ref>. Ablation Study on CrowdHumanBetaHead BetaMask MaskLoss BetaNMS MR ?2 /% AP/%</figDesc><table><row><cell>43.8</cell><cell>85.2</cell></row><row><cell>43.5</cell><cell>85.5</cell></row><row><cell>41.3</cell><cell>87.1</cell></row><row><cell>41.1</cell><cell>87.2</cell></row><row><cell>40.3</cell><cell>88.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of various NMS strategies</figDesc><table><row><cell>Strategy</cell><cell cols="3">Threshold MR ?2 /% AP/%</cell></row><row><cell>fIoU NMS</cell><cell>0.5</cell><cell>41.1</cell><cell>87.2</cell></row><row><cell>vIoU NMS</cell><cell>0.35</cell><cell>42.0</cell><cell>88.1</cell></row><row><cell>fIoU + vIoU NMS</cell><cell>0.5/0.35</cell><cell>41.0</cell><cell>88.1</cell></row><row><cell>SoftNMS</cell><cell>-</cell><cell>41.1</cell><cell>88.0</cell></row><row><cell>BetaNMS</cell><cell>6</cell><cell>40.6</cell><cell>89.6</cell></row><row><cell>BetaNMS</cell><cell>7</cell><cell>40.3</cell><cell>88.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>SOTA comparisons on CrowdHuman</figDesc><table><row><cell>Methods</cell><cell>MR ?2 /%</cell><cell>AP/%</cell></row><row><cell>CrowdHuman [1]</cell><cell>50.4</cell><cell>85.0</cell></row><row><cell>Adaptive NMS [22]</cell><cell>49.7</cell><cell>84.7</cell></row><row><cell>PBM [18]</cell><cell>43.3</cell><cell>89.3</cell></row><row><cell>CrowdDet [32]</cell><cell>41.4</cell><cell>90.7</cell></row><row><cell>Beta R-CNN(KL th = 7)</cell><cell>40.3</cell><cell>88.2</cell></row><row><cell>Beta R-CNN(KL th = 6 )</cell><cell>40.6</cell><cell>89.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>SOTA comparisons on CityPersons</figDesc><table><row><cell>Methods</cell><cell>Reasonable</cell><cell>Heavy</cell><cell>Partial</cell><cell>Bare</cell></row><row><cell>OR-CNN [8]</cell><cell>12.8</cell><cell>55.7</cell><cell>15.3</cell><cell>6.7</cell></row><row><cell>TLL [33]</cell><cell>15.5</cell><cell>53.6</cell><cell>17.2</cell><cell>10.0</cell></row><row><cell>RepLoss [19]</cell><cell>13.2</cell><cell>56.9</cell><cell>16.8</cell><cell>7.6</cell></row><row><cell>ALFNet [34]</cell><cell>12.0</cell><cell>51.9</cell><cell>11.4</cell><cell>8.4</cell></row><row><cell>CSP [35]</cell><cell>11.0</cell><cell>49.3</cell><cell>10.4</cell><cell>7.3</cell></row><row><cell>Beta R-CNN</cell><cell>10.6</cell><cell>47.1</cell><cell>10.3</cell><cell>6.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparisons on CityPersonsMethodsSize Reasonable Heavy Partial Bare distinguishes them very well, benefiting from Beta Representation and BetaNMS. Besides, we randomly draw the corresponding Beta Representation over the last image for better understanding.</figDesc><table><row><cell>Adapted Faster RCNN [2]</cell><cell>?1</cell><cell>15.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OR-CNN [8]</cell><cell>?1</cell><cell>12.8</cell><cell>55.7</cell><cell>15.3</cell><cell>6.7</cell></row><row><cell>RepLoss [19]</cell><cell>?1</cell><cell>13.2</cell><cell>56.9</cell><cell>16.8</cell><cell>7.6</cell></row><row><cell>Adaptive NMS [22]</cell><cell>?1</cell><cell>12.0</cell><cell>51.2</cell><cell>11.9</cell><cell>6.8</cell></row><row><cell>TLL [33]</cell><cell>?1</cell><cell>15.5</cell><cell>53.6</cell><cell>17.2</cell><cell>10.0</cell></row><row><cell>TLL + MRF [33]</cell><cell>?1</cell><cell>14.4</cell><cell>52.0</cell><cell>15.9</cell><cell>9.2</cell></row><row><cell>ALFNet [34]</cell><cell>?1</cell><cell>12.0</cell><cell>51.9</cell><cell>11.4</cell><cell>7.3</cell></row><row><cell>CSP [35]</cell><cell>?1</cell><cell>11.0</cell><cell>49.3</cell><cell>10.4</cell><cell>7.3</cell></row><row><cell>Beta R-CNN</cell><cell>?1</cell><cell>10.6</cell><cell>47.1</cell><cell>10.3</cell><cell>6.4</cell></row><row><cell cols="2">Adapted Faster RCNN [2] ?1.3</cell><cell>12.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OR-CNN [8]</cell><cell>?1.3</cell><cell>11.0</cell><cell>51.3</cell><cell>13.7</cell><cell>5.9</cell></row><row><cell>RepLoss [19]</cell><cell>?1.3</cell><cell>11.6</cell><cell>55.3</cell><cell>14.8</cell><cell>7.0</cell></row><row><cell>Adaptive NMS [22]</cell><cell>?1.3</cell><cell>10.8</cell><cell>54.0</cell><cell>11.4</cell><cell>6.2</cell></row><row><cell>CrowdDet [32]</cell><cell>?1.3</cell><cell>10.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Beta R-CNN</cell><cell>?1.3</cell><cell>9.9</cell><cell>45.8</cell><cell>9.1</cell><cell>6.0</cell></row><row><cell>still detects and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">FIoU and vIoU are the IoU calculated based on full-body/visible boxes respectively.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">CrowdHuman: A Benchmark for Detecting Human in a Crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<idno>arXiv: 1805.00123</idno>
		<ptr target="http://arxiv.org/abs/1805.00123" />
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">CityPersons: A Diverse Dataset for Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05693</idno>
		<idno>arXiv: 1702.05693</idno>
		<ptr target="http://arxiv.org/abs/1702.05693" />
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick ; R-Cnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08083</idno>
		<idno>arXiv: 1504.08083</idno>
		<ptr target="http://arxiv.org/abs/1504.08083" />
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SSD: Single Shot MultiBox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<idno>arXiv: 1708.02002</idno>
		<ptr target="http://arxiv.org/abs/1708.02002" />
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Occlusion-Aware R-CNN: Detecting Pedestrians in a Crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/978-3-030-01219-9_39</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-030-01219-9_39" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11207</biblScope>
			<biblScope unit="page" from="657" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pedestrian detection via body part semantic and contextual information with dnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3148" to="3159" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">PedHunter: Occlusion Robust Pedestrian Detector in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06826</idno>
		<idno>arXiv: 1909.06826</idno>
		<ptr target="http://arxiv.org/abs/1909.06826" />
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Learning Strong Parts for Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/document/7410578/" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1904" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Handling occlusions with franken-classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1505" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1904" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-label Learning of Part Detectors for Heavily Occluded Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/document/8237639/" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Venice</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="3506" to="3515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2056" to="2063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mask-Guided Attention Network for Occluded Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bi-box Regression for Pedestrian Detection and Occlusion Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">http:/link.springer.com/10.1007/978-3-030-01246-5_9</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-030-01246-5_9" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11205</biblScope>
			<biblScope unit="page" from="138" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">NMS by Representative Region: Towards Crowded Pedestrian Detection by Proposal Pairing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yoshie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12729</idno>
		<idno>arXiv: 2003.12729</idno>
		<ptr target="http://arxiv.org/abs/2003.12729" />
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Repulsion Loss: Detecting Pedestrians in a Crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/8578909/" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="7774" to="7783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A Convnet for Non-maximum Suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>Pattern Recognition, B. Rosenhahn and B. Andres</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="192" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Soft-nms -improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adaptive NMS: Refining Pedestrian Detection in a Crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bounding Box Regression With Uncertainty for Accurate Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/8953889/" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="2883" to="2892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Annotating object instances with a polygon-rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrej?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4485" to="4493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Statistical interspace models (sims): Application to robust 3d spine segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Castro-Mateos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Pozo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perea?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lekadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1663" to="1675" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2978" to="2991" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Object as Distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fridman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12929</idno>
		<idno>arXiv: 1907.12929</idno>
		<ptr target="http://arxiv.org/abs/1907.12929" />
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03144</idno>
		<idno>arXiv: 1612.03144</idno>
		<ptr target="http://arxiv.org/abs/1612.03144" />
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/document/7780459/" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detection in crowded scenes: One proposal, multiple predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Small-Scale Pedestrian Detection Based on Topological Line Localization and Temporal Feature Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>M. Hebert, C. Sminchisescu, and Y. Weiss</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="554" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning Efficient Single-Stage Pedestrian Detectors by Asymptotic Localization Fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>M. Hebert, C. Sminchisescu, and Y. Weiss</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="643" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02948</idno>
		<idno>arXiv: 1904.02948</idno>
		<ptr target="http://arxiv.org/abs/1904.02948" />
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
