<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Yew</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">DAMO Academy, Alibaba Group DeCLaRe Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chia</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">DAMO Academy, Alibaba Group DeCLaRe Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
							<email>l.bing@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">DAMO Academy, Alibaba Group DeCLaRe Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
							<email>sporia@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">DAMO Academy, Alibaba Group DeCLaRe Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Declare</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">DAMO Academy, Alibaba Group DeCLaRe Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">DAMO Academy, Alibaba Group DeCLaRe Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of Zero-Shot Relation Triplet Extraction (Ze-roRTE) to encourage further research in lowresource relation extraction methods. Given an input sentence, each extracted triplet consists of the head entity, relation label, and tail entity where the relation label is not seen at the training stage. To solve ZeroRTE, we propose to synthesize relation examples by prompting language models to generate structured texts. Concretely, we unify language model prompts and structured text approaches to design a structured prompt template for generating synthetic relation samples when conditioning on relation label prompts (RelationPrompt). To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot relation classification. Our code and data are available at github.com/declare-lab/RelationPrompt.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction aims to predict relationships between entities in unstructured text, which has applications such as knowledge graph construction <ref type="bibr" target="#b15">(Lin et al., 2015)</ref> and question answering <ref type="bibr" target="#b36">(Xu et al., 2016)</ref>. However, existing approaches often require large datasets of annotated samples which are costly to annotate and have a fixed set of relations. Currently, less research is focused on the zero-shot setting <ref type="bibr" target="#b32">(Wang et al., 2019)</ref> where models need to generalize to unseen relation sets without available annotated samples <ref type="bibr" target="#b32">(Wang et al., 2019)</ref>. * * Yew Ken is a student under the Joint PhD Program between Alibaba and SUTD. ? Corresponding author.  Although there are existing zero-shot relation task settings, they do not require extracting the full relation triplets. The task setting of Zero-Shot Relation Classification 1 (ZeroRC) was previously introduced by <ref type="bibr" target="#b2">Chen and Li (2021)</ref> to classify the relation between a given head and tail entity pair for unseen labels. However, it is not always practical or realistic to assume that the ground-truth entities are readily available. Zero-Shot Relation Slot-Filling <ref type="bibr" target="#b12">(Levy et al., 2017)</ref> aims to predict the tail entity based on the provided head entity and relation, but also relies on other methods for entity detection. Thus, it also faces the challenge of error propagation in practice <ref type="bibr" target="#b41">(Zhong and Chen, 2021)</ref>. Hence, we propose a new and challenging task setting called Zero-Shot Relation Triplet Extraction (ZeroRTE). The goal of ZeroRTE is to extract triplets of the form (head entity, tail entity, relation label) from each sentence despite not having any annotated training samples that contain the test relation labels. For a clear comparison between task settings, we provide a summary in <ref type="table" target="#tab_1">Table 1</ref>. To our knowledge, this is the first work to extend the task of Relation Triplet Extraction to the zero-shot setting. For example in <ref type="figure" target="#fig_1">Figure 1</ref>, the training samples may belong to the seen relation set {Sibling, Man-ufacturer, Architect}, while the test samples may belong to the unseen relation set {Military Rank, Position Played, Record Label}. Given the annotated training samples in <ref type="figure" target="#fig_1">Figure 1a</ref>, ZeroRTE aims to extract triplets such as <ref type="bibr">(Nicolas Tindal, Military Rank, Captain)</ref> in <ref type="figure" target="#fig_1">Figure 1b</ref>.</p><p>To solve the challenges of data scarcity, there are several existing approaches. Although distant supervision <ref type="bibr" target="#b8">(Ji et al., 2017)</ref> can be used to construct a relation corpus with a many relation types, this approach generally results in lower annotation quality than human annotation. Furthermore, distant supervision remains limited to a fixed set of relation types in the existing knowledge base <ref type="bibr" target="#b30">(Smirnova and Cudr?-Mauroux, 2018)</ref>. Another approach is to formulate the task objective such that the label space is unconstrained. For instance, zero-shot sentence classification can be reframed as entailment <ref type="bibr" target="#b24">(Puri and Catanzaro, 2019)</ref> or embedding similarity <ref type="bibr" target="#b25">(Pushp and Srivastava, 2017)</ref> objectives. However, the existing formulations are designed for sequence classification tasks, which cannot be directly applied to structured prediction tasks such as relation triplet extraction. A third direction is to leverage pre-trained language models using task-specific prompt templates  which enables the models to generalize to new tasks with little to no training samples, such as zero-text classification . This zero-shot potential is possible by leveraging the semantic information in prompts to query the language comprehension capabilities of pre-trained language models <ref type="bibr" target="#b26">(Radford et al., 2019)</ref>.</p><p>Hence, we propose RelationPrompt which reframes the zero-shot problem as synthetic data generation. The core concept is to leverage the semantics of relation labels, prompting language models to generate synthetic training samples which can express the desired relations. The synthetic data can then be used to train another model to perform the zero-shot task. This capability is supported by the finding that language models can be prompted to control task-specific aspects of the generated text, such as domain and content <ref type="bibr" target="#b9">(Keskar et al., 2019)</ref>. For instance, given the relation label "Military Rank" in <ref type="figure" target="#fig_1">Figure 1c</ref>, it is reasonable to condition the language model and compose a sentence demonstrating the relationship that a person has been bestowed with a certain position in the armed forces. Hence, a possible sentence could be "She is the wife of Lieutenant Colonel George Hocham.",</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Sentence</head><p>Sibling She was the mother of Michael and Joel Douglas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manufacturer</head><p>In late 2012 , Samsung announced its NX300 camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architect</head><p>His house was designed by Henry Hob Richardson.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Sentence</head><p>Military Rank Their grandson was Group Captain Nicolas Tindal.</p><p>Position Played Made Chad Brown the highest paid linebacker in NFL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Record Label</head><p>Deadsy signed onto Immortal Records to release "Phantasmagore".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train Relation Generator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train Relation Extractor</head><p>Generate Synthetic Unseen Samples</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-Tune Relation Extractor on Synthetic Samples</head><p>Triplet Extraction on Unseen Sentences  The head and tail entities are shown in blue and orange, respectively. The ZeroRTE train samples (a) and test samples (b) contain triplets that belong to disjoint relation label sets. We formulate ZeroRTE as generating synthetic samples (c) for the unseen test relation labels. The synthetic data can then be used to train another model to extract relation triplets from the test sentences. We also present more data samples in Appendix A.1.</p><p>where the head entity is "George Hocham" and the tail entity is "Lieutenant Colonel". Given generated samples of sufficient quality and diversity, the synthetic dataset can effectively supervise another model to perform ZeroRTE.</p><p>To encode the relation triplet information as text sequences which can be generated by language models, we unify prompt templates with structured text formats <ref type="bibr">(Paolini et al., 2020)</ref>. Structured texts use special markers to encode the structured information which can be easily decoded as triplets. However, it is challenging to generate sentences which contain multiple different relation triplets. Designing a complex structured prompt template to encode multiple triplets may compromise the generation quality as the language model needs to manipulate multiple relations at once. Hence, we focus on generating single-triplet samples and explore how this limitation can be overcome by the downstream relation extractor model. Concretely, we propose a method named Triplet Search Decoding which allows the extraction of multiple triplets at prediction time despite training on synthetic samples which contain a single triplet each.</p><p>Contributions. In summary, our main contributions include: (1) We introduce the ZeroRTE task setting which overcomes limitations in prior task settings by extending the Relation Triplet Extraction task to the zero-shot setting. ZeroRTE is released as a publicly available benchmark based on the reorganized FewRel <ref type="bibr" target="#b5">(Han et al., 2018)</ref> and Wiki-ZSL <ref type="bibr" target="#b2">(Chen and Li, 2021)</ref> datasets. (2) In order to make ZeroRTE solvable in a supervised manner, we propose RelationPrompt to generate synthetic relation examples by prompting language models to generate structured texts. (3) We propose Triplet Search Decoding to overcome the limitation for extracting multiple relation triplets in a sentence. (4) RelationPrompt surpasses prior ZeroRC methods and baselines on ZeroRTE, setting the bar for future work. Our analysis shows that the generated samples are reasonable and diverse, hence serving as effective synthetic training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RelationPrompt: Methodology</head><p>To extract triplets for unseen relation labels in Ze-roRTE, we propose a framework called Relation-Prompt which uses relation labels as prompts to generate synthetic relation examples of target unseen labels. The synthetic data can then be used to supervise any downstream relation extraction model. Hence, our framework requires two models: a Relation Generator for synthetic relation samples, and a Relation Extractor that will be trained on the synthetic data and used to predict triplets for unseen relations. In order to represent the relation triplet information to be processed by language models, we design structured prompt templates. The relation extractor is designed to support both ZeroRTE and ZeroRC tasks. We further propose Triplet Search Decoding to overcome the challenge of generating relation samples with multiple triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Formulation</head><p>In ZeroRTE, the goal is to learn from the seen dataset D s and generalize to the unseen dataset D u . The datasets D s and D u are used for training and testing respectively, and are originally split from the full dataset which is defined as D = (S, T, Y ) where S denotes the input sentences, T denotes the output triplets and Y denotes the set of relation labels present in D. The seen and unseen label sets are predefined and denoted as Y s = {y 1 s , ..., y n s } and Y u = {y 1 u , ..., y m u } respectively, where n = |Y s | and m = |Y u | are the size of seen and unseen label sets respectively. Hence, the label sets of D s and D u are disjoint, i.e., Y s ? Y u = ?. Each data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Context: &lt;Sentence&gt;.  sample contains the input sentence s ? S which corresponds to a list t ? T which can contain one or more output triplets. A relation triplet is defined as (e head , e tail , y) which denotes the head entity, tail entity and relation label respectively. To solve ZeroRTE, we formulate the following algorithm:</p><p>Algorithm 1 RelationPrompt: Prompting language models to generate synthetic data for ZeroRTE. Define:</p><formula xml:id="formula_0">Dataset D = (Sentences S, Triplets T, Labels Y ) Require: Train Dataset D s , Test Dataset D u , Re- lation Generator M g , Relation Extractor M e . Ensure: Y s ? Y u = ?. 1: M g,f inetune ? T rain(M g , D s ) 2: M e,f inetune ? T rain(M e , D s ) 3: D synthetic ? Generate(M g,f inetune , Y u ) 4: M e,f inal ? T rain(M e,f inetune , D synthetic ) 5:T u ? P redict(M e,f inal , S u ) 6: return Extracted TripletsT u</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relation Generator</head><p>Language models are implicitly capable of zeroshot generalization based on their general and largescale pre-training <ref type="bibr" target="#b26">(Radford et al., 2019)</ref>. Furthermore, text generation has been shown to be effectively controllable <ref type="bibr" target="#b9">(Keskar et al., 2019)</ref>. Hence, we prompt the language model to generate synthetic samples by conditioning on the target unseen relation labels. As shown in Algorithm 1, relation generator M g is first fine-tuned on samples for the seen dataset D s (line 1) and then prompted by</p><p>x 1</p><p>x 2 x 3 x 4</p><p>x 5 x 6</p><p>x 7 x 8 x 9</p><p>x 10 x 11</p><p>x 12 x 13</p><p>Decoder Relation: Sibling. Context: mother of Michael and Joel. Head Entity: Michael, Tail Entity:</p><p>Sibling. Context: Joel.</p><p>x 2 x 3 x 4</p><p>x 5 x 6</p><p>x 7 x 8 x 9</p><p>x 10 x 11</p><p>x 12</p><p>x 13</p><p>x 14</p><p>x 1 x 2 x 3 x 4</p><p>x 5 x 6</p><p>Encoder Context: Their grandson was Captain Nicolas. Head Entity: Nicolas, Tail Entity:</p><p>Decoder Entity: Nicolas, Tail Entity: Captain,</p><formula xml:id="formula_1">t 1 t 2 t 3 t 4 t 5 t 6 t 2 t 3 t 4 t 5 t 6 t 7</formula><p>Captain, Relation: Military</p><formula xml:id="formula_2">t 7 t 8</formula><p>Relation: Military Rank.</p><formula xml:id="formula_3">t 8 t 9</formula><p>She was</p><p>x 14</p><p>x 15   relation labels Y u to generate the synthetic samples D synthetic (line 3). As shown in <ref type="figure" target="#fig_2">Figure 2a</ref>, the relation generator takes as input a structured prompt in the form of "Relation: y" and outputs a structured output in the form of "Context: s. Head Entity: e head , Tail Entity: e tail .". We employ a causal language model as our relation generator to sample the structured sequence in an autoregressive manner. As shown in 3a, the model M g is trained using the standard language modeling objective of next-word prediction <ref type="bibr" target="#b1">(Bengio et al., 2001)</ref>. Given each sequence x = [x 1 , x 2 , ..., x n ], the goal is to learn the conditional generation probability:</p><formula xml:id="formula_4">p(x) = n i=1 p(x i |x &lt;i )<label>(1)</label></formula><p>To generate diverse output sequences for each input relation prompt, we use sampling with temperature t <ref type="bibr" target="#b6">(Hinton et al., 2015)</ref> over the output logits o and vocabulary size V with temperature tp:</p><formula xml:id="formula_5">p(x i |x &lt;i ) = exp(o i /tp) |V | j=1 exp(o j /tp)<label>(2)</label></formula><p>The output sequences are decoded into relation triplets by splitting on the special terms "Context:", "Head Entity:" and "Tail Entity:". In case of decoding errors where an entity is not found in the generated context, we discard that sample and continue generating until a fixed amount of valid samples is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relation Extractor</head><p>Given the generated samples of unseen relations, we can train a relation extractor model M e to predict the relation triplets in a zero-shot setting. As shown in Algorithm 1, relation extractor M e is first fine-tuned on samples for the seen dataset D s (line 2) and finally tuned on the synthetic samples D synthetic (line 4). Lastly, M e is used to predict and extract relation tripletsT u from the test sentences S u (lines 5 and 6). We adopt a sequence-tosequence learning approach which is flexible and effective for structured prediction tasks <ref type="bibr" target="#b3">(Cui et al., 2021;</ref><ref type="bibr">Paolini et al., 2020)</ref>. As shown in <ref type="figure" target="#fig_2">Figure  2b</ref>, the relation extractor takes as input a structured prompt containing the sentence s in the form of "Context: s". It then generates a structured output sequence containing a single pair of entities e head and e tail satisfying the relation y, in the form of "Head Entity: e head , Tail Entity: e tail , Relation: y". As shown in <ref type="figure" target="#fig_4">Figure 3b</ref>, we use a standard sequenceto-sequence objective <ref type="bibr" target="#b13">(Lewis et al., 2020)</ref> for training and greedy decoding for generation. To predict a single relation triplet in a given sentence s, we can generate the model outputs without any initial decoder input, as seen in <ref type="figure">Figure 4a</ref>. In case of invalid entity or relation, we treat it as null prediction for that sample. On the other hand, prediction for ZeroRC is easily supported by providing the entity information as the initial decoder input. As shown in <ref type="figure">Figure 4b</ref>, the model takes "Context: s, Head Entity: e head , Tail Entity: e tail , Relation:" as decoder input to generate "y" as output. Hence, our method naturally encompasses both ZeroRTE and ZeroRC as this change affects model prediction and not training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Extracting Multiple triplets using Triplet Search Decoding</head><p>We further propose a generation decoding method in order to improve the zero-shot extraction performance on sentences which contain multiple triplets.</p><p>For the RelationPrompt generation of synthetic data, each sample is limited to contain a single relation triplet. Hence, conventional models for triplet extraction most likely cannot perform well with our framework for multi-triplet ZeroRTE as they normally assume that the training samples may contain multiple triplets per sentence. The inference method of multi-turn question answering <ref type="bibr" target="#b14">(Li et al., 2019)</ref> may mitigate this issue, but cannot scale easily to unseen relations as it relies on hand-crafted question templates which are specific to certain relation and entity types. Hence, we propose Triplet Search Decoding which improves multi-triplet ZeroRTE for the relation extractor. Given the relation extractor which takes a sentence as input and generates output sequences in an autoregressive fashion, greedy decoding as in <ref type="figure">Figure 4a</ref> can output a single sequence. However, Triplet Search Decoding as shown in <ref type="figure">Figure 4c</ref> can output multiple sequences that each correspond to a different candidate relation triplet. We then apply a likelihood threshold to filter the final output sequences. The core concept is enumerating multiple output sequences during generation by considering multiple candidates for the head entity, tail entity and relation label respectively. Starting from the special sub-sequence "Head Entity:", it follows from our template in <ref type="figure" target="#fig_4">Figure 3b</ref> that the next generated token should be the first token of the head entity, such as "Nicolas". For the i th possible first token of the head entity, we denote the softmax probability as p(e head,i ). We only consider the probability of the first token as it can mostly determine the following generated tokens of the entity <ref type="bibr" target="#b39">(Zhao et al., 2021)</ref>. Instead of greedily decoding the entire sequence, we branch the generation into  <ref type="figure">Figure 4</ref>: Comparison of generation decoding methods with our proposed Triplet Search Decoding. The head entities, tail entities and relation labels are shown in blue, orange and dark red respectively. Unconditional decoding (a) can be used to predict one relation triplet in each sentence for ZeroRTE. Entity-conditioned decoding (b) can be used to predict only the relation label between the given entity pair for ZeroRC. Our proposed triplet search decoding (c) can be used to predict multiple triplets in each sentence for ZeroRTE.</p><p>b sequences based on the tokens with the top b highest p(e head,i ). Thereafter, the sequence is greedily decoded until the special sub-sequence "Tail Entity:" is generated. The following token will then be the first token of the tail entity, such as "Captain". The j th tail entity first token probability is denoted as p(e tail,j |e head,i ). Hence, the generation is branched such that for each head entity, there will be another b sequences based on the tokens with the top b highest p(e tail,j |e head,i ). Thereafter, the sequence is greedily decoded until the special sub-sequence "Relation:" is generated. The next generated token will be the first token of the relation label, such as "Military" in "Military Rank". The k th relation first token probability is denoted as p(y k |e head,i , e tail,j ). We branch the generation such that for each pair of head entity and tail entity, there will be another b sequences based on the tokens with the top b highest p(y k |e head,i , e tail,j ). For each sequence, the generation proceeds greedily until the end token is reached, and the overall inference probability is aggregated as: p(triplet i,j,k ) = p(e head,i , e tail,j , y k ) = p(y k |e head,i , e tail,j ) ? p(e tail,j |e head,i ) ? p(e head,i )</p><p>We note that the conditional assumption does not directly consider the other context tokens as they consist of the special sub-sequences which are fixed as part of our generation template. The input sentence s is also not included in the formulation as it is the same when considering multiple output triplets for one sample. At this point, there will be b 3 sequences, each corresponding to a different candidate relation triplet. To filter the final output sequences, we use a probability threshold over that is tuned on the validation F 1 metric, with hyperparameter details in Section A.2. Compared to previous generative extraction methods <ref type="bibr">(Paolini et al., 2020;</ref><ref type="bibr" target="#b19">Nayak and Ng, 2020)</ref>, Triplet Search Decoding allows the probability p(triplet i,j,k ) of each output triplet to be directly calculated and hence control the number of output triplets using the threshold. Compared to other decoding methods such as beam search, Triplet Search Decoding leverages the specific relation triplet structure in our structured text templates. Hence, it can ensure that each output sequence corresponds to a different relation triplet. Furthermore, Triplet Search Decoding is more interpretable than existing generative methods for triplet extraction as it can directly provide the prediction probability for each triplet. More importantly for ZeroRTE, this decoding process allows the relation extractor to naturally predict multiple triplets at test time despite training on synthetic samples which have a single triplet each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We use the following two datasets for our experiments. FewRel <ref type="bibr" target="#b5">(Han et al., 2018)</ref> was handannotated for few-shot relation extraction, but we made it suitable for the zero-shot setting after data splitting into disjoint relation label sets for training, validation and testing. Wiki-ZSL <ref type="bibr" target="#b2">(Chen and Li, 2021)</ref> is constructed through distant supervision over Wikipedia articles and the Wikidata knowledge base. The dataset statistics are shown in <ref type="table" target="#tab_6">Table  2</ref>. To partition the data into seen and unseen label sets, we follow the same process as <ref type="bibr" target="#b2">Chen and Li (2021)</ref> to be consistent. For each dataset, a fixed number of labels are randomly selected as unseen labels while the remaining labels are treated as seen labels during training. To study the performance of methods under different settings of unseen label set size m, we use m ? {5, 10, 15} in our experiments. In order to reduce the effect of experimental noise, the label selection process is repeated for  five different random seeds to produce different data folds. For each data fold, the test set consists of the sentences containing unseen labels. Five validation labels from the seen labels are used to select sentences for early stopping and hyperparameter tuning. The remaining sentences are treated as the train set. Hence, the zero-shot setting ensures that train, validation and test sentences belong to disjoint label sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Settings</head><p>For the relation generator, we fine-tune the pretrained GPT-2 <ref type="bibr" target="#b26">(Radford et al., 2019)</ref> which has 124M parameters. For the relation extractor, we fine-tune the pre-trained BART <ref type="bibr" target="#b13">(Lewis et al., 2020)</ref> which has 140M parameters. In both cases, the finetuning is performed on the training set for up to five epochs and early stopping is based on the validation loss. The learning rate is 3e-5 with linear warm up for the first 20% of training steps and batch size is set to 128. During the training process, we use the AdamW optimizer <ref type="bibr" target="#b17">(Loshchilov and Hutter, 2019)</ref>. The relation generator is used to generate synthetic samples based on the validation and test set label names. A fixed amount of sentences will be generated for each relation. The relation extractor is fine-tuned again on the synthetic relation sentences and then used for evaluation on the test set. 2</p><p>To perform evaluation for ZeroRTE, we evaluate the triplet extraction results separately for sentences containing single triplets and multiple triplets. To evaluate multiple triplet extraction, we use the Micro F 1 metric which is standard in structured prediction tasks <ref type="bibr">(Paolini et al., 2020)</ref> and report the precision (P.) and recall (R.). Evaluating single triplet extraction involves only one possible triplet for each sentence, hence the metric used is Accuracy (Acc.). We evaluate on ZeroRC using the Macro F 1 metric to be consistent with <ref type="bibr" target="#b2">Chen and Li (2021)</ref>. <ref type="table" target="#tab_8">Table 3</ref> and 4 report the average results across five data folds as detailed in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unseen Labels Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Triplet</head><p>Multi Triplet  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baseline Methods</head><p>ZeroRTE As ZeroRTE is a new task setting, we provide two baseline methods for comparison with our RelationPrompt method. Firstly, our relation extractor can be made to perform ZeroRTE without fine-tuning on synthetic samples as it is trained to extract triplets on the sentences of the seen relation set. At prediction time, we constrain the generated labels to be selected from the target label names by masking the generated token probabilities. We denote this model as "NoGen" to indicate that it does not use generated synthetic samples for training. Secondly, we use an existing triplet extraction model known as TableSequence <ref type="bibr" target="#b31">(Wang and Lu, 2020)</ref>. As it is normally unable to perform ZeroRTE, we provide supervision using synthetic samples from our relation generator.</p><p>ZeroRC There are three main categories of competing methods for ZeroRC. Firstly, R-BERT (Wu and He, 2019) is a relation classification model but can be adapted to the zero-shot setting by using the sentence representations to perform nearest neighbor search over label embeddings. Next, CIM <ref type="bibr" target="#b29">(Rockt?schel et al., 2016)</ref> is an entailmentbased method which takes the sentence and each possible relation as input to perform binary classification whether the label matches the sentence semantically. Lastly, ZS-BERT <ref type="bibr" target="#b2">(Chen and Li, 2021)</ref> generates sentence representations that are conditioned on the provided entity pair information, and performs nearest neighbor search over embeddings of the candidate relation descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental Results</head><p>Triplet   <ref type="table" target="#tab_8">Table 3</ref>. In both single-triplet and multi-triplet evaluation, our method consistently outperforms the baseline methods in terms of Accuracy and F 1 metrics respectively. Although we do not observe a consistent advantage in precision and recall scores for multi-triplet extraction, the baseline methods cannot achieve a balanced precision-recall ratio, leading to poor overall F 1 results. The results difference between NoGen and RelationPrompt also indicate that using the synthetic samples from the relation generator is critical, as the F 1 score can be improved by more than two times in some cases. This also suggests that the relation generator can produce reasonable-quality synthetic sentences as training data for the downstream relation extractor. We also observe that the choice of relation extractor for ZeroRTE is not trivial, as the third-party TableSequence <ref type="bibr" target="#b31">(Wang and Lu, 2020)</ref> has significantly worse performance when compared to RelationPrompt, especially for multitriplet extraction. Although the TableSequence model is able to perform multi-triplet extraction  by design, it assumes that the training data may contain multi-triplet sentences, whereas our synthetic data is limited to single triplet samples. On the other hand, our proposed relation extractor and decoding method effectively overcomes this challenge by naturally enumerating and ranking multiple triplets at inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FewRel datasets in</head><p>Relation Classification RelationPrompt naturally supports the ZeroRC task without additional training by providing the entity pair information in the prompt. In <ref type="table" target="#tab_10">Table 4</ref>, we observe consistent improvements compared to the prior state-of-the-art method ZS-BERT <ref type="bibr" target="#b2">(Chen and Li, 2021)</ref>. Notably, our method is able to maintain a relatively high classification F 1 performance when the unseen label set size m increases, whereas ZS-BERT shows a sharper drop in performance. The trend suggests that RelationPrompt is able to scale better to larger unseen label sets, which is more important for opendomain applications. This advantage may further indicate that our method can leverage the semantic information of relation labels more effectively through the token-level conditional generation and extraction stages. On the other hand, ZS-BERT relies on sequence-level representations which can only preserve the high-level label semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Study</head><p>We conduct an ablation study to examine the performance of our decoding method and task-specific fine-tuning on the seen relation set for multi-triplet ZeroRTE, and the results are shown in <ref type="table" target="#tab_12">Table 5</ref>. The comparison is conducted on the Wiki-ZSL validation set with 10 unseen labels. The large performance gap shows that Triplet Search Decoding is critical for multi-triplet ZeroRTE, and suggests that the enumeration and ranking of relation triplet candidates are of sufficiently high quality. Secondly, we observe a significant drop in performance when the relation extractor is not fine-tuned on seen relation samples from the train set before the final tuning on generated synthetic samples for unseen labels. This case suggests that the initial fine-tuning on sentences for seen relations is useful for learning the general task of relation triplet extraction. The learned representations can then be further finetuned on the synthetic samples to adapt specifically for the unseen relations to achieve optimal results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effect of Generated Data Size</head><p>We further study how the number of generated synthetic samples effects the multi-triplet ZeroRTE performance. The evaluation is based on Wiki-ZSL validation set with 10 unseen labels, and the results are shown in <ref type="figure" target="#fig_6">Figure 6</ref>. Increasing the amount from 125 to 250 samples per label improves F 1 score. However, further increasing the generated size up to 2000 does not improve the final performance. This indicates that although the synthetic data is beneficial for ZeroRTE, excessive amounts can lead to over-fitting due to noise. We further analyze the generation diversity in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Analysis</head><p>To assess how the relation data generator generalizes to relations in the wild, we present several samples of real and generated samples in <ref type="figure" target="#fig_5">Figure  5</ref>. The relation labels and real sentences were collected from factual articles. Given the relations "Investor", "Defeated By" and "Currency Of", the generator is able to determine the correct semantic meaning of the relations and compose reasonable sentences. In most cases, the generated head and tail entity pairings can match the given relations and have a similar context to the real sentences. However, in the last case for relation "Political Partner", the generated entity pair does not match the relation meaning despite being grounded in a political context. Instead, the generated sentence expresses a relationship that is closer to "Political Party". This suggests that a future area of improvement could be to match the generated head and tail entity more closely to the given relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Zero-Shot Relation Extraction Zero-shot relation extraction was previously framed as a slotfilling task and solved by reading comprehension methods <ref type="bibr" target="#b12">(Levy et al., 2017)</ref>. However, their approach requires manual template design for each relation label, which cannot scale well to new relation types. Another approach to zero-shot relation extraction is the formulation into an entailment task <ref type="bibr" target="#b20">(Obamuyide and Vlachos, 2018)</ref>, which is not con-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Investor</head><p>In October 1999 , Alibaba received a US $25 million investment from Goldman Sachs and SoftBank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Defeated By</head><p>National shuttler Loh Kean Yew recorded a 24-22, 21-14 win over All-England champion Lee Zii Jia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Currency Of</head><p>El Salvador, on the other hand, has accepted Bitcoin (BTC) as a legal currency Political Partner Beijing and the Holy See reached a provisional agreement in 2018 on the appointment of Chinese bishops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train Relation Generator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train Relation Extractor</head><p>Generate Synthetic Unseen Samples</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-Tune Relation Extractor on Synthetic Samples</head><p>Triplet Extraction on Unseen Sentences</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Military Rank</head><p>William Riggins was a Canadian veteran of the Second Canada First Infantry Division.</p><p>Position Played Taita played for the New Zealand national team from 1998 to 2001.</p><p>Record Label "Sorel" was co -produced and toured by the San Francisco -based DJ Khaled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nominated For</head><p>He received praise from his father , actor Bill McKibben, for portraying in the film "The Wounded".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Director</head><p>The movie , titled "Amistad" ( 1964 ) , was created by Mark J. Taylor, and directed by William Saldana.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real Sentence Generated Sentence</head><p>Investor In October 1999, Alibaba received a US $25 million investment from Goldman Sachs and SoftBank.</p><p>In 2001, CSC announced that it is investing in a record 65 plants in South Africa and plans to invest $2 billion in hydropower, hydrological services and agriculture.</p><p>Defeated By National shuttler Loh Kean Yew recorded a 24-22, 21-14 win over All-England champion Lee Zii Jia.</p><p>The team lost three consecutive draws to the New York Knicks, including a 9-2 win against the Lakers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Currency Of</head><p>El Salvador, on the other hand, has accepted Bitcoin (BTC) as a legal currency.</p><p>In 2001, as the euro strengthened, Italy introduced an extension of the Eurobill (Eurobills), the currency of the Eurozone.</p><p>Political Partner Beijing and the Holy See reached a provisional agreement in 2018 on the appointment of Chinese bishops.</p><p>His son, Thomas, was a leading Republican, elected to the Massachusetts State Senate in 1881.  strained to a fixed relation label space. Instead, the entailment approach determines if arbitrary pairs of sentences and relation labels are compatible. However, it is designed for sentence classification and cannot be applied to ZeroRTE.</p><p>Data Augmentation A popular method for improving model performance in supervised lowresource tasks is data augmentation. Simple heuristics such as token manipulation <ref type="bibr" target="#b10">(Kobayashi, 2018)</ref> were initially developed, new methods in language modeling improved the quality of augmented samples <ref type="bibr" target="#b35">(Xie et al., 2020;</ref><ref type="bibr" target="#b33">Wei and Zou, 2019)</ref>. Although there are data augmentation methods that can be applied to structured tasks such as named entity recognition <ref type="bibr" target="#b4">(Ding et al., 2020)</ref> and relation extraction <ref type="bibr" target="#b22">(Papanikolaou and Pierleoni, 2020;</ref><ref type="bibr" target="#b40">Lee et al., 2021)</ref>, they require existing training samples and cannot be easily adapted to zero-shot tasks.</p><p>Knowledge Retrieval RelationPrompt also leverages the knowledge stored in language models <ref type="bibr" target="#b28">(Roberts et al., 2020)</ref> to compose relation samples that are grounded in realistic contexts. To ensure that the generated samples are factually accurate, the language model requires strong knowledge retrieval capabilities <ref type="bibr" target="#b23">(Petroni et al., 2019)</ref>.</p><p>Language Model Prompts Prompting-based methods have shown promise as a new paradigm for zero-shot or few-shot inference in natural language processing . Another advantage is the potential to adapt very large language models <ref type="bibr" target="#b27">(Reynolds and McDonell, 2021)</ref> to new tasks without relatively expensive fine-tuning. Concurrent works <ref type="bibr" target="#b18">(Meng et al., 2022;</ref><ref type="bibr" target="#b38">Ye et al., 2022)</ref> also show that language models can generate synthetic training data. However, such methods have not yet proven effective for more complex tasks such as triplet extraction.</p><p>Structured Prediction RelationPrompt generates synthetic data for relation triplet extraction, which is a structured prediction task. Hence, it can be widely applicable to other structured prediction tasks such as named entity recognition <ref type="bibr" target="#b0">(Aly et al., 2021)</ref>, event extraction <ref type="bibr" target="#b7">(Huang et al., 2018)</ref> or aspect sentiment triplet extraction <ref type="bibr" target="#b37">(Xu et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this work, we introduce the task setting of Zero-Shot Relation Triplet Extraction (ZeroRTE) to overcome fundamental limitations in previous task settings and encourage further research in lowresource relation extraction. To solve ZeroRTE, we propose RelationPrompt and show that language models can effectively generate synthetic training data through relation label prompts to output structured texts. To overcome the limitation for extracting multiple relation triplets in a sentence, we propose the Triplet Search Decoding method which is effective and interpretable. Results show that our method surpasses prior ZeroRC methods as well as strong baselines on ZeroRTE, setting the bar for future work. As mentioned in Section 4.3, a future direction for improvement could be to ensure that the generated entity spans are more compatible with the semantics of the relation in the language model prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mouth of Watercourse</head><p>It drains into the Pacific Ocean via the Pat?a River.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position Played</head><p>Made Chad Brown the highest paid linebacker in NFL history.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>League</head><p>The Diamondbacks compete in the National League West division.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Military Branch</head><p>The 47th Liaison Squadron is an inactive United States Air Force unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head of Government</head><p>Following the September 2014 general elections in Montserrat, Reuben Meade's government was replaced by new government led by Donaldson Romeo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Director</head><p>The Locket is a 1946 film directed by John Brahm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Military Rank</head><p>General Sir Bernard Paget died on 16 February 1961.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residence</head><p>Diederik van Dijk is married and lives in Benthuizen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Location</head><p>He gave the Bampton Lectures at Oxford in 1824.</p><p>Original Language Her latest Tamil film was "Jaihind 2".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train Relation Generator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train Relation Extractor</head><p>Generate Synthetic Unseen Samples</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-Tune Relation Extractor on Synthetic Samples</head><p>Triplet Extraction on Unseen Sentences</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A) Annotation Samples of Seen Relations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Sentence</head><p>Sibling She was the mother of Michael and Joel Douglas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manufacturer</head><p>In late 2012 , Samsung announced its NX300 camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architect</head><p>His house was designed by Henry Hob Richardson.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B) Annotation Samples of Unseen Relations for Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Sentence</head><p>Military Rank Their grandson was Group Captain Nicolas Tindal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position Played</head><p>Made Chad Brown the highest paid linebacker in NFL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Record Label</head><p>Deadsy signed onto Immortal Records to release "Phantasmagore".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C) Generated Synthetic Relation Samples of Unseen Relations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Military Rank</head><p>The squadron is commanded by Sir Robert Davis, the fourth British marine Lieutenant General.</p><p>Position Played However, it was Dario Argentino who defended the midfield.</p><p>Record Label "The Sun" was first recorded by Pavement in 1982.</p><p>(a) Annotation Samples of Unseen Relations in FewRel Dataset indal. er in NFL.</p><p>ase "Phantasmagore".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Sentence</head><p>Employer Martha Crago is Vice President of Research at Dalhousie University.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Award Received</head><p>Private Bernard McQuirt won the Victoria Cross at Rowa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sports Discipline</head><p>Andrii Toptun is a Ukrainian marathon runner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spouse</head><p>Messalina, Roman wife of Claudius.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Country of Citizenship</head><p>Jarmo Saari is Finnish a guitarist , composer and producer .</p><p>Part Of Line 2 of Metro Bilbao starts at Basauri and reaches Santurtzi.</p><p>Official Language Mass media in Israel in a language other than Hebrew.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Drafted By</head><p>Sihugo Green from Duquesne University was selected first overall by the Rochester Royals.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Narrative Location</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Additional Data Samples Dataset Samples To further illustrate the datasets used, we show test samples in <ref type="figure" target="#fig_7">Figure 7</ref>. The samples are taken from the FewRel (a) and Wiki-ZSL (b) test sets respectively with 10 unseen relation labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic Samples</head><p>To further examine the output of the relation generator, we show test samples in <ref type="figure">Figure 8</ref>. The samples are generated from the FewRel (a) and Wiki-ZSL (b) test set labels respectively with 10 unseen relation labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Implementation Details</head><p>Generating Structured Texts We use the relation generator model to generate synthetic sentences in an autoregressive fashion. To convert the structured text outputs to relation triplet samples, we perform simple string processing on the output templates shown in <ref type="figure" target="#fig_4">Figure 3a</ref> to separate the structured content from the natural text. In case of a small amount of conversion errors, we continue to generate samples until the amount of sentences</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mouth of Watercourse</head><p>The Cascades River is a freshwater estuary in Florida.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position Played</head><p>In 2009, Wojciech Szczerbi?ski was named head coach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>League</head><p>The 2014 FIFA World Cup, played at D?sseldorf stadium.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Military Branch</head><p>At this time the Army continued to deploy to Somalia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head of Government</head><p>The Prime Minister is the Prime Minister of Pakistan.</p><p>Director "T?l?chargier" was directed by the director Olivier Delpierre.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Military Rank</head><p>He was a former admiral named Thomas J. Tarr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residence</head><p>Toretto was born and raised in Nieuwland, Norway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Location</head><p>The district was originally assigned to the Northern Romanovs of Moscow.</p><p>Original Language It was also written by the Finnish filmmaker Mikael Njoro.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train Relation Generator</head><p>Train Position Played Made Chad Brown the highest paid linebacker in NFL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Record Label</head><p>Deadsy signed onto Immortal Records to release "Phantasmagore".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Employer</head><p>Bewley was signed into the HGV at the age of 17.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Award Received</head><p>In 1962 he won Best Director for Unrequited Love.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sports Discipline</head><p>Thomas Stuestor was a champion of tennis in 1872.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spouse</head><p>It was created for Harry M. Truman's wife Nancy in 1950.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Country of Citizenship</head><p>Peter Paul Rubens was a Czechoslovak politician and businessman.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part Of</head><p>The main source of power in the Middle East was Saudi Arabia and Egypt.</p><p>Official Language The first official English translation is by Robert Knecht.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Drafted By</head><p>In addition, the Cincinnati Bengals drafted quarterback Danny Franklin.</p><p>Narrative Location A story from the English drama series The Tudors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Educated At</head><p>Tchaikov attended the Krasnoyarsk Academy (1960s) in Moscow.</p><p>(b) Generated Samples of Unseen Relations in Wiki-ZSL Dataset <ref type="figure">Figure 8</ref>: Additional synthetic samples from the generated outputs. The head and tail entities are shown in blue and orange, respectively. generated per label is reached. For the relation extractor model, we perform a similar processing on the output templates in <ref type="figure" target="#fig_4">Figure 3b</ref> to extract the predicted relation triplets. However, in case of processing errors, we do not continue generation and instead treat it as a prediction failure for that input sample.</p><p>Hyperparameters We show more detailed hyperparameters used in <ref type="table" target="#tab_16">Table 6</ref>. We run a grid search on the Wiki-ZSL validation set with 10 unseen labels for multi-triplet ZeroRTE F 1 metric. A grid search is used to tune the hyperparameters. For number of generated samples per label, we consider the values {125, 250, 500, 1000, 2000}. To tune the Triplet Search Decoding threshold, we consider fifty evenly-spaced values from the interval over the minimum and maximum output scores of all candidate triplets on the validation set. Due to computational constraints, we consider the number of branches to consider at each stage a fixed value, and do not tune it as a hyperparameter.</p><p>Computing Infrastructure The experiments are conducted on NVIDIA V100 GPUs, and each experiment is run on a single GPU with 32 GB of   memory and mixed precision settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Further Analysis</head><p>Generated Sample Diversity Our method for ZeroRTE heavily depends on the quality of the generated data. Hence, we compare the diversity of real and synthetic data samples. Concretely, we measure the number of unique words and entities present in the texts. We used the Wiki-ZSL validation set sentences with five unique labels and generate an equal amount of synthetic sentences for comparison. <ref type="table" target="#tab_17">Table 7</ref> shows that the diversity of unique entities is actually greater for the generated sentences. However, the generated sentences have lower diversity of overall unique words. This may be explained by the fact that entity names tend to be unique, and the generator language model has seen a vast number of unique entity names during the large-scale pre-training. On the other hand, the total unique words are mostly determined by the non-entity words. By using prompts to condition the generation of sentences specifically for unseen relation labels, this may constrain the diversity of contextual information in the output sentences.</p><p>Performance Across Relations To study how the performance varies across different relation labels, we evaluate single-triplet ZeroRTE on the Wiki-ZSL test set with 10 unseen labels. <ref type="figure" target="#fig_8">Figure 9</ref> shows that the model is able to perform well for relations such as "Drafted By" and "Sports Discipline Competed In". However, it performs more poorly for relations such as "Official Language" and "Employer". This suggests that RelationPrompt per-  forms best for relations which are highly specific to constrain the output context more effectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b) Annotation samples of unseen relations for evaluation. (a) Annotation samples of seen relations for training. (c) Generated synthetic samples of unseen relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Example relation triplet data for ZeroRTE and our formulation as synthetic sentence generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>RelationPrompt structured templates. The head entities, tail entities and relation labels are shown in blue, orange and dark red respectively. The relation generator (a) takes the relation label as input and outputs the context and entity pair. The relation extractor (b) takes the sentence as input and outputs the relation triplet which consists of entity pair and relation label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(b) Training process for relation extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Model training process. Each head entity, tail entity and relation label is shown in blue, orange and dark red respectively. To conserve space, the sentences shown are shortened and punctuation is not separated. The relation generator (a) is trained with the standard language modeling objective to condition on the relation label and generate the sentence and entity pair. The relation extractor (b) is trained with the standard sequence-to-sequence objective to condition on the input sentence and output the relation triplet of entity pair and relation label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Case study between real and generated samples for relations in the wild. The head and tail entities are shown in blue and orange respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FFigure 6 :</head><label>6</label><figDesc>Effect of generated data size on ZeroRTE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Additional sentence samples from the datasets. The head and tail entities are shown in blue and orange, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FFigure 9 :</head><label>9</label><figDesc>Separate evaluation on relation labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Their grandson was Group Captain Nicolas Tindal .</figDesc><table><row><cell cols="3">S: y: Military Rank</cell><cell></cell></row><row><cell></cell><cell>e tail</cell><cell>e head</cell><cell></cell></row><row><cell>Task Setting</cell><cell>Input</cell><cell>Output</cell><cell>Supervision</cell></row><row><cell>Relation Classification</cell><cell>S, ehead, etail</cell><cell>y</cell><cell>Full</cell></row><row><cell>Zero-Shot Relation Classification</cell><cell>S, ehead, etail</cell><cell>y</cell><cell>Zero-Shot</cell></row><row><cell>Zero-Shot Relation Slot-Filling</cell><cell>S, ehead, y</cell><cell>etail</cell><cell>Zero-Shot</cell></row><row><cell>Relation Triplet Extraction</cell><cell>S</cell><cell>ehead, etail, y</cell><cell>Full</cell></row><row><cell>Zero-Shot Relation Triplet Extraction</cell><cell>S</cell><cell cols="2">ehead, etail, y Zero-Shot</cell></row><row><cell>Train Relation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Generator</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Generate Synthetic Unseen Samples</cell><cell cols="2">Fine-Tune Relation Extractor on Synthetic Samples</cell><cell>Triplet E Unseen</cell></row><row><cell>Train Relation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Extractor</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of task settings with our proposed Zero-Shot Relation Triplet Extraction (ZeroRTE). To our knowledge, ZeroRTE is the first task to extract full relation triplets in the zero-shot setting.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Example Context: Their grandson was Captain Nicolas Tindal.</figDesc><table><row><cell>Input</cell><cell>Relation: &lt;Label&gt;.</cell></row><row><cell cols="2">Example Relation: Military Rank.</cell></row><row><cell>Output</cell><cell>Context: &lt;Sentence&gt;. Head Entity: &lt;Subject&gt;, Tail Entity: &lt;Object&gt;.</cell></row><row><cell cols="2">Example Context: Their grandson was Captain Nicolas Tindal. Head Entity: Nicolas</cell></row><row><cell></cell><cell>Tindal, Tail Entity: Captain.</cell></row><row><cell></cell><cell>(a) Structured template for relation generator.</cell></row><row><cell>Output</cell><cell>Head Entity: &lt;Subject&gt;, Tail Entity: &lt;Object&gt;, Relation: &lt;Label&gt;.</cell></row><row><cell cols="2">Example Head Entity: Nicolas Tindal, Tail Entity: Captain, Relation: Military Rank.</cell></row><row><cell></cell><cell>(b) Structured template for relation extractor.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Head Entity: Nicolas. Tail Entity: Captain, Relation: Military Rank.Head Entity: Nicolas. , Tail Entity: Captain , Relation: Military Rank .</figDesc><table><row><cell cols="4">(a) Unconditional decoding for single-triplet extraction.</cell></row><row><cell cols="4">(b) Entity-conditioned decoding for relation classification.</cell></row><row><cell></cell><cell>Nicolas</cell><cell>Captain</cell><cell>Military Rank</cell></row><row><cell></cell><cell>p(e head,1 )</cell><cell>p(e tail,1 | e head,i )</cell><cell>p(y 1 | e tail,j , e head,i )</cell></row><row><cell></cell><cell>grandson</cell><cell>Nicolas</cell><cell>Position Played</cell></row><row><cell></cell><cell>p(e head,i )</cell><cell>p(e tail,j | e head,i )</cell><cell>p(y k | e tail,j , e head,i )</cell></row><row><cell></cell><cell>Captain</cell><cell>grandson</cell><cell>Record Label</cell></row><row><cell></cell><cell>p(e head,b )</cell><cell>p(e tail,b | e head,i )</cell><cell>p(y b | e tail,j , e head,i )</cell></row><row><cell cols="4">(c) Triplet search decoding for multi-triplet extraction.</cell></row><row><cell></cell><cell>Nicolas</cell><cell>Captain</cell><cell>Military Rank</cell></row><row><cell></cell><cell>p head,1</cell><cell>p tail,1</cell><cell>p relation,1</cell></row><row><cell></cell><cell>grandson</cell><cell>Nicolas</cell><cell>Position Played</cell></row><row><cell></cell><cell>p head,2</cell><cell>p tail,2</cell><cell>p relation,2</cell></row><row><cell></cell><cell>Captain</cell><cell>grandson</cell><cell>Record Label</cell></row><row><cell></cell><cell>p head,3</cell><cell>p tail,3</cell><cell>p relation,3</cell></row><row><cell>Prompt</cell><cell cols="2">Enumerate Head Entities</cell><cell>Enumerate Tail Entities</cell><cell>Enumerate Relations</cell></row><row><cell></cell><cell>p head,1</cell><cell></cell><cell>p tail,1</cell><cell>p relation,1</cell></row><row><cell>Head Entity:</cell><cell cols="2">Joe Hogsett. Tail Entity:</cell><cell>Indianapolis. Relation:</cell><cell>Residence.</cell></row><row><cell></cell><cell>p head,2</cell><cell></cell><cell>p tail,2</cell><cell>p relation,2</cell></row><row><cell></cell><cell cols="2">Democrat. Tail Entity:</cell><cell>Joe Hogsett. Relation:</cell><cell>Head of government.</cell></row><row><cell></cell><cell>p head,3</cell><cell></cell><cell>p tail,3</cell><cell>p relation,3</cell></row><row><cell></cell><cell cols="2">Indianapolis. Tail Entity:</cell><cell>Mayor. Tail Entity:</cell><cell>Position head.</cell></row></table><note>Head Entity: Nicolas, Tail Entity: Captain, Relation: Military Rank.Head Entity: Nicolas, Tail Entity: Captain, Relation: Military Rank.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics. "Sentence Length" refers to the average number of words in each sentence.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Results for Zero-Shot Relation Triplet Extraction (ZeroRTE).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>NoGen 54.45 29.43 37.45 66.49 40.05 49.38 RelationPrompt 63.69 67.93 65.74 74.33 72.51 73.40</figDesc><table><row><cell cols="2">Unseen Model</cell><cell></cell><cell>Wiki-ZSL</cell><cell></cell><cell></cell><cell>FewRel</cell></row><row><cell>Labels</cell><cell></cell><cell>P.</cell><cell>R.</cell><cell>F 1</cell><cell>P.</cell><cell>R.</cell><cell>F 1</cell></row><row><cell></cell><cell>R-BERT</cell><cell cols="6">39.22 43.27 41.15 42.19 48.61 45.17</cell></row><row><cell></cell><cell>CIM</cell><cell cols="6">49.63 48.81 49.22 58.05 61.92 59.92</cell></row><row><cell>m=5</cell><cell>ZS-BERT</cell><cell cols="6">71.54 72.39 71.96 76.96 78.86 77.90</cell></row><row><cell></cell><cell>NoGen</cell><cell cols="6">51.78 46.76 48.93 72.36 58.61 64.57</cell></row><row><cell></cell><cell cols="7">RelationPrompt 70.66 83.75 76.63 90.15 88.50 89.30</cell></row><row><cell></cell><cell>R-BERT</cell><cell cols="6">26.18 29.69 27.82 25.52 33.02 28.20</cell></row><row><cell></cell><cell>CIM</cell><cell cols="6">46.54 47.90 45.57 47.39 49.11 48.23</cell></row><row><cell>m=10</cell><cell>ZS-BERT</cell><cell cols="6">60.51 60.98 60.74 56.92 57.59 57.25</cell></row><row><cell></cell><cell>NoGen</cell><cell cols="6">54.87 36.52 43.80 66.47 48.28 55.61</cell></row><row><cell></cell><cell cols="7">RelationPrompt 68.51 74.76 71.50 80.33 79.62 79.96</cell></row><row><cell></cell><cell>R-BERT</cell><cell cols="6">17.31 18.82 18.03 16.95 19.37 18.08</cell></row><row><cell></cell><cell>CIM</cell><cell cols="6">29.17 30.58 29.86 31.83 33.06 32.43</cell></row><row><cell>m=15</cell><cell>ZS-BERT</cell><cell cols="6">34.12 34.38 34.25 35.54 38.19 36.82</cell></row></table><note>Extraction We compare RelationPrompt with the baselines on ZeroRTE for Wiki-ZSL and</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Zero-Shot Relation Classification (ZeroRC).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Extractor Fine-Tuning (Seen Relations) 13.57 -14.84    </figDesc><table><row><cell>Model</cell><cell>F 1</cell><cell>?F 1</cell></row><row><cell>Full Method</cell><cell>28.41</cell><cell></cell></row><row><cell>? Triplet Search Decoding</cell><cell cols="2">14.53 -13.88</cell></row><row><cell>?</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc>Ablation results for multi-triplet ZeroRTE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 6 :</head><label>6</label><figDesc>Additional hyperparameters.</figDesc><table><row><cell></cell><cell cols="3">Samples Unique Entities Unique Words</cell></row><row><cell>Real Data</cell><cell>3461</cell><cell>3090</cell><cell>14736</cell></row><row><cell>Generated Data</cell><cell>3461</cell><cell>4949</cell><cell>10558</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 7 :</head><label>7</label><figDesc>Data diversity comparison.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">As relation classification and relation extraction are sometimes interchangeable, we refer to relation classification.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">See Appendix A.2 for more implementation details.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Leveraging type descriptions for zero-shot named entity recognition and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?jean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zs-bert: Towards zero-shot relation extraction with attribute representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Te</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Template-based named entity recognition using BART</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DAGA: Data augmentation with a generation approach for low-resource tagging tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bosheng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canasai</forename><surname>Kruengkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Zero-shot transfer learning for event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><surname>Voss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with sentence-level attention and entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Nitish Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05858</idno>
		<title level="m">Ctrl: A conditional transformer language model for controllable generation. CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contextual augmentation: Data augmentation by words with paradigmatic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01335</idno>
		<title level="m">Tim Dozat, and Hyung Won Chung. 2021. Neural data augmentation via example extrapolation. CoRR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Zero-shot relation extraction via reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Entity-relation extraction as multi-turn question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Generating training data with language models: Towards zero-shot language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.04538</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective modeling of encoder-decoder architecture for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapas</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Zeroshot relation classification as textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abiola</forename><surname>Obamuyide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of FEVER</title>
		<meeting>of FEVER</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2020. Structured prediction as translation between augmented natural languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">DARE: data augmented relation extraction with GPT-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Papanikolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Pierleoni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13845</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-IJCNLP</title>
		<meeting>of EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Zero-shot text classification with generative language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10165</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Train once, test anywhere: Zero-shot learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpankar</forename><surname>Kumar Pushp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muktabh Mayank</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05972</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>OpenAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Prompt programming for large language models: Beyond the few-shot paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laria</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.1145/3411763.3451760</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
		<meeting>of CHI</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How much knowledge can you pack into the parameters of a language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Kocisk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Relation extraction using distant supervision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alisa</forename><surname>Smirnova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Cudr?-Mauroux</surname></persName>
		</author>
		<idno type="DOI">10.1145/3241741</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CSUR</publisher>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two are better than one: Joint entity and relation extraction with tablesequence encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A survey of zero-shot learning: Settings, methods, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3293318</idno>
	</analytic>
	<monogr>
		<title level="j">TIST</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Eda: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-IJCNLP</title>
		<meeting>of EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Enriching pretrained language model with entity information for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanchan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/abs/10.1145/3357384.3358119</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of CIKM</title>
		<meeting>of CIKM</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Question answering on freebase via relation extraction and textual evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning span-level interactions for aspect sentiment triplet extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Zerogen: Efficient zero-shot learning via dataset generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qintong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07922</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristy</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04670</idno>
		<title level="m">Meta-tuning language models to answer prompts better. CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A frustratingly easy approach for entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
