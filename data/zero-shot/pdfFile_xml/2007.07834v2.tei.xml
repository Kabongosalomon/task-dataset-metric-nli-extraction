<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">INFOXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewen</forename><surname>Chi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology ? Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
							<email>lidong1@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology ? Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<email>fuwei@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology ? Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology ? Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology ? Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology ? Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology ? Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology ? Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology ? Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology ? Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">INFOXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present an informationtheoretic framework that formulates crosslingual language model pre-training as maximizing mutual information between multilingual-multi-granularity texts.</p><p>The unified view helps us to better understand the existing methods for learning cross-lingual representations. More importantly, inspired by the framework, we propose a new pretraining task based on contrastive learning. Specifically, we regard a bilingual sentence pair as two views of the same meaning and encourage their encoded representations to be more similar than the negative examples. By leveraging both monolingual and parallel corpora, we jointly train the pretext tasks to improve the cross-lingual transferability of pre-trained models. Experimental results on several benchmarks show that our approach achieves considerably better performance. The code and pre-trained models are available at https://aka.ms/infoxlm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning cross-lingual language representations plays an important role in overcoming the language barrier of NLP models. The recent success of crosslingual language model pre-training <ref type="bibr" target="#b10">(Devlin et al., 2019;</ref><ref type="bibr" target="#b7">Conneau and Lample, 2019;</ref><ref type="bibr" target="#b6">Conneau et al., 2020a;</ref><ref type="bibr" target="#b5">Chi et al., 2020;</ref> significantly improves the cross-lingual transferability in various downstream tasks, such as cross-lingual classification, and question answering.</p><p>State-of-the-art cross-lingual pre-trained models are typically built upon multilingual masked language modeling (MMLM; <ref type="bibr" target="#b10">Devlin et al. 2019;</ref><ref type="bibr" target="#b6">Conneau et al. 2020a)</ref>, and translation language modeling (TLM; <ref type="bibr" target="#b7">Conneau and Lample 2019)</ref>. The goal of both pretext tasks is to predict masked tokens given input context. The difference is that MMLM uses monolingual text as input, while TLM feeds bilingual parallel sentences into the model. Even without explicit encouragement of learning universal representations across languages, the derived models have shown promising abilities of cross-lingual transfer.</p><p>In this work, we formulate cross-lingual pretraining from a unified information-theoretic perspective. Following the mutual information maximization principle <ref type="bibr" target="#b21">Kong et al., 2020)</ref>, we show that the existing pretext tasks can be viewed as maximizing the lower bounds of mutual information between various multilingualmulti-granularity views.</p><p>Specifically, MMLM maximizes mutual information between the masked tokens and the context in the same language while the anchor points across languages encourages the correlation between cross-lingual contexts. Moreover, we present that TLM can maximize mutual information between the masked tokens and the parallel context, which implicitly aligns encoded representations of different languages. The unified informationtheoretic framework also inspires us to propose a new cross-lingual pre-training task, named as cross-lingual contrast (XLCO). The model learns to distinguish the translation of an input sentence from a set of negative examples. In comparison to TLM that maximizes token-sequence mutual information, XLCO maximizes sequence-level mutual information between translation pairs which are regarded as cross-lingual views of the same meaning. We employ the momentum contrast <ref type="bibr" target="#b14">(He et al., 2020)</ref> to realize XLCO. We also propose the mixup contrast and conduct the contrast on the universal layer to further facilitate the cross-lingual transferability.</p><p>Under the presented framework, we develop a cross-lingual pre-trained model (INFOXLM) to leverage both monolingual and parallel corpora. We jointly train INFOXLM with MMLM, TLM and XLCO. We conduct extensive experiments on several cross-lingual understanding tasks, including cross-lingual natural language inference <ref type="bibr" target="#b8">(Conneau et al., 2018)</ref>, cross-lingual question answering , and cross-lingual sentence retrieval <ref type="bibr" target="#b2">(Artetxe and Schwenk, 2019)</ref>. Experimental results show that INFOXLM outperforms strong baselines on all the benchmarks. Moreover, the analysis indicates that INFOXLM achieves better cross-lingual transferability.</p><p>2 Related Work 2.1 Cross-Lingual LM Pre-Training Multilingual BERT (mBERT; <ref type="bibr" target="#b10">Devlin et al. 2019)</ref> is pre-trained with the multilingual masked language modeling (MMLM) task on the monolingual text. mBERT produces cross-lingual representations and performs cross-lingual tasks surprisingly well <ref type="bibr" target="#b38">(Wu and Dredze, 2019)</ref>. XLM <ref type="bibr" target="#b7">(Conneau and Lample, 2019)</ref> extends mBERT with the translation language modeling (TLM) task so that the model can learn cross-lingual representations from parallel corpora. Unicoder <ref type="bibr" target="#b18">(Huang et al., 2019)</ref> tries several pre-training tasks to utilize parallel corpora. ALM  extends TLM to codeswitched sequences obtained from translation pairs. XLM-R <ref type="bibr" target="#b6">(Conneau et al., 2020a)</ref> scales up MMLM pre-training with larger corpus and longer training. LaBSE <ref type="bibr" target="#b13">(Feng et al., 2020)</ref> learns cross-lingual sentence embeddings by an additive translation ranking loss.</p><p>In addition to learning cross-lingual encoders, several pre-trained models focus on generation. MASS <ref type="bibr" target="#b32">(Song et al., 2019)</ref> and mBART  pretrain sequence-to-sequence models to improve machine translation. XNLG <ref type="bibr" target="#b5">(Chi et al., 2020)</ref> focuses on the cross-lingual transfer of language generation, such as cross-lingual question generation, and abstractive summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mutual Information Maximization</head><p>Various methods have successfully learned visual or language representations by maximizing mutual information between different views of input. It is difficult to directly maximize mutual information. In practice, the methods resort to a tractable lower bound as the estimator, such as InfoNCE <ref type="bibr" target="#b27">(Oord et al., 2018)</ref>, and the variational form of the KL divergence <ref type="bibr" target="#b26">(Nguyen et al., 2010)</ref>. The estimators are also known as contrastive learning <ref type="bibr" target="#b0">(Arora et al., 2019)</ref> that measures the representation similarities between the sampled positive and negative pairs. In addition to the estimators, various view pairs are employed in these methods. The view pair can be the local and global features of an image , the random data augmentations of the same image <ref type="bibr" target="#b33">(Tian et al., 2019;</ref><ref type="bibr" target="#b14">He et al., 2020;</ref>, or different parts of a sequence <ref type="bibr" target="#b27">(Oord et al., 2018;</ref><ref type="bibr" target="#b15">Henaff, 2020;</ref><ref type="bibr" target="#b21">Kong et al., 2020)</ref>. <ref type="bibr" target="#b21">Kong et al. (2020)</ref> show that learning word embeddings or contextual embeddings can also be unified under the framework of mutual information maximization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Information-Theoretic Framework for</head><p>Cross-Lingual Pre-Training</p><p>In representation learning, the learned representations are expected to preserve the information of the original input data. However, it is intractable to directly model the mutual information between the input data and the representations. Alternatively, we can maximize the mutual information between the representations from different views of the input data, e.g., different parts of a sentence, a translation pair of the same meaning.</p><p>In this section, we start from a unified information-theoretic perspective, and formulate cross-lingual pre-training with the mutual information maximization principle. Then, under the information-theoretic framework, we propose a new cross-lingual pre-training task, named as crosslingual contrast (XLCO). Finally, we present the pre-training procedure of our INFOXLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multilingual Masked Language Modeling</head><p>The goal of multilingual masked language modeling (MMLM; <ref type="bibr" target="#b10">Devlin et al. 2019</ref>) is to recover the masked tokens from a randomly masked sequence. For each input sequence of MMLM, we sample a text from the monolingual corpus for pretraining. Let (c 1 , x 1 ) denote a monolingual text sequence, where x 1 is the masked token, and c 1 is the corresponding context. Intuitively, we need to maximize their dependency (i.e., I(c 1 ; x 1 )), so that the context representations are predictive for masked tokens <ref type="bibr" target="#b21">(Kong et al., 2020)</ref>.</p><p>For the example pair (c 1 , x 1 ), we construct a set N that contains x 1 and |N | ? 1 negative samples drawn from a proposal distribution q. According to the InfoNCE (Oord et al., 2018) lower bound, we have:</p><formula xml:id="formula_0">I(c 1 ; x 1 ) E q(N ) log f ? (c 1 , x 1 ) x ?N f ? (c 1 , x ) + log |N | (1)</formula><p>where f ? is a function that scores whether the input c 1 and x 1 is a positive pair. Given context c 1 , MMLM learns to minimize the cross-entropy loss of the masked token x 1 :</p><formula xml:id="formula_1">L MMLM = ? log exp(g ? T (c 1 ) g ? E (x 1 )) x ?V exp(g ? T (c 1 ) g ? E (x ))<label>(2)</label></formula><p>where V is the vocabulary, g ? E is a look-up function that returns the token embeddings, g ? T is a Transformer that returns the final hidden vectors in position of x 1 . According to Equation <ref type="formula">(1)</ref> and Equation <ref type="formula" target="#formula_1">(2)</ref>, if N = V and f ? (c 1 , x 1 ) = exp(g ? T (c 1 ) g ? E (x 1 )), we can find that MMLM maximizes a lower bound of I(c 1 ; x 1 ). Next, we explain why MMLM can implicitly learn cross-lingual representations. Let (c 2 , x 2 ) denote a MMLM instance that is in different language as (c 1 , x 1 ). Because the vocabulary, the position embedding, and special tokens are shared across languages, it is common to find anchor points <ref type="bibr" target="#b28">(Pires et al., 2019;</ref> where x 1 = x 2 (such as subword, punctuation, and digit) or I(x 1 , x 2 ) is positive (i.e., the representations are associated or isomorphic). With the bridge effect of {x 1 , x 2 }, MMLM obtains a v-structure dependency "c 1 ? {x 1 , x 2 } ? c 2 ", which leads to a negative co-information (i.e., interaction information) I(c 1 ; c 2 ; {x 1 , x 2 }) <ref type="bibr" target="#b36">(Tsujishita, 1995)</ref>. Specifically, the negative value of I(c 1 ; c 2 ; {x 1 , x 2 }) indicates that the variable {x 1 , x 2 } enhances the correlation between c 1 and c 2 <ref type="bibr" target="#b12">(Fano, 1963)</ref>.</p><p>In summary, although MMLM learns to maximize I(c 1 , x 1 ) and I(c 2 , x 2 ) in each language, we argue that the task encourages the cross-lingual correlation of learned representations. Notice that for the setting without word-piece overlap <ref type="bibr" target="#b1">(Artetxe et al., 2020;</ref><ref type="bibr" target="#b9">Conneau et al., 2020b;</ref><ref type="bibr" target="#b19">K et al., 2020)</ref>, we hypothesize that the information bottleneck principle <ref type="bibr" target="#b35">(Tishby and Zaslavsky, 2015)</ref> tends to transform the cross-lingual structural similarity into isomorphic representations, which has similar bridge effects as the anchor points. Then we can explain how the cross-lingual ability is spread out as above. We leave more discussions about the setting without word-piece overlap for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Translation Language Modeling</head><p>Similar to MMLM, the goal of translation language modeling (TLM; Conneau and Lample 2019) is also to predict masked tokens, but the prediction is conditioned on the concatenation of a translation pair. We try to explain how TLM pre-training enhances cross-lingual transfer from an informationtheoretic perspective.</p><p>Let c 1 and c 2 denote a translation pair of sentences, and x 1 a masked token taken in c 1 . So c 1 and x 1 are in the same language, while c 1 and c 2 are in different ones. Following the derivations of MMLM in Section 3.1, the objective of TLM is maximizing the lower bound of mutual information I(c 1 , c 2 ; x 1 ). By re-writing the above mutual information, we have:</p><formula xml:id="formula_2">I(c 1 , c 2 ; x 1 ) = I(c 1 ; x 1 ) + I(c 2 ; x 1 |c 1 ) (3)</formula><p>The first term I(c 1 ; x 1 ) corresponds to MMLM, which learns to use monolingual context. In contrast, the second term I(c 2 ; x 1 |c 1 ) indicates crosslingual mutual information between c 2 and x 1 that is not included by c 1 . In other words, I(c 2 ; x 1 |c 1 ) encourages the model to predict masked tokens by using the context in a different language. In conclusion, TLM learns to utilize the context in both languages, which implicitly improves the crosslingual transferability of pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cross-Lingual Contrastive Learning</head><p>Inspired by the unified information-theoretic framework, we propose a new cross-lingual pre-training task, named as cross-lingual contrast (XLCO). The goal of XLCO is to maximize mutual information between the representations of parallel sentences c 1 and c 2 , i.e., I(c 1 , c 2 ). Unlike maximizing tokensequence mutual information in MMLM and TLM, XLCO targets at cross-lingual sequence-level mutual information.</p><p>We describe how the task is derived as follows. Using InfoNCE (Oord et al., 2018) as the lower bound, we have:</p><formula xml:id="formula_3">I(c 1 ; c 2 ) E q(N ) log f ? (c 1 , c 2 ) c ?N f ? (c 1 , c ) + log |N |<label>(4)</label></formula><p>where N is a set that contains the positive pair c 2 and |N |?1 negative samples. In order to maximize the lower bound of I(c 1 ; c 2 ), we need to design the function f ? that measures the similarity between the input sentence and the proposal distribution q(N ). Specifically, we use the following similarity function f ? :</p><formula xml:id="formula_4">f ? (c 1 , c 2 ) = exp(g ? (c 1 ) g ? (c 2 )) (5)</formula><p>where g ? is the Transformer encoder that we are pre-training. Following <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, a special token [CLS] is added to the input, whose hidden vector is used as the sequence representation. Additionally, we use a linear projection head after the encoder in g ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Momentum</head><p>Contrast Another design choice is how to construct N . As shown in Equation <ref type="formula" target="#formula_3">(4)</ref>, a large |N | improves the tightness of the lower bound, which has been proven to be critical for contrastive learning .</p><p>In our work, we employ the momentum contrast <ref type="bibr" target="#b14">(He et al., 2020)</ref> to construct the set N , where the previously encoded sentences are progressively reused as negative samples. Specifically, we construct two encoders with the same architecture which are the query encoder g ? Q and the key encoder g ? K . The loss function of XLCO is:</p><formula xml:id="formula_5">L XLCO = ? log exp(g ? Q (c 1 ) g ? K (c 2 )) c ?N exp(g ? Q (c 1 ) g ? K (c ))<label>(6)</label></formula><p>During training, the query encoder g ? Q encodes c 1 and is updated by backpropagation. The key encoder g ? K encodes N and is learned with momentum update <ref type="bibr" target="#b14">(He et al., 2020)</ref> towards the query encoder. The negative examples in N are organized as a queue, where a newly encoded example is added while the oldest one is popped from the queue. We initialize the query encoder and the key encoder with the same parameters, and pre-fill the queue with a set of encoded examples until it reaches the desired size |N |. Notice that the size of the queue remains constant during training.</p><p>Mixup Contrast For each pair, we concatenate it with a randomly sampled translation pair from another parallel corpus. For example, consider the pairs c 1 , c 2 and d 1 , d 2 sampled from two different parallel corpora. The two pairs are concatenated in a random order, such as c 1 d 1 , c 2 d 2 , and c 1 d 2 , d 1 c 2 . The data augmentation of mixup encourages pre-trained models to learn sentence boundaries and to distinguish the order of multilingual texts.</p><p>Contrast on Universal Layer As a pre-training task maximizing the lower bound of sequencelevel mutual information, XLCO is usually jointly learned with token-sequence tasks, such as MMLM, and TLM. In order to make XLCO more compatible with the other pretext tasks, we propose to conduct contrastive learning on the most universal (or transferable) layer in terms of MMLM and TLM.</p><p>In our implementations, we instead use the hidden vectors of [CLS] at layer 8 to perform contrastive learning for base-size (12 layers) models, and layer 12 for large-size (24 layers) models. Because previous analysis <ref type="bibr" target="#b30">(Sabet et al., 2020;</ref><ref type="bibr" target="#b9">Conneau et al., 2020b)</ref> shows that the specific layers of MMLM learn more universal representations and work better on crosslingual retrieval tasks than other layers. We choose the layers following the same principle.</p><p>The intuition behind the method is that MMLM and TLM encourage the last layer to produce language-distinguishable token representations because of the masked token classification. But XLCO tends to learn similar representations across languages. So we do not directly use the hidden states of the last layer in XLCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Cross-Lingual Pre-Training</head><p>We pretrain a cross-lingual model INFOXLM by jointly maximizing the lower bounds of three types of mutual information, including monolingual token-sequence mutual information (MMLM), cross-lingual token-sequence mutual information (TLM), and cross-lingual sequence-level mutual information (XLCO). Formally, the loss of crosslingual pre-training in INFOXLM is defined as:</p><formula xml:id="formula_6">L = L MMLM + L TLM + L XLCO<label>(7)</label></formula><p>where we apply the same weight for the loss terms. Both TLM and XLCO use parallel data. The number of bilingual pairs increases with the square of the number of languages. In our work, we set English as the pivot language following <ref type="bibr" target="#b7">(Conneau and Lample, 2019)</ref>, i.e., we only use the parallel corpora that contain English.</p><p>In order to balance the data size between highresource and low-resource languages, we apply a multilingual sampling strategy <ref type="bibr" target="#b7">(Conneau and Lample, 2019)</ref> for both monolingual and parallel data. An example in the language l is sampled with the probability p l ? (n l /n) 0.7 , where n l is the number of instances in the language l, and n refers to the total number of data. Empirically, the sampling algorithm alleviates the bias towards high-resource languages <ref type="bibr" target="#b6">(Conneau et al., 2020a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first present the training configuration of INFOXLM. Then we compare the finetuning results of INFOXLM with previous work on three cross-lingual understanding tasks. We also conduct ablation studies to understand the major components of INFOXLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Corpus We use the same pre-training corpora as previous models <ref type="bibr" target="#b6">(Conneau et al., 2020a;</ref><ref type="bibr" target="#b7">Conneau and Lample, 2019)</ref>. Specifically, we reconstruct CC-100 <ref type="bibr" target="#b6">(Conneau et al., 2020a)</ref> for MMLM, which remains 94 languages by filtering the language code larger than 0.1GB. Following (Conneau and Lample, 2019), for the TLM and XLCO tasks, we employ 14 language pairs of parallel data that involves English. We collect translation pairs from MultiUN <ref type="bibr" target="#b40">(Ziemski et al., 2016)</ref>, IIT Bombay <ref type="bibr" target="#b22">(Kunchukuttan et al., 2018)</ref>, OPUS (Tiedemann, 2012), and WikiMatrix . The size of parallel corpora is about 42GB. More details about the pre-training data are described in the appendix.</p><p>Model Size We follow the model configurations of XLM-R <ref type="bibr" target="#b6">(Conneau et al., 2020a)</ref>. For the Transformer <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref> architecture, we use 12 layers and 768 hidden states for INFOXLM (i.e., base size), and 24 layers and 1,024 hidden states for INFOXLM LARGE (i.e., large size). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>We conduct experiments over three cross-lingual understanding tasks, i.e., cross-lingual natural language inference, cross-lingual sentence retrieval, and cross-lingual question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Lingual Natural Language Inference</head><p>The Cross-Lingual Natural Language Inference corpus (XNLI; <ref type="bibr" target="#b8">Conneau et al. 2018</ref>) is a widely used cross-lingual classification benchmark. The goal of NLI is to identify the relationship of an input sentence pair. We evaluate the models under the following two settings. (1) Cross-Lingual Transfer: fine-tuning the model with English training set and directly evaluating on multilingual test sets. <ref type="formula" target="#formula_1">(2)</ref> Translate-Train-All: fine-tuning the model with the English training data and the pseudo data that are translated from English to the other languages.</p><p>Cross-Lingual Sentence Retrieval The goal of the cross-lingual sentence retrieval task is to extract parallel sentences from bilingual comparable corpora. We use the subset of 36 language pairs of the Tatoeba dataset (Artetxe and Schwenk, 2019) for the task. The dataset is collected from Tatoeba 1 , which is an open collection of multilingual parallel sentences in more than 300 languages. Following <ref type="bibr" target="#b17">(Hu et al., 2020)</ref>, we use the averaged hidden vectors in the seventh Transformer layer to compute cosine similarity for sentence retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Lingual</head><p>Question Answering We use the Multilingual Question Answering (MLQA; ) dataset for the crosslingual QA task. MLQA provides development and test data in seven languages in the format of SQuAD v1.1 <ref type="bibr" target="#b29">(Rajpurkar et al., 2016)</ref>. We follow the fine-tuning method introduced in <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> that concatenates the question-passage pair as the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We compare INFOXLM with the following pretrained Transformer models: (1) Multilingual BERT (MBERT; <ref type="bibr" target="#b10">Devlin et al. 2019</ref>) is pre-trained with MMLM on Wikipedia in 102 languages; (2) XLM <ref type="bibr" target="#b7">(Conneau and Lample, 2019)</ref>     Cross-Lingual Sentence Retrieval In <ref type="table">Table 2</ref> and   <ref type="table" target="#tab_4">Table 3</ref>: Evaluation results on Tatoeba cross-lingual sentence retrieval. We report the top-1 accuracy scores of 22 language pairs that are not covered by parallel data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Lingual Natural Language Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Lingual Question Answering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis and Discussion</head><p>To understand INFOXLM and the cross-lingual contrast task more deeply, we conduct analysis from the perspectives of cross-lingual transfer and cross-lingual representations. Furthermore, we perform comprehensive ablation studies on the major components of INFOXLM, including the crosslingual pre-training tasks, mixup contrast, the contrast layer, and the momentum contrast. To reduce the computation load, we use INFOXLM15 in our ablation studies, which is trained on 15 languages for 100K steps.</p><p>Cross-Lingual Transfer Gap Cross-lingual transfer gap <ref type="bibr" target="#b17">(Hu et al., 2020)</ref> is the difference between the performance on the English test set and the averaged performance on the test sets of all other languages. A lower cross-lingual transfer gap score indicates more end-task knowledge from the English training set is transferred to other languages. In <ref type="table" target="#tab_8">Table 5</ref>, we compare the cross-lingual transfer gap scores of INFOXLM with baseline models on MLQA and XNLI. Note that we do not include the results of XLM because it is pre-trained on 15 languages or using #M=N. The results show that INFOXLM reduces the gap scores on both MLQA and XNLI, providing better cross-lingual transferability than the baselines.</p><p>Cross-Lingual Representations In addition to cross-lingual transfer, learning good cross-lingual representations is also the goal of cross-lingual pre- training. In order to analyze how the cross-lingual contrast task affects the alignment of the learned cross-lingual representations, we evaluate the representations of different middle layers on the Tatoeba test sets of the 14 languages that are covered by parallel data. <ref type="figure" target="#fig_1">Figure 1</ref> presents the averaged top-1 accuracy of cross-lingual sentence retrieval in the direction of xx ? en. INFOXLM outperforms XLM-R on all of the 12 layers, demonstrating that our proposed task improves the cross-lingual alignment of the learned representations. From the results of XLM-R, we observe that the model suffers from a performance drop in the last few layers. The reason is that MMLM encourages the representations of the last hidden layer to be similar to token embeddings, which is contradictory with the goal of learning cross-lingual representations. In  <ref type="table">Table 4</ref>: Evaluation results on MLQA cross-lingual question answering. We report the F1 and exact match (EM) scores. Results with "*" are taken from . "(reimpl)" is our reimplementation of finetuning, which is the same as INFOXLM.  contrast, INFOXLM still provides high retrieval accuracy at the last few layers, which indicates that INFOXLM provides better aligned representations than XLM-R. Moreover, we find that the performance is further improved when removing TLM, demonstrating that XLCO is more effective than TLM for aligning cross-lingual representations, although TLM helps to improve zero-shot cross-lingual transfer.</p><p>Effect of Cross-Lingual Pre-training Tasks To better understand the effect of the cross-lingual pre-training tasks, we perform ablation studies on the pre-training tasks of INFOXLM, by removing XLCO, TLM, or both. We present the experimental results in   on the universal layer improves cross-lingual pretraining. As shown in <ref type="table" target="#tab_10">Table 6</ref>, we compare the evaluation results of four variants of INFOXLM, where XLCO is applied on the layer 8 (i.e., universal layer) or on the layer 12 (i.e., the last layer). We find that contrast on the layer 8 provides better results for INFOXLM. However, conducting XLCO on layer 12 performs better when the TLM task is excluded. The results show that maximizing context-sequence (TLM) and sequence-level (XLCO) mutual information at the last layer tends to interfere with each other. Thus, we suggest applying XLCO on the universal layer for pre-training INFOXLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Mixup Contrast</head><p>We conduct an ablation study on the mixup contrast strategy. We pre-train a model that directly uses translation pairs for XLCO without mixup contrast (?TLM?Mixup). As shown in <ref type="table" target="#tab_9">Table 7</ref>, we present the evaluation results on XNLI and MLQA. We observe that mixup contrast improves the performance of INFOXLM on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Momentum Contrast</head><p>In order to show whether our pre-trained model benefits from momentum contrast, we pretrain a revised version of INFOXLM without momentum contrast. In other words, the parameters of the key encoder are always the same as the query encoder. As shown in <ref type="table" target="#tab_9">Table 7</ref>, we report evaluation results (indicated by "?TLM?Momentum") of removing momentum contrast on XNLI and MLQA. We observe a performance descent after removing the momentum contrast from INFOXLM, which indicates that momentum contrast improves the learned language representations of INFOXLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a cross-lingual pre-trained model INFOXLM that is trained with both monolingual and parallel corpora. The model is motivated by the unified view of cross-lingual pretraining from an information-theoretic perspective. Specifically, in addition to the masked language modeling and translation language modeling tasks, INFOXLM is jointly pre-trained with a newly introduced cross-lingual contrastive learning task. The cross-lingual contrast leverages bilingual pairs as the two views of the same meaning, and encourages their encoded representations to be more similar than the negative examples. Experimental results on several cross-lingual language understanding tasks show that INFOXLM can considerably improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Ethical Considerations</head><p>Currently, most NLP research works and applications are English-centric, which makes non-English users hard to access to NLP-related services. Our work focuses on cross-lingual language model pretraining. With the pre-trained model, we are able to transfer end-task knowledge from high-resource languages to low-resource languages, which helps to build more accessible NLP applications. Additionally, incorporating parallel corpora into the pretraining procedure improves the training efficiency, which potentially reduces the computational cost for building multilingual NLP applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Pre-Training Data</head><p>We reconstruct CCNet 2 and follow <ref type="bibr" target="#b6">(Conneau et al., 2020a)</ref> to reproduce the CC-100 corpus for monolingual texts. The resulting corpus contains 94 languages. <ref type="table" target="#tab_13">Table 8</ref> reports the language codes and data size in our work. Notice that several languages can share the same ISO language code, e.g., zh represents both Simplified Chinese and Traditional Chinese. Moreover, <ref type="table" target="#tab_14">Table 9</ref> shows the statistics of the parallel data.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Results of Training From Scratch</head><p>We conduct experiments under the setting of training from scratch. The Transformer size and hyperparameters follow BERT-base <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>. The parameters are randomly initialized from U [?0.02, 0.02]. We optimize the models with 2 https://github.com/facebookresearch/ cc_net  Adam using a batch size of 256 for a total of 1M steps. The learning rate is scheduled with a linear decay with 10K warmup steps, where the peak learning rate is set as 0.0001. For cross-lingual contrast, we set the queue length as 16, 384. We use a warmup of 200K steps for the key encoder and then enable cross-lingual contrast. We use an inverse square root scheduler to set the momentum coefficient, i.e., m = min(1 ? t ?0.51 , 0.9995), where t is training step. <ref type="table" target="#tab_2">Table 10</ref> shows the results of INFOXLM SCRATCH and various ablations. INFOXLM SCRATCH significantly outperforms MMLM SCRATCH on both XNLI and MLQA. We also evaluate the pre-training objectives of INFOXLM, where we ablate XLCO, TLM, and MMLM, respectively. The findings agree with the results in <ref type="table" target="#tab_9">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyperparameters for Pre-Training</head><p>As shown in <ref type="table" target="#tab_2">Table 11</ref>, we present the hyperparameters for pre-training INFOXLM. We use the same vocabulary with XLM-R <ref type="bibr" target="#b6">(Conneau et al., 2020a)</ref>.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Hyperparameters for Fine-Tuning</head><p>In <ref type="table" target="#tab_2">Table 12</ref> and <ref type="table" target="#tab_2">Table 13</ref>, we present the hyperparameters for fine-tuning on XNLI and MLQA. For each task, the hyperparameters are searched on the joint validation set of all languages (#M=1). For XNLI, we evaluate the model every 5,000 steps, and select the model with the best accuracy score on the validation set. For MLQA, we directly use the final learned model. The final scores are averaged over five random seeds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Results with "*" are taken from Conneau et al. (2020a). "(reimpl)" is our reimplementation of fine-tuning, which is the same as INFOXLM. Results of INFOXLM and XLM-R (reimpl) are averaged over five runs. "?XLCO" is the model without cross-lingual contrast. languages; (3) XLM-R (Conneau et al., 2020a) scales up MMLM to the large CC-100 corpus in 100 languages with much more training steps; (4) UNICODER (Liang et al., 2020) continues training XLM-R with MMLM and TLM. (5) IN-FOXLM?XLCO continues training XLM-R with MMLM and TLM, using the same pre-training datasets with INFOXLM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Evaluation results of different layers on Tatoeba cross-lingual sentence retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We initialize the parameters of INFOXLM with XLM-R. We optimize the model with Adam (Kingma and Ba, 2015) using a batch size of 2048 for a total of 150K steps for INFOXLM, and 200K steps for INFOXLM LARGE . The same number of training examples are fed to three tasks. The learning rate is scheduled with a linear decay with 10K warmup steps, where the peak learning rate is set as 0.0002 for INFOXLM, and 0.0001 for INFOXLM LARGE . The momentum coefficient is set as 0.9999 and 0.999 for IN-FOXLM and INFOXLM LARGE , respectively. The length of the queue is set as 131, 072. The training procedure takes about 2.3 days ? 2 Nvidia DGX-2 stations for INFOXLM, and 5 days ? 16 Nvidia DGX-2 stations for INFOXLM LARGE . Details about the pre-training hyperparameters can be found in the appendix.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Evaluation results on XNLI cross-lingual natural language inference. We report test accuracy in 15 languages. The model number #M=N indicates the model selection is done on each language's validation set (i.e., each language has a different model), while #M=1 means only one model is used for all languages.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>reports the classification accuracy on each</cell></row><row><cell>test of XNLI under the above evaluation settings.</cell></row><row><cell>The final scores on test set are averaged over five</cell></row><row><cell>random seeds. INFOXLM outperforms all base-</cell></row><row><cell>line models on the two evaluation settings of XNLI.</cell></row><row><cell>In the cross-lingual transfer setting, INFOXLM</cell></row><row><cell>achieves 76.5 averaged accuracy, outperforming</cell></row><row><cell>XLM-R (reimpl) by 1.5. Similar improvements can</cell></row><row><cell>be observed for large-size models. Moreover, the</cell></row><row><cell>ablation results "?XLCO" show that cross-lingual</cell></row><row><cell>contrast is helpful for zero-shot transfer in most</cell></row><row><cell>languages. We also find that INFOXLM improves</cell></row><row><cell>the results in the translate-train-all setting.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>, we report the top-1 accuracy scores of</cell></row><row><cell>cross-lingual sentence retrieval with the base-size</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 Table 2 :</head><label>42</label><figDesc>en 36.8 67.6 60.7 89.9 53.7 74.1 54.2 72.5 74.0 18.7 38.3 61.1 36.6 68.4 57.6 INFOXLM xx ? en 59.0 78.6 86.3 93.9 62.1 79.4 87.1 83.8 88.2 39.5 84.9 83.3 73.0 89.6 77.8 ?XLCO xx ? en 42.9 65.5 69.5 91.1 55.6 76.4 71.6 74.9 74.8 20.5 68.1 69.8 51.6 81.8 65.3 XLM-R en ? xx 38.6 69.9 60.3 89.4 57.3 74.3 49.3 73.0 74.6 14.4 58.4 64.0 36.9 72.5 59.5 INFOXLM en ? xx 68.6 78.6 86.4 95.1 72.6 84.0 88.3 85.7 87.2 40.8 91.2 84.7 73.3 92.0 80.6 ?XLCO en ? xx 45.4 64.0 69.3 88.1 56.5 72.3 69.6 73.6 71.5 22.1 79.7 64.3 48.2 79.8 64.6 Evaluation results on Tatoeba cross-lingual sentence retrieval. We report the top-1 accuracy of 14 language pairs that are covered by parallel data.</figDesc><table><row><cell>compares INFOXLM with baseline models on</cell></row><row><cell>MLQA, where we report the F1 and the exact</cell></row><row><cell>match (EM) scores on each test set. Both IN-</cell></row><row><cell>FOXLM and INFOXLM LARGE obtain the best re-</cell></row><row><cell>sults against the four baselines. In addition, the</cell></row><row><cell>results of the ablation variant "?XLCO" indicate</cell></row><row><cell>that the proposed cross-lingual contrast is benefi-</cell></row><row><cell>cial on MLQA.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Cross-lingual transfer gap scores, i.e., aver-</cell></row><row><cell>aged performance drop between English and other lan-</cell></row><row><cell>guages in zero-shot transfer. Smaller gap indicates</cell></row><row><cell>better transferability. "?XLCO" is the model without</cell></row><row><cell>cross-lingual contrast.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>Comparing the results of ?TLM and ?XLCO with the results of ?TLM?XLCO, we find that both XLCO and TLM effectively improve cross-lingual transferability of the pre-trained INFOXLM model. TLM is more effective for XNLI while XLCO is more effective for MLQA. Moreover, the performance can be further improved by jointly learning XLCO and TLM.</figDesc><table><row><cell>Effect of Contrast on Universal Layer We con-</cell></row><row><cell>duct experiments to investigate whether contrast</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Contrast on the universal layer v.s. on the last layer. Results are averaged over five runs. "?TLM" is the ablation variant without TLM.</figDesc><table><row><cell></cell><cell>Model</cell><cell>XNLI</cell><cell>MLQA</cell></row><row><cell>[0]</cell><cell>INFOXLM15</cell><cell cols="2">76.45 67.87 / 49.58</cell></row><row><cell>[1]</cell><cell>[0]?XLCO</cell><cell cols="2">76.24 67.43 / 49.23</cell></row><row><cell>[2]</cell><cell>[0]?TLM</cell><cell cols="2">75.85 67.84 / 49.54</cell></row><row><cell>[3]</cell><cell>[2]?XLCO</cell><cell cols="2">75.33 66.86 / 48.82</cell></row><row><cell>[4]</cell><cell>[2]?Mixup</cell><cell cols="2">75.43 67.21 / 49.19</cell></row><row><cell>[5]</cell><cell>[2]?Momentum</cell><cell cols="2">75.32 66.58 / 48.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Ablation results on components of INFOXLM. Results are averaged over five runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>The statistics of CCNet used corpus for pretraining.</figDesc><table><row><cell>ISO Code</cell><cell>Size (GB)</cell><cell>ISO Code</cell><cell>Size (GB)</cell></row><row><cell>en-ar</cell><cell>5.88</cell><cell>en-ru</cell><cell>7.72</cell></row><row><cell>en-bg</cell><cell>0.49</cell><cell>en-sw</cell><cell>0.06</cell></row><row><cell>en-de</cell><cell>4.21</cell><cell>en-th</cell><cell>0.47</cell></row><row><cell>en-el</cell><cell>2.28</cell><cell>en-tr</cell><cell>0.34</cell></row><row><cell>en-es</cell><cell>7.09</cell><cell>en-ur</cell><cell>0.39</cell></row><row><cell>en-fr</cell><cell>7.63</cell><cell>en-vi</cell><cell>0.86</cell></row><row><cell>en-hi</cell><cell>0.62</cell><cell>en-zh</cell><cell>4.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Parallel data used for pre-training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>55.02 / 37.90 INFOXLMSCRATCH 70.71 59.71 / 41.46 ?XLCO 70.64 57.70 / 40.21 ?TLM 69.76 58.22 / 40.78 ?MMLM 63.06 52.81 / 35.01</figDesc><table><row><cell>Model</cell><cell>XNLI</cell><cell>MLQA</cell></row><row><cell>Metrics</cell><cell>Acc.</cell><cell>F1 / EM</cell></row><row><cell>MMLMSCRATCH</cell><cell>69.40</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Ablation results of the models pre-trained from scratch. Results are averaged over five runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>Hyperparameters used for INFOXLM pretraining. *: the momentum coefficient uses an inverse square root scheduler m = min(1 ? t ?0.51 , 0.9995).</figDesc><table><row><cell></cell><cell>XNLI</cell><cell>MLQA</cell></row><row><cell>Batch size</cell><cell>32</cell><cell>{16, 32}</cell></row><row><cell>Learning rate</cell><cell>{5e-6, 7e-6, 1e-5}</cell><cell>{2e-5, 3e-5, 5e-5}</cell></row><row><cell>LR schedule</cell><cell>Linear</cell><cell>Linear</cell></row><row><cell>Warmup</cell><cell>12,500 steps</cell><cell>10%</cell></row><row><cell>Weight decay</cell><cell>0</cell><cell>0</cell></row><row><cell>Epochs</cell><cell>10</cell><cell>{2, 3, 4}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 12 :</head><label>12</label><figDesc>Hyperparameters used for fine-tuning BASEsize models on XNLI and MLQA.</figDesc><table><row><cell></cell><cell>XNLI</cell><cell>MLQA</cell></row><row><cell>Batch size</cell><cell>32</cell><cell>32</cell></row><row><cell>Learning rate</cell><cell>{4e-6, 5e-6, 6e-6}</cell><cell>{2e-5, 3e-5, 5e-5}</cell></row><row><cell>LR schedule</cell><cell>Linear</cell><cell>Linear</cell></row><row><cell>Warmup</cell><cell>5,000 steps</cell><cell>10%</cell></row><row><cell>Weight decay</cell><cell>{0, 0.01}</cell><cell>0</cell></row><row><cell>Epochs</cell><cell>10</cell><cell>{2, 3, 4}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 13 :</head><label>13</label><figDesc>Hyperparameters used for fine-tuning LARGE-size models on XNLI and MLQA.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We appreciate the helpful discussions with Bo Zheng, Shaohan Huang, Shuming Ma, and Yue Cao.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishikesh</forename><surname>Khandeparkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Orestis Plevrakis, and Nikunj Saunshi</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07-05" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4623" to="4637" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="597" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15509" to="15519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-lingual natural language generation via pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7570" to="7577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7057" to="7067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">XNLI: Evaluating cross-lingual sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1269</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2475" to="2485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Emerging cross-lingual structure in pretrained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.536</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6022" to="6034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identifying elements essential for BERT&apos;s multilinguality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Dufter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.358</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4423" to="4437" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Fano</surname></persName>
		</author>
		<idno type="DOI">https:/aapt.scitation.org/doi/10.1119/1.1937609</idno>
		<title level="m">Transmission of Information: A Statistical Theory of Communications. M.I</title>
		<imprint>
			<publisher>T. Press</publisher>
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Languageagnostic BERT sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangxiaoyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01852</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="4411" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1252</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2485" to="2494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-lingual ability of multilingual bert: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A mutual information maximization perspective of language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyprien</forename><surname>De Masson D&amp;apos;autume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The IIT Bombay English-Hindi parallel corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MLQA: Evaluating cross-lingual extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7315" to="7330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">XGLUE: A new benchmark datasetfor cross-lingual pre-training, understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenfei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sining</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiun-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winnie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.484</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6008" to="6018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanlong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5847" to="5861" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How multilingual is multilingual BERT?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1493</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4996" to="5001" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masoud Jalili</forename><surname>Sabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Dufter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08728</idno>
		<title level="m">Simalign: High quality word alignments without parallel training data using static and contextualized embeddings</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05791</idno>
		<title level="m">Wiki-Matrix: Mining 135M parallel sentences in 1620 language pairs from wikipedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MASS: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno>abs/1906.05849</idno>
		<title level="m">Contrastive multiview coding. ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in OPUS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2214" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On triple mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tsujishita</surname></persName>
		</author>
		<idno type="DOI">10.1006/aama.1995.1013</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="274" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1077</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="833" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Alternating language modeling for cross-lingual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The united nations parallel corpus v1. 0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Ziemski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Pouliquen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3530" to="3534" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
