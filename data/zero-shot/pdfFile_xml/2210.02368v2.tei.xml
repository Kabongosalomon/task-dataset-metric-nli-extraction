<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatio-Temporal Learnable Proposals for End-to-End Video Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><forename type="middle">Azeem</forename><surname>Hashmi</surname></persName>
							<email>khurram_azeem.hashmi@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="department">German Research Centre for Artificial Intelligence (DFKI)</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Kaiserslautern</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
							<email>didier.stricker@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="department">German Research Centre for Artificial Intelligence (DFKI)</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Kaiserslautern</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeshan</forename><surname>Muhammamd</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Afzal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">German Research Centre for Artificial Intelligence (DFKI)</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Kaiserslautern</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammamd_Zeshan</forename><forename type="middle">Afzal@dfki</forename><surname>De</surname></persName>
						</author>
						<title level="a" type="main">Spatio-Temporal Learnable Proposals for End-to-End Video Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>HASHMI ET AL: SPATIO-TEMPORAL LEARNABLE PROPOSALS FOR END-TO-END VOD 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the novel idea of generating object proposals by leveraging temporal information for video object detection. The feature aggregation in modern regionbased video object detectors heavily relies on learned proposals generated from a singleframe RPN. This imminently introduces additional components like NMS and produces unreliable proposals on low-quality frames. To tackle these restrictions, we present Spar-seVOD, a novel video object detection pipeline that employs Sparse R-CNN to exploit temporal information. In particular, we introduce two modules in the dynamic head of Sparse R-CNN. First, the Temporal Feature Extraction module based on the Temporal RoI Align operation is added to extract the RoI proposal features. Second, motivated by sequence-level semantic aggregation, we incorporate the attention-guided Semantic Proposal Feature Aggregation module to enhance object feature representation before detection. The proposed SparseVOD effectively alleviates the overhead of complicated post-processing methods and makes the overall pipeline end-to-end trainable. Extensive experiments show that our method significantly improves the single-frame Sparse R-CNN by 8%-9% in mAP. Furthermore, besides achieving state-of-the-art 80.3% mAP on the ImageNet VID dataset with ResNet-50 backbone, our SparseVOD outperforms existing proposal-based methods by a significant margin on increasing IoU thresholds (IoU &gt; 0.5).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video Object Detection (VOD) aims to localize and classify objects in a series of subsequent video frames. Recent efforts in video object detection demonstrate that exploiting feature aggregation of temporal information <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> produce superior performance than leveraging temporal information at the post-processing stage <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref>. The former approaches mainly enhance the target frame feature representation through aggregating features from neighbouring frames or an entire video clip by designing a specific module, thereby boosting detection results. The majority of these works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> employ two-stage detectors such as Faster R-CNN <ref type="bibr" target="#b30">[31]</ref> or R-FCN <ref type="bibr" target="#b5">[6]</ref> to design their VOD pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal/Semantic</head><p>Feature Aggregation support frame t-s support frame t+s target frame t  Albeit the enormous success, it is important to highlight that the temporal feature aggregation scheme of all prior region-based VOD methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> heavily relies on object proposals from RPN <ref type="bibr" target="#b30">[31]</ref> trained without any temporal information. Consequently, these methods suffer from several underlying restrictions. First, they require an additional step of NMS <ref type="bibr" target="#b29">[30]</ref> at the beginning to perform dense-to-sparse matching of hand-crafted anchors, making the overall VOD pipeline not end-to-end optimizable. Second, on low-quality frames (appearance deterioration), the generated object proposals are unreliable, leading to ineffective temporal feature aggregation. <ref type="figure" target="#fig_0">Fig. 1(a)</ref> illustrates that although the detector leverages spatio-temporal information from support frames t-s and t+s, it fails to detect Turtle at the target frame t. The main reason for such missed prediction is that generated proposals from single-frame RPN overlooks Turtle in the target frame t. This corrupts object features during instance-level feature aggregation. The third restriction is that these methods require several support frames to calibrate proposal feature representation for the target frame, decreasing the run time performance. Fourth, since the RPN in these methods is optimized on a single IoU level (generally IoU=0.5), they struggle to provide high-quality detections (IoU &gt; 0.5) despite producing impressive performance on a lower IoU threshold. Moreover, these methods necessitate complex post-processing methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref> or additional proposal classifier networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> to accomplish state-of-the-art performance.</p><p>To tackle the aforementioned challenges, we propose SparseVOD, an end-to-end trainable framework that exploits temporal information to learn sparse (merely 100) object proposals for VOD. The SparseVOD employs the recently introduced Sparse R-CNN <ref type="bibr" target="#b36">[37]</ref> that has shown impressive performance by eliminating the need for dense priors enumerating over frames and alleviating the interaction between object queries and dense frame features. In particular, motivated from <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42]</ref>, we incorporate a Temporal (Region of Interest) RoI Feature Extraction (TFE) head that replaces a single image RoI extractor in the dynamic instance interactive head in <ref type="bibr" target="#b36">[37]</ref>. Furthermore, inspired by <ref type="bibr" target="#b41">[42]</ref>, we fuse attention guided Semantic Proposal Feature Aggregation (SPFA) module that enhances the feature representation of object proposals in target frames through semantic level sequence aggregation from support frames.</p><p>Contrary to most prior VOD works, our SparseVOD operates on a sparse-in sparse-out matching scheme. This not only eliminates components like post-processing and NMS but also enables faster training network convergence without pre-training the detector as done in <ref type="bibr" target="#b18">[19]</ref>. Furthermore, thanks to the spatio-temporal learning, iterative refinement of object proposals lead to successful predictions even on a low-quality target frame t as shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. Moreover, our spatio-temporal proposal learning alleviates the need for several support frames in a video and brings significant performance gains on increasing IoU thresholds (see <ref type="figure">Fig. 4</ref>).</p><p>Herein, our main contributions are as follows. (1) We propose SparseVOD, a novel endto-end trainable video object detection method. To our knowledge, this is the first work that exploits temporal information to learn object proposals for video object detection. (2) We extend the design of Sparse R-CNN <ref type="bibr" target="#b36">[37]</ref> by introducing a Temporal Feature Extraction (TFE) module that leverages temporal information to extract RoI proposal features. Furthermore, we fuse Semantic Proposal Feature Aggregation (SPFA) in <ref type="bibr" target="#b36">[37]</ref> to enhance object feature representation before final detection inspired by <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42]</ref>. (3) By introducing the proposed TFE and SPFA modules in <ref type="bibr" target="#b36">[37]</ref>, our SparseVOD improves the baseline by far (5-6% mAP). Without bells and whistles, our SparseVOD achieves the new best mAP of 80.3% on the ImageNet VID benchmark using ResNet-50 as the backbone. Moreover, it surpasses prior state-of-the-art methods by far in terms of high-quality detections (higher IoU thresholds, see <ref type="figure">Fig. 4</ref>) and achieves optimal speed-accuracy tradeoff ( <ref type="figure">Fig. 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Proposal Learning for Image Object Detection. Ren et al. <ref type="bibr" target="#b30">[31]</ref> introduce the Region Proposal Network (RPN) in Faster R-CNN to predict object proposals. The RPN consists of a small fully convolutional network <ref type="bibr" target="#b27">[28]</ref> that receives an anchor as an input, classifies it as an object or background, and performs box regression. This design is widely incorporated in later two-stage approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26]</ref>. MetaAnchor <ref type="bibr" target="#b43">[44]</ref> proposes to exploit metalearning to generate anchors dynamically. Cascade RPN <ref type="bibr" target="#b38">[39]</ref> improves the object proposal quality of the conventional RPN through multi-stage refinement and adaptive convolution. Recently, Sparse R-CNN <ref type="bibr" target="#b36">[37]</ref> introduces a sparse-in sparse-out paradigm that simplifies the sophisticated two-stage object detection pipeline by alleviating complex components such as Non-Maximum Suppression (NMS) <ref type="bibr" target="#b29">[30]</ref> and dense priors. Following a similar line of work, this paper proposes spatio-temporal learnable proposals to simplify the video object detection pipeline. Exploiting Temporal Information in Video Object Detection. Exploiting temporal information from other frames in a video is a natural choice to tackle the challenges of video object detection, and our work derives from the same idea. Existing approaches leveraging temporal information mainly follow one of the two directions. The first line of works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref> mainly employ temporal information to make still-image detection results more coherent and stable. The performance of such VOD methods is sub-optimal because they are not end-to-end trainable and heavily rely on the capabilities of the initial still-image detector. On the contrary, the other direction of methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> utilizes temporal information during the course of training. Earlier works in this category adopt FlowNet <ref type="bibr" target="#b8">[9]</ref> to propagate warp features across frames <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. However, temporal exploitation of optical flow-based works is limited to neighbouring frames, yielding inferior performance in occlusions. PSLA <ref type="bibr" target="#b11">[12]</ref> proposes to learn the spatial correspondence between neighbouring frames by employing the progressive sparser stride. All these methods can only capitalize temporal information from a small number of nearby frames to refine the target frame features. Alternatively, global feature aggregation methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref> have been proposed to utilize long-term semantic information. Recent VOD methods adopt this aggregation scheme and propose blending of temporal features <ref type="bibr" target="#b4">[5]</ref>, class-aware feature aggregation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, temporal RoIAlign <ref type="bibr" target="#b10">[11]</ref>, and temporal meta-adaptor <ref type="bibr" target="#b39">[40]</ref> to achieve stateof-the-art results. Although these methods produce superior performance from prior efforts, their feature aggregation rely on object proposals generated without temporal information. Refining Object Proposals in Video Object Detection. Recent efforts have shown that enhancing object proposal features can alleviate the obstacles of object confusion in videos <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref>. Shvets et al. <ref type="bibr" target="#b34">[35]</ref> refine the proposal for the target frame by learning similarities between proposals from different frames. LSTS <ref type="bibr" target="#b20">[21]</ref> models the spatio-temporal correspondence to alleviate misalignment before aggregating features from different frames. Han et al. <ref type="bibr" target="#b14">[15]</ref> propose integrating inter-video and intra-video proposals to boost target proposal features. Despite the promising improvements, the effectiveness of all these methods heavily relies on the initial quality of object proposals retrieved from single-frame RPN <ref type="bibr" target="#b30">[31]</ref>. Alternatively, this paper exploits temporal information to generate object proposals for video object detection.</p><p>Recently, MAMBA <ref type="bibr" target="#b35">[36]</ref> proposes to extract region proposals from the enhanced feature maps through a pixel-level memory bank. TransVOD <ref type="bibr" target="#b18">[19]</ref> proposes the transformerbased VOD pipeline by extending Deformable DETR <ref type="bibr" target="#b46">[47]</ref> to exploit temporal information in videos. Despite the simple and end-to-end trainable framework, the temporal transformer in TransVOD depends on object queries generated by the spatial transformer optimized without temporal information. Furthermore, owing to the interaction between each object query and the dense features of an entire frame, <ref type="bibr" target="#b18">[19]</ref> is not a pure sparse method <ref type="bibr" target="#b36">[37]</ref>. As a result of this dense interaction, TransVOD requires pre-training the detector on a similar dataset <ref type="bibr" target="#b24">[25]</ref>. On the contrary, our method leverages temporal information to generate object proposals. Moreover, following <ref type="bibr" target="#b36">[37]</ref>, our proposed SparseVOD operates on a pure sparse paradigm and does not require pre-training the detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Overview. This section explains our proposed SparseVOD, which consists of two main components: Temporal RoI extraction and Semantic Proposal Feature Aggregation incorporated in the Temporal Dynamic Head. Finally, we discuss network optimization. Note that due to space constraints, the detailed explanation of the still-image object detector Sparse R-CNN <ref type="bibr" target="#b36">[37]</ref> is omitted here and can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SparseVOD Architecture</head><p>The SparseVOD is a simple, end-to-end trainable framework, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. It receives a target frame and multiple support frames from the same video as input and outputs the class and location of objects in the target frame. For each target and support frame, the extracted feature maps from the backbone, proposal boxes and corresponding proposal features are fed into the iterative Temporal Dynamic head consisting of multiple stages. We create Temporal Dynamic Head by incorporating two main components into the dynamic head used in <ref type="bibr" target="#b36">[37]</ref> to effectively exploit the temporal information in videos. First, inspired by <ref type="bibr" target="#b10">[11]</ref>, we leverage support frame RoI features to extract temporal RoI features for the target frame. Then,  the corresponding Instance Interactive Head (IIH) produces object feature representation for each video frame. Subsequently, the Semantic Proposal Feature Aggregation module enhances the representation of the target frame by intelligently aggregating object features from support frames. The optimal object features are fed to the corresponding feed-forward network for classification and regression. Similar to <ref type="bibr" target="#b36">[37]</ref>, we follow the iterative architecture in our SparseVOD. In the next iteration, the newly refined object features and the bounding box predictions serve as the target frame's proposal features and proposal boxes. Temporal RoI Feature Extraction. The Sparse R-CNN <ref type="bibr" target="#b36">[37]</ref> applies the conventional RoIAlign <ref type="bibr" target="#b17">[18]</ref> pooling operation to extract proposal features, and it is widely adopted in existing VOD methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>. However, the naive RoIAlign operation only restricts the proposal feature extraction to exploit intra-frame features. Therefore, motivated by <ref type="bibr" target="#b10">[11]</ref>, we incorporate the Temporal Feature Extraction (TFE) module in our Temporal Dynamic Head, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. The TFE leverages temporal information for the same object instance across support frames in a video. Since the features of the same object instance have high semantic resemblance across video frames, we calculate cosine similarities between target proposal features and support frame feature maps. Given target proposal features P t and feature map from support frame F t+s , the cosine similarity Sim t+s (m) is computed as P t (m) ? F t+s , where m represents the spatial location of P t and ? denotes dot product. Note that the target frame proposal is mapped on the most similar feature maps from support frames to extract the most similar RoI features. Following the temporal attentional feature aggregation in <ref type="bibr" target="#b10">[11]</ref>, we adopt the self-attention mechanism <ref type="bibr" target="#b37">[38]</ref> to aggregate the RoI features of target and support frames.</p><p>Semantic Proposal Feature Aggregation. The Semantic Proposal Feature Aggregation (SPFA) head aims to learn the enhanced feature representation from target RoI features (containing temporal information of the same instance) and support frame RoI features to perform final classification and regression. Since we already have temporal RoI features, we follow the spirits of <ref type="bibr" target="#b41">[42]</ref> and adopt semantic similarity as the metric to aggregate features from support frame proposals. We compute semantic similarity between target and support proposal features in the same way as in TFE. For effective feature aggregation, the SPFA applies multi-head attention <ref type="bibr" target="#b37">[38]</ref> on temporal proposal features of a target frame P t and support proposal features P s as follows:</p><formula xml:id="formula_0">W t = so f tmax( ? (P s ) ? (? (P t )) T d ? (P t ) ) ? ? (P s )<label>(1)</label></formula><p>where ? (.), ? (.), and ? (.) are some linear transformations. The symbol T represents transposition, d denotes the size of transformed P t , and W t is the enhanced proposal representation of a target frame. This rich instance representation improves robustness against inherent challenges of VOD, such as appearance deterioration. Loss Function. Since our SparseVOD operates on a one-to-one label matching, our method's loss function and training process are similar to the original Sparse R-CNN. We adopt set predictions loss <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b46">47]</ref>, which aims to optimize the bipartite matching among the ground truth and predictions. Following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37]</ref>, the cost function is defined as L = ? cls ? L cls + ? L1 ? L L1 + ? giou ? L giou , where L cls is the focal loss <ref type="bibr" target="#b26">[27]</ref> for classification. L L1 and L giou are L1 loss and generalized IoU loss <ref type="bibr" target="#b31">[32]</ref> for regression, respectively. ? cls , ? L1 , and ? giou are coefficients to balance the loss. We employ an identical setting to balance these losses as in <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We perform experiments on the ImageNet VID dataset <ref type="bibr" target="#b32">[33]</ref>, which comprises 3862 training videos and 555 validation videos. Following prior works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>, we train our model on a combination of ImageNet VID and DET datasets and evaluate the results on the validation set. We adopt ImageNet pre-trained <ref type="bibr" target="#b7">[8]</ref> ResNet-50 <ref type="bibr" target="#b16">[17]</ref>, ResNet-101, and ResNeXt-101 <ref type="bibr" target="#b42">[43]</ref> backbones to compare performance with recent state-of-the-art methods. We train our network for 12 epochs with a batch size of 8 on 8 GPUs. Analogous to <ref type="bibr" target="#b36">[37]</ref>, we use AdamW <ref type="bibr" target="#b28">[29]</ref> optimizer with a weight decay of 10 ?4 . Initially, the learning rate is set to 2.5 ? 10 ?5 and divided by 10 at the 8-th and 11-th epochs. Following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47]</ref>, we set ? cls =2, ? L1 =5, and ? giou =2. We follow the basic settings of <ref type="bibr" target="#b36">[37]</ref> and set the number of iterative stages, proposal boxes, and the corresponding proposal features to 6, 100, and 100, respectively. We refer readers to supplementary materials for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>We compare the performance of the proposed SparseVOD with prior state-of-the-art VOD methods on ImageNet VID dataset in <ref type="table">Table 1</ref>. Besides the conventional mAPs @IoU=0.5, we compute mAPs @IoU=0.75 and @IOU=0.5:95 as in <ref type="bibr" target="#b24">[25]</ref> to analyze the precision of detections. Owing to the unavailability of code at the time of experiments, apart from <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36]</ref>, we reproduce the results of existing methods from the code provided by the original papers for direct comparison. It is important to mention that all the results shown in <ref type="table">Table 1</ref> are without any post-processing. By looking at results under the backbone of ResNet-50, it is evident that our SparseVOD outperforms recent methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>, mainly relying on feature aggregation of region proposals. Furthermore, it surpasses the previous best score of 79.9% by <ref type="bibr" target="#b18">[19]</ref> and achieves a new best score of 80.3% on ResNet-50. Note that alongside the  <ref type="table">Table 1</ref>: Comparison with other state-of-the-art methods on the ImageNet VID dataset. Results with * are reproduced. The two best results are highlighted in red and blue. improvement on mAP @IoU=0.5, our SparseVOD demonstrates a significant increase (7.3 and 6 points) in the precise localization on mAPs @IoU=0.75 and @IOU=0.5:95 from the previous best method <ref type="bibr" target="#b10">[11]</ref>, reflecting the superiority of spatio-temporal learnable proposals. When stronger backbones of ResNet-101 and ResNeXt-101 are incorporated into Spar-seVOD, the performance (mAP 50 ) further increases to 81.9% and 83.1%, respectively. Note that although MAMBA <ref type="bibr" target="#b35">[36]</ref>, MEGA <ref type="bibr" target="#b3">[4]</ref>, and TROI <ref type="bibr" target="#b10">[11]</ref> demonstrate better results at mAP 50 , our SparseVOD supersedes them by far (4?5 points in mAP) on higher IoU thresholds. These results correspond to our argument that while prior VOD methods operating on dense to sparse detection pipelines show impressive results on mAP 50 , they fail to produce confident and precise predictions. Furthermore, our SparseVOD boosts the single-frame baseline (Sparse R-CNN <ref type="bibr" target="#b36">[37]</ref>) by a strong margin (8%?9% mAP 50 ) with all backbone networks. This noticeable increase in mAP highlights the importance of leveraging temporal information to generate object proposals in VOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Iterative proposal Visualization Analysis</head><p>We visualize the behaviour of learned proposals boxes of a trained model on a video clip from the validation set in <ref type="figure" target="#fig_2">Fig. 3</ref>. Note that these proposals cover almost all potential regions in the target frames. This ensures high recall performance even with sparse proposals. Moreover, each stage in the cascaded architecture refines the bounding box offset and removes duplication. This makes our pipeline independent of any post-processing techniques to produce precise predictions. <ref type="figure" target="#fig_2">Fig. 3(d)</ref> further exhibits the robustness of our SparseVOD by producing high-quality predictions in challenging scenarios with camera defocus and part-occlusions. Please see supplementary materials for more qualitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>This section discusses the effect of key components and validates the design choices in our proposed method. Following <ref type="bibr" target="#b18">[19]</ref>, we perform all experiments on ImageNet VID dataset with ResNet-50 as the backbone. The run time (FPS) is tested on a single DGX A100 GPU. More ablation studies can be found in supplementary materials.</p><p>Effectiveness of each component in SparseVOD. <ref type="table" target="#tab_5">Table 2</ref> summarizes the impact of adding each component to build our proposed method. Beginning with the single-frame baseline, we incorporate the TFE module (explained in Section 3.1) and boost the AP 50 from 71.1% to 76.9%. This demonstrates the benefit of exploiting inter-frame information to extract RoI proposal features. On the other hand, by separately plugging the SPFA module (explained in Section 3.1) into a single-frame baseline, we achieve substantial gains in AP 50 from 71.1% to 79.1%. Finally, by combining both TFE and SPFA to build the proposed SparseVOD, we gain a further boost of 1.2% in AP 50 , accomplishing 80.3%. These results establish the superiority of introducing spatio-temporal feature aggregation for learnable proposals.     <ref type="table">Table 3</ref>: Ablation on the number of stages. <ref type="figure">Figure 4</ref>: The detection performance of recent VOD methods and our SparseVOD on increasing IoU threshold.   <ref type="table">Table 5</ref>: Ablation on number of support frames. <ref type="figure">Figure 5</ref>: Speed-accuracy tradeoff between SparseVOD and previous best competitor (TROI+SELSA <ref type="bibr" target="#b10">[11]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Stages.</head><p>The impact of increasing stages in an iterative architecture is summarized in <ref type="table">Table 3</ref>. Note that without iterative architecture, even though the performance AP 50 (%) reaches 60.0, the AP 75 is merely 27.2%. Since the input proposal boxes at the first stage are just random distribution of possible object locations, this result (AP 50 =60.0%) indicates that computing AP on a single IoU threshold of 0.5 is not a reliable evaluation metric. By increasing iterative stages to 3, the performance already reaches a comparable AP 50 of 78% and surpasses prior methods on AP 75 with 58.3%. Finally, similar to <ref type="bibr" target="#b36">[37]</ref>, the performance saturates at 6 stages. Hence, we adopt 6 stages in our experiments.</p><p>Comparing High-Quality Detection. <ref type="figure">Fig. 4</ref> shows the AP curves of recent state-of-the-art VOD methods and our SparseVOD under increasing IoU thresholds. It is evident that the proposed method consistently outperforms prior works with a significant margin on all the evaluation metrics. Note that although the difference is mild with the previous best competitor TROI <ref type="bibr" target="#b10">[11]</ref>, on low IoU threshold (0.5), it rises on higher IoU thresholds. These results reflect the superiority of sparse spatio-temporal learnable proposals over hand-crafted dense priors optimized on a single IoU level in existing VOD methods.</p><p>Effectiveness of Multi-head Attention in TFE and SPFA. We demonstrate the effectiveness of multi-head attentional blocks <ref type="bibr" target="#b37">[38]</ref> in Temporal Feature Extraction (TFE) and Semantic Proposal Feature Aggregation (SPFA) modules in <ref type="table" target="#tab_7">Table 4</ref>. For direct comparison, we conduct baseline experiments where attentional weights in TFE and SPFA are replaced by simple averaging to aggregate features from target and support frames. As shown in <ref type="table" target="#tab_7">Ta-ble 4</ref>, with averaging in both modules, the AP 50 reaches 75.8%, reflecting the benefit of leveraging temporal information to refine object proposals. Furthermore, we conduct experiments by switching attention to averaging in one of the two modules. With TFE (Avg) and SPFA (MSA), we observe an impressive speed-accuracy tradeoff of 79.4% AP 50 and 16.3 FPS. Since the averaging in TFE is performed on the most similar RoI features computed with cosine similarities, when combined with SPFA (MSA), it already provides an acceptable object feature representation. However, these results are still inferior (-0.9 points in AP 50 ) to the performance achieved when multi-head attention is plugged in both TFE and SPFA. These results (80.3% AP 50 ) indicate the superiority of multi-head attentional aggregation in our method. Number of Support Frames. <ref type="table">Table 5</ref> presents the ablations on the number of support frames. We follow the identical frame sampling strategy as in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42]</ref>, where support frames are uniformly sampled from the entire video. We can see that the AP 50 already reaches 79.0% with 2 support frames. Upon increasing support frames, the performance keeps increasing and tends to stabilize after reaching the AP 50 of 80.3% with 6 support frames. Speed-accuracy Tradeoff. <ref type="table" target="#tab_5">Table 2</ref> shows that the computational load in our SparseVOD stems from Temporal Feature Extraction (TFE) and Semantic Proposal Feature Aggregation (SPFA). Since the results of <ref type="bibr" target="#b18">[19]</ref> are not reproducible, for direct comparison, we analyse the speed-accuracy tradeoff of the second best method TROI <ref type="bibr" target="#b10">[11]</ref> and our SparseVOD in <ref type="figure">Fig. 5</ref>. Note that TROI <ref type="bibr" target="#b10">[11]</ref> is built upon SELSA <ref type="bibr" target="#b41">[42]</ref> to enhance performance. With only 6 support frames sampled from the entire video, our SparseVOD achieves a new best AP 50 of 80.3% with a run time of 14.4 FPS. In contrast, TROI manages to reach its best performance (AP 50 of 78.8%) with a run time of 7.5 FPS after utilizing 14 support frames. The results in <ref type="figure">Fig. 5</ref> demonstrate that the temporal feature aggregation from several support frames in prior works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42]</ref> lead to a major increase in run time, producing a sub-optimal speed-accuracy tradeoff. Contrarily, thanks to the spatio-temporal learnable proposals, our SparseVOD yields an optimal speed-accuracy tradeoff (79.0% AP 50 and 17.7 FPS) with merely 2 support frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposes SparseVOD, a novel video object detection pipeline which introduces spatio-temporal feature aggregation to refine object proposals. The SparseVOD effectively eliminates hand-crafted dense priors and provides reliable proposal features even with deteriorated input frames. Particularly, the SparseVOD incorporates attention-guided Temporal Feature Extraction and Semantic Proposal Feature Aggregation modules in Sparse R-CNN <ref type="bibr" target="#b36">[37]</ref>. Extensive experiments validate that SparseVOD significantly improves the baseline performance by 8%-9% in mAP and achieves the state-of-the-art 80.3% mAP 50 on the ImageNet VID dataset with ResNet-50 backbone. Besides, our SparseVOD beats existing methods in terms of high-quality predictions and optimal speed-accuracy tradeoff. To our knowledge, our work is the first one that exploits temporal information in directly generating a sparse set of object proposals for video object detection. We hope similar work can be applied to other video analysis tasks like object tracking and video instance segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison between previous region-based and the proposed video object detection methods. Subplot (a): Despite the temporal feature aggregation from support frames t-s and t+s in existing proposal-based approaches, the detector overlooks Turtle in pink on a low-quality target frame t due to unreliable region proposals. Subplot (b): To tackle such limitations, we propose a novel SparseVOD framework that iteratively refines region proposals through leveraging spatio-temporal feature aggregation prior to final detection. The Temporal Dynamic Head is explained inFig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Our SparseVOD Framework. The input for the first stage of the temporal dynamic head consists of initial proposal boxes, proposal features, and spatial features for the target and support frames. The TFE extracts RoI proposal features for the target frame by exploiting RoI features from support frames. Then, SPFA receives object features from each IIH (identical as in<ref type="bibr" target="#b36">[37]</ref>) and aggregates object features with guided attention heads. This enhanced object feature representation from SPFA and box predictions after FFN serve as input proposal features and proposal boxes for the next iterative stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the learned proposals and bounding box predictions at different iterative stages from a converged model. For brevity, we only visualize predictions with a confidence score greater than 0.3. Note that learned proposal boxes in (a) cover possible regions in all three video frames while the cascading heads in each stage enhance detections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Modern Video Object Detection Pipelines Proposed SparseVOD Subplot (a)</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>stage 1</cell><cell>stage 3</cell><cell>stage 5</cell></row><row><cell></cell><cell>target frame t</cell><cell>support frame t-s</cell><cell>Initial Proposal Boxes</cell><cell>Proposal Features</cell><cell cols="2">Temporal</cell><cell>target frame t</cell></row><row><cell>Missed Prediction</cell><cell>Singe Frame RPN Region Proposals from Support Features Flow Final Prediction</cell><cell>support frame t+s target frame t</cell><cell>Subplot (b)</cell><cell></cell><cell cols="2">Dynamic Head Support Features Flow Predictions Initial Proposal Boxes</cell><cell>Refined Proposal Boxes/ Features Flow Refined Proposal Boxes Refined Proposal Features turtle|0.67 Final Prediction</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Ablation on effectiveness of each module in SparseVOD.</figDesc><table><row><cell cols="4">Stages AP 50 (%) AP 75 (%) FPS</cell></row><row><cell>1</cell><cell>60.0</cell><cell>27.2</cell><cell>42.2</cell></row><row><cell>2</cell><cell>74.5</cell><cell>52.8</cell><cell>29.4</cell></row><row><cell>3</cell><cell>78.0</cell><cell>58.3</cell><cell>23.7</cell></row><row><cell>4</cell><cell>78.5</cell><cell>58.6</cell><cell>19.8</cell></row><row><cell>5</cell><cell>79.1</cell><cell>59.8</cell><cell>16.1</cell></row><row><cell>6</cell><cell>80.3</cell><cell>60.1</cell><cell>14.4</cell></row><row><cell>12</cell><cell>77.1</cell><cell>55.2</cell><cell>5.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation on the effectiveness of multi-head attention in TFE (Temporal Feature Extraction) and SPFA (Semantic Proposal Feature Aggregation) modules. The first row represents results from a single-frame baseline. The terms Avg. and MSA denote simple averaging and multi-head self-attention.</figDesc><table><row><cell cols="4">N ref AP 50 (%) AP 75 (%) FPS</cell></row><row><cell>1</cell><cell>71.1</cell><cell>52.4</cell><cell>24.3</cell></row><row><cell>2</cell><cell>79.0</cell><cell>58.2</cell><cell>17.7</cell></row><row><cell>4</cell><cell>79.9</cell><cell>59.5</cell><cell>15.9</cell></row><row><cell>6</cell><cell>80.3</cell><cell>60.1</cell><cell>14.4</cell></row><row><cell>10</cell><cell>80.2</cell><cell>60.2</cell><cell>11.7</cell></row><row><cell>14</cell><cell>80.3</cell><cell>60.2</cell><cell>9.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2022. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">HASHMI ET AL: SPATIO-TEMPORAL LEARNABLE PROPOSALS FOR END-TO-END VOD</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">HASHMI ET AL: SPATIO-TEMPORAL LEARNABLE PROPOSALS FOR END-TO-END VOD</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">HASHMI ET AL: SPATIO-TEMPORAL LEARNABLE PROPOSALS FOR END-TO-END VOD</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimizing video object detection via a scale-time lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7814" to="7823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Memory enhanced global-local aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10337" to="10346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tf-blender: Temporal feature blender for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongfang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8138" to="8147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kaiming He, and Jian Sun. R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object guided external memory network for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanming</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongpu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6678" to="6687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3038" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal roi align for video object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huamin</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1442" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Progressive sparse local attention for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3909" to="3918" />
		</imprint>
	</monogr>
	<note>Shiming Xiang, Veronique Prinet, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting better feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaozheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1469" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Class-aware feature aggregation network for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaozheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mining inter-video proposal relations for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="431" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooya</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.08465</idno>
		<title level="m">Seqnms for video object detection</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end video object detection with spatial-temporal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1507" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shiming Xiang, and Chunhong Pan. Video object detection with locally-weighted deformable neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8529" to="8536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shiming Xiang, and Chunhong Pan. Learning where to focus for efficient video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="18" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="817" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object detection in videos with tubelet proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="727" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Tubelets with convolutional neural networks for object detection from videos. IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2896" to="2907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient non-maximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Neubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Pattern Recognition (ICPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="850" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust and efficient postprocessing for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sabater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10536" to="10542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Leveraging long-range temporal relationships between proposals for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhailo</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9756" to="9764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mamba: Multi-level aggregation via memory bank for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanxiong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2620" to="2627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14454" to="14463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cascade rpn: Delving into high-quality region proposal network with adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjun</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Trung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal meta-adaptor for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fully motion-aware network for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="542" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sequence level semantics aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9217" to="9225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Metaanchor: Learning to detect objects with customized anchors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2349" to="2358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
