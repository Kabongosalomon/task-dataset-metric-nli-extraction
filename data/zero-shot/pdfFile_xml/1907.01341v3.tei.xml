<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020">2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hafner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
						</author>
						<title level="a" type="main">Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</title>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<biblScope unit="page">1</biblScope>
							<date type="published" when="2020">2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Monocular depth estimation</term>
					<term>Single-image depth prediction</term>
					<term>Zero-shot cross-dataset transfer</term>
					<term>Multi-dataset training !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks. Armed with these tools, we experiment with five diverse training datasets, including a new, massive data source: 3D films. To demonstrate the generalization power of our approach we use zero-shot cross-dataset transfer, i.e. we evaluate on datasets that were not seen during training. The experiments confirm that mixing data from complementary sources greatly improves monocular depth estimation. Our approach clearly outperforms competing methods across diverse datasets, setting a new state of the art for monocular depth estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>D EPTH is among the most useful intermediate representations for action in physical environments <ref type="bibr" target="#b0">[1]</ref>. Despite its utility, monocular depth estimation remains a challenging problem that is heavily underconstrained. To solve it, one must exploit many, sometimes subtle, visual cues, as well as long-range context and prior knowledge. This calls for learning-based techniques <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>To learn models that are effective across a variety of scenarios, we need training data that is equally varied and captures the diversity of the visual world. The key challenge is to acquire such data at sufficient scale. Sensors that provide dense ground-truth depth in dynamic scenes, such as structured light or time-of-flight, have limited range and operating conditions <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Laser scanners are expensive and can only provide sparse depth measurements when the scene is in motion. Stereo cameras are a promising source of data <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, but collecting suitable stereo images in diverse environments at scale remains a challenge. Structurefrom-motion (SfM) reconstruction has been used to construct training data for monocular depth estimation across a variety of scenes <ref type="bibr" target="#b10">[11]</ref>, but the result does not include independently moving objects and is incomplete due to the limitations of multiview matching. On the whole, none of the existing datasets is sufficiently rich to support the training of a model that works robustly on real images of diverse scenes. At present, we are faced with multiple datasets that may usefully complement each other, but are individually biased and incomplete.</p><p>In this paper, we investigate ways to train robust monocular depth estimation models that are expected to perform across ? R. Ranftl, D. Hafner, and V. Koltun are with the Intelligent Systems Lab, Intel Labs.</p><p>? K. Lasinger and K. Schindler are with the Institute of Geodesy and Photogrammetry, ETH Z?rich.</p><p>*Equal contribution diverse environments. We develop novel loss functions that are invariant to the major sources of incompatibility between datasets, including unknown and inconsistent scale and baselines. Our losses enable training on data that was acquired with diverse sensing modalities such as stereo cameras (with potentially unknown calibration), laser scanners, and structured light sensors. We also quantify the value of a variety of existing datasets for monocular depth estimation and explore optimal strategies for mixing datasets during training. In particular, we show that a principled approach based on multi-objective optimization <ref type="bibr" target="#b11">[12]</ref> leads to improved results compared to a naive mixing strategy. We further empirically highlight the importance of high-capacity encoders, and show the unreasonable effectiveness of pretraining the encoder on a large-scale auxiliary task.</p><p>Our extensive experiments, which cover approximately six GPU months of computation, show that a model trained on a rich and diverse set of images from different sources, with an appropriate training procedure, delivers state-of-the-art results across a variety of environments. To demonstrate this, we use the experimental protocol of zero-shot cross-dataset transfer. That is, we train a model on certain datasets and then test its performance on other datasets that were never seen during training. The intuition is that zero-shot cross-dataset performance is a more faithful proxy of "real world" performance than training and testing on subsets of a single data collection that largely exhibit the same biases <ref type="bibr" target="#b12">[13]</ref>.</p><p>In an evaluation across six different datasets, we outperform prior art both quantitatively and qualitatively, and set a new state of the art for monocular depth estimation. Example results are shown in <ref type="figure">Figure 1</ref>.</p><p>arXiv:1907.01341v3 [cs.CV] 25 Aug 2020 <ref type="figure">Fig. 1</ref>. We show how to leverage training data from multiple, complementary sources for single-view depth estimation, in spite of varying and unknown depth range and scale. Our approach enables strong generalization across datasets. Top: input images. Middle: inverse depth maps predicted by the presented approach. Bottom: corresponding point clouds rendered from a novel view-point. Point clouds rendered via Open3D <ref type="bibr" target="#b3">[4]</ref>. Input images from the Microsoft COCO dataset <ref type="bibr" target="#b4">[5]</ref>, which was not seen during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Early work on monocular depth estimation used MRF-based formulations <ref type="bibr" target="#b2">[3]</ref>, simple geometric assumptions <ref type="bibr" target="#b1">[2]</ref>, or nonparametric methods <ref type="bibr" target="#b13">[14]</ref>. More recently, significant advances have been made by leveraging the expressive power of convolutional networks to directly regress scene depth from the input image <ref type="bibr" target="#b14">[15]</ref>. Various architectural innovations have been proposed to enhance prediction accuracy <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. These methods need ground-truth depth for training, which is commonly acquired using RGB-D cameras or LiDAR sensors. Others leverage existing stereo matching methods to obtain ground truth for supervision <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. These methods tend to work well in the specific type of scenes used to train them, but do not generalize well to unconstrained scenes, due to the limited scale and diversity of the training data.</p><p>Garg et al. <ref type="bibr" target="#b8">[9]</ref> proposed to use calibrated stereo cameras for self-supervision. While this significantly simplifies the acquisition of training data, it still does not lift the restriction to a very specific data regime. Since then, various approaches leverage selfsupervision, but they either require stereo images <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> or exploit apparent motion <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, and are thus difficult to apply to dynamic scenes.</p><p>We argue that high-capacity deep models for monocular depth estimation can in principle operate on a fairly wide and unconstrained range of scenes. What limits their performance is the lack of large-scale, dense ground truth that spans such a wide range of conditions. Commonly used datasets feature homogeneous scene layouts, such as street scenes in a specific geographic region <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> or indoor environments <ref type="bibr" target="#b29">[30]</ref>. We note in particular that these datasets show only a small number of dynamic objects. Models that are trained on data with such strong biases are prone to fail in less constrained environments.</p><p>Efforts have been made to create more diverse datasets. Chen et al. <ref type="bibr" target="#b33">[34]</ref> used crowd-sourcing to sparsely annotate ordinal relations in images collected from the web. Xian et al. <ref type="bibr" target="#b31">[32]</ref> collected a stereo dataset from the web and used off-the-shelf tools to extract dense ground-truth disparity; while this dataset is fairly diverse, it only contains 3,600 images. Li and Snavely <ref type="bibr" target="#b10">[11]</ref> used SfM and multi-view stereo (MVS) to reconstruct many (predominantly static) 3D scenes for supervision. Li et al. <ref type="bibr" target="#b37">[38]</ref> used SfM and MVS to construct a dataset from videos of people imitating mannequins (i.e. they are frozen in action while the camera moves through the scene). Chen et al. <ref type="bibr" target="#b38">[39]</ref> propose an approach to automatically assess the quality of sparse SfM reconstructions in order to construct a large dataset. Wang et al. <ref type="bibr" target="#b32">[33]</ref> build a large dataset from stereo videos sourced from the web, while Cho et al. <ref type="bibr" target="#b39">[40]</ref> collect a dataset of outdoor scenes with handheld stereo cameras. Gordon et al. <ref type="bibr" target="#b40">[41]</ref> estimate the intrinsic parameters of YouTube videos in order to leverage them for training. Large-scale datasets that were collected from the Internet <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref> require a large amount of pre-and post-processing. Due to copyright restrictions, they often only provide links to videos, which frequently become unavailable. This makes reproducing these datasets challenging.</p><p>To the best of our knowledge, the controlled mixing of multiple data sources has not been explored before in this context. Ummenhofer et al. <ref type="bibr" target="#b41">[42]</ref> presented a model for two-view structure  <ref type="bibr" target="#b35">[36]</ref> High Medium Synthetic (Metric) 1064 KITTI <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> ()</p><formula xml:id="formula_0">() Medium Low Laser/Stereo Metric 93K NYUDv2 [30] () Medium Low RGB-D Metric 407K TUM-RGBD [37] () Medium Low RGB-D Metric 80K</formula><p>and motion estimation and trained it on a dataset of (static) scenes that is the union of multiple smaller datasets. However, they did not consider strategies for optimal mixing, or study the impact of combining multiple datasets. Similarly, Facil et al. <ref type="bibr" target="#b42">[43]</ref> used multiple datasets with a naive mixing strategy for learning monocular depth with known camera intrinsics. Their test data is very similar to half of their training collection, namely RGB-D recordings of indoor scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXISTING DATASETS</head><p>Various datasets have been proposed that are suitable for monocular depth estimation, i.e. they consist of RGB images with corresponding depth annotation of some form <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Datasets differ in captured environments and objects (indoor/outdoor scenes, dynamic objects), type of depth annotation (sparse/dense, absolute/relative depth), accuracy (laser, time-of-flight, SfM, stereo, human annotation, synthetic data), image quality and camera settings, as well as dataset size. Each single dataset comes with its own characteristics and has its own biases and problems <ref type="bibr" target="#b12">[13]</ref>. High-accuracy data is hard to acquire at scale and problematic for dynamic objects <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b46">[47]</ref>, whereas large data collections from Internet sources come with limited image quality and depth accuracy as well as unknown camera parameters <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Training on a single dataset leads to good performance on the corresponding test split of the same dataset (same camera parameters, depth annotation, environment), but may have limited generalization capabilities to unseen data with different characteristics. Instead, we propose to train on a collection of datasets, and demonstrate that this approach leads to strongly enhanced generalization by testing on diverse datasets that were not seen during training. We list our training and test datasets, together with their individual characteristics, in <ref type="table" target="#tab_0">Table 1</ref>. Training datasets. We experiment with five existing and complementary datasets for training. ReDWeb <ref type="bibr" target="#b31">[32]</ref> (RW) is a small, heavily curated dataset that features diverse and dynamic scenes with ground truth that was acquired with a relatively large stereo baseline. MegaDepth <ref type="bibr" target="#b10">[11]</ref> (MD) is much larger, but shows predominantly static scenes. The ground truth is usually more accurate in background regions since wide-baseline multi-view stereo reconstruction was used for acquisition. WSVD <ref type="bibr" target="#b32">[33]</ref> (WS) consists of stereo videos obtained from the web and features diverse and dynamic scenes. This dataset is only available as a collection of links to the stereo videos. No ground truth is provided. We thus recreate the ground truth according to the procedure outlined by the original authors. DIML Indoor <ref type="bibr" target="#b30">[31]</ref> (DL) is an RGB-D dataset of predominantly static indoor scenes, captured with a Kinect v2. Test datasets. To benchmark the generalization performance of monocular depth estimation models, we chose six datasets based on diversity and accuracy of their ground truth. DIW <ref type="bibr" target="#b33">[34]</ref> is highly diverse but provides ground truth only in the form of sparse ordinal relations. ETH3D <ref type="bibr" target="#b34">[35]</ref> features highly accurate laserscanned ground truth on static scenes. Sintel <ref type="bibr" target="#b35">[36]</ref> features perfect ground truth for synthetic scenes. KITTI <ref type="bibr" target="#b28">[29]</ref> and NYU <ref type="bibr" target="#b29">[30]</ref> are commonly used datasets with characteristic biases. For the TUM dataset <ref type="bibr" target="#b36">[37]</ref>, we use the dynamic subset that features humans in indoor environments <ref type="bibr" target="#b37">[38]</ref>. Note that we never fine-tune models on any of these datasets. We refer to this experimental procedure as zero-shot cross-dataset transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">3D MOVIES</head><p>To complement the existing datasets we propose a new data source: 3D movies (MV). 3D movies feature high-quality video frames in a variety of dynamic environments that range from human-centric imagery in story-and dialogue-driven Hollywood films to nature scenes with landscapes and animals in documentary features. While the data does not provide metric depth, we can use stereo matching to obtain relative depth (similar to RW and WS). Our driving motivation is the scale and diversity of the data. 3D movies provide the largest known source of stereo pairs that were captured in carefully controlled conditions. This offers the possibility of tapping into millions of high-quality images from an ever-growing library of content. We note that 3D movies have been used in related tasks in isolation <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. We will show that their full potential is unlocked by combining them with other, complementary data sources. In contrast to similar data collections in the wild <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref>, no manual filtering of problematic content was required with this data source. Hence, the dataset can easily be extended or adapted to specific needs (e.g. focus on dancing humans or nature documentaries). Challenges. Movie data comes with its own challenges and imperfections. The primary objective when producing stereoscopic film is providing a visually pleasing viewing experience while avoiding discomfort for the viewer <ref type="bibr" target="#b50">[51]</ref>. This means that the disparity range for any given scene (also known as the depth budget) is limited and depends on both artistic and psychophysical considerations. For example, disparity ranges are often increased in the beginning and the end of a movie, in order to induce a very noticeable stereoscopic effect for a short time. Depth budgets in the middle may be lower to allow for more comfortable viewing. Stereographers thus adjust their depth budget depending on the content, transitions, and even the rhythm of scenes <ref type="bibr" target="#b51">[52]</ref>.</p><p>In consequence, focal lengths, baseline, and convergence angle between the cameras of the stereo rig are unknown and vary between scenes even within a single film. Furthermore, in contrast to image pairs obtained directly from a standard stereo camera, stereo pairs in movies usually contain both positive and negative disparities to allow objects to be perceived either in front of or behind the screen. Additionally, the depth that corresponds to the screen is scene-dependent and is often modified in postproduction by shifting the image pairs. We describe data extraction and training procedures that address these challenges.</p><p>Movie selection and preprocessing. We selected a diverse set of 23 movies. The selection was based on the following considerations: 1) We only selected movies that were shot using a physical stereo camera. (Some 3D films are shot with a monocular camera and the stereoscopic effect is added in post-production by artists.) 2) We tried to balance realism and diversity. 3) We only selected movies that are available in Blu-ray format and thus allow extraction of high-resolution images.</p><p>We extract stereo image pairs at 1920x1080 resolution and 24 frames per second (fps). Movies have varying aspect ratios, resulting in black bars on the top and bottom of the frame, and some movies have thin black bars along frame boundaries due to post-production. We thus center-crop all frames to 1880x800 pixels. We use the chapter information (Blu-ray meta-data) to split each movie into individual chapters. We drop the first and last chapters since they usually include the introduction and credits.</p><p>We use the scene detection tool of FFmpeg <ref type="bibr" target="#b52">[53]</ref> with a threshold of 0.1 to extract individual clips. We discard clips that are shorter than one second to filter out chaotic action scenes and highly correlated clips that rapidly switch between protagonists during dialogues. To balance scene diversity, we sample the first 24 frames of each clip and additionally sample 24 frames every four seconds for longer clips. Since multiple frames are part of the same clip, the complete dataset is highly correlated. Hence, we further subsample the training set at 4 fps and the test and validation sets at 1 fps.</p><p>Disparity extraction. The extracted image pairs can be used to estimate disparity maps using stereo matching. Unfortunately, state-of-the-art stereo matchers perform poorly when applied to movie data, since the matchers were designed and trained to match only over positive disparity ranges. This assumption is appropriate for the rectified output of a standard stereo camera, but not to image pairs extracted from stereoscopic film. Moreover, disparity ranges encountered in 3D movies are usually smaller than ranges that are common in standard stereo setups due to the limited depth budget.</p><p>To alleviate these problems, we apply a modern optical flow algorithm <ref type="bibr" target="#b53">[54]</ref> to the stereo pairs. We retain the horizontal component of the flow as a proxy for disparity. Optical flow algorithms naturally handle both positive and negative disparities and usually perform well for displacements of moderate size. For each stereo pair we use the left camera as the reference and extract the optical flow from the left to the right image and vice versa. We perform a left-right consistency check and mark pixels with a disparity difference of more than 2 pixels as invalid. We automatically filter out frames of bad disparity quality following the guidelines of Wang et al. <ref type="bibr" target="#b32">[33]</ref>: frames are rejected if more than 10% of all pixels have a vertical disparity &gt;2 pixels, the horizontal disparity range is &lt;10 pixels, or the percentage of pixels passing the left-right consistency check is &lt;70%. In a final step, we detect pixels that belong to sky regions using a pre-trained semantic segmentation model <ref type="bibr" target="#b54">[55]</ref> and set their disparity to the minimum disparity in the image.</p><p>The complete list of selected movies together with the number of frames that remain after filtering with the automatic cleaning pipeline is shown in <ref type="table" target="#tab_1">Table 2</ref>. Note that discrepancies in the number of extracted frames per movie occur due to varying runtimes as well as varying disparity quality. We use frames from 19 movies for training and set aside two movies for validation and two movies for testing, respectively. Example frames from the resulting dataset are shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TRAINING ON DIVERSE DATA</head><p>Training models for monocular depth estimation on diverse datasets presents a challenge because the ground truth comes in different forms (see <ref type="table" target="#tab_0">Table 1</ref>). It may be in the form of absolute depth (from laser-based measurements or stereo cameras with known calibration), depth up to an unknown scale (from SfM), or disparity maps (from stereo cameras with unknown calibration). The main requirement for a sensible training scheme is to carry out computations in an appropriate output space that is compatible with all ground-truth representations and is numerically wellbehaved. We further need to design a loss function that is flexible enough to handle diverse sources of data while making optimal use of all available information.</p><p>We identify three major challenges. 1) Inherently different representations of depth: direct vs. inverse depth representations. 2) Scale ambiguity: for some data sources, depth is only given up to an unknown scale. 3) Shift ambiguity: some datasets provide disparity only up to an unknown scale and global disparity shift that is a function of the unknown baseline and a horizontal shift of the principal points due to post-processing <ref type="bibr" target="#b32">[33]</ref>.</p><p>Scale-and shift-invariant losses. We propose to perform prediction in disparity space (inverse depth up to scale and shift) together with a family of scale-and shift-invariant dense losses to handle the aforementioned ambiguities. Let M denote the number of pixels in an image with valid ground truth and let ? be the parameters of the prediction model. Let d = d(?) ? R M be a disparity prediction and let d * ? R M be the corresponding ground-truth disparity. Individual pixels are indexed by subscripts.</p><p>We define the scale-and shift-invariant loss for a single sample as</p><formula xml:id="formula_1">L ssi (d,d * ) = 1 2M M i=1 ? d i ?d * i ,<label>(1)</label></formula><p>whered andd * are scaled and shifted versions of the predictions and ground truth, and ? defines the specific type of loss function. Let s : R M ? R + and t : R M ? R denote estimators of the scale and translation. To define a meaningful scale-and shift-invariant loss, a sensible requirement is that prediction and ground truth should be appropriately aligned with respect to their scale and shift, i.e. we need to ensure that s(d) ? s(d * ) and t(d) ? t(d * ). We propose two different strategies for performing this alignment.</p><p>The first approach aligns the prediction to the ground truth based on a least-squares criterion:</p><formula xml:id="formula_2">(s, t) = arg min s,t M i=1 (sd i + t ? d * i ) 2 , d = sd + t,d * = d * ,<label>(2)</label></formula><p>whered andd * are the aligned prediction and ground truth, respectively. The factors s and t can be efficiently determined in closed form by rewriting (2) as a standard least-squares problem: Let d i = (d i , 1) and h = (s, t) , then we can rewrite the objective as</p><formula xml:id="formula_3">h opt = arg min h M i=1 d i h ? d * i 2 ,<label>(3)</label></formula><p>which has the closed-form solution</p><formula xml:id="formula_4">h opt = M i=1 d i d i ?1 M i=1 d i d * i .<label>(4)</label></formula><p>We set ?(x) = ? mse (x) = x 2 to define the scale-and shiftinvariant mean-squared error (MSE). We denote this loss as L ssimse . The MSE is not robust to the presence of outliers. Since all existing large-scale datasets only provide imperfect ground truth, we conjecture that a robust loss function can improve training. We thus define alternative, robust loss functions based on robust estimators of scale and shift:</p><formula xml:id="formula_5">t(d) = median(d), s(d) = 1 M M i=1 |d ? t(d)|.<label>(5)</label></formula><p>We align both the prediction and the ground truth to have zero translation and unit scale:</p><formula xml:id="formula_6">d = d ? t(d) s(d) ,d * = d * ? t(d * ) s(d * ) .<label>(6)</label></formula><p>We define two robust losses. The first, which we denote as L ssimae , measures the absolute deviations ? mae (x) = |x|. We define the second robust loss by trimming the 20% largest residuals in every image, irrespective of their magnitude:</p><formula xml:id="formula_7">L ssitrim (d,d * ) = 1 2M Um j=1 ? mae d j ?d * j ,<label>(7)</label></formula><p>with |d j ?d * j | ? |d j+1 ?d * j+1 | and U m = 0.8M (set empirically based on experiments on the ReDWeb dataset). Note that this is in contrast to commonly used M-estimators, where the influence of large residuals is merely down-weighted. Our reasoning for trimming is that outliers in the ground truth should never influence training. Related loss functions. The importance of accounting for unknown or varying scale in the training of monocular depth estimation models has been recognized early. Eigen et al. <ref type="bibr" target="#b14">[15]</ref> proposed a scale-invariant loss in log-depth space. Their loss can be written as</p><formula xml:id="formula_8">L silog (z, z * ) = min s 1 2M M i=1 log(e s z i ) ? log(z * i ) 2 ,<label>(8)</label></formula><p>where z i = d ?1 i and z * i = (d * i ) ?1 are depths up to unknown scale. Both <ref type="bibr" target="#b7">(8)</ref> and L ssimse account for the unknown scale of the predictions, but only L ssimse accounts for an unknown global disparity shift. Moreover, the losses are evaluated on different depth representations. Our loss is defined in disparity space, which is numerically stable and compatible with common representations of relative depth.</p><p>Chen et al. <ref type="bibr" target="#b33">[34]</ref> proposed a generally applicable loss for relative depth estimation based on ordinal relations:</p><formula xml:id="formula_9">? ord (z i ? z j ) = log(1 + exp(?(z i ? z j )l ij ), l ij = 0 (z i ? z j ) 2 , l ij = 0,<label>(9)</label></formula><p>where l ij ? {?1, 0, 1} encodes the ground-truth ordinal relation of point pairs. This encourages pushing points as far apart as possible when l ij = 0 and pulling them to the same depth when l ij = 0.</p><p>Xian et al. <ref type="bibr" target="#b31">[32]</ref> suggest to sparsely evaluate this loss by randomly sampling point pairs from the dense ground truth. In contrast, our proposed losses take all available data into account.</p><p>Recently, Wang et al. <ref type="bibr" target="#b32">[33]</ref> proposed the normalized multiscale gradient (NMG) loss. To achieve shift invariance in addition to scale invariance in disparity space, they evaluate the gradient difference between ground-truth and rescaled estimates at multiple scales k:</p><formula xml:id="formula_10">L nmg (d, d * ) = K k=1 M i=1 |s? k x d ? ? k x d * | + |s? k y d ? ? k y d * |.<label>(10)</label></formula><p>In contrast, our losses are evaluated directly on the ground-truth disparity values, while also accounting for unknown scale and shift. While both the ordinal loss and NMG can, conceptually, be applied to arbitrary depth representations and are thus suited for mixing diverse datasets, we will show that our scale-and shiftinvariant loss variants lead to consistently better performance.</p><p>Final loss. To define the complete loss, we adapt the multiscale, scale-invariant gradient matching term <ref type="bibr" target="#b10">[11]</ref> to the disparity space. This term biases discontinuities to be sharp and to coincide with discontinuities in the ground truth. We define the gradient matching term as</p><formula xml:id="formula_11">L reg (d,d * ) = 1 M K k=1 M i=1 |? x R k i | + |? y R k i | ,<label>(11)</label></formula><p>where R i =d i ?d * i , and R k denotes the difference of disparity maps at scale k. We use K = 4 scale levels, halving the image resolution at each level. Note that this term is similar to L nmg , but with different approaches to compute the scaling s.</p><p>Our final loss for a training set l is <ref type="bibr" target="#b11">(12)</ref> where N l is the training set size and ? is set to 0.5. Mixing strategies. While our loss and choice of prediction space enable mixing datasets, it is not immediately clear in what proportions different datasets should be integrated during training with a stochastic optimization algorithm. We explore two different strategies in our experiments. The first, naive strategy is to mix datasets in equal parts in each minibatch. For a minibatch of size B, we sample B/L training samples from each dataset, where L denotes the number of distinct datasets. This strategy ensures that all datasets are represented equally in the effective training set, regardless of their individual size.</p><formula xml:id="formula_12">L l = 1 N l N l n=1 L ssi d n , (d * ) n + ? L reg d n , (d * ) n ,</formula><p>Our second strategy explores a more principled approach, where we adapt a recent procedure for Pareto-optimal multi-task learning to our setting <ref type="bibr" target="#b11">[12]</ref>. We define learning on each dataset as a separate task and seek an approximate Pareto optimum over datasets (i.e. a solution where the loss cannot be decreased on any training set without increasing it for at least one of the others). Formally, we use the algorithm presented in <ref type="bibr" target="#b11">[12]</ref> to minimize the multi-objective optimization criterion</p><formula xml:id="formula_13">min ? L 1 (?), . . . , L L (?) ,<label>(13)</label></formula><p>where model parameters ? are shared across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We start from the experimental setup of Xian et al. <ref type="bibr" target="#b31">[32]</ref> and use their ResNet-based <ref type="bibr" target="#b55">[56]</ref> multi-scale architecture for singleimage depth prediction. We initialize the encoder with pretrained ImageNet <ref type="bibr" target="#b56">[57]</ref> weights and initialize other layers randomly. We use Adam <ref type="bibr" target="#b57">[58]</ref> with a learning rate of 10 ?4 for randomly initialized layers and 10 ?5 for pretrained layers, and set the exponential decay rate to ? 1 = 0.9 and ? 2 = 0.999. Images are flipped horizontally with a 50% chance, and randomly cropped and resized to 384 ? 384 to augment the data and maintain the aspect ratio across different input images. No other augmentations are used. Subsequently, we perform ablation studies on the loss function and, since we conjecture that pretraining on ImageNet data has significant influence on performance, also the encoder architecture. We use the best-performing pretrained model as the starting point for our dataset mixing experiments. We use a batch size of 8L, i.e. when mixing three datasets the batch size is 24. When comparing datasets of different sizes, the term epoch is not well-defined; we thus denote an epoch as processing 72,000 images, roughly the size of MD and MV, and train for 60 epochs. We shift and scale the ground-truth disparity to the range [0, 1] for all datasets. Test datasets and metrics. For ablation studies of loss and encoders, we use our held-out validation sets of RW (360 images), MD (2,963 images -official validation set), and MV (3,058 images -see <ref type="table" target="#tab_1">Table 2</ref>). For all training dataset mixing experiments and comparisons to the state of the art, we test on a collection of datasets that were never seen during training: DIW, ETH3D, Sintel, KITTI, NYU, and TUM. For DIW <ref type="bibr" target="#b33">[34]</ref> we created a validation set of 10,000 images from the DIW training set for our ablation studies and used the official test set of 74,441 images when comparing to the state of the art. For NYU we used the official test split (654 images). For KITTI we used the intersection of the official validation set for depth estimation (with improved ground-truth depth <ref type="bibr" target="#b58">[59]</ref>) and the Eigen test split <ref type="bibr" target="#b59">[60]</ref> (161 images). For ETH3D and Sintel we used the whole dataset for which ground truth is available (454 and 1,064 images, respectively). For the TUM dataset, we use the dynamic subset that features humans in indoor environments <ref type="bibr" target="#b37">[38]</ref> (1,815 images).</p><p>For each dataset, we use a single metric that fits the ground truth in that dataset. For DIW we use the Weighted Human Disagreement Rate (WHDR) <ref type="bibr" target="#b33">[34]</ref>. For datasets that are based on relative depth, we measure the root mean squared error in disparity space (MV, RW, MD). For datasets that provide accurate absolute depth (ETH3D, Sintel), we measure the mean absolute value of the relative error  RW MD MV mean <ref type="figure">Fig. 4</ref>. Relative performance of different encoders across datasets (higher is better). ImageNet performance of an encoder is predictive of its performance in monocular depth estimation.</p><formula xml:id="formula_14">(1/M ) M i=1 |z i ? z * i | /z * i in depth space (AbsRel</formula><p>maximum value for datasets that are evaluated in depth space. For ETH3D, KITTI, NYU, and TUM, the depth cap was set to the maximum ground-truth depth value (72, 80, 10, and 10 meters, respectively). For Sintel, we evaluate on areas with ground-truth depth below 72 meters and accordingly use a depth cap of 72 meters. For all our models and baselines, we align predictions and ground truth in scale and shift for each image before measuring errors. We perform the alignment in inverse-depth space based on the least-squares criterion. Since absolute numbers quickly become hard to interpret when evaluating on multiple datasets, we also present the relative change in performance compared to an appropriate baseline method. Input resolution for evaluation. We resize test images so that the larger axis equals 384 pixels while the smaller axis is resized to a multiple of 32 pixels (a constraint imposed by the encoder), while keeping an aspect ratio as close as possible to the original aspect ratio. Due to the wide aspect ratio in KITTI this strategy would lead to very small input images. We thus resize the smaller axis to be equal to 384 pixels on this dataset and adopt the same strategy otherwise to maintain the aspect ratio. Most state-of-the-art methods that we compare to are specialized to a specific dataset (with fixed image dimensions) and thus did not specify how to handle different image sizes and aspect ratios during inference. We tried to find the best-performing setting for all methods, following their evaluation scripts and training dimensions. For approaches trained on square patches <ref type="bibr" target="#b31">[32]</ref>, we follow our setup and set the larger axis to the training image axis length and adapt the smaller one, keeping the aspect ratio as close as possible to the original. For approaches with nonsquare patches <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b37">[38]</ref> we fix the smaller axis to the smaller training image axis dimension. For DORN <ref type="bibr" target="#b18">[19]</ref> we followed their tiling protocol, resizing the images to the dimensions stated for their NYU and KITTI evaluation, respectively. For Monodepth2 <ref type="bibr" target="#b23">[24]</ref> and Struct2Depth <ref type="bibr" target="#b26">[27]</ref>, which were both trained on KITTI and thus expect a very wide aspect ratio, we pad the input image using reflection padding to obtain the same aspect ratio, resize to their specific input dimension, and crop the resulting prediction to the original target dimensions. For methods where model weights were available for different training resolutions we evaluated all of them and report numbers for the best-performing variant.</p><p>All predictions were rescaled to the resolution of the ground truth for evaluation. Comparison of loss functions. We show the effect of different loss functions on the validation performance in <ref type="figure" target="#fig_1">Figure 3</ref>. We used RW to train networks with different losses. For the ordinal loss (cf. Equation <ref type="formula" target="#formula_9">(9)</ref>), we sample 5,000 point pairs randomly <ref type="bibr" target="#b31">[32]</ref>. Where appropriate, we combine losses with the gradient regularization term <ref type="bibr" target="#b10">(11)</ref>. We also test a scale-invariant, but not shift-invariant, MSE in disparity space L simse by fixing t = 0 in (1). The model trained with L ord corresponds to our reimplementation of Xian et al. <ref type="bibr" target="#b31">[32]</ref>. <ref type="figure" target="#fig_1">Figure 3</ref> shows that our proposed trimmed MAE loss yields the lowest validation error over all datasets. We thus conduct all experiments that follow using L ssitrim + L reg . Comparison of encoders. We evaluate the influence of the encoder architecture in <ref type="figure">Figure 4</ref>. We define the model with a ResNet-50 <ref type="bibr" target="#b55">[56]</ref> encoder as used originally by Xian et al. <ref type="bibr" target="#b31">[32]</ref> as our baseline and show the relative improvement in performance when swapping in different encoders (higher is better). We tested ResNet-101, ResNeXt-101 <ref type="bibr" target="#b60">[61]</ref> and DenseNet-161 <ref type="bibr" target="#b61">[62]</ref>. All encoders were pretrained on ImageNet <ref type="bibr" target="#b56">[57]</ref>. For ResNeXt-101, we additionally use a variant that was pretrained with a massive corpus of weakly-supervised data (WSL) <ref type="bibr" target="#b62">[63]</ref> before training on ImageNet. All models were fine-tuned on RW.</p><p>We observe that a significant performance boost is achieved by using better encoders. Higher-capacity encoders perform better than the baseline. The ResNeXt-101 encoder that was pretrained on weakly-supervised data performs significantly better than the same encoder that was only trained on ImageNet. We found pretraining to be crucial. A network with a ResNet-50 encoder with random initialization performs on average 35% worse than its pretrained counterpart. In general, we find that ImageNet performance of an encoder is a strong predictor for its performance in monocular depth estimation. This is encouraging, since advancements made in image classification can directly yield gains in robust monocular depth estimation. The performance gain over the baseline is remarkable: up to 15 % relative improvement, without any task-specific adaptations. We use ResNeXt-101-WSL for all subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 3</head><p>Relative performance with respect to the baseline in percent when fine-tuning on different single training sets (higher is better). Performance better than the baseline in green, worse performance in red. Best performance is bold, second best is underlined. The absolute errors of the RW baseline are shown on the top row. While some datasets provide better performance on individual, similar datasets, average performance for zero-shot cross-dataset transfer degrades.   Training on diverse datasets. We evaluate the usefulness of different training datasets for generalization in <ref type="table">Table 3</ref> and <ref type="table" target="#tab_4">Table 4</ref>. While more specialized datasets reach better performance on similar test sets (DL for indoor scenes or MD for ETH3D), performance on the remaining datasets declines. Interestingly, every single dataset used in isolation leads to worse generalization performance on average than just using the small, but curated, RW dataset, i.e. the gains on compatible datasets are offset on average by the decrease on the other datasets. The difference in performance for RW, MV, and WS is especially interesting since they have similar characteristics. Although substantially larger than RW, both MV and WS show worse individual performance. This could be explained partly by redundant data due to the video nature of these datasets and possibly more rigorous filtering in RW (human experts pruned samples that had obvious flaws). Comparing WS and MV, we see that MV leads to more general models, likely because of higher-quality stereo pairs due to the more controlled nature of the images.</p><p>For our subsequent mixing experiments, we use <ref type="table">Table 3</ref> as reference, i.e. we start with the best performing individual training dataset and consecutively add datasets to the mix. We show which datasets are included in the individual training sets in <ref type="table" target="#tab_5">Table 5</ref>. To better understand the influence of the Movies dataset, we additionally show results where we train on all datasets except Movies (MIX 4). We always start training from the pretrained RW    baseline. <ref type="table" target="#tab_6">Tables 6 and 7</ref> show that, in contrast to using individual datasets, mixing multiple training sets consistently improves performance with respect to the baseline. However, we also see that adding datasets does not unconditionally improve performance when naive mixing is used (see MIX 1 vs. <ref type="bibr">MIX 2)</ref>. <ref type="table" target="#tab_8">Tables 8  and 9</ref> report the results of an analogous experiment with Paretooptimal dataset mixing. We observe that this approach improves over the naive mixing strategy. It is also more consistently able to leverage additional datasets. Combining all five datasets with Pareto-optimal mixing yields our best-performing model. We show a qualitative comparison of the resulting models in <ref type="figure">Figure 5</ref>.</p><p>Comparison to the state of the art. We compare our bestperforming model to various state-of-the-art approaches in <ref type="table" target="#tab_0">Table 10 and Table 11</ref>. The top part of each table compares to baselines that were not fine-tuned on any of the evaluated datasets (i.e. zero-shot transfer, akin to our model). The bottom parts show baselines that were fine-tuned on a subset of the datasets for reference. In the training set column, MC refers to Mannequin Challenge <ref type="bibr" target="#b37">[38]</ref> and CS to Cityscapes <ref type="bibr" target="#b44">[45]</ref>. A ? B indicates pretraining on A and fine-tuning on B.</p><p>Our model outperforms the baselines by a comfortable margin in terms of zero-shot performance. Note that our model outperforms the Mannequin Challenge model of Li et al. <ref type="bibr" target="#b37">[38]</ref> on a subset of the TUM dataset that was specifically curated by Li et al. to showcase the advantages of their model. We show additional results on a variant of our model that has a smaller encoder based on ResNet-50 (Ours -small). This architecture is equivalent to the network proposed by Xian et al. <ref type="bibr" target="#b31">[32]</ref>. The smaller model also outperforms the state of the art by a comfortable margin. This shows that the strong performance of our model is not only due to increased network capacity, but fundamentally due to the proposed training scheme.</p><p>Some models that were trained for one specific dataset (e.g. KITTI or NYU in the lower part of the table) perform very well on those individual datasets but perform significantly worse on all other test sets. Fine-tuning on individual datasets leads to strong priors about specific environments. This can be desirable in some applications, but is ill-suited if the model needs to generalize. A qualitative comparison of our model to the four best-performing competitors is shown in <ref type="figure">Figure 6</ref>. Additional qualitative results. <ref type="figure" target="#fig_3">Figure 7</ref> shows additional qualitative results on the DIW test set <ref type="bibr" target="#b33">[34]</ref>. We show results on a diverse set of input images depicting various objects and scenes, including humans, mammals, birds, cars, and other man-made and natural objects. The images feature indoor, street and nature scenes, various lighting conditions, and various camera angles. Additionally, subject areas vary from close-up to long-range shots.</p><p>We show qualitative results on the DAVIS video dataset <ref type="bibr" target="#b63">[64]</ref> in our supplementary video https://youtu.be/D46FzVyL9I8. Note that every frame was processed individually, i.e. no temporal information was used in any way. For each clip, the inverse depth maps were jointly scaled and shifted for visualization. The dataset consists of a diverse set of videos and includes humans, animals, and cars in action. This dataset was filmed with monocular cameras, hence no ground-truth depth information is available.</p><p>Hertzmann <ref type="bibr" target="#b64">[65]</ref> recently observed that our publicly available Input Xian et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[32]</head><p>Wang et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[33]</head><p>Li et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[38]</head><p>Li &amp; Snavely <ref type="bibr" target="#b10">[11]</ref> Ours <ref type="figure">Fig. 6</ref>. Qualitative comparison of our approach to the four best competitors on images from the Microsoft COCO dataset <ref type="bibr" target="#b4">[5]</ref>.</p><p>model provides plausible results even on abstract line drawings. Similarly, we show results on drawings and paintings with different levels of abstraction in <ref type="figure">Figure 8</ref>. We can qualitatively confirm the findings in <ref type="bibr" target="#b64">[65]</ref>: The model shows a surprising capability to estimate plausible relative depth even on relatively abstract inputs. This seems to be true as long as some (coarse) depth cues such as shading or vanishing points are present in the artwork.</p><p>Failure cases. We identify common failure cases and biases of our model. Images have a natural bias where the lower parts of the image are closer to the camera than the higher image regions. When randomly sampling two points and classifying the lower point as closer to the camera, <ref type="bibr" target="#b33">[34]</ref> achieved an agreement rate of 85.8% with human annotators. This bias has also been learned by our network and can be observed in some extreme cases that are shown in the first row of <ref type="figure">Figure 9</ref>. In the example on the left, the model fails to recover the ground plane, likely because the input image was rotated by 90 degrees. In the right image, pellets at approximately the same distance to the camera are reconstructed closer to the camera in the lower part of the image. Such cases could be prevented by augmenting training data with rotated images. However, it is not clear if invariance to image rotations is a desired property for this task.</p><p>Another interesting failure case is shown in the second row of <ref type="figure">Figure 9</ref>. Paintings, photos, and mirrors are often not recognized as such. The network estimates depth based on the content that is depicted on the reflector rather than predicting the depth of the reflector itself.</p><p>Additional failure cases are shown in the remaining rows. Strong edges can lead to hallucinated depth discontinuities. Thin structures can be missed and relative depth arrangement between disconnected objects might fail in some situations. Results tend to get blurred in background regions, which might be explained by the limited resolution of the input images and imperfect ground truth in the far range.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>The success of deep networks has been driven by massive datasets. For monocular depth estimation, we believe that existing datasets are still insufficient and likely constitute the limiting factor. Motivated by the difficulty of capturing diverse depth datasets at scale, we have introduced tools for combining complementary sources of data. We have proposed a flexible loss function and a principled dataset mixing strategy. We have further introduced a dataset based on 3D movies that provides dense ground truth for diverse dynamic scenes.</p><p>We have evaluated the robustness and generality of models via zero-shot cross-dataset transfer. We find that systematically testing models on datasets that were never seen during training is a better proxy for their performance "in the wild" than testing on a heldout portion of even the most diverse datasets that are currently available.</p><p>Our work advances the state of the art in generic monocular depth estimation and indicates that the presented ideas substantially improve performance across diverse environments. We hope that this work will contribute to the deployment of monocular depth models that meet the requirements of practical applications. Our models are freely available at https://github.com/intel-isl/MiDaS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 10</head><p>Relative performance of state of the art methods with respect to our best model (top row) -higher is better. Top: models that were not fine-tuned on any of the datasets. Bottom: models that were fine-tuned on a subset of the tested datasets.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Sample images from the 3D Movies dataset. We show images from some of the films in the training set together with their inverse depth maps. Sky regions and invalid pixels are masked out. Each image is taken from a different film. 3D movies provide a massive source of diverse data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>g + r e g n m g s im s e + r e g s s im s e + r e g s s im a e + r e Relative performance of different loss functions (higher is better) with the best performing loss L ssitrim + Lreg used as reference. All our four proposed losses (white area) outperform current state-of-theart losses (gray area).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Qualitative results on the DIW test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Results on paintings and drawings. Top row: A Friend in Need, Cassius Marcellus Coolidge, and Bathers at Asni?res, Georges Pierre Seurat. Bottom row: Mittagsrast, Vincent van Gogh, and Vector drawing of central street of old european town, Vilnius, @Misha Failure cases. Subtle failures in relative depth arrangement or missing details are highlighted in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Datasets used in our work. Top: Our training sets. Bottom: Our test sets. No single real-world dataset features a large number of diverse scenes with dense and accurate ground truth.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Indoor Outdoor Dynamic Video Dense Accuracy Diversity Annotation</cell><cell>Depth</cell><cell># Images</cell></row><row><cell cols="2">DIML Indoor [31]</cell><cell cols="2">Medium Medium</cell><cell>RGB-D</cell><cell>Metric</cell><cell>220K</cell></row><row><cell>MegaDepth [11]</cell><cell>()</cell><cell cols="2">() Medium Medium</cell><cell>SfM</cell><cell>No scale</cell><cell>130K</cell></row><row><cell>ReDWeb [32]</cell><cell></cell><cell>Medium</cell><cell>High</cell><cell>Stereo</cell><cell>No scale &amp; shift</cell><cell>3600</cell></row><row><cell>WSVD [33]</cell><cell></cell><cell>Medium</cell><cell>High</cell><cell>Stereo</cell><cell cols="2">No scale &amp; shift 1.5M</cell></row><row><cell>3D Movies</cell><cell></cell><cell>Medium</cell><cell>High</cell><cell>Stereo</cell><cell>No scale &amp; shift</cell><cell>75K</cell></row><row><cell>DIW [34]</cell><cell></cell><cell>Low</cell><cell>High</cell><cell>User clicks</cell><cell>Ordinal pair</cell><cell>496K</cell></row><row><cell>ETH3D [35]</cell><cell></cell><cell>High</cell><cell>Low</cell><cell>Laser</cell><cell>Metric</cell><cell>454</cell></row><row><cell>Sintel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>List of films and the number of extracted frames in the 3D Movies dataset after automatic processing.</figDesc><table><row><cell>Movie title</cell><cell># frames</cell></row><row><cell>Training set</cell><cell>75074</cell></row><row><cell>Battle of the Year (2013)</cell><cell>4821</cell></row><row><cell>Billy Lynn's Long Halftime Walk (2016)</cell><cell>4178</cell></row><row><cell>Drive Angry (2011)</cell><cell>328</cell></row><row><cell>Exodus: Gods and Kings (2014)</cell><cell>8063</cell></row><row><cell>Final Destination 5 (2011)</cell><cell>1437</cell></row><row><cell>A very Harold &amp; Kumar 3D Christmas (2011)</cell><cell>3690</cell></row><row><cell>Hellbenders (2012)</cell><cell>120</cell></row><row><cell>The Hobbit: An Unexpected Journey (2012)</cell><cell>8874</cell></row><row><cell>Hugo (2011)</cell><cell>3189</cell></row><row><cell>The Three Musketeers (2011)</cell><cell>5028</cell></row><row><cell>Nurse 3D (2013)</cell><cell>492</cell></row><row><cell>Pina (2011)</cell><cell>1215</cell></row><row><cell>Dawn of the Planet of the Apes (2014)</cell><cell>5571</cell></row><row><cell>The Amazing Spider-Man (2012)</cell><cell>5618</cell></row><row><cell>Step Up 3D (2010)</cell><cell>509</cell></row><row><cell>Step Up: All In (2014)</cell><cell>2187</cell></row><row><cell>Transformers: Age of Extinction (2014)</cell><cell>8740</cell></row><row><cell>Le Dernier Loup / Wolf Totem (2015)</cell><cell>4843</cell></row><row><cell>X-Men: Days of Future Past (2014)</cell><cell>6171</cell></row><row><cell>Validation set</cell><cell>3058</cell></row><row><cell>The Great Gatsby (2013)</cell><cell>1815</cell></row><row><cell>Step Up: Miami Heat / Revolution (2012)</cell><cell>1243</cell></row><row><cell>Test set</cell><cell>788</cell></row><row><cell>Doctor Who -The Day of the Doctor (2013)</cell><cell>508</cell></row><row><cell>StreetDance 2 (2012)</cell><cell>280</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Absolute performance when fine-tuning on different single training sets -lower is better.This table corresponds to Table 3.</figDesc><table><row><cell></cell><cell>DIW</cell><cell>ETH3D</cell><cell>Sintel</cell><cell>KITTI</cell><cell>NYU</cell><cell>TUM</cell></row><row><cell></cell><cell>WHDR</cell><cell>AbsRel</cell><cell cols="4">AbsRel ?&gt;1.25 ?&gt;1.25 ?&gt;1.25</cell></row><row><cell>RW ? RW</cell><cell>14.59</cell><cell>0.151</cell><cell>0.349</cell><cell>27.95</cell><cell>18.74</cell><cell>21.69</cell></row><row><cell>RW ? DL</cell><cell>20.08</cell><cell>0.148</cell><cell>0.364</cell><cell>48.35</cell><cell>12.68</cell><cell>17.48</cell></row><row><cell>RW ? MV</cell><cell>18.39</cell><cell>0.175</cell><cell>0.403</cell><cell>25.12</cell><cell>20.65</cell><cell>22.44</cell></row><row><cell>RW ? MD</cell><cell>19.18</cell><cell>0.145</cell><cell>0.383</cell><cell>34.73</cell><cell>19.05</cell><cell>32.96</cell></row><row><cell>RW ? WS</cell><cell>19.31</cell><cell>0.196</cell><cell>0.359</cell><cell>37.59</cell><cell>24.72</cell><cell>20.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Combinations of datasets used for training.</figDesc><table><row><cell>Mix</cell><cell>RW DL MV MD WS</cell></row><row><cell>MIX 1</cell><cell></cell></row><row><cell>MIX 2</cell><cell></cell></row><row><cell>MIX 3</cell><cell></cell></row><row><cell>MIX 4</cell><cell></cell></row><row><cell>MIX 5</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6 Relative</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="7">performance of naive dataset mixing with respect to the RW</cell></row><row><cell cols="8">baseline (top row) -higher is better. While we usually see an</cell></row><row><cell cols="8">improvement when adding datasets, adding datasets can hurt</cell></row><row><cell></cell><cell cols="6">generalization performance with naive mixing.</cell><cell></cell></row><row><cell></cell><cell>DIW</cell><cell cols="3">ETH3D Sintel KITTI</cell><cell>NYU</cell><cell>TUM</cell><cell>Mean [%]</cell></row><row><cell>RW</cell><cell>14.6</cell><cell>0.2</cell><cell>0.3</cell><cell>28.0</cell><cell>18.7</cell><cell>21.7</cell><cell>-</cell></row><row><cell>MIX 1</cell><cell>10.9</cell><cell>9.9</cell><cell>?3.7</cell><cell>18.0</cell><cell>41.4</cell><cell>33.0</cell><cell>18.3</cell></row><row><cell>MIX 2</cell><cell>6.7</cell><cell>8.6</cell><cell>3.2</cell><cell>9.2</cell><cell>40.8</cell><cell>35.7</cell><cell>17.3</cell></row><row><cell>MIX 3</cell><cell>13.5</cell><cell>10.6</cell><cell>4.9</cell><cell>13.9</cell><cell>43.8</cell><cell>29.1</cell><cell>19.3</cell></row><row><cell>MIX 4</cell><cell>11.7</cell><cell>11.3</cell><cell>5.2</cell><cell>11.3</cell><cell>38.8</cell><cell>35.5</cell><cell>19.0</cell></row><row><cell>MIX 5</cell><cell>12.3</cell><cell>12.6</cell><cell>7.2</cell><cell>9.1</cell><cell>38.5</cell><cell>37.2</cell><cell>19.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7</head><label>7</label><figDesc>Absolute performance of naive dataset mixing -lower is better. This table corresponds to Table 6.</figDesc><table><row><cell></cell><cell>DIW</cell><cell>ETH3D</cell><cell>Sintel</cell><cell>KITTI</cell><cell>NYU</cell><cell>TUM</cell></row><row><cell></cell><cell>WHDR</cell><cell>AbsRel</cell><cell cols="4">AbsRel ?&gt;1.25 ?&gt;1.25 ?&gt;1.25</cell></row><row><cell>RW</cell><cell>14.59</cell><cell>0.151</cell><cell>0.349</cell><cell>27.95</cell><cell>18.74</cell><cell>21.69</cell></row><row><cell>MIX 1</cell><cell>13.00</cell><cell>0.136</cell><cell>0.362</cell><cell>22.91</cell><cell>10.98</cell><cell>14.53</cell></row><row><cell>MIX 2</cell><cell>13.62</cell><cell>0.138</cell><cell>0.338</cell><cell>25.39</cell><cell>11.10</cell><cell>13.94</cell></row><row><cell>MIX 3</cell><cell>12.62</cell><cell>0.135</cell><cell>0.332</cell><cell>24.06</cell><cell>10.54</cell><cell>15.39</cell></row><row><cell>MIX 4</cell><cell>12.88</cell><cell>0.134</cell><cell>0.331</cell><cell>24.78</cell><cell>11.46</cell><cell>14.00</cell></row><row><cell>MIX 5</cell><cell>12.79</cell><cell>0.132</cell><cell>0.324</cell><cell>25.41</cell><cell>11.52</cell><cell>13.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 8</head><label>8</label><figDesc>Relative performance of dataset mixing with multi-objective optimization with respect to the RW baseline (top row) -higher is better. Principled mixing dominates the solutions found by naive mixing.</figDesc><table><row><cell></cell><cell>DIW</cell><cell cols="3">ETH3D Sintel KITTI</cell><cell>NYU</cell><cell>TUM</cell><cell>Mean [%]</cell></row><row><cell>RW</cell><cell>14.6</cell><cell>0.2</cell><cell>0.3</cell><cell>28.0</cell><cell>18.7</cell><cell>21.7</cell><cell>-</cell></row><row><cell>MIX 1</cell><cell>9.4</cell><cell>7.3</cell><cell>?7.7</cell><cell>13.2</cell><cell>44.1</cell><cell>33.2</cell><cell>16.6</cell></row><row><cell>MIX 2</cell><cell>14.1</cell><cell>8.6</cell><cell>0.9</cell><cell>17.5</cell><cell>45.5</cell><cell>32.0</cell><cell>19.8</cell></row><row><cell>MIX 3</cell><cell>15.8</cell><cell>11.9</cell><cell>5.2</cell><cell>11.7</cell><cell>47.8</cell><cell>32.4</cell><cell>20.8</cell></row><row><cell>MIX 4</cell><cell>15.4</cell><cell>13.9</cell><cell>1.7</cell><cell>17.2</cell><cell>43.4</cell><cell>38.2</cell><cell>21.6</cell></row><row><cell>MIX 5</cell><cell>15.9</cell><cell>14.6</cell><cell>6.3</cell><cell>14.5</cell><cell>49.0</cell><cell>34.1</cell><cell>22.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 9</head><label>9</label><figDesc>Absolute performance of dataset mixing with multi-objective optimization -lower is better. Thistable corresponds to Table 8.</figDesc><table><row><cell></cell><cell>DIW</cell><cell>ETH3D</cell><cell>Sintel</cell><cell>KITTI</cell><cell>NYU</cell><cell>TUM</cell></row><row><cell></cell><cell>WHDR</cell><cell>AbsRel</cell><cell cols="4">AbsRel ?&gt;1.25 ?&gt;1.25 ?&gt;1.25</cell></row><row><cell>RW</cell><cell>14.59</cell><cell>0.151</cell><cell>0.349</cell><cell>27.95</cell><cell>18.74</cell><cell>21.69</cell></row><row><cell>MIX 1</cell><cell>13.22</cell><cell>0.140</cell><cell>0.376</cell><cell>24.26</cell><cell>10.48</cell><cell>14.50</cell></row><row><cell>MIX 2</cell><cell>12.54</cell><cell>0.138</cell><cell>0.346</cell><cell>23.05</cell><cell>10.21</cell><cell>14.76</cell></row><row><cell>MIX 3</cell><cell>12.29</cell><cell>0.133</cell><cell>0.331</cell><cell>24.68</cell><cell>9.78</cell><cell>14.66</cell></row><row><cell>MIX 4</cell><cell>12.35</cell><cell>0.130</cell><cell>0.343</cell><cell>23.13</cell><cell>10.61</cell><cell>13.41</cell></row><row><cell>MIX 5</cell><cell>12.27</cell><cell>0.129</cell><cell>0.327</cell><cell>23.90</cell><cell>9.55</cell><cell>14.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 11</head><label>11</label><figDesc>Absolute performance of state of the art methods, sorted by average rank. This table corresponds to Table 10. NYU 28.79 0.195 0.433 61.61 8.69 24.65 7.3 Chen [34] NYU DIW 14.47 0.221 0.440 36.30 28.33 30.16 8.5 Casser [27] KITTI 33.49 0.217 0.409 11.93 36.08 37.03 8.7 Fu [19] KITTI 30.39 0.216 0.432 7.13 40.61 40.13 9.2</figDesc><table><row><cell></cell><cell>Training sets</cell><cell cols="2">DIW ETH3D Sintel KITTI NYU TUM Rank</cell></row><row><cell></cell><cell></cell><cell>WHDR AbsRel AbsRel ?&gt;1.25 ?&gt;1.25 ?&gt;1.25</cell><cell></cell></row><row><cell>Ours</cell><cell>MIX 5</cell><cell>12.46 0.129 0.327 23.90 9.55 14.29</cell><cell>2.0</cell></row><row><cell>Ours -small</cell><cell>MIX 5</cell><cell>12.48 0.155 0.330 21.81 15.73 17.00</cell><cell>2.7</cell></row><row><cell>Li [11]</cell><cell>MD</cell><cell>23.15 0.181 0.385 36.29 27.52 29.54</cell><cell>5.7</cell></row><row><cell>Li [38]</cell><cell>MC</cell><cell>26.52 0.183 0.405 47.94 18.57 17.71</cell><cell>5.7</cell></row><row><cell>Wang [33]</cell><cell>WS</cell><cell>19.09 0.205 0.390 31.92 29.57 20.18</cell><cell>6.0</cell></row><row><cell>Xian [32]</cell><cell>RW</cell><cell>14.59 0.186 0.422 34.08 27.00 25.02</cell><cell>6.1</cell></row><row><cell>Casser [27]</cell><cell>CS</cell><cell>32.80 0.235 0.422 21.15 39.58 37.18</cell><cell>9.6</cell></row><row><cell>Godard [24]</cell><cell>KITTI</cell><cell>29.67 0.189 0.406 5.53 33.29 36.03</cell><cell>6.7</cell></row><row><cell>Fu [19]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Does computer vision matter for action?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">30</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic photo pop-up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Make3D: Learning 3D scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Open3D: A modern library for 3D data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accuracy and resolution of Kinect depth data for indoor mapping applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Khoshelham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Elberink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hansard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<title level="m">Time-of-Flight Cameras: Principles, Methods and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kinect v2 for mobile robot navigation: Evaluation and modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fankhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bl?sch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kaestner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Robotics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised CNN for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MegaDepth: Learning single-view depth prediction from Internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-task learning as multi-objective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep attentionbased classification network for robust depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Single view stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3D geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion: A structured approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep monocular depth estimation via integration of global and local predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Monocular relative depth perception with web stereo data supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Web stereo video supervision for depth prediction from dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Single-image depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A multi-view stereo benchmark with highresolution images and multi-camera videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sch?ps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A benchmark for the evaluation of RGB-D SLAM systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Engelhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning the depths of moving people by watching frozen people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning single-image depth from videos using quality assessment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A large RGB-D dataset for semisupervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10230</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DeMoN: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">CAM-Convs: Camera-aware multi-scale convolutions for single-view depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Facil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ScanNet: Richly-annotated 3D reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Tanks and temples: Benchmarking large-scale scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vasiljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Daniele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00463</idno>
		<title level="m">DIODE: A Dense Indoor and Outdoor DEpth Dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lebeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<title level="m">Hollywood 3D: What are the best 3D features for action recognition?&quot; IJCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">121</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Stereoscopic cinema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Devernay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Beardsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image and Geometry Processing for 3-D Cinematography</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bolt 3D: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Neuman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stereoscopic Displays and Applications XX</title>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">7237</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">FFmpeg developers</title>
		<ptr target="https://ffmpeg.org" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>FFmpeg</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of DNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Why do line drawings work? a realism hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
