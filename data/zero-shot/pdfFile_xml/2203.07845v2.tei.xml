<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-Vision Dataset, Human-Machine Synergy</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale datasets play a vital role in computer vision. But current datasets are annotated blindly without differentiation to samples, making the data collection inefficient and unscalable. The open question is how to build a mega-scale dataset actively. Although advanced active learning algorithms might be the answer, we experimentally found that they are lame in the realistic annotation scenario where out-of-distribution data is extensive. This work thus proposes a novel active learning framework for realistic dataset annotation. Equipped with this framework, we build a high-quality vision dataset-Bamboo, which consists of 69M image classification annotations with 119K categories and 28M object bounding box annotations with 809 categories. We organize these categories by a hierarchical taxonomy integrated from several knowledge bases. The classification annotations are four times larger than ImageNet22K, and that of detection is three times larger than Object365. Compared to ImageNet22K and Objects365, models pre-trained on Bamboo achieve superior performance among various downstream tasks (6.2% gains on classification and 2.1% gains on detection). We believe our active learning framework and Bamboo are essential for future work. Code and dataset are available at https://github.com/ZhangYuanhan-AI/Bamboo.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>L ARGE-scale pre-trained models, either trained in a supervised <ref type="bibr" target="#b25">[27]</ref>, <ref type="bibr" target="#b35">[37]</ref>, <ref type="bibr" target="#b69">[71]</ref> or unsupervised <ref type="bibr" target="#b8">[10]</ref>, <ref type="bibr" target="#b12">[14]</ref>, <ref type="bibr" target="#b27">[29]</ref> manner, have become a foundation for modern computer vision. Pre-trained models <ref type="bibr" target="#b51">[53]</ref> bring various applications by transferring to downstream tasks. Most importantly, the fuel of this foundation models <ref type="bibr" target="#b4">[6]</ref> relies on the availability of increasingly large and diverse datasets <ref type="bibr" target="#b16">[18]</ref>, <ref type="bibr" target="#b38">[40]</ref>, <ref type="bibr" target="#b42">[44]</ref>, <ref type="bibr" target="#b58">[60]</ref>.</p><p>Building a high-quality dataset requires careful consideration in selecting data. However, public datasets are annotated blindly with no differentiation to samples, which brings a colossal waste of annotation budget: Citovsky et al <ref type="bibr" target="#b15">[17]</ref> indicates that only 70% data in OpenImages <ref type="bibr" target="#b39">[41]</ref> can achieve on-par performance to its complete set. Though active learning (AL) researchers extensively study how to select the most valuable samples-informative and indistribution-from unlabeled data pool <ref type="bibr" target="#b24">[26]</ref>, <ref type="bibr" target="#b26">[28]</ref>, <ref type="bibr" target="#b31">[33]</ref>, <ref type="bibr" target="#b32">[34]</ref>, <ref type="bibr" target="#b56">[58]</ref>, <ref type="bibr" target="#b59">[61]</ref>, <ref type="bibr" target="#b62">[64]</ref>, <ref type="bibr" target="#b64">[66]</ref>, <ref type="bibr" target="#b73">[75]</ref>, we experimentally observe that the current advanced active learning methods, e.g. Cluster-Margin <ref type="bibr" target="#b15">[17]</ref>, Margin <ref type="bibr" target="#b52">[54]</ref> and CoreSet <ref type="bibr" target="#b55">[57]</ref>, are lame in the realistic annotation scenario. Specifically, AL methods select high-informative data at the expense of choosing out-ofdistribution data discarded by annotators and not used for model supervised learning. Random sampling selects less high-informative data than AL but includes much more in-distribution data than AL. As the performance gain bought by data quantity is superior to data quality when annotated data of AL is 70% less than that of random sampling (as shown in <ref type="figure" target="#fig_5">Fig. 6</ref>), AL is inferior to random sampling for improving model performance. Given this shortage, we propose a novel active learning framework, which cleans off the out-of-distribution data in the unlabeled data pool before active sampling, ensuring the sampled data under active learning is informative and meanwhile in-distribution. This novel framework achieves better than random sampling for boosting supervised learning model performance.</p><p>We aim to annotate a mega-scale classification and object detection dataset with our proposed active learning framework. First, we build a comprehensive label system for querying diverse data covering numerous semantics. Specifically, we form a unified label system with a hierarchical structure consisting of 304,048 categories. It integrates label systems from 19 latest public image classification datasets and five object detection datasets and also absorbs 170,586 new categories from knowledge bases, e.g. Wikidata <ref type="bibr" target="#b65">[67]</ref>. Then, we contribute Bamboo Dataset, a megascale and information-dense dataset for the pre-training of both classification and detection, which is active annotating by human-machine synergy. Bamboo has three appealing properties:</p><p>? Comprehensive. It consists of 69M image classification annotations and 28M object bounding box annotations, spanning over 119K visual categories. The scale of the label system and the annotated data are the largest among all the publicly available datasets. We illustrate the comparison of Bamboo and publicly available datasets in the <ref type="figure" target="#fig_0">Fig. 1(c</ref>  Bamboo is a new mega-scale vision dataset built on a comprehensive label system with human-machine synergy. (a) Our label system continually extends from WordNet with our solutions. Concepts in the label system are distinguished as "common visual", "non-common visual" or "non-visual" concepts. (b) Raw data crawled by the query word person includes both the in-distribution (ID) data and out-of-distribution (OOD) data. OOD data implies noisy, covariate shift, and semantic shift data. Noisy data does not present useful semantic information. Covariate shift data implies semantic information, i.e. person. However, such semantic information is of poor quality, annotators thus hard to annotate. Semantic shift data also implies sxemantic information, i.e. tree. But the tree is not related to the query word person. OOD rectification mitigates the ineffectiveness of active learning through filtering OOD data. (c) Bamboo collects 69M classification annotation and 28M bounding box annotations.</p><p>formative through the label system and the annotated data. The label system is constructed by thoroughly integrating public datasets and knowledge bases. Our active annotation pipeline specifically selects the annotated data to reduce model uncertainty. ? Continual. Our label system keeps the dataset growing with the automatic concept linking strategies. We can constantly absorb new categories in the real world and integrate them into our label system. Moreover, leveraging the ever-increasing internet data, our active annotation pipeline will steadily sustainably expand the Bamboo dataset size. Extensive experiments demonstrate that Bamboo dataset is an effective pre-training source. The Bamboo pre-trained model significantly outperforms CLIP ViT B/16 <ref type="bibr" target="#b51">[53]</ref> pretrained model with 6.2% gain (85.6% vs 91.8%) on classification, and outperforms Objects365 <ref type="bibr" target="#b57">[59]</ref> pre-trained model with 2.1% gain (14.7% vs 12.6%) on CityPersons <ref type="bibr" target="#b77">[79]</ref>. In addition, we provide valuable observations regarding largescale pre-training from over 1,000 experiments. We hope the Bamboo dataset and these observations will pave the way for developing more general and effective vision models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Learning of Representation at Scale. Representation learning has advanced thanks to improvements in various learning paradigms and large-scale datasets. Supervised learning models <ref type="bibr" target="#b13">[15]</ref>, <ref type="bibr" target="#b28">[30]</ref> leverage label information to supervise representation learning, achieving excellent performance among various downstream tasks. To avoid the need for annotations that require a tremendous amount of human and labeling cost, weakly-supervised and self-supervised pre-training methods have been proposed. Self-supervised methods <ref type="bibr" target="#b7">[9]</ref>, <ref type="bibr" target="#b8">[10]</ref>, <ref type="bibr" target="#b11">[13]</ref>, <ref type="bibr" target="#b17">[19]</ref>, <ref type="bibr" target="#b27">[29]</ref>, <ref type="bibr" target="#b40">[42]</ref>, <ref type="bibr" target="#b41">[43]</ref>, <ref type="bibr" target="#b47">[49]</ref>, <ref type="bibr" target="#b66">[68]</ref>, <ref type="bibr" target="#b76">[78]</ref> with contrastive learning have shown that unsupervised pre-training produces features surpassing the supervised feature representations on many downstream tasks <ref type="bibr" target="#b37">[39]</ref>, <ref type="bibr" target="#b38">[40]</ref>, <ref type="bibr" target="#b48">[50]</ref>, <ref type="bibr" target="#b49">[51]</ref>, <ref type="bibr" target="#b67">[69]</ref>. Large weakly-supervised datasets, such as Instagram hashtags <ref type="bibr" target="#b44">[46]</ref> and JFT <ref type="bibr" target="#b58">[60]</ref>, helps model <ref type="bibr" target="#b68">[70]</ref>, <ref type="bibr" target="#b69">[71]</ref>, <ref type="bibr" target="#b79">[81]</ref> achieve significant gains on downstream tasks. In addition, CLIP <ref type="bibr" target="#b51">[53]</ref> pre-train models on both the image signal and text signal, achieving good performance for the zero-shot evaluation. Our study is part of a larger body of work on training models on sizeable supervised image datasets. As the labeling cost that hurdles the supervised learning dataset is becoming increasingly significant, we systematically investigate how to collect, annotate and build a mega-scale dataset efficiently, actively and continually. Active Learning. Active learning (AL) aims at finding the minimum number of labeled images to have a supervised learning algorithm reach a certain performance <ref type="bibr" target="#b24">[26]</ref>, <ref type="bibr" target="#b26">[28]</ref>, <ref type="bibr" target="#b31">[33]</ref>, <ref type="bibr" target="#b32">[34]</ref>, <ref type="bibr" target="#b56">[58]</ref>, <ref type="bibr" target="#b59">[61]</ref>, <ref type="bibr" target="#b62">[64]</ref>, <ref type="bibr" target="#b64">[66]</ref>, <ref type="bibr" target="#b73">[75]</ref>. The main component in an active learning loop is sampling strategies. The existing AL research is conducted on the curated datasets. Each data point in the labeled and unlabeled pool of these datasets is valid, i.e. available for labeling. However, curated datasets can hardly imitate the annotation in realistic scenarios where out-of-distribution data that is unavailable for labeling appears on a large scale in the unlabeled pool. From our experiments, we find the existing AL methods lag in realistic scenarios. Therefore, we propose a novel active annotation pipeline to improve the performance of AL methods in realistic scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LABEL SYSTEM CONSTRUCTION</head><p>In this section, we briefly introduce how to build a comprehensive label system. The number of concepts decides the data amount upper bound-we crawl data based on querying these labels. Starting from WordNet <ref type="bibr" target="#b46">[48]</ref>, we enrich its concepts from another two concept resources (Sec. 3.1) through three designed linking strategies (Sec. 3.2)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-visual Concepts Visual Concepts</head><p>Visual vs Non-visual Samples Iris Radio Economists Vitamin <ref type="figure">Fig. 2</ref>. The illustration of visual and Non-visual concept. Vitamin do not share any common semantic information. Economists implies common semantic information-Man-but economists are not necessarily men.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Concepts Resources</head><p>WordNet. WordNet is a lexical database of semantic relations between concepts in more than 200 languages. Each meaningful concept in WordNet, possibly described by multiple words or phrases, is called a "synset". Referring to ImageNet22K <ref type="bibr" target="#b16">[18]</ref>, we only use the Noun words of WordNet. WordNet is the foundation of our label system. Public Datasets. We collect 24 public datasets, including ImageNet22K <ref type="bibr" target="#b16">[18]</ref>, OpenImages <ref type="bibr" target="#b39">[41]</ref>, COCO <ref type="bibr" target="#b42">[44]</ref>, iNaturalist <ref type="bibr" target="#b63">[65]</ref>, and etc. 1 These datasets cover a wide range of datasets in both image classification and object detection. Wikidata. Wikidata <ref type="bibr" target="#b65">[67]</ref> contains a large number of concepts, such as different kinds of foods, animals, and locations. As the number of concepts in Wikidata continues to grow, so far, we have included 170,586 concepts from it. These concepts are the leaf nodes in their taxonomy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Concepts Integration</head><p>WordNet is a lexical graph whose concepts imply semantic relation. For example, the father node of "British Shorthair" is "Domestic Cat". How to integrate concepts from public datasets and Wikidata into this WordNet is an open question. We propose three parallel solutions to integrate these categories into WordNet in this work. Solution 1: Leveraging on the subclassOf. The taxonomy of Wikidata is contributed by adding the "subclassOf" that is related to the hypernyms relationship in the taxonomy of WordNet. Referred to <ref type="bibr" target="#b60">[62]</ref>, we link Wikidata leaf node concepts to the WordNet by leveraging the "subclassOf". Solution 2: Parsing the Concept. Referred to the previous work <ref type="bibr" target="#b21">[23]</ref>, we can also link the concept to the Word-Net through word parsing. For example, for the concept Sumatran Orangutan, we parse this concept <ref type="bibr" target="#b29">[31]</ref> and get its head compound "Orangutan". In this way, we add Sumatran Orangutan as the new hyponym of the "Orangutan" if "Orangutan" exists in WordNet. Solution 3: Linking to the Closed Synset. We calculate the word embedding of both the synsets and given concepts through Spacy <ref type="bibr" target="#b29">[31]</ref>. If a given concept cannot be linked to WordNet, we add this category to the hyponym of its nearest cosine distance synset.</p><p>1. The complete list of public datasets is reported in Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Concepts Tagging</head><p>Visuality. Yang et al has mentioned the non-visual category problem in their work <ref type="bibr" target="#b70">[72]</ref>. We illustrate visual and nonvisual words in <ref type="figure">Fig. 2</ref>. To mitigate this problem, we conduct visual concept tagging for our build label system. Specifically, a concept is non-visual if three out of five annotators think this word is less concrete, and its sample images can rarely imply a common semantic meaning. We illustrate the concept tagging in <ref type="figure" target="#fig_1">Fig. 3(a)</ref>. Commonality. Based on the visual concepts, we further conduct common concept annotation for all visual concepts. Referred to COCO <ref type="bibr" target="#b42">[44]</ref>, "common concept" refers to entrylevel categories that are commonly used by humans when describing objects (e.g. dog, chair, person). Specifically, a concept is positive only if it receives at least three-fifths of the votes. Based on the proposed annotation method, we retain 809 common concepts for the annotation of object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ACTIVE DATASET CONSTRUCTION -BAMBOO</head><p>Equipped with the unified and comprehensive label system, we start to construct Bambooactively. In this section. We first introduce the active learning pipeline for building Bamboo in Sec 4.1. We summarize this pipeline in Algorithm 1. Then in Sec. 4.2, we discuss the superiority of our newly proposed active learning methods-we are the first time beat the random sampling in selecting the most valuable data for data pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Outline of AL Framework</head><p>input : Raw unlabeled pool ?; Number of active learning rounds T; Neural network ?; P L (0) ? Annotating a few data from ? and adding all inherited data as cold start;</p><formula xml:id="formula_0">P U (0) ? ? ? P L (0) ? ?; Initializing model ?(0) based on P L (0); for r ? 1 to T do 2 P R (r) ? Rectifying P U (r ? 1) w/ ?(r ? 1); S U (r) ? Sampling in P R (r) w/ ?(r ? 1); S L (r) ? Annotating valid data from S U (r); P U (r) ? P U (r ? 1) ? S U (r); P L (r) ? P L (r ? 1) ? S L (r);</formula><p>Training ?(r) on P L (r); end</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Active Learning Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Building Unlabeled Data Pool</head><p>For image classification, one query word has one visual concept mentioned in Sec. 3.3. For object detection, one query has two concepts, i.e., common concept + scene semantic word or common concept + common concept. For example, dog + street or dog + ball. To further enrich the searching results, any given query word can be converted to its synonyms or its Chinese, Spanish, Dutch and Italian version for querying. Totally, we obtain a 170M unlabeled pool for classification and a 200M unlabeled pool for detection.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Cold Start</head><p>Cold start is the very first step for active learning. The labeled date pool P L (0) to initialize the model ?(0) for the cold start phase include two parts as follows. Public Dataset. As mentioned in Sec. 3.1, we use 24 datasets as concept resources, including 19 image classification datasets and 5 object detection datasets. Refereed to the evaluation suite of popular transfer learning study <ref type="bibr" target="#b27">[29]</ref>, <ref type="bibr" target="#b36">[38]</ref>, <ref type="bibr" target="#b75">[77]</ref>, we select 12 datasets for downstream evaluation. We include the annotation of the other 12 datasets-9 image classification datasets and 3 object detection datasets. In total, we inherit 27,848,477 classification annotations and 21,983,223 object bounding box annotations from those 12 datasets. New Annotated Data. For concepts not included in public datasets, we sample images from unlabeled pool ? and annotate data for them until they have 50 annotated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">OOD Rectification</head><p>Image Classification. In this step, we rectify the latest unlabeled data pool P U C (r ? 1). As shown in <ref type="figure" target="#fig_3">Fig 4 (b)</ref>, in each round r, we firstly utilize ? C (r ? 1) trained on P L C (r ? 1) to acquire predictions of each image in P U C (r ? 1). We infer an image is out-of-distribution if its top-5 predicted categories do not overlap with its related categories. Specifically, we define the related categories of an image as the sub-population of hypernyms of its query word. If an image is not out-of-distribution, we add it into P R C (r) for further data sampling. In Sec. 4.2, we empirically observe that OOD rectification is essential for AL to function in realistic scenarios. Object Detection. Similar to classification, we acquire proposal predictions of each image in P U D (r ? 1) by ? D (r ? 1).</p><p>On the one hand, we filter out the image with less than two proposals. Such images might be noisy data or semantic shift data. On the other hand, we filter out the image with more than 15 identical semantic proposals since such image might be the covariate shift data. As shown in <ref type="figure" target="#fig_3">Fig 4 (b)</ref>, the remaining in-distribution data forms P R D (r) for the data sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Data Sampling.</head><p>In this step, we use ClusterMargin <ref type="bibr" target="#b15">[17]</ref>, which considers both the uncertainty and diversity in data, to select the most valuable data S U (r) from the latest rectified data pool P R (r) for annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Data Annotation.</head><p>Finally, we rely on an online platform to annotate valid data-its querying word accurately describes the semantic meaning of this data-in S U (r), forming the labeled data set S L (r). We illustrate our online platform in <ref type="figure" target="#fig_1">Fig. 3</ref>, and introduce the details of annotations as follows. Image Classification. To provide a comprehensive definition of each category, we construct reference images that are collected by querying Google image search and Wikipedia <ref type="bibr" target="#b19">[21]</ref>. For each image in S U (r), its meta-information has two parts: the query word of this image and the reference images of the query word. We then ask the five annotators whether this image conforms to its meta information. An image is annotated and added into S L (r)-valid data-only if at least 3 out of 5 annotators give the positive answer to the question as mentioned above. Object Detection. Following Objects365 <ref type="bibr" target="#b57">[59]</ref>, one annotator is responsible for annotating a specific category, which improves the annotation efficiency and quality. Similar to     OpenImages <ref type="bibr" target="#b39">[41]</ref>, meta information of an image includes not only its reference images but also its pseudo labels that include i) the query words of this image. ii) the category predictions of available detection models. iii) re-labeling predictions <ref type="bibr" target="#b74">[76]</ref> of the latest trained classification model ? C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Studies on Active Annotation</head><p>In academic active learning (AL) works <ref type="bibr" target="#b15">[17]</ref>, <ref type="bibr" target="#b30">[32]</ref>, researchers conduct data sampling on the leave-out "unlabeled" data pool that are separated from a curated dataset, e.g. ImageNet <ref type="bibr" target="#b15">[17]</ref> and CIFAR10 <ref type="bibr" target="#b52">[54]</ref>. All the data in this "unlabeled" data pool is strictly valid. <ref type="bibr" target="#b1">3</ref> However, in realistic annotation scenarios, the real unlabeled data pool is composed of valid data and invalid data that is mostly outof-distribution data, as shown in <ref type="figure" target="#fig_3">Fig. 4(a)</ref>. Therefore, can AL methods are effective when the invalid data is in the unlabeled data pool is an open question. And we found that: Current Active Learning Methods are Ineffective for Sampling Valuable Data in the Real unlabeled data pool.</p><p>As shown in <ref type="figure" target="#fig_5">Fig. 6(a)</ref>, we illustrate the number of S L (1) (the first round valid data set of Bamboo) when P R (1) ? P U (0) (the academic active learning framework). We observe that AL sampling would retain fewer data in S L (1) than random sampling. For example, Entropy Sampling selects 70% less data than random sampling, resulting in worse downstream performance. The Devils are in Uncertainty Modeling. As discussed in <ref type="bibr" target="#b18">[20]</ref>, <ref type="bibr" target="#b34">[36]</ref>, there are mainly two types of uncertainty for the deep models: Aleatoric and Epistemic. Both uncertainties are informative, but the aleatoric uncertainty is the outof-distribution data, and the epistemic uncertainty is the in-distribution data. Considering P U (0) where aleatoricuncertain data, epistemic-uncertain data, and other lessinformative data exist, when P R (1) ? P U (0), S U (1) under AL sampling would have more aleatoric-uncertain data than that under random sampling, as AL methods tend to select uncertain data. Eventually, S L (1) under AL sampling should has less data than that under random sampling as aleatoric uncertain data is invalid for annotators. We illustrate this phenomenon in <ref type="figure" target="#fig_4">Fig. 5</ref> left. As shown in <ref type="figure" target="#fig_5">Fig. 6</ref> (a), with much less S L (1), AL methods' performances are hence worse than RS. OOD Rec. Boosts AL Performance. When P R (1) ? Rectifying P U (0) w/ ?(0) (our active learning framework), our proposed OOD rectification filters out the aleatoric uncertain data in P U (0). Therefore, P R (1) is only comprised of 3. Annotator had deleted invalid data as dataset established. epistemic-uncertain data-which is informative-and other less-informative data. Since AL methods would select more epistemic uncertain data in P R (1) than random sampling, they eventually perform better. We illustrate how OOD rectification helps active learning performs better in realistic scenarios in <ref type="figure" target="#fig_4">Fig. 5</ref> right. As shown in <ref type="figure" target="#fig_5">Fig. 6(b,c)</ref>, with OOD rectification, in both classification and detection tasks, AL methods (ClusterMargin and Core-Set) that consider both the uncertainty and diversity select the most valuable data for model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DATASET STATISTICS</head><p>As shown in <ref type="figure">Fig. 7</ref>, we illustrate the sorted distribution of image numbers per category in the Bamboo. Generally, we emphasize that the new annotated data in the Bamboo-CLS and Bamboo-DET are a powerful complement to the current public datasets-This data mostly belongs to tail classes of public datasets and new classes. In the following, we briefly describe the data statistics of Bamboo. Image Classification (Bamboo-CLS). Bamboo-CLS has 68,884,828 images spread across 119,035 categories. 42,648,217 out of 68,884,828 images are newly annotated, which is twice of ImageNet22K. In addition, 20,000 out of 119,035 categories are from Wikidata. These categories mainly are fine-grained concepts, such as Folland Midge (one type of fighter) and hemaria hemishphaerica (a species of fungi). To our knowledge, Bamboo-CLS is the largest clean image dataset available to the vision research community, in terms of the total number of images and categories. Object Detection (Bamboo-DET). Bamboo-DET has 3,104,012 images across 809 categories. Specifically, 557,457 images are newly annotated, and 150 concepts are from the Wikidata. In addition, we provide the statistics on instances per image of Bamboo-DET. As shown in <ref type="table" target="#tab_4">Table 1</ref>, Our newly annotated data has 8.3 instances (on average) per image, which is dense than existing datasets, i.e. MS-COCO, Object-365, and OpenImages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setups</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Upstream Pre-training</head><p>Hyper-parameter. We train the models on 64 A100 GPUs for image classification, with a total batch size of <ref type="bibr" target="#b6">8,</ref><ref type="bibr">192</ref>. We employ an AdamW <ref type="bibr" target="#b43">[45]</ref> optimizer of 30 epochs using a cosine decay scheduler with two epochs of linear warmup. The ResNet-50 backbone is initialized from the model officially offered by PyTorch. The ViT B/16 backbone is initialized from ImageNet1K model provided by timm. <ref type="bibr" target="#b2">4</ref> The weight decay, and warm-up learning rate are 2?10 ?8 , 10 ?6 , and 2 ? 10 ?2 . Datasets. Beyond the new annotated data, we include Ima-geNet22K <ref type="bibr" target="#b16">[18]</ref>, INaturalist2021 <ref type="bibr" target="#b63">[65]</ref>, Herbarium2021, 5 , Danish Fungi 2020 <ref type="bibr" target="#b50">[52]</ref>, iWildCam2020 <ref type="bibr" target="#b2">[4]</ref>, TsinghuaDogs <ref type="bibr" target="#b80">[82]</ref>, Places <ref type="bibr" target="#b78">[80]</ref>, FoodX-251 <ref type="bibr" target="#b33">[35]</ref>, CompCars <ref type="bibr" target="#b72">[74]</ref> in the upstream classification dataset training. We train the models on 48 A100 GPUs for detection, with a total batch size of 384, a total learning rate of 0.4, SGD optimizer of momentum 0.9, and a weight decay of 0.0001. We use the MultiStep learning rate scheduler with the decay rate of 0.1 on <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b20">22]</ref> epochs and train for 26 epochs in total. We also applied the warm-up learning rate of 0.0004 for 1 epoch. We used Cross-Entropy-Loss for categorization and Smoothed-L1-Loss for bounding box regression. Beyond the new annotated data, we include COCO <ref type="bibr" target="#b42">[44]</ref>, Objects365 <ref type="bibr" target="#b57">[59]</ref> and OpenImages <ref type="bibr" target="#b39">[41]</ref> in the upstream object detection dataset training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Downstream Evaluation</head><p>Datasets. In the following sections, we adopt the downstream datasets that are widely used in the transfer learning study <ref type="bibr" target="#b27">[29]</ref>, <ref type="bibr" target="#b36">[38]</ref>, <ref type="bibr" target="#b75">[77]</ref>. For models pre-trained on the image classification datasets, we use CIFAR10 <ref type="bibr" target="#b38">[40]</ref>, CIFAR100 <ref type="bibr" target="#b38">[40]</ref>, OxfordFlower <ref type="bibr" target="#b48">[50]</ref>, Food101 <ref type="bibr" target="#b5">[7]</ref>, Caltech101 <ref type="bibr" target="#b22">[24]</ref>, Oxford-Pets <ref type="bibr" target="#b49">[51]</ref>, DTD <ref type="bibr" target="#b14">[16]</ref>, StanfordCars <ref type="bibr" target="#b37">[39]</ref>, FGVC-Aircraft <ref type="bibr" target="#b45">[47]</ref>   <ref type="bibr" target="#b67">[69]</ref>, ImageNet1K <ref type="bibr" target="#b53">[55]</ref> as the downstream evaluation datasets. As for the object detection task, we select PASCAL VOC <ref type="bibr" target="#b20">[22]</ref> and CityPersons <ref type="bibr" target="#b77">[79]</ref> as the downstream evaluation datasets. These datasets cover a wide range of image domains. The number of images in each dataset ranges from 2,000 to 80,000, and the number of classes in each dataset ranges from 10 to 8,000. Evaluation Protocol. For the classification task, we use image features taken from the penultimate layer of each model, ignoring any classification layer provided. We train a logistic regression classifier for the linear probe evaluation setting.</p><p>We finetune the entire model loaded with its backbone and FPN weights for the detection task. We only report the evaluation performance of models on downstream datasets. We finetune the model on 8 1080-Ti GPUs for detection, with the batch size of 16, SGD optimizer of momentum 0.9, and weight decay 0.0001 by loading the weights of backbone and FPN. We conduct a grid search on learning rate among [5 ? 10 ?4 , 1 ? 10 ?3 , 5 ? 10 ?3 , 1 ? 10 ?2 ]. The learning rate is decayed by 0.1 at 16 and 18 and stopped training at 19 epochs. <ref type="table" target="#tab_5">Table 2</ref>, ResNet-50 (RN50) pre-trained on CLIP (400M) or IG-1B (1B) achieves better downstream task performance   <ref type="figure" target="#fig_0">+1.1) 12.6 (+2.1) 43.9 (+4.4)</ref> than BiT pre-trained on ImageNet1K (IN1K) <ref type="bibr" target="#b53">[55]</ref>. However, compared to RN50 pre-trained on Bamboo, CLIP-RN50 or RN50 pre-trained on IG-1B achieves inferior performance. It indicates that the amount of informative-dense annotations instead of the sheer number of annotations is much more essential for model pre-training. Compared to CLIP, which leverages the vast amount of image-text pairs on the web for pre-training, our Bamboo presents an active and continual framework that collects and annotates fullysupervised samples in a highly scalable manner. Comprehensive Label System Helps. As shown in Table 2, most methods pre-trained on IN1K, IG-1B, or WIT achieve more than 90% accuracy on the OxfordPets and OxfordFlower. But they only achieve less than 80% accuracy on the StanfordCars and FGVC-Aircraft. It indicates that these pre-trained datasets might include more semantic concepts related to OxfordPets and OxfordFlower. Our BambooTX spreads a large spectrum of concepts. Notably, it includes much more concepts that are neglected in the current public and nonpublic datasets. As a result, models pre-trained on Bamboo achieve much better performance than other methods. Beyond general object detection, it is also important to validate the generalization ability on specific object detection problems like pedestrian detection. Bamboo is an Effective Pre-Training Source. Compared to other methods, Bamboo achieves the best performance among downstream tasks on average. As shown in <ref type="table" target="#tab_5">Table 2</ref>, ViT B/16 pretrained on Bamboo outperforms CLIP with 6.2 points gain. It indicates that our annotation is much more informative and hence more helpful for the model pretraining. In addition, <ref type="table" target="#tab_6">Table 3</ref> presents that ResNet-50 with FPN pretrained on Bamboo outperforms Objects365 with 1.1 points gain on PASCAL VOC and 2.1 points gain on CityPersons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Power of Bamboo as Pre-Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information-Dense Annotations Matter. As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Further Analysis</head><p>The Influence Of Similar Semantic Proposals. The total annotation cost for the object detection task depends on the number of proposals. Images with dense proposals are more expensive than sparse ones. Based on our observation, many proposals with similar semantics tend to form a group in a single image. To evaluate their effectiveness, we conduct the following experiments on Objects365 <ref type="bibr" target="#b57">[59]</ref> dataset.</p><p>Firstly, we define an image as a crowded image if it contains at least one category with more than 15 proposals. By removing all 27K crowded images from the full Objects365 dataset, we denote the remaining part as Objects365-sparse. Keeping the number of proposals the same as Objects365-sparse, we randomly removed 90K images from the full Objects365 dataset and marked the remaining part as Objects365-random. Furthermore, keeping the total object amount the same as Objects365-sparse, we randomly removed 101K non-crowded images from the full Objects365 dataset and denoted the remaining part as Objects365-dense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Images Given the same annotation budget, we find that choosing to label non-crowded images yields better results for pretraining performance. Therefore, as mentioned in Sec. 3.3.2 of the main paper, we filter out covariate shift data in the OOD rectification step. Finetuning Transfer. We compared our model pre-trained on Bamboo to various with the ResNet-50 backbone. We  present the finetuning transfer performance of the models pre-trained on Bamboo. The finetuning strategy among each downstream task is followed by the SimCLR <ref type="bibr" target="#b11">[13]</ref>. <ref type="table" target="#tab_9">Table 5</ref> shows the comparison. Bamboo model achieves a 1.3% average accuracy gain compared to BiT-M pre-trained on the current largest public classification dataset: Ima-geNet22K. It indicates a larger, carefully annotated dataset can continually improve the performance of models. Besides, Bamboo model achieves a 0.5% average accuracy gain compared to SWSL, pre-trained on the IG-1B with 1B weakly supervised hashtags. Bamboo is 20 times smaller than IG-1B, which indicates that the amount of informativedense annotations instead of the sheer number of weak annotations is much more essential for model pre-training.</p><p>Zero-Shot Transfer. We present the zero-shot transfer performance of the models pre-trained on Bamboo. We compared our model pre-trained on Bamboo to CLIP models with the same backbone. Robustness to Natural Distribution Shift. We conduct experiments on the ObjectNet <ref type="bibr" target="#b0">[2]</ref> to compare Bamboo models with other models when evaluated on the data with controls for rotation, background, and viewpoint. ObjectNet is a dataset collected in the real world, where multiple objects are always present. There are 313 object classes in total, with 113 overlapping with ImageNet1K. We follow the literature <ref type="bibr" target="#b35">[37]</ref>, <ref type="bibr" target="#b51">[53]</ref> and evaluate our models on those 113 classes. As shown above, we compare Bamboo models with the state-of-the-art model with the same backbone. Specifically, ResNet-50 pre-trained on Bamboo achieves 1.2% gains compared with ResNet-50 pre-trained on JFT-300M. ViT B/16 pre-trained on Bamboo achieves 3.2% gains compared with ViT B/16 pre-trained on Anno-1.3B. Even though JFT-300M and Anno-1.3B are much larger than Bamboo, the informative data in Bamboo is more helpful for pre-trained models in real scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">SOCIAL IMPACT</head><p>The proposed Bamboo dataset and pre-training model shows the capacity and generalization of learned image representation which could benefit many applications of computer vision. However, our data usage might bring several risks, such as data overlapping, privacy, and inappropriate content. We discuss these risks and their mitigation strategies as follows. Data Overlapping. A concern with pre-training on an extensive dataset is unintentional overlap with downstream evaluation <ref type="bibr" target="#b51">[53]</ref>. To enable a meaningful test of generalization, we identify and remove all duplicates among upstream data. Specifically, we utilize Difference Hash (DHash) <ref type="bibr" target="#b3">[5]</ref> to present the information of each image. We calculate the hash-code of each downstream image and each crawled image, and two images with the same hash-code are regarded as similar ones. Then, we filter out the crawled images that are similar to downstream images. Based on the above method, we discard 122,939 images for classification and 1,046 images for detection from the unlabeled pool. Copyright. We crawl only the data under the Creative Commons license (CC-BY) for the Bamboo-DET. This license allows free use, redistribution, and adaptation for noncommercial purposes. For the Bamboo-CLS data, 30% of data is under the CC-BY license because of its large volume of data. For Bamboo-CLS data that is not under the CC-BY license, referred to LAION-400M <ref type="bibr" target="#b54">[56]</ref> and Conceptual 12M <ref type="bibr" target="#b10">[12]</ref>, we only present the lists of URLs to this data without redistribution. We build the meta file as follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[image url] [class index]</head><p>Referred to Authors Guild, Inc. v. Google Inc. <ref type="bibr" target="#b6">[8]</ref>, training data on the copyrighted works might be considered as transformative uses and was thus might be regarded as Fair Use 6 . In addition, referred to Article 30-4 of the new Copyright Act [1], there are no restrictions on the subject, purpose, and method of data analysis, and there is no obligation to compensate the copyright holder. However, we admit that using copyright material as training data is still a controversial issue in Artificial Intelligence, and we would no doubt follow the newest law worldwide.</p><p>Bamboo is specifically open for non-commercial research and/or educational purposes to respect the copyright law. For researchers and educators who wish to use copyrighted images for that purpose, training or benchmarking models with copyrighted works would be qualified as transformative uses and thus not infringe copyright law in the U.S.6. Nevertheless, the users must strictly follow the Flickr Terms of Use. <ref type="bibr" target="#b5">7</ref> And the users of these images accept full responsibility for the use of the image. Problematic Content. The inappropriate contents such as drugs, nudity, and other offensive content exist in the web data. we ask annotators to discard such images instead of conducting annotation. Privacy. To mitigate privacy issues with public visual datasets, researchers have attempted to obfuscate private information before publishing the data <ref type="bibr" target="#b23">[25]</ref>, <ref type="bibr" target="#b71">[73]</ref>. We plan to follow this line of work to blur faces, and license plates in our new annotated data. In addition, if the original picture found at the URL present on the Bamboo on the record states users' names, phone numbers, or any personal information, users can request a takedown of this image. Bias. The images were crawled from Flickr, thus inheriting all the biases of that website. The usage of user-generated data might bring the risk of bias. We plan to tackle this problem by balancing various categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In our work, with a human-machine synergy, we actively and continually build a mega-scale and information-dense <ref type="bibr" target="#b4">6</ref>. https://www.copyright.gov/fair-use/index.html 7. https://www.flickr.com/help/terms/api dataset, namely Bamboo. Bamboo is the largest clean image dataset available to the vision research community, in terms of the total number of images and the number of categories, for classification and detection tasks. Our key insight is that a unified and visually-oriented label system is crucial for model pre-training, and rectifying OOD samples is indispensable for AL to function in realistic scenarios. We have demonstrated the effectiveness of Bamboo as a better pretraining dataset for various downstream tasks and provided several valuable observations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The overview of Bamboo Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>User interfaces for concept tagging and annotation. (a) The meta information of the concept tagging consists of tags, descriptions, and reference images. (b) Interface for image classification and object detection. For the object detection task. The image is assigned to different annotators based on its multiple pseudo labels. In addition, annotators should choose the attribute of the bounding box. The criteria for the attribute options are described in detail in Supplementary Material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>OOD Samples (query word: "person")Model Inference</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>(a) The illustration of out-of-distribution (OOD) data in realistic scenarios. Mainly, three types of OOD data exist in the unlabeled data pool, including noisy data, covariate shift data (i.e., OOD samples from a different domain), and semantic shift data (i.e., OOD samples are drawn from different classes). (b) The illustration of OOD rectification. OOD rectification filters OOD data in the unlabeled data pool, which is crucial for active learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>The Illustration of how our OOD rectification step helps active learning performs better in realistic scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>The study of active annotation in Bamboo. (a) current AL methods struggle in realistic scenarios. Random sampling achieves better performance than each AL method. OOD Rectification boosts all AL methods to outperform random sampling. AL methods are still more helpful for model training with less valid data. It implies that the valid data that AL methods selected are much more informative. (b) and (c) in both classification and detection tasks, AL methods (ClusterMargin and Core-Set) that consider both the uncertainty and diversity select the most valuable data for model training. S L C refers annotated valid data from a given AL batch. Average accuracy denotes the average performance of models on the downstream datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>, 4 .e t e s o c h r a c e a b e n t le y o x t a i l_ s o u p o x y c a r e n u s la v a t e r aFig. 7 .</head><label>47</label><figDesc>https://github.com/rwightman/pytorch-imagemodels/tree/master/timm 5. https://www.kaggle.com/c/herbarium-2021-fgvc8/overview a t _ g r a n d c h il d jo n q u il h o r s e _ r a c i n g m e a d o w _ s p it t le b u g a d o b o w a ll e a r d t a b le d o g e a r a u t o m o b il e g r il l e le t t u c e o y s t e is s a n t b a ll d r e s s b a r b e c u e c h ic k e n b ic y c le r Sorted distribution of image number per category in the Bamboo. (a-i) Bamboo-CLS contains 68,884,828 images spread across 119,035 categories. Category names are shown for every 250 intervals. Bamboo-CLS includes some fine-grained concepts that not be included in the current public datasets, such as Folland Midge. (a-ii) The new classification annotated data accounts for 60.71% of images in Bamboo. (b-i) Bamboo-DET contains 3,104,012 images across 809 categories. Category names are shown for every 16 intervals. (b-ii) The new detection annotated data accounts for 11% of images in Bamboo.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Cat Subtree (a) Unified and Comprehensive Label System:</head><label></label><figDesc>We guarantee Bamboo is highly in-arXiv:2203.07845v2 [cs.CV] 24 Aug 2022</figDesc><table><row><cell></cell><cell>(b) Active Annotation:</cell><cell></cell><cell>(c)</cell><cell>Bamboo:</cell></row><row><cell>Bamboo WordNet Cat Subtree Comprehensive WordNet = + Public Dataset + ? Clingy Kittens: Wikidata (Non-Visual Concept) Wild Cat ? Continues to Extend ? Domestic Cat Norwegian Forest Cat British Shorthair : (Non-common Visual Concept) (Common Visual Concept)</cell><cell>Ours Sampling In-Distribution Rectification OOD Query Word: Person Out-Of-Active Learning Unlabeled Pool Distribution Noisy Data Noisy Data Covariate Shift Semantic Shift</cell><cell>#Images #Boxes</cell><cell cols="2">COCO 4 X L a r g e r 0.05M 14M CIFAR100 ImageNet Objects365 Image Classification Ours 69M #Categories L a r g e r 119K 5 X 0.1K 20K Ours 0.8M 10M 30M 809 Object Detection 3 X L a r g e r 2 X L a r g e r #Categories 80 365</cell></row></table><note>).? Information-Dense.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Is this a Golden Retriever ? Description:Is this a Golden Retriever ? Description:</head><label></label><figDesc>An English breed having a long silky golden coat. An English breed having a long silky golden coat. ://www.goog https://www.goog https://www.goog https://www.</figDesc><table><row><cell cols="3">(a) Visuality and Commonality</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Visuality</cell><cell></cell><cell></cell><cell>Image Classification</cell><cell></cell><cell></cell></row><row><cell cols="2">Is Parliamentarian a visual concept ? Description: An elected member of the British Parliament ? Reference Image</cell><cell>? Yes ? No</cell><cell>Unlabeled Image</cell><cell>Reference Image ? Yes ? No</cell><cell>Unlabeled Image</cell><cell>? Yes ? No Reference Image</cell></row><row><cell>https://www.google/imag</cell><cell>https://www.goog</cell><cell>https://www.goog</cell><cell>https://www.flickr</cell><cell></cell><cell>https://www. flickr</cell></row><row><cell>Commonality</cell><cell></cell><cell></cell><cell>Object Detection</cell><cell></cell><cell></cell></row><row><cell cols="2">Is Willow Flute a Common concept ? Description: Nordic folk flute?</cell><cell>? Yes ? No</cell><cell></cell><cell>Your choice: ? Normal bbox ? Group bbox ? Invalid Image</cell><cell></cell><cell>Your choice: ? Normal bbox ? Group bbox ? Invalid Image</cell></row><row><cell cols="2">Reference Image</cell><cell></cell><cell></cell><cell>Reference Image</cell><cell></cell><cell>Reference Image</cell></row><row><cell>https://www.google/imag</cell><cell>https://www.goog</cell><cell>https://www.goog</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Pseudo Label: Person Description: A human being</cell><cell></cell><cell>Pseudo Label: Car Description: A motor vehicle?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Annotator_1</cell><cell></cell><cell>Annotator_2</cell></row></table><note>httpsgoog (b) Image Classification and Object Detection</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 1 Left: The statistics of the number of bounding boxes per image.Right: Summary of Bamboo.10 11 12 13 14 15 16 17 18 19 20</head><label>1</label><figDesc>Quantitatively, our new annotated data has 8.3 instances (on average) per image, which is more dense compared with the other datasets like COCO and OpenImages. Bamboo is the largest fully annotated vision dataset available to the general research community, in terms of the total number of images, the number of concepts, and the number of bounding boxes (for object detection task).</figDesc><table><row><cell>Percentage of Images</cell><cell>0% 13% 25%</cell><cell>Object365 New Annotated 0 1 2 3 4 5 6 7 8 9 COCO OpenImages COCO Number of Instances</cell><cell>OpenImages Object365</cell><cell>New Annotated 4.3 5.5 4.5 9 Average Instance Per Image 7.9 8.3 0</cell><cell>Datasets YFCC-100M [63] ImageNet22K [18] Bamboo-CLS COCO [44] Objects365 [59] OpenImages [41] Bamboo-DET</cell><cell>Concepts -22K 119K 80 365 600 809</cell><cell>Images 100M 14M 69M 118K 609K 2M 3M</cell><cell>Boxes ---1M 10M 14M 27M</cell><cell>Anno. No Yes Yes Yes Yes Partial Yes</cell></row><row><cell cols="3">SUN397</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 Downstream classification tasks performance among different pre-training methods.</head><label>2</label><figDesc>Bamboo achieves the state-of-the-arts linear probe performance on the downstream tasks. Lang. indicates image-text pair. Numbers in red are the performance gain on the same backbone network. Bamboo here refers to the Bamboo-CLS. Pets indicates OxfordPets. Flowers indicates OxfordFlower. Cars indicates StanfordCars. Aircraft indicates FGVC-Aircraft. IN1K indicates ImageNet1K. Results reported by the author are marked in gray. We mainly compare with the methods conducted on supervised learning. Other performance of current methods are also presented.</figDesc><table><row><cell>Method</cell><cell>Data</cell><cell>Annotation</cell><cell>Model</cell><cell>Paradigm</cell><cell>CIFAR10</cell><cell>CIFAR100</cell><cell>Food101</cell><cell>Pets</cell><cell>Flowers</cell><cell>SUN397</cell><cell>Cars</cell><cell>DTD</cell><cell>Caltech101</cell><cell>Aircraft</cell><cell>IN1K</cell><cell>AVG?</cell></row><row><cell cols="2">SwAV [10] IN1K</cell><cell>1.2M</cell><cell>RN50</cell><cell>Self.</cell><cell cols="12">92.5 76.6 76.4 88.0 93.0 65.5 60.5 78.1 91.0 56.0 66.9 76.8</cell></row><row><cell>DINO [11]</cell><cell>IN1K</cell><cell>1.2M</cell><cell>RN50</cell><cell>Self.</cell><cell cols="12">93.7 79.2 77.2 89.2 96.2 66.0 68.3 77.6 92.3 63.1 83.3 79.8</cell></row><row><cell>SWSL [71]</cell><cell>IG-1B</cell><cell>1B</cell><cell>RN50</cell><cell>Semi.</cell><cell cols="12">94.7 79.5 79.1 94.4 94.6 67.8 65.9 77.8 96.1 58.4 81.2 80.9</cell></row><row><cell>WSL [46]</cell><cell>IG-1B</cell><cell>1B</cell><cell cols="2">RX101 Weak.</cell><cell cols="12">95.0 78.2 83.5 95.5 90.8 67.9 72.3 75.3 93.3 53.9 83.3 81.0</cell></row><row><cell>CLIP [53]</cell><cell>WIT</cell><cell cols="2">400M RN50</cell><cell>Lang.</cell><cell cols="12">88.7 70.3 86.4 88.2 96.1 73.3 78.3 76.4 89.6 49.1 73.3 79.1</cell></row><row><cell>CLIP [53]</cell><cell>WIT</cell><cell cols="2">400M B/16</cell><cell>Lang.</cell><cell cols="12">96.2 83.1 92.8 93.1 98.1 78.4 86.7 79.2 94.7 59.5 80.2 85.6</cell></row><row><cell>BiT [37]</cell><cell>IN1K</cell><cell>1.2M</cell><cell>RN50</cell><cell>Sup.</cell><cell cols="12">91.7 74.8 72.5 92.3 92.0 61.1 53.5 72.4 91.2 52.5 75.2 73.6</cell></row><row><cell>BiT [37]</cell><cell>IN22K</cell><cell>14M</cell><cell>RN50</cell><cell>Sup.</cell><cell cols="12">94.9 82.2 83.3 91.5 99.4 69.9 59.0 77.3 93.9 55.6 76.7 80.3</cell></row><row><cell>RN50</cell><cell cols="2">Bamboo 69M</cell><cell>RN50</cell><cell>Sup.</cell><cell cols="12">93.9 81.2 85.3 92.0 99.4 72.2 91.1 76.5 93.2 84.0 77.2 86.0 (+5.1)</cell></row><row><cell>B/16</cell><cell cols="2">Bamboo 69M</cell><cell>B/16</cell><cell>Sup.</cell><cell cols="12">98.2 90.2 92.9 95.1 99.8 79.0 93.3 81.2 97.0 88.1 83.6 91.8 (+6.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 Comparisons of downstream detection tasks performance.</head><label>3</label><figDesc>Pre-trained model on Bamboo achieves significant performance gain. Bamboo here refers to the Bamboo-DET. VOC means the PASCAL VOC dataset<ref type="bibr" target="#b20">[22]</ref>. CITY. means the CityPersons dataset<ref type="bibr" target="#b77">[79]</ref>.</figDesc><table><row><cell>Data</cell><cell>Anno.</cell><cell>VOC AP50 ?</cell><cell>CITY. MR ?</cell><cell>COCO mmAP ?</cell></row><row><cell>COCO [59]</cell><cell>1M</cell><cell>85.1</cell><cell>16.2</cell><cell>-</cell></row><row><cell cols="2">OpenImages [59] 14M</cell><cell>82.4</cell><cell>16.8</cell><cell>37.4</cell></row><row><cell>Objects365</cell><cell>10M</cell><cell>86.4</cell><cell>14.7</cell><cell>39.3</cell></row><row><cell>Bamboo</cell><cell>27M</cell><cell>87.5 (</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 4 Comparisons of zero-shot downstream classification tasks performance among different pre-training methods.</head><label>4</label><figDesc>Bamboo achieves the state-of-the-arts linear probe performance on the downstream tasks. Lang. indicates image-text pair. Numbers in red are the performance gain on the same backbone network. Bamboo here refers to the Bamboo-CLS. Pets indicates OxfordPets. Flowers indicates OxfordFlower. Cars indicates StanfordCars. Aircraft indicates FGVC-Aircraft. IN1K indicates ImageNet1K.Results reported by the author are marked in gray. We mainly compare with the methods conducted on supervised learning. Other performance of current methods are also presented.</figDesc><table><row><cell>Method</cell><cell>Data</cell><cell>Annotation</cell><cell>Model</cell><cell>Paradigm</cell><cell>CIFAR10</cell><cell>CIFAR100</cell><cell>Food101</cell><cell>Pets</cell><cell>Flowers</cell><cell>SUN397</cell><cell>Cars</cell><cell>DTD</cell><cell>Caltech101</cell><cell>Aircraft</cell><cell>IN1K</cell><cell>AVG?</cell></row><row><cell cols="2">CLIP [53] WIT</cell><cell cols="3">400M RN50 Lang.</cell><cell cols="8">91.6 68.7 89.2 88.9 70.4 65.2 65.6 46</cell><cell cols="4">89.3 27.1 68.6 70.0</cell></row><row><cell>RN50</cell><cell cols="2">Bamboo 69M</cell><cell cols="2">RN50 Sup.</cell><cell cols="12">93.8 67.7 81.6 74.3 87.3 58.7 63.0 51.1 88.4 87.2 82.5 76.0 (+6.0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 5 Comparisons of fine-tuning downstream classification tasks performance among different pre-training methods.</head><label>5</label><figDesc>Bamboo achieves the state-of-the-arts fine-tuning performance on the downstream tasks.</figDesc><table><row><cell>Method</cell><cell>Data</cell><cell>Annotation</cell><cell>Model</cell><cell>Paradigm</cell><cell>CIFAR10</cell><cell>CIFAR100</cell><cell>Food101</cell><cell>Pets</cell><cell>Flowers</cell><cell>SUN397</cell><cell>Cars</cell><cell>DTD</cell><cell>Caltech101</cell><cell>Aircraft</cell><cell>IN1K</cell><cell>AVG?</cell></row><row><cell>DINO</cell><cell>IN1K</cell><cell cols="3">1.2M RN50 Self.</cell><cell cols="12">97.1 84.0 86.3 90.0 96.1 65.2 84.6 77.6 91.4 81.8 66.5 83.7</cell></row><row><cell cols="2">SWAV IN1K</cell><cell cols="3">1.2M RN50 Self.</cell><cell cols="12">97.2 84.2 86.0 90.3 95.7 64.4 83.9 77.2 91.7 81.2 66.9 83.5</cell></row><row><cell>SWSL</cell><cell>IG-1B</cell><cell>1B</cell><cell cols="2">RN50 Semi.</cell><cell cols="12">97.0 86.5 87.3 94.4 97.0 66.0 88.5 78.3 93.8 84.0 81.7 86.8</cell></row><row><cell>BiT-S</cell><cell>IN1K</cell><cell cols="3">1.2M RN50 Sup.</cell><cell cols="12">97.0 85.0 85.7 92.8 95.0 60.3 87.5 74.7 92.0 83.8 75.2 84.5</cell></row><row><cell>BiT-M</cell><cell>IN22K</cell><cell>14M</cell><cell cols="2">RN50 Sup.</cell><cell cols="12">97.6 86.2 87.9 91.5 98.1 64.2 88.2 78.4 92.9 84.3 76.7 86.0</cell></row><row><cell>RN50</cell><cell cols="2">Bamboo 69M</cell><cell cols="2">RN50 Sup.</cell><cell cols="12">97.3 87.0 87.5 92.0 99.4 72.2 91.4 77.1 93.9 85.9 77.1 87.3 (+0.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4</head><label>4</label><figDesc>shows the comparison. We can indicate that Bamboo model conclusively outperforms CLIP model with the same backbone: RN50. Specifically, Bamboo model achieves a 6% average accuracy gain. On the FGVC-Aircraft, Bamboo model achieves 87.2%, despite having never seen any training images from this dataset. Bamboo includes all the concepts in the downstream tasks. However, we conduct data overlap analysis of Bamboo in Sec. 7, ensuring Bamboo rarely includes downstream data.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. This step is not included in the current active learning research.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Billion-scale pretraining with vision transformers for multi-task visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kislyuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.05887</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arushi</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elijah</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vighnesh</forename><surname>Birodkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03494</idno>
		<title level="m">The iwildcam 2021 competition dataset</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Duplicate image detection with perceptual hashing in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoyt</forename><surname>Ben</surname></persName>
		</author>
		<ptr target="https://benhoyt.com/writings/duplicate-image-detection/#difference-hash-dhash" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simran</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<title level="m">On the opportunities and risks of foundation models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Food-101 -mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Authors guild v. google, inc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DePaul J. Art Tech. &amp; Intell. Prop. L</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">59</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui</forename><surname>Citovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lazaros</forename><surname>Karydas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.14263</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Batch active learning at scale</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A tale of two long tails</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Daniel D&amp;apos;souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chirag</forename><surname>Nussbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13098</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Estimation lemma -Wikipedia, the free encyclopedia</title>
		<imprint>
			<date type="published" when="2010-09" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
	<note>Estimation lemma</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A core of semantic knowledge unifying wordnet and wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kasneci</forename><surname>Ms Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gjergji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gerhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="178" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale privacy protection in google street view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Abdulkader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Zennaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartmut</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2373" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.11353</idno>
		<title level="m">Multi-task self-training for learning general representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Query by committee made real</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Gilad-Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Navot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised active learning with temporal output discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3447" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Combining generative and discriminative models for semantic segmentation of ct scans via active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Eugenio</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ender</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Montillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biennial International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiclass active learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ajay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2372" to="2379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Foodx-251: A dataset for fine-grained food classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parneet</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06167</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04977</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Do better imagenet models transfer better? In CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="577" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical contrastive learning of unsupervised representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Finegrained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">WordNet: An electronic lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A visual vocabulary for flower classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luk??</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji??</forename><surname>Milan?ulc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Laess?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fr?slev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10107</idno>
		<title level="m">Danish fungi 2020-not just another image recognition dataset</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Margin-based active learning for structured output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Laion-400m: Open dataset of clipfiltered 400 million image-text pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kaczmarczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Active learning for convolutional neural networks: A core-set approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR. OpenReview.net</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8430" to="8439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Active testing for face detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Jedynak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1914" to="1920" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Yago 4: A reason-able knowledge base. The Semantic Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Thomas Pellissier Tanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suchanek</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">12123</biblScope>
			<biblScope unit="page" from="583" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Weakly supervised structured output learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="845" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Sun database: Exploring a large collection of scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>I Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klint</forename><surname>Qinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="547" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqueline</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06191</idno>
		<title level="m">A study of face obfuscation in imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A large-scale car dataset for fine-grained categorization and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3973" to="3981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Multi-class active learning by uncertainty sampling with diversity maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="127" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Re-labeling imagenet: from single to multi-labels, from global to localized labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05022</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><forename type="middle">Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04867</idno>
		<title level="m">Alexey Dosovitskiy, et al. A largescale study of representation learning with the visual task adaptation benchmark</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Citypersons: A diverse dataset for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Places: A 10 million image database for scene recognition. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Rethinking pre-training and selftraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A new dataset of dog breed images and a benchmark for finegrained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding-Nan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Hai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
