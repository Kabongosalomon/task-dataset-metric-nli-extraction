<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">XnODR and XnIDR: Two Accurate and Fast Fully Connected Layers For Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department Of Computer Science</orgName>
								<orgName type="institution">University of Denver</orgName>
								<address>
									<addrLine>2155 E Wesley Ave</addrLine>
									<postCode>80210</postCode>
									<settlement>Denver</settlement>
									<region>Colorado</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Pourramezan</forename><surname>Fard</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department Of Computer Engineering</orgName>
								<orgName type="institution">University of Denver</orgName>
								<address>
									<addrLine>2155 E Wesley Ave</addrLine>
									<postCode>80210</postCode>
									<settlement>Denver</settlement>
									<region>Colorado</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department Of Computer Engineering</orgName>
								<orgName type="institution">University of Denver</orgName>
								<address>
									<addrLine>2155 E Wesley Ave</addrLine>
									<postCode>80210</postCode>
									<settlement>Denver</settlement>
									<region>Colorado</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">XnODR and XnIDR: Two Accurate and Fast Fully Connected Layers For Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>A R T I C L E I N F O</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>CapsNet XNOR-Net Dynamic Routing Binarization Xnorization Machine Learning Neural Network</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B S T R A C T</head><p>Although Capsule Network is powerful at defining the positional relationship between features in deep neural networks for visual recognition tasks, it is computationally expensive and not suitable for running on mobile devices. The bottleneck is in the computational complexity of the Dynamic Routing mechanism used between the capsules. On the other hand, XNOR-Net is fast and computationally efficient, though it suffers from low accuracy due to information loss in the binarization process. To address the computational burdens of the Dynamic Routing mechanism, this paper proposes new Fully Connected (FC) layers by xnorizing the linear projector outside or inside the Dynamic Routing within the CapsFC layer. Specifically, our proposed FC layers have two versions, XnODR (Xnorize the Linear Projection Linear Projector Outside Dynamic Routing) and XnIDR (Xnorize the Linear Projection Linear Projector Inside Dynamic Routing). To test the generalization of both XnODR and XnIDR, we insert them into two different networks, MobileNet V2 and ResNet-50. Our experiments on three datasets, MNIST, CIFAR-10, and MultiMNIST validate their effectiveness. The results demonstrate that both XnODR and XnIDR help networks to have high accuracy with lower FLOPs and fewer parameters (e.g., 95.32% correctness with 2.99M parameters and 312.04M FLOPs on CIFAR-10).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the advancement of new computing devices, Convolutional Neural Networks (CNNs) show dominance in image classification due to the CNNs' powerful and efficient feature extraction ability. Despite their power, CNNs have some limitations in capturing the positional relation between features in images. For example, a face with randomly ordered eyes, ears, nose, mouth, and eyebrows will be wrongly recognized as a human face <ref type="bibr" target="#b24">[25]</ref>.</p><p>In 2017, CapsuleNet (CapsNet) was proposed to address this problem. CapsNet introduces a new concept called Capsule, which is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part <ref type="bibr" target="#b47">[47]</ref>. In better words, a capsule is a vector, where its length size means the possibility of the appearance of an object or an image property. Its direction represents the object's image property, such as location, shape, size, direction, etc. The capsule's direction is mutually exclusive to its length. To deploy Capsule, <ref type="bibr" target="#b47">[47]</ref> took the idea of K-Means clustering, created a Dynamic Routing (DR) mechanism as the classifier, embedded it into the network's final FC layer, and called it CapsFC layer.</p><p>The CapsFC layer contains a linear projector and DR, which is an iterative process (see Section 3.1 for more details). The linear projector outside the iterative structure takes 5-dimensional capsules as input variables, while the usual linear projector only accepts 2-dimensional flatten tensors. Dimension expansion causes the surge of parameters, multiplication and addition operations (MADD <ref type="bibr" target="#b48">[48]</ref>), and processing time. Routing iterations have a similar influence too. Therefore, compared with the usual FC layers, the CapsFC layer is slow on both training and inference due to the dimension expansion and routing iterations. Furthermore, <ref type="table" target="#tab_5">Table 4</ref> shows that CapsNet has 99.65% accuracy with 6.80M parameters on MNIST, while MobileNet V2 (with upsampling) achieves 99.50% accuracy (a comparable result) with a network containing 3.05M parameters (less than half of CapsNet's parameter). In practice, the large numbers of network parameters and its slow inference speed weaken the effect of CapsNet on mobile devices. In terms of model speed, Xnorization has become a simple and efficient approximation technique to CNNs <ref type="bibr" target="#b38">[39]</ref> (see Section 3.3 and Appexdix A). We call a network XNOR-Net if all the layers, except the first one, are the xnorized ones. The experimental results on MNIST reported in <ref type="bibr" target="#b38">[39]</ref> demonstrate that XNOR-Net can achieve high accuracy with fewer operations and faster speed. Additionally, the experiments show that XNOR-Net is more accurate than Binary Weight Network (BWN) on ImageNet <ref type="bibr" target="#b9">[10]</ref>. However, the drawback of Xnorization is information loss, which results in lower accuracy compared to the full-precision AlexNet on ImageNet. Frankly, ImageNet usually requires a deep neural network, which has more convolution layers, such as CoCa and CoAtNet-7 <ref type="bibr" target="#b63">[62]</ref>, <ref type="bibr" target="#b7">[8]</ref>. However, the more layers XNOR-Net xnorizes, the more information it loses. Too much information loss impairs the XNOR-Net to classify small-size images, small object images, or complex images. Therefore, XNOR-Net only results in 44.2% Top-1 accuracy on ImageNet <ref type="bibr" target="#b38">[39]</ref>. In general, XNOR-Net fits to classify the small-scale datasets such as MNIST well rather than the large-scale and complex datasets like ImageNet.</p><p>Both CapsNet and XNOR-Net have pros and cons. Intuitively, we decide to define a layer to take advantages of both networks. We envision a layer that enables the network to maintain comparable or higher accuracy like CapsNet while increasing the network speed like XNOR-Net. Hence, we fuse the CapsFC layer and Xnorization into the same FC layer.</p><p>Specifically, the CapsFC layer has two linear projectors, and (see Sections 3.1 and 3.3). is outside the DR, while is inside it. We xnorize and separately in our proposed FC layers to reduce the parameters and floating point of operations (FLOPs). In summary, we propose two different layers XnODR (Xnorize the Linear Projection Outside Dynamic Routing) and XnIDR (Xnorize the Linear Projection Inside Dynamic Routing).</p><p>To test the generalization of XnODR and XnIDR, we utilize them separately to replace the usual FC layers of the typical lightweight model, MobileNet V2. In addition, we do the same procedure on the representative heavyweight model, ResNet-50. We validate these variants on MNIST, CIFAR-10, and MultiMNIST datasets. The experimental results show that XnODR and XnIDR can properly replace the dense layers on the lightweight and heavyweight models. Section 4 shows that both XnODR and XnIDR help speed the model computation and maintain comparable or even better accuracy.</p><p>Overall, the contributions of this paper are as follows:</p><p>? We propose a new Fully Connected Layer called XnODR by xnorizing the linear projection outside the dynamic routing.</p><p>? We propose a new Fully Connected Layer called XnIDR by xnorizing the linear projection inside the dynamic routing.</p><p>? We calculated and recorded the binary operations of XNOR operation in XnODR and XnIDR during the experiments.</p><p>? XnODR and XnIDR improves the performance (i.e., better accuracy, less FLOPS, and parameters) of both lightweight (MobileNet V2) and heavyweight (ResNet-50) models.</p><p>The remainder of the paper is organized as follows. Section 2 provides an overview of related work. Section 3 explains the new proposed FC layers. Section 4 introduces databases used in this work and presents the experimental configuration, evaluation metrics, experimental results, ablation study, and analyses. We discuss our work in Section 5 and finally conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Capsule Network</head><p>CNNs focus more on extracting features from images rather than orientational and relative spatial relationships between those features. The max-pooling layer helps CNNs to work surprisingly better than previous CNN models. The max-pooling layer increases the CNNs' performance and covers their orientational and relative spatial relationships problem. However, the max-pooling layer still loses much valuable information. CapsNet has solved this problem <ref type="bibr" target="#b47">[47]</ref>. It consists of Convolutional layers, PrimaryCapsule layers, CapsFC layers, and a decoder 1 . The PrimaryCaps layer consists of the convolutional operation and max-pooling. The effect of the CapsFC layer is a classifier like the usual FC layer. However, the CapsFC layer makes up of an affine transformation and the DR mechanism (iteratively weighted-sum), which significantly improve the accuracy and interpretation of classification.</p><p>Researchers, who were inspired by CapsNet, improved the original CapsNet with different ideas, such as better performance on complex datasets <ref type="bibr" target="#b55">[54]</ref>, providing solid equivariance and invariance property <ref type="bibr" target="#b30">[31]</ref>, and detecting objects from features on a one-dimensional linear subspace <ref type="bibr" target="#b2">[3]</ref>. Then, Aff-CapsNets improved the robustness of affine transformations and dropped the dynamic routing mechanism <ref type="bibr" target="#b15">[16]</ref>. MRCapsNet and Res-CapsNet focused on enhancing feature extraction capability <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b25">[26]</ref>. The aforementioned papers pay more attention to elevating the accuracy of the CapsNet.</p><p>Other researchers have explored the other possibilities of CapsuleNet, such as addressing the visual distortion problem <ref type="bibr" target="#b34">[35]</ref>, fine-grained classification <ref type="bibr" target="#b35">[36]</ref>, text classification <ref type="bibr" target="#b26">[27]</ref>, wind speed prediction <ref type="bibr" target="#b31">[32]</ref>, and environmental monitoring <ref type="bibr" target="#b65">[64]</ref>. These papers extended the good performance of CapsNet to other tasks beyond purely image classification.</p><p>In our work, we focus on improving CapsNet's speed. CapsNet is time-consuming to train and run inference, especially on complex datasets like CIFAR-10, because of the DR's iterative structure and large-scale floatingpoint operations during convolutional calculation and linear projection. This paper presents a solution to address the lowspeed performance of the CapsNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">XNOR Network</head><p>CNNs' excessive parameters usually cause inefficient computation and memory over-utilization. Researchers have proposed several methods to address this problem.</p><p>Some researchers proposed the theory of Shallow networks and did related experiments <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b62">[61]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. The core idea of Shallow networks is to mimic deep neural networks to get similar numbers of parameters and equivalent accuracy. Otherwise, Shallow networks return less comparable accuracy on ImageNet <ref type="bibr" target="#b8">[9]</ref>.</p><p>It is also sensible to assemble CNNs with compact blocks that cost less memory and FLOPs. For example, GoogleNet, ResNet, and SqueezeNet proposed new layers or structures and achieved several benchmarks with the cost of fewer parameters <ref type="bibr" target="#b51">[50]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Then, HGCNet was capable to elevate representation capability by fusing feature maps from different groups <ref type="bibr" target="#b56">[55]</ref>.</p><p>Since CNNs can achieve good performance without the need of high precision parameters, parameter quantization is a viable option to speed up the network computation. Therefore, researchers proposed many novel ideas, such as quantizing the weights of FC layers <ref type="bibr" target="#b14">[15]</ref>, using ternary weights and 3-bits activations <ref type="bibr" target="#b22">[23]</ref>, only quantizing neurons during the back-propagation process <ref type="bibr" target="#b33">[34]</ref>, and vector quantization method <ref type="bibr" target="#b13">[14]</ref>.</p><p>Other researchers focused on improving either the accuracy or the speed of the networks using the quantization methods as well <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b59">[58]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>XNOR-Net uses standard deep architectures instead of shallow ones, and trains networks from scratch rather than implementing pre-trained networks or networks with compact layers. Moreover, it quantizes the weights and input values with two factors, +1, -1, instead of +1, 0, -1 <ref type="bibr" target="#b0">[1]</ref>. <ref type="bibr" target="#b38">[39]</ref> stated that the typical CNNs would cost more time as the size of tensors increases because that causes more multiplication and division operations while doing the convolutional calculations. To reduce the processing time and maintain the prediction accuracy, <ref type="bibr" target="#b38">[39]</ref> proposed a new concept called Xnorization (see Appendices A and B). The advantage of the XNOR operation is that it uses plus and minus to do convolutional calculations rather than multiplication and division. Therefore, XNOR can save substantial processing time during the training time. It is worth mentioning that XNOR-Net has comparable performance on MNIST and CIFAR10 compared to BNN(Binary Neural Network) <ref type="bibr" target="#b38">[39]</ref>.</p><p>XNOR-Net has many variants too. Ternary Sparse XNOR-Net <ref type="bibr" target="#b61">[60]</ref>, XNOR-Net++ <ref type="bibr" target="#b3">[4]</ref>, and Bi-Real-Net are good examples <ref type="bibr" target="#b36">[37]</ref>. They put effort on improving model representational capability. Zhu et al. proposed XOR-Net to offer a pipeline for binary networks both with and without scaling factors <ref type="bibr" target="#b68">[67]</ref>. The above papers aimed to reduce memory usage, speed up the inference time, and improve accuracy by creating different quantization methods or the new pipeline. Simultaneously, XNOR-Net has broad prospects on application as well, such as bird sound detection <ref type="bibr" target="#b64">[63]</ref>, reducing the impact of RRAM noise, and improving energy efficiency <ref type="bibr" target="#b67">[66]</ref>.</p><p>We also found that Xnorization causes the network to lose much information. Insufficient information prevents XNOR-Net from achieving as high accuracy as CNNs, such as MobileNet V2 and ResNet-50. In this work, we focus on improving accuracy. Our idea is to add a routing mechanism rather than modifying the sign function, scaling factor, or the CNNs' main body. In this paper, CNNs' main body represents the CNNs without any FC layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Different Routing Mechanisms</head><p>Designing a new routing mechanism becomes popular after the appearance of DR. EM Routing algorithm <ref type="bibr" target="#b46">[46]</ref> and VB Routing method <ref type="bibr" target="#b44">[45]</ref> are good examples. EM Routing is more accurate than DR with the help of Expectation-Maximization algorithm and Gaussian Mixture model, but it becomes more time-consuming than DR because of expanding the capsule to 2-dimensional pose matrix and more for loops. People without many computing resources may hesitate to select EM Routing to train large-scale datasets such as ImageNet. VB Routing, on the other hand, has more flexible control over capsule complexity by tuning priors to induce sparsity, and reducing the variance-collapse singularities inherent to MLE-based mixture models such as EM. Generally, VB Routing helps overcome overfitting and unstable training issues found in EM Routing. However, researchers only validated VB Routing and EM Routing on small-scale datasets, such as MNIST and CIFAR-10, instead of the large-scale ones, such as ImageNet or MS-COCO <ref type="bibr" target="#b32">[33]</ref>. It would be more helpful if VB Routing helped reach high accuracy on large-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">New Fully Connected Layer: XnODR and XnIDR</head><p>This section introduces the concept of XnODR and XnIDR and the steps to fuse the CapsFC layer and the Xnorization. Both XnODR and XnIDR obtain the properties of high accuracy from CapsNet and fast inference speed from XNOR-Net, and they are modified based on CapsF-CLayer. See Section 3.3 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dynamic Routing Review</head><p>CapsFCLayer has two parts, affine transformation and DR. DR <ref type="bibr" target="#b47">[47]</ref> helps to activate core capsules, suppresses unimportant ones from lower layers, and highlight the core capsules with high probability. At the same time, as an iterative process, it improves the performance by iteratively updating the output capsules. It is used in XnODR and XnIDR as the routing mechanism too. Next, we describe its concept in detail.</p><p>Traditional neurons have three steps, weighting, sum, and nonlinearity activation, which can be summarized as:</p><formula xml:id="formula_0">= ? + ? = ( )<label>(1)</label></formula><p>where is the input value from the neuron , is randomly generated weight for , is the bias, is the ? neuron's output of linear projection, is the nonlinearity activation function, ? is the ? neuron's final output. In CapsFCLayer, <ref type="bibr" target="#b47">[47]</ref> took the capsules as the input variable and designed a new activation function, Squash function. Then, they added affine transformation before the weighting step. This makes the CapsFCLayer have four steps, affine transformation, weighting, sum, and nonlinearity activation, which can be summarized as:</p><formula xml:id="formula_1">| = (2) = ? ? | (3) = || || 2 1 + || || 2 ? || ||<label>(4)</label></formula><p>where denotes the capsule from lower layer , and denotes the weight matrix between capsule and . | is the prediction capsules from layer to layer + 1. Here, Eq. 2, denoted as , is the affine transformation. The vectors are multiplied by the corresponding weight matrices that encode important spatial and other relationships between the lower level features (eyes, mouth and nose) and higher level feature (face). The meaning of affine transformation here is to observe the object from different views and angles. Then, multipling by returns the predicted capsules? | . Next, in Eq. 3, represents the weight that multiplies the predicted vector? | from the lower-level capsules and serves as the input to a higher level capsule. In simple terms, is the coupling coefficient that depicts the relationship between capsule and capsule .</p><p>is the output of capsule from layer + 1. In the meanwhile, measures the probability that activates . is determined by the Softmax function and a new coefficient (Eq. 5), which is a temporary value that is updated iteratively. The sum of is 1. At the start of the training, the value of is initialized to zero.</p><formula xml:id="formula_2">= ( ) ? ( )<label>(5)</label></formula><p>Finally, they apply the Squash function (Eq. 4), an activation function like Relu, to activate the output capsules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>||</head><p>|| is the L2-norm of . is the output of capsule from layer + 1 after squashing. The Squash function controls the size of to be less than 1 and preserves its direction.</p><p>is the final output vector of the capsule , which represents the activated capsule. Here, also helps update , which can be formulated as:</p><formula xml:id="formula_3">= +? | ? .<label>(6)</label></formula><p>Eq. 6 shows that the new equals to the old plus the dot product of the activated capsule and the predicted capsule? | . We denote? | ? as . The dot product looks at the similarity between predicted capsules and activated capsules. Also, the lower-level capsule will send its output to the higher-level one, whose output is similar to the predicted one. This similarity is captured by the dot product. The larger the dot product, the higher the correlation between the activated capsule and the predicted capsule is, and the larger the is. Then, referring to Section 3.3, we modified Eq. 2 in the proposed XnODR, while changing Eq. 6 to get XnIDR.</p><p>The aforementioned parameters get updated iteratively. According to <ref type="bibr" target="#b47">[47]</ref>, the model would perform well when setting the iteration number to 3.</p><p>In general, Sabour et al.summarized the above formulations to an iterative process called DR <ref type="bibr" target="#b47">[47]</ref>. Iterative routing process implements the property of local feature maps to calculate and decide whether or not to activate capsules. Moreover, with the help of the capsules, DR takes the feature maps' location, direction, size, and other detailed information into consideration rather than simply detecting features such as CNNs. For example, we can make either a house or a sailboat with one square and one triangle. If we train the network by the house and test it on a sailboat, CNNs would wrongly classify it as a "house" since it only detects features independently <ref type="bibr" target="#b66">[65]</ref>. Oppositely, CapsNet with Dynamic Routing would activate related sailboat capsules, avoid mistakes after comprehensive analysis, and help to improve the prediction result by updating capsules in the FC layer. Section 3.3 provides the defection analysis of DR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">XnorNet Review</head><p>Binarization and XnorConvLayer are two vital concepts introduced in <ref type="bibr" target="#b38">[39]</ref> and used to define new Fully Connect layer. We refer the reader to Appendixes A and B for more details. Next, we directly introduce XnODR and XnIDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">XnODR and XnIDR</head><p>Referring to Section 1, a usual dense layer only has one linear projector, while the CapsFC layer has and . Too many floating-point calculations in , as one of the reasons, cause the time-consuming issue. The usual dense layer only takes a 2-dimensional tensor as input, while requires to expand the input feature from three dimensions to five dimensions. Therefore, costs more MADD operations than the original dense layer <ref type="bibr" target="#b48">[48]</ref>. Simultaneously, the is in DR. The iterative structure of the DR mechanism is another reason to slow down the model speed. The DR uses more trainable parameters so that it enlarges disparity on MADD from the usual dense layer and spends more time on inference. Simply stated, to seek implicit information, CapsNet trades off accuracy with speed. For example, CapsNet achieves a Top-1 error rate of less than 0.5% on small-scale and simple datasets, such as MNIST.</p><p>Given that "fully connected layers can be implemented by convolution", which means that the FC layer is like a convolution layer with kernel size 1 ? 1 <ref type="bibr" target="#b38">[39]</ref>, it is also feasible to binarize the FC layer in XNOR-Net. Binarization is a very convenient function. However, it averages the pixel values among each channel as a scaling factor that breaks the hierarchy of the pixel values and fails to collect many implicit features. Furthermore, it only approximates the pixels simply by the product of the sign matrix and scaling factor, which exacerbates the information loss, a very apparent negative influence. Thereby, Xnorization at different layers aggravates the network's information loss, which prevents XNOR-Net from performing as well as fullprecision CNN-based models. This disparity is minor on small-scale and simple datasets but is apparent on largescale complex datasets such as ImageNet and AffectNet <ref type="bibr" target="#b41">[42]</ref>. For example, in ImageNet, compared to 56.6% Top-1 accuracy at the full-precision AlexNet, AlexNet with Xnorization only achieves 44.2% top1 accuracy, which warns us of the importance of xnorizing the correct layers. The accuracy of network will be closer to that of a fullprecision model if we only xnorize the final dense layer instead of the second convolution layer. The reason is that the network already extracts enough feature maps before the last Dense layer. Xnorizing the last Dense layer causes less information loss than xnorizing the second layer, where the network exactly starts mining feature maps.</p><p>Intuitively, we fuse the CapsFC layer from CapsuleNet and Xnorization from XNOR-Net to create a new FC layer, a more accurate and faster layer. During the training and inference, this layer implements Xnorization to simplify operations and speed up the model. It also helps maintain a comparable prediction accuracy by taking advantage of Capsules and DR to extract the direction, location, and other sophisticated information among feature maps. Furthermore, the new FC layer would do binarization before the linear projector, and replace multiplications with additions and subtractions. In detail, we xnorize to get the first  new FC layer while xnorizing to get the second one. In total, there are two versions.</p><p>Given that both Eqs. 2 and 6 take one capsule as the input, we rewrite two equations as Eqs. 7 and 8 before applying to all capsules.</p><formula xml:id="formula_4">= (7) = +? ?<label>(8)</label></formula><p>where represents the input capsules, is the weight,? represents the predicted capsules, represents all temporary values, and represents the activated capsules. <ref type="table" target="#tab_0">Table 1</ref> shows the related size of each variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">XnODR(Xnorizes the Linear Projector Outside the Dynamic Routing)</head><p>The core of XnODR is to xnorize the affine transformation, Eq. 7. Let denote output tensors from the PrimaryCap layer. <ref type="table" target="#tab_1">Table 2</ref> summarized the specific steps. 5 Affine Transformation:</p><formula xml:id="formula_5">* ? ( ? ) ? . 6 Let [ , , , ?, ?] = * , ? [0, ], ? [0, _ ], ? [0, _ ]. 7 = _ ( ).</formula><p>and are the binary filters, while and are the scaling factors. We also illustrate them in <ref type="figure" target="#fig_2">Fig. 2</ref> Given that we average the last channel of , the size of is [ , , , According to the theory of <ref type="bibr" target="#b38">[39]</ref>, the total number of operations in a standard convolution is , where is the channel number, = ?, = ? . "With the current generation of CPUs, we can perform 64 binary operations in one clock of CPU" <ref type="bibr" target="#b38">[39]</ref>. The total parameters from the xnorized convolution is 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="64">+</head><p>, where is the binary operation, is the non-binary operation. The speed-ratio equation, also known as Speed Up, is summarized as Eq. 9. It represents the times of the FLOPs of convolutional operation over the xnorized convolution. </p><p>In our case, we only compare the operations of the linear projector before DR, since we only binarize this part in XnODR. We take as the input for the linear projector, and as each capsule's dimension. We also take as the related weight, and as the product of and . Therefore, we formulate the operations of the usual linear projector as Eq. 10:</p><p>? .</p><p>The operations of binary one is formulated as Eq. 11:</p><formula xml:id="formula_9">1 64 ? + .<label>(11)</label></formula><p>Therefore, the speed-ratio is: 2  <ref type="table">Table 3</ref> XnIDR Algorithm</p><formula xml:id="formula_10">1 64 2 + .<label>(12)</label></formula><formula xml:id="formula_11">Procedure 2: XnIDR 1 Expand ? . 2? = . 3 it = 0 while it &lt; iteration_number: 4 = 0 5 = ( ) ? ( ) 6 = ? ? | 7 = || || 2 1+|| || 2 || || 8 Binarize? ? ? and ? . 9</formula><p>Binarize ? and .</p><formula xml:id="formula_12">10 = + ? ? ? ? 11 return .</formula><p>We show the result in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">XnIDR(Xnorize the Linear Projector Inside Dynamic Routing)</head><p>The core of XnIDR is to xnorize the linear projector inside DR (see Eq. 8). We summarized the whole procedure in <ref type="table">Table 3</ref>. ? and are the binary filters, ? and are the scaling factors. We plot this algorithm in <ref type="figure" target="#fig_4">Fig. 3</ref>.</p><p>Given that we average the last channel of? , the size of ? is [ , , , In the meanwhile, we only compare the operation times of linear projector within DR for Speed Up, since we only xnorize this part in XnIDR. Here, we take? as the input of linear projector, which is a tensor of capsules, and select to represent the weight. Therefore, we formulate the operations of the usual linear projector as Eq. 13:</p><formula xml:id="formula_14">_ ? _ ? _ 2 .<label>(13)</label></formula><p>The operations of binary one is formulated as Eq. 14:</p><formula xml:id="formula_15">1 64 _ ? _ ? _ 2 + _ . (14)</formula><p>The speed-ratio is formulated as Eq. 15:</p><formula xml:id="formula_16">_ ? _ ? _ 2 1 64 _ ? _ ? _ 2 + _ .<label>(15)</label></formula><p>The result is shown in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Summary</head><p>There are two linear projectors in the original CapsF-CLayer <ref type="bibr" target="#b47">[47]</ref>. One is outside DR, while the other is inside DR. XnODR xnorizes the linear projector outside DR. XnIDR xnorizes the one inside DR. Therefore, XnODR and XnIDR are two different variants of CapsFCLayer. They simplify operations by xnorizing linear projectors at different positions. However, XnODR causes more information loss than XnIDR, because XnIDR preserves all information in the outer linear projector. In specific, Eq. 7 prepares the input values for the following DR in XnODR, which means the information loss may exacerbate during the iterative process. In XnIDR, on the other hand, Eq. 8 prepares the temporary value , which is to update , which means the information loss has few negative effects on the softmax process directly. Therefore, the information loss of XnIDR has a weaker impact than that of XnODR, which contributes to better performance on accuracy.</p><p>Furthermore, we xnorize? (the size is [ , , , ). As we can see, the second dimension and the fourth one of has 1 channel, while the first dimension and the third dimension of has and channels. and are much larger than one so that XnIDR generates less FLOPs during binarization than XnODR. In total, the FLOPs of XnIDR is less than that of XnODR.</p><p>Hence, XnIDR is theoretically better than XnODR. Simultaneously, if the network implements Xnorization operation on both linear projectors simultaneously, it predicts awfully due to lacking too much information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>In this section, we first introduce the datasets, evaluation metrics, and implementation details. Then, we explain our experiments, present the results, report the ablation study, and analyze the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We pick both small-scale datasets (MNIST, CIFAR-10) and large-scale datasets (MultiMnist) to validate our proposed XnODR and XnIDR. They would separately work as the only FC layer in the MobileNet V2 (lightweight model) and the ResNet-50 (heavyweight model) to replace the original dense layers. The goal of these variants is to validate XnODR's and XnIDR's effectiveness on these datasets.</p><p>MNIST: The National Institute of Standards and Technology was in charge of creating the MNIST dataset <ref type="bibr" target="#b29">[30]</ref>. It consists of 70,000 28?28 gray-scale images in 10 classes. There are 60,000 training images and 10,000 test images. The American Census Bureau employees contributed half of the training images, and American high school students contributed the other half. Test images have the same background. The categories are 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9.</p><p>CIFAR-10: This dataset was collected by <ref type="bibr" target="#b28">[29]</ref>. It consists of 60,000 32?32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. The categories consist of the airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.</p><p>MultiMNIST: This dataset is generated out of MNIST to prove the effectiveness of CapsNet. Our proposed layers get inspired by CapsNet. We, therefore, validate the XnODR and XnIDR by MultiMNIST <ref type="bibr" target="#b47">[47]</ref>.</p><p>We create MultiMNIST 2 following the instruction from <ref type="bibr" target="#b47">[47]</ref>, except generating four rather than 1K MultiMNIST examples for each digit in the MNIST dataset, because we find that the model can converge to accuracy higher than 99% without a large volume dataset. So the training set consists of 240,000 36?36 gray-scale images in 10 classes, and the test set size is 40,000 36?36 gray-scale images in 10 classes. The categories are 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>This paper uses the prediction accuracy (the maximum value among five random training), the number of network parameters, Speed Up (see Eq. 9), and FLOPs as the metrics to evaluate and compare the model performance. Moreover, we train the ResNet-50, and MobileNet V2 from scratch and record these metrics for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>For the MNIST classification task, we take gray-scale images with the shape of <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b0">1]</ref> as the input values and convert the labels to categorical values with the size of <ref type="bibr">[bs, 10]</ref>.</p><p>For the CIFAR-10 classification task, we take color images with the shape of <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b2">3]</ref> as the input values and convert the labels to categorical values with the size of [bs, 10]. To enhance the performance, we do random data augmentation on CIFAR-10 before training. For the MultiMNIST classification task, we take grayscale images with the shape of <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b0">1]</ref> as the input values  <ref type="bibr" target="#b4">[5]</ref> 99.87% - <ref type="bibr" target="#b57">[56]</ref> 99.56% --85.96% --92.46% --ACGAN <ref type="bibr" target="#b5">[6]</ref> 96.25% - <ref type="bibr" target="#b58">[57]</ref> 99.67% --85.69% --94.88% --CPPN <ref type="bibr" target="#b60">[59]</ref> 97.00% -----65.6% --CapsuleNet <ref type="bibr" target="#b47">[47]</ref> 99.65% -6 In the original papers, the authors of MobileNet V2 and those of ResNet-50 disclosed the experiment results on other datasets instead of MNIST, CIFAR-10, and Mul-tiMNIST <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Thus, we would validate the original MobileNet V2 and ResNet-50 on three datasets.</p><formula xml:id="formula_18">1.5M 89.23% - - - - - PLM(BCE+CB) [11] - - - 70.33% - - 89.37% - - CaiT-M-36 224 [51] - - - 99.40% 53.7B 270.9M - - - L1/FC</formula><formula xml:id="formula_19">- - - - - - - EnsNet [22] 99.84% - - 76.25% - - - - - LaNet-L [52] - - - 99.01% - 44.1M - - - SCAE [28] 99.0% - - 33.48% - - - - - capsnet+PA</formula><p>Moreover, the MobileNet V2 and ResNet-50 requires the size of the input images to be larger than 32?32. ResNet-50 has this requirement too. Therefore, we upsampled the MNIST to 32?32 (This is the only upsampling in the second experiment) and expanded the channel number to three. The adjusted input size became <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b2">3]</ref>. We changed the MultiMNIST to <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b2">3]</ref> too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Experiment With Upsampling</head><p>To pursue better performance, the experiment on MNIST and CIFAR-10 took the upsampled images with the resolution of 224?224 as the input variable. That on Mul-tiMNIST trained the input variable with the resolution of 108?108 (Upsampling to 244 would cost too much time on training. To save time, we upsample MultiMNIST to the resolution of 108). The original MobileNet V2 and ResNet-50 uses a sparse categorical cross-entropy loss function as the cost function, while models with XnODR/XnIDR take square hinge / marginal loss function as the cost function. The sparse categorical cross-entropy loss function helps MobileNet V2 and ResNet-50 to converge well (they use marginal loss on MultiMNIST because the size of categorical values is [bs, 10]). The squared hinge loss is the loss function of the XNOR-Net, while the marginal loss is the loss function of the CapsNet. Either of them helps the new proposed models to converge well. All three of them utilize Adam Optimizer with a starting learning rate of 1e-3. Moreover, they call a cyclic learning rate scheduler, where the starting learning rate is 1e-3, the ending learning rate is 1e-9, the step size is 6000, and the mode is triangular2. The epoch number is 30 for MNIST, and 80 for CIFAR-10, and 20 for MultiMNIST.</p><p>After training, we collect and record the Top-1 accuracy, trainable parameters, and FLOPs for comparison. We coded the network using Tensorflow (2.2.2) and Keras (2.4.3) framework and ran experiments on the NVIDIA GTX 1080Ti GPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Experiment Without Upsampling</head><p>For MNIST and CIFAR-10, we trained MobileNet V2, ResNet-50, and the new proposed models with the image resolution of 32?32. On MultiMNIST, we used the input variable with a resolution of 36. The marginal loss function fits the CapsNet, while the squared hinge loss function suits the XNOR-Net. At the same time, given that models with XnODR/XnIDR are the variants of dynamic routing, they take the marginal loss function as the cost function. For the sake of doing a fair experiment, the original MobileNet V2 and ResNet-50 used the marginal loss function as the cost function. The aforementioned models utilize the same optimizer, learning rate, learning rate scheduler, epoch numbers as Section 4.3.1 does.</p><p>However, the structure of the typical MobileNet V2 and ResNet-50 is designed for the ImageNet dataset. To fit the images with low resolution and maximizely keep the original structure, we changed the stride of the first convolutional layer from two to one in MobileNet V2. In ResNet-50, we changed the kernel size of the first convolutional layer from seven to three. Additionally, we tune its stride from two to one. We also modified the size of the first max pooling layer to one with the stride of one.</p><p>The framework and coding environment are the same as those mentioned in Section 4.3.1. After training, we take note of the evaluation metrics again. <ref type="table" target="#tab_5">Table 4</ref> shows all experimental results from models with upsampling and those without upsampling. For each model setting, the highest accuracy among different models on each dataset is in bold. The lowest FLOPs and fewest <ref type="table">Table 6</ref> This table shows experiment eesults on CIFAR-10 from cited papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10 Accuracy</head><p>Aff-CapsNets <ref type="bibr" target="#b15">[16]</ref> 76.28 CapsNetSIFT <ref type="bibr" target="#b34">[35]</ref> 91.27 HGCNet-91 <ref type="bibr" target="#b56">[55]</ref> 94.47 Ternary connect + Quantized backprop <ref type="bibr" target="#b33">[34]</ref> 87.99 Greedy Algorithm for Quantizing <ref type="bibr" target="#b37">[38]</ref> 88.88 SLB on ResNet20 <ref type="bibr" target="#b59">[58]</ref> 92.1 SLB on VGG small <ref type="bibr" target="#b59">[58]</ref> 94.1 DoReFa-Net on VGG-11 <ref type="bibr" target="#b17">[18]</ref> 86.30 DoReFa-Net on ResNet14 <ref type="bibr" target="#b17">[18]</ref> 89.84 parameters are in bold too. In total, there are 36 subexperiments. <ref type="table" target="#tab_7">Table 5</ref> is to compare the FLOPs of the original FC layers with that of XnODR/XnIDR. To be specific, we also listed the FLOPs of binarization process and XNOR operation, which are two parts of Xnorization (see Appendices A and B). Given that XNOR is binary operation, we initially calculated its BOPs (binary point of operations) and then divided it by 64 to get the related FLOPs. The ratio to original FC means that the FLOPs of models with XnODR/XnIDR over that of the original models. <ref type="table" target="#tab_8">Table 7</ref> shows the related Speed Up ratioes. According to <ref type="table" target="#tab_5">Table 4</ref>, both ResNet_XnODR and ResNet _XnIDR achieve higher accuracy by costing fewer FLOPs and less parameters than the original ResNet-50 on all three datasets. Especially on MultiMNIST, when we embed either XnODR or XnIDR into ResNet-50, we achieve better accuracy than other cited models in <ref type="table">Table 6</ref>. The experiment results of MobileNet V2 and MobileNet_XnIDR also support this view. In other words, the heavyweight models with either XnODR or XnIDR enhance the accuracy of the original model with faster speed, while the lightweight models with XnIDR improve the accuracy of the original model with faster speed.</p><p>Moreover, <ref type="table" target="#tab_5">Table 4</ref> shows that ResNet_XnIDR achieved comparable or better accuracy than ResNet_XnODR by costing fewer FLOPs on all three dataset. Additionally, the two models have the same number of trainable parameters. Although <ref type="table" target="#tab_8">Table 7</ref> presents that ResNet_XnIDR has a slightly less Speed Up ratio than ResNet_XnODR, ResNet_XnIDR is objectively better.</p><p>When it turns to MobileNet_XnODR (with upsampling) and MobileNet_XnIDR (with upsampling), the conclusion is different. The number of trainable parameters is still the same. On the MNIST and MultiMNIST, however, Mo-bileNet_XnIDR (with upsampling) achieved comparable accuracy to MobileNet_XnODR (with upsampling) by costing less FLOPs and a fewer Speed Up ratio. Then, on the complex dataset CIFAR-10, MobileNet_XnIDR (with upsampling) performs as well as MobileNet_XnODR (with upsampling) by costing fewer FLOPs. Comprehensively, we support that models with XnIDR performed better than those with XnODR.</p><p>Then, MobileNet_XnIDR (without upsampling) is better than MobileNet_XnODR (without upsampling) on MNIST and MultiMNIST. On CIFAR-10, the accuracy of Mo-bileNet_XnIDR (without upsampling) is comparable to that of MobileNet_XnODR (without upsampling). Furthermore, MobileNet_XnIDR (without upsampling) costs less FLOPs than MobileNet_XnODR on all three datasets. After a comprehensive comparison, we support that models with XnIDR perform better than those with XnODR again. <ref type="table" target="#tab_7">Table 5</ref> presents that ResNet_XnODR and ResNet_XnIDR cost much less FLOPs than ResNet-50, the highest ratio is merely 19.96%. However, MobileNet_XnODR cost more FLOPs than MobileNet V2 due the FLOPs of binarization process. See Section 4.6.2 for specific analysis and explanation. <ref type="table" target="#tab_8">Table 7</ref> summarizes that models with XnODR have higher Speed Up ratio than those with XnIDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>To show the effectiveness of XnODR and XnIDR, we perform ablation studies to evaluate the influence of components on the classification task. The experiments were launched on the MNIST, CIFAR-10, and MultiMNIST. Its evaluation metrics and configuration are the same as those in Sections 4.2 and 4.3.</p><p>In XnODR and XnIDR, the hyperparameters are batch size, input capsule number, and output capsule number, which are subject to dataset number, the size of input images, and the dataset's category number. <ref type="bibr" target="#b47">[47]</ref> did many experiments to tune them, especially iteration number. We referred to their paper and decided to use the same hyperparameters because our target is to show the efficiency of XnODR/XnIDR with the minimum change. Hence, it is less necessary to tune them to strengthen the rationality of our work.</p><p>Influence of Dynamic Routing: Dynamic Routing mechanism is the basic framework of XnODR and XnIDR. To show the necessity of proposing XnODR and XnIDR, we do the experiment by inserting the original DR into ResNet-50 and MobileNet V2. It replaces all the original FC layers and acts as the only one. Let ResNet_DR denote the ResNet-50 with DR, and MobileNet_DR denote the MobileNet V2 with DR. To match the former experiments, we also did experiments on models with upsampling and models without upsampling. <ref type="table" target="#tab_9">Table 8</ref> shows the experimental results. <ref type="table">Table 9</ref> compares the FLOPs of the FC layers in the original models with that of DR in the new model.</p><p>Referring to <ref type="table" target="#tab_5">Table 4</ref>, we report 36 experimental results. Half of them are on the model with upsampling, while the other half are from the experiments on the model without upsampling. Compared to <ref type="table">Table 9</ref>, models with DR were better than those with XnODR/XnIDR in 19 experiments. However, on the CIFAR-10, only ResNet_DR (without upsampling) and MobileNet_DR (without upsampling) surpassed those with XnODR/XnIDR in 3 subexperiments. Furthermore, ResNet_DR (with upsampling) and MobileNet_DR (with upsampling) do not outperform the typical ResNet-50 and MobileNet V2 on CIFAR-10. Although the models with DR performed better than those with XnODR/XnIDR in 16 out of 24 experiments (66.7%) on MNIST and MultiMNIST, models with XnODR and XnIDR also performed near perfect. For example, the accuracy of MobileNet_XnIDR (with upsampling) on MNIST is 0.39% less than that of MobileNet_DR, but it is already 99.25% which means that the prediction is very good. In the meanwhile, we put more weight on CIFAR-10 than on MNIST and MultiMNIST because it is more challenging. After a comprehensive analysis, ResNet_DR is less comparable to ResNet_XnODR and ResNet_XnIDR. Moreover, MobileNet_XnODR and MobileNet_XnIDR predicted comparable or better accuracy than MobileNet_DR. Hence, models with XnODR and XnIDR assist in predicting better than those with DR. Objectively, models with DR cost fewer FLOPs than those with XnODR and XnIDR. However, it is the binarization process rather than the XNOR operation generating too many FLOPs. For example, the FLOPs of XnODR in ResNet_XnODR (without sampling) is 3.56M, of which 3.13M is from binarization. The remainder is merely 0.43M, which is much less than ResNet_DR (without sampling)'s 1.08M. Moreover, blindly pursuing speed damages our core target of good prediction, which means a high accuracy.</p><p>In summary, models with XnODR and XnIDR are better than those with DR. It is meaningful to propose XnODR and XnIDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Analysis and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1.">Speed Analysis</head><p>Eq. 9 is the Speed Up function, which represents the number of operations in the original convolution divided by that operation in XnorConv. The output is a ratio, which is the MADDs spent by the usual convolutional operation over XnorConv. The larger value means the faster speed. <ref type="table" target="#tab_8">Table 7</ref> shows that the Speed Up of the linear projector in XnIDR is a little less than that in XnODR. However, both XnODR and XnIDR have more steps other than linear projection. Refering to <ref type="table" target="#tab_7">Table 5</ref>, XnODR, generally, has more FLOPs than XnIDR. The main reason is that the Xn-ODR has larger _ , which causes binarization process costing larger FLOPs than XnIDR does. Comprehensively, models with XnIDR can run faster than those with XnODR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2.">Comprehensive Analysis</head><p>Why do models with XnODR/XnIDR outperform the original model?: The affine transformation (Eq. 2, which becomes Eq. 7 in XnODR) is one reason. It enriches the features so that the model learns the image from different angles. Eq. 3, on the other hand, transports the needed capsules from the lower level to the higher level. Then, Eq. 4 helps to enlarge the disparity among different capsules and activate the target capsules. In the meanwhile, Eq. 6, which becomes Eq. 8 in XnIDR, monitors the similarity between predicted capsules and activated capsules (the final output values). This similarity helps update . The iterative structure takes the updated to the next round. The aforementioned equations cooperate to help improve the prediction. Moreover, <ref type="bibr" target="#b47">[47]</ref> calls the combination of the aforementioned equations the DR mechanism. Therefore, the DR mechanism enhances accuracy. Moreover, Xnorization helps reduce the parameter and speed up the calculation process. Hence, our proposed XnODR and XnIDR suit both lightweight models and heavyweight models.</p><p>Why is XnIDR better than XnODR?: The channel number of each dimension benefits decreasing the FLOPs. Refer to Section 3.3 for more theoretical analysis.</p><p>Results from MobileNet_XnODR (without upsampling) and MobileNet_XnIDR (without upsampling) in <ref type="table" target="#tab_5">Table 4</ref> support the theoretical analysis.</p><p>Why does MobileNet_XnODR do more FLOPs than MobileNet V2?: The main reason is the number of FC layers. According to Tables 5 and 10, the current MobileNet V2 has four FC layers that take 1.64M FLOPs. If MobileNet V2 uses ResNet-50's three FC layers, it would do 3.68M FLOPs, which is larger than XnODR's FLOPs, 2.23M.</p><p>The other reason is due to the Binarization process. <ref type="table" target="#tab_7">Table 5</ref> shows that XnODR did 2.23M FLOPs during training all three datasets on MobileNet_XnODR. The XNOR operation only cost 76.8K binary operations <ref type="bibr" target="#b0">(1,</ref><ref type="bibr">200</ref> FLOPs), which slightly affected the total FLOPs of XnODR in MobileNet_XnODR. However, MobileNet_XnODR cost 1.96M FLOPs in Binarization which is even 320K more than FLOPs of all FC layers (1.64M) in the MobileNet V2. Therefore, the total FLOPs of XnODR in MobileNet_XnODR is 135.60% of that of all FC layers in the MobileNet V2. The above explanation supports the view that the lightweight models with XnIDR rather than XnODR improve the accuracy of the original model with faster speed.</p><p>Simultaneously, the core of the classification task is predicting well. MobileNet_XnODR outperformed the Mo-bileNet V2 on accuracy and parameters across all three datasets. Without good prediction results, it is in vain to increase the inference speed of the models. Comprehensively, lightweight models with XnODR outperformed the original model as well.</p><p>Why do the FLOPs and SpeedUp stay the same on the XnODR of both ResNet_XnODR and MobileNet_ XnODR across all three datasets?: The reason is that the input size of XnODR is [BS, 160, 1, 8] on all three datasets. Moreover, the structure of XnODR stayed the same. Therefore, we calculate the FLOPs of XnODR and the Speed Up of the related step three times by the same input values. And, we finally got the same FLOPs and Speed Up. <ref type="table" target="#tab_7">Tables 5 and 7</ref> tell that the same pattern happened to the XnIDR as well. This reason is the same.</p><p>Why do models without upsampling achieve the mediocre accuracy on CIFAR-10?: On MNIST and Mul-tiMNIST, <ref type="table" target="#tab_5">Table 4</ref> shows that the experiment results of models without upsampling are comparable to those with upsampling. However, on CIFAR-10, the experiment results of models without upsampling very noticeably fade to low accuracy. The typical ResNet-50 and MobileNet V2 are for complex image datasets with high resolution. Even though we slightly modified them to fit the low-resolution image dataset (see Section 4.3.2), it is less possible to guarantee good results. Moreover, according to <ref type="table" target="#tab_5">Table 4</ref>, <ref type="bibr" target="#b52">[51]</ref> showed that CaiT-M-36 224 achieved 99.40% on CIFAR-10 with the resolution of 224. It also took 224?224 as input size. Additionally, Thin MobileNet did accept 32?32 as input, but it is the variant of the typical MobileNet V2 <ref type="bibr" target="#b50">[49]</ref>. <ref type="bibr" target="#b42">[43]</ref> showed that BootstrapNAS' ResNet-50 supernetwork achieved 93.70% Top-1 accuracy on CIFAR-10, while BootstrapNAS' MobilenetV2 supernetwork returned 93.91% Top-1 accuracy on CIFAR-10. However, both of the two are variants, and none of their accuracies is higher than ours. Hence, it is uneasy for MobileNet V2 and ResNet-50 to achieve good performance on the complex image dataset with low resolution. Upsampling is a typical method to enhance performance. Modifying model structure and training strategy, proposed in the aforementioned papers, are other approaches. Given that the goal is to validate the XnODR and XnIDR without changing the main body of ResNet-50 and MobileNet V2, it is acceptable and normal to do upsampling in the experiment.</p><p>In summary, the first takeaway is that heavyweight models and lightweight models with XnODR/XnIDR perform better than the original models. The second take-away is that model taking XnIDR as an FC layer is better than models taking XnODR because of higher accuracy and lower FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>The CNNs main body with our proposed FC layers (XnODR and XnIDR) demonstrate that they outperform the original models both in terms of accuracy and speed by their solid performance in our experiments. For example, ResNet_XnODR and ResNet_XnIDR achieve over 96.5% accuracy with the cost of 3.862B FLOPs and 23.86M parameters on CIFAR-10 while ResNet-50 returns accuracy lower than 96% with more FLOPs and parameters. As we can see, fusing xnorization into the DR mechanism helps speed the model while maintaining a comparable or even better accuracy. Hence, we can implement XnODR and XnIDR as effective FC layers in both lightweight and heavyweight models.</p><p>XnODR and XnIDR can do more than we introduced above. In order to improve the network's representative capability, either XnODR or XnIDR can work as a parallel branch in CNNs to provide rotation invariance, increase the accuracy and avoid loss of time. In addition, it is helpful to load its output into relabeling mechanism as a contrast.</p><p>However, XnODR and XnIDR also have drawbacks. We have not validated our method on large-scale complex datasets such as ImageNet yet, but only validated them on small-scale complex datasets such as CIFAR-10. Specifically, while using the Xnorization method, the network is less likely to do a comparable performance on complex datasets due to losing too much information, such as a less satisfying performance on ImageNet in <ref type="bibr" target="#b38">[39]</ref> and the decayed performance of ViT on ImageNet (drop from 90.45% to 71.2% after using Xnorization) <ref type="bibr" target="#b40">[41]</ref>. The computationally expensive and poor performance of CapsNet are obstacles <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b54">[53]</ref>, <ref type="bibr" target="#b16">[17]</ref>. For example, <ref type="bibr" target="#b43">[44]</ref> showed that CapsNet merely returned 18% accuracy after training for 18 hours (35 epochs). Furthermore, the weakness of CapsNet in distinguishing closely similar objects is a big drawback to classify ImageNet successfully as well. Therefore, a sagacious move is to design a new powerful conv layer (see Section 6) rather than purely experiment with current XnODR/XnIDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>High accuracy and fast processing speed are two highlights of CapsNet and XnorNet. Combining these two advantages helps the network to speed the training while maintaining good or even better performance. Inspired by this idea, we proposed XnODR and XnIDR as the alternative options for the usual FC layer. Then, we inserted them into the MobileNet V2 and ResNet-50 and experimented on three datasets, MNIST, CIFAR-10, and MultiMnist. The results show that the models with either XnODR or XnIDR takes fewer parameters and less FLOPs than the original one. Furthermore, the variants reach higher accuracy.</p><p>In the future, we would work on creating a new Xnorization algorithm to approximate convolutional operation and avoid overt information loss, while simultaneously, we will fuse this new Xnorization method and EM Routing <ref type="bibr" target="#b46">[46]</ref> to create a new xnorized CapsConv layer. Finally, we plan to build a new network with the XnorCapsConv layer and XnODR/XnIDR. The target is to achieve good performance on large-scale complex datasets such as AffectNet <ref type="bibr" target="#b41">[42]</ref> and ImageNet <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Binarization</head><p>Xnorization speeds the calculation by three steps. The first step is to binarize the input values and weights before the convolutional operation. Secondly, it introduces a binary dot product with XNOR-Bitcounting operations. The last step is to replace the multiplication operations with the addition operation during the convolutional operation. Simply stated, it has two parts, binarization process and XNOR operation. We introduce binarization firstly and include XNOR operation in Appendix B.</p><p>Binarization process is to split the tensor into 2 parts. One is sign matrix (spanned from 2 values {-1, 1}), the other one is scaling factor.</p><p>Let ? be a set of tensors. And = ? ( =1,..., ) , ? ? ? ?? represents the input tensor for the ? layer of network, where ( , , ? ) means channel, width and height. We split the tensor into two values, binary filter ? {+1, ?1} ? ?? and scaling factor ? ? + , and use them to estimate ? . We first discuss the Sign and the binary filter. According to <ref type="bibr" target="#b38">[39]</ref> -bit Quantization is ( ) = 2(</p><formula xml:id="formula_21">[(2 ?1)( +1 2 )] 2 ?1 ? 1 2 )</formula><p>. The sign function is 1-bit Quantization, such that 1 ( ) = 2( </p><formula xml:id="formula_22">[(2 1 ?1)( +1 2 )] 2 1 ?1 ? 1 2 ) = 2( +1 2 ? 1 2 ), where the inner function,</formula><p>where is the output of Hard Sigmoid, is the Min-Max Normalization result of . Its range is [0,1]. Round function will round value bigger than 0.5 to be 1, less than or equal to 0.5 to be 0. And it leaves only 2 values, 0 and 1 after rounding, then the output of Hard Sigmoid function ? {0.5, 1}. To control the value of between 0 and 1, we call Clip function and round its output, such that</p><formula xml:id="formula_24">= ( ) = (0, (1, )) = ( ).<label>(17)</label></formula><p>Therefore, we get , which only has 2 values, 0 and 1. To get the expected binary filter , we load into Tanh function, = ?( ) = 2 ? ? 1 ? {?1, +1}. Now, we calculate the sign of out.</p><p>About scaling factor, according to <ref type="bibr" target="#b38">[39]</ref>, we use the average of to represent it.</p><formula xml:id="formula_25">= 1 ( ) = ? | | = 1 || || 1 ( 1 ? )<label>(18)</label></formula><p>Eq. 18 is the formula to get scaling factor, where represents the scaling factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. XnorConvLayer</head><p>XnorConvLayer is similar to the standard Conv layer, except binaring input and weight before doing convolution. Moreover, we use XNOR operation to do convolution in the XnorConvLayer. We formulate it as the following.</p><p>Let denote ? tensor of , denote ? scaling factor, denote binary filter of . Then ? is the estimate of after xnorize, where = { 0 , 1 , ..., ? }. Then, let ? be a set of tensors, and represent the ? weight filter in the ? layer of the network such that = ? ( =1,..., ) .</p><p>is the number of weight filters in the ? layer of the network. What's more, ? ? ? ?? , where ? , ? ? ? . Next, we start estimating with binary filter, , and scaling filter, , such that ? . According to XNOR-Net <ref type="bibr" target="#b38">[39]</ref>, Xnorization replaces multiplication in convolutional operations with additions and subtractions. And it causes 58? faster convolutional operations and 32? memory savings. This process is called Binary Dot Product.</p><p>To approximate the dot product between and , such that ? 1 2 , where , ? {+1, ?1} , 1 , 2 ? ? + , the paper solved and proved the following optimization: * 1 , * , * 2 , * = 1 , , 2 , || ? ? 1 2 ? || <ref type="formula" target="#formula_0">(19)</ref> where ? represents element-wise product.</p><p>In the meanwhile, for input tensors, , and weight, , we need to compute scaling factor, , for all possible subtensors in with same size as during convolution. To overcome the redundant computations caused by overlaps between sub-tensors, the paper firstly computed a matrix = ? | ?,?, | , which is the average over absolute values of the elements in the input across the channel, c. Then the paper convolved with a 2D filter ? ? ?? , = * , where ? = 1 ?? and * is a convolutional operation. contains scaling factors for all sub-tensors in the input I.</p><p>The above description proves that it makes sense to estimate * by ( ? ) ? , which can be formulated as Eq. 20: * ? ( ? ) ? <ref type="bibr" target="#b19">(20)</ref> where ? denotes the convolutional operation using XNOR and the bitcount operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Structure of Fully Connected layers on different models</head><p>The structure of Fully Connected layers on the typical ResNet-50 and MobileNet V2 deeply affect the paper's conclusion. Hence, we list the detail in <ref type="table" target="#tab_0">Table 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Accuracy and Loss plots on CIFAR-10</head><p>Given that CIFAR-10 is a complex dataset, it is more representative to reflect the capability of the models. So, we show the experimental results on CIFAR-10 in the following figures. <ref type="figure" target="#fig_6">Fig. 4</ref> shows the accuracy of MobileNet-Related models without upsampling on CIFAR-10. MobileNet_XnODR  and MobileNet_XnIDR have better accuracy than Mo-bileNet_DR and MobileNet V2. <ref type="figure" target="#fig_7">Fig. 5</ref> shows the loss of MobileNet-Related models without upsampling on CIFAR-10. MobileNet_XnODR and MobileNet_XnIDR have less loss value than Mo-bileNet_DR and MobileNet V2. <ref type="figure" target="#fig_8">Fig. 6</ref> shows the accuracy of ResNet-Related models without upsampling on CIFAR-10. ResNet_XnODR and ResNet_XnIDR have better accuracy and less fluctuation than ResNet_DR and ResNet-50. <ref type="figure" target="#fig_9">Fig. 7</ref> shows the loss of ResNet-Related models without upsampling on CIFAR-10. ResNet_XnODR and ResNet_XnIDR have less loss value and less fluctuation than ResNet_DR and ResNet-50. <ref type="figure" target="#fig_10">Fig. 8</ref> shows the accuracy comparison of models with XnODR and XnIDR on CIFAR-10. ResNet_XnODR and  ResNet_XnIDR have better accuracy than MobileNet_XnODR and MobileNet_XnIDR. <ref type="figure" target="#fig_11">Fig. 9</ref> shows the loss Comparison of models with XnODR and XnIDR on CIFAR-10. ResNet_XnODR and ResNet_XnIDR have less loss value than MobileNet_XnODR and MobileNet_XnIDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. What is 2.5 ?</head><p>The term fine particles, or particulate matter 2.5 ( 2.5 ), refers to tiny particles or droplets in the air that are two and one half microns or less in width. Like inches, meters and miles, a micron is a unit of measurement for distance. There are about 25,000 microns in an inch. The widths of the larger particles in the 2.5 size range would be about thirty times smaller than that of a human hair. The smaller particles are so small that several thousand of them could fit on the period at the end of this sentence <ref type="bibr" target="#b20">[21]</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Jian.Sun86@du.edu (J. Sun); Ali.Pourramezanfard@du.edu (A.P. Fard); mmahoor@du.edu (M.H. Mahoor) https://sites.google.com/view/sunjian/home (J. Sun) ORCID(s): 0000-0002-9367-0892 (J. Sun); 0000-0002-3807-0798 (A.P. Fard)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a) is the structure of typical CNN-based models; (b) is the structure of models with XnODR/XnIDR. The layers within light green box are the modified part. We exchange the last convolutional layer and all FC layers with PrimaryCaps layer and XnODR/XnIDR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>XnODR (Xnorizes the Linear Projection Outside Dynamic Routing), the Version 1 of the proposed Fully Connected layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>XnIDR(Xnorize the Linear Projection Inside Dynamic Routing), the Version 2 of proposed Fully Connected layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>+1 2 , 2 )</head><label>22</label><figDesc>is Hard Sigmoid function, the outer function, 2( ? 1 , is Tanh function. Therefore, the sign function can be formulated as shown in the Eq. 16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Accuracy of MobileNet-Related models without upsampling on CIFAR-10. MobileNet_XnODR and Mo-bileNet_XnIDR converged to higher accuracy than Mo-bileNet_DR and MobileNet V2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Loss of MobileNet-Related models without upsampling on CIFAR-10. MobileNet_XnODR and Mo-bileNet_XnIDR converged to lower loss value than Mo-bileNet_DR and MobileNet V2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Accuracy of ResNet-Related models without upsampling on CIFAR-10. ResNet_XnODR and ResNet_XnIDR converged to higher accuracy than ResNet_DR and ResNet-50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Loss of ResNet-Related models without upsampling on CIFAR-10. ResNet_XnODR and ResNet_XnIDR converged to lower loss value than ResNet_DR and ResNet-50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Accuracy comparison of models with XnODR and XnIDR on CIFAR-10. ResNet_XnODR and ResNet_XnIDR converged to higher accuracy than MobileNet_XnODR and Mobile_XnIDR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Loss Comparison of models with XnODR and XnIDR on CIFAR-10. ResNet_XnODR and ResNet_XnIDR converged to lower loss value than MobileNet_XnODR and Mobile_XnIDR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>This table shows the size of variables for XnODR and XnIDR.is the batch size of the input value, is the number of capsules loaded into this layer, is the number of the capsules output from this layer, 1 means the capsule is a 1-dimensional vector, is the element number of this each input capsule, is the dimension of each output capsule.</figDesc><table><row><cell>Variables</cell><cell cols="2">Size</cell><cell></cell><cell></cell></row><row><cell cols="2">[ ,</cell><cell>,</cell><cell>]</cell><cell></cell></row><row><cell>[ ,</cell><cell>,</cell><cell></cell><cell>, 1,</cell><cell>]</cell></row><row><cell>[</cell><cell>,</cell><cell>,</cell><cell>,</cell><cell>]</cell></row><row><cell>[ ,</cell><cell>,</cell><cell cols="2">, 1,</cell><cell>]</cell></row><row><cell cols="2">[ , 1,</cell><cell>, 1,</cell><cell>]</cell><cell></cell></row><row><cell>[ ,</cell><cell>,</cell><cell></cell><cell>, 1, 1]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>XnODR Algorithm</figDesc><table><row><cell cols="2">Procedure 1: XnODR</cell><cell></cell><cell></cell></row><row><cell>1 Expand</cell><cell cols="2">(3-dimensional) ?</cell><cell>(5-dimensional).</cell></row><row><cell>2 Initialize</cell><cell>.</cell><cell></cell><cell></cell></row><row><cell>3 Binarize</cell><cell>?</cell><cell>and</cell><cell>.</cell></row><row><cell>4 Binarize</cell><cell>?</cell><cell>and</cell><cell>.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.1 introduces the detail of the DR mechanism.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">1]. When, we average</cell></row><row><cell cols="2">the third channel of</cell><cell cols="2">, the size of</cell><cell>is [</cell><cell>,</cell></row><row><cell>, 1,</cell><cell cols="5">]. In addition, the binarization changes</cell></row><row><cell cols="5">capsule's value instead of its size. Therefore,</cell><cell>has</cell></row><row><cell>the size of [ ,</cell><cell>,</cell><cell>, 1,</cell><cell></cell><cell>], while</cell></row><row><cell>has the size of [</cell><cell>,</cell><cell>,</cell><cell>,</cell><cell cols="2">]. ? denotes</cell></row><row><cell cols="6">the convolutional operation using XNOR and the bitcount</cell></row><row><cell cols="5">operations. ? represents the element-wise product.</cell></row><row><cell cols="2">represents the results.</cell><cell>_</cell><cell></cell><cell cols="2">represents the DR.</cell></row><row><cell cols="5">represents the final output, where its size is [ ,</cell><cell>_ ,</cell></row><row><cell>_ , 1,</cell><cell>_ ].</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Section 3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>This table shows the comparison betweenmodels under different datasets. The results include models from cited papers, the experiments of models with upsampling, and those without upsampling.</figDesc><table><row><cell>Method</cell><cell>Top1</cell><cell>MNIST FLOPs</cell><cell>PARA</cell><cell>Top1</cell><cell>CIFAR-10 FLOPs</cell><cell>PARA</cell><cell>Top2</cell><cell>MultiMNIST FLOPs</cell><cell>PARA</cell></row><row><cell>Efficient-CapsNet [40]</cell><cell>99.84%</cell><cell>-</cell><cell>161K</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>94.90%</cell><cell>-</cell><cell>154K</cell></row><row><cell>HVCs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>This table showsthe FLOPs of the original FC layers and that of XnODR/XnIDR. For the sake of well comparison, we list the FLOPs of binarization, the BOPs (binary operations) of XNOR operation, the FLOPs of XNOR operation, and the ratio of the FLOPs of XnODR/XnIDR over that of the original FC layers.</figDesc><table><row><cell>Datasets</cell><cell>Details</cell><cell>ResNet-50</cell><cell>ResNet _XnODR</cell><cell>ResNet _XnIDR</cell><cell>MobileNet V2</cell><cell>MobileNet _XnODR</cell><cell>MobileNet _XnIDR</cell></row><row><cell></cell><cell>Total</cell><cell>68.17M</cell><cell>3.56M</cell><cell>2.28M</cell><cell>1.64M</cell><cell>2.23M</cell><cell>1.43M</cell></row><row><cell>MNIST and CIFAR-10 (with upsampling)</cell><cell>Binarization XNOR BOPs XNOR FLOPs</cell><cell></cell><cell>3.13M 122.88K 1,920</cell><cell>740.16K 15.36K 240</cell><cell></cell><cell>1.96M 76.8K 1,200</cell><cell>463.68K 9,600 150</cell></row><row><cell></cell><cell>Ratio to original FC</cell><cell></cell><cell>5.22%</cell><cell>3.34%</cell><cell></cell><cell>135.98%</cell><cell>87.20%</cell></row><row><cell></cell><cell>Total</cell><cell>17.84M</cell><cell>3.56M</cell><cell>2.28M</cell><cell>1.64M</cell><cell>2.23M</cell><cell>1.43M</cell></row><row><cell>MultiMNIST (with upsampling)</cell><cell>Binarization XNOR BOPs XNOR FLOPs</cell><cell></cell><cell>3.13M 122.88K 1,920</cell><cell>740.16K 15,360 240</cell><cell></cell><cell>1.96M 76.8K 1,200</cell><cell>463.68K 9,600 150</cell></row><row><cell></cell><cell>Ratio to original FC</cell><cell></cell><cell>19.96%</cell><cell>12.78%</cell><cell></cell><cell>135.98%</cell><cell>87.20%</cell></row><row><cell></cell><cell>Total</cell><cell>17.84M</cell><cell>3.56M</cell><cell>2.28M</cell><cell>1.64M</cell><cell>2.23M</cell><cell>1.43M</cell></row><row><cell>MNIST and CIFAR-10 (without sampling)</cell><cell>Binarization XNOR BOPs XNOR FLOPs</cell><cell></cell><cell>3.13M 122.88K 1,920</cell><cell>740.16K 15,360 240</cell><cell></cell><cell>1.96M 76.8K 1,200</cell><cell>463.68K 9,600 150</cell></row><row><cell></cell><cell>Ratio to original FC</cell><cell></cell><cell>19.96%</cell><cell>12.78%</cell><cell></cell><cell>135.98%</cell><cell>87.20%</cell></row><row><cell></cell><cell>Total</cell><cell>38.81M</cell><cell>3.56M</cell><cell>2.28M</cell><cell>1.64M</cell><cell>2.23M</cell><cell>1.43M</cell></row><row><cell>MultiMNIST (without sampling)</cell><cell>Binarization XNOR BOPs XNOR FLOPs</cell><cell></cell><cell>3.13M 122.88K 1,920</cell><cell>740.16K 15.36K 240</cell><cell></cell><cell>1.96M 76.8K 1,200</cell><cell>463.68K 9,600 150</cell></row><row><cell></cell><cell>Ratio to original FC</cell><cell></cell><cell>9.17%</cell><cell>5.87%</cell><cell></cell><cell>135.98%</cell><cell>87.20%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc>This table shows speed comparison among proposed models, ResNet-50 with XnODR/XnIDR and MobileNet V2 with XnODR/XnIDR, under different datasets.</figDesc><table><row><cell>Models (With Upsampling)</cell><cell>MNIST</cell><cell cols="2">Speed Up (Unit: ratio) CIFAR-10 MultiMNIST</cell></row><row><cell>ResNet_XnODR</cell><cell>63.99</cell><cell>63.99</cell><cell>63.99</cell></row><row><cell>ResNet_XnIDR</cell><cell>63.90</cell><cell>63.90</cell><cell>63.98</cell></row><row><cell>MobileNet_XnODR</cell><cell>63.98</cell><cell>63.98</cell><cell>63.99</cell></row><row><cell>MobileNet_XnIDR</cell><cell>63.80</cell><cell>63.80</cell><cell>63.95</cell></row><row><cell>Models (Without Upsampling)</cell><cell>MNIST</cell><cell cols="2">Speed Up (Unit: ratio) CIFAR-10 MultiMNIST</cell></row><row><cell>ResNet_XnODR</cell><cell>63.99</cell><cell>63.99</cell><cell>63.99</cell></row><row><cell>ResNet_XnIDR</cell><cell>63.90</cell><cell>63.90</cell><cell>63.90</cell></row><row><cell>MobileNet_XnODR</cell><cell>63.99</cell><cell>63.94</cell><cell>63.99</cell></row><row><cell>MobileNet_XnIDR</cell><cell>63.84</cell><cell>63.84</cell><cell>63.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc>This table shows experiment results on models with the typical DR mechanism.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell>Top1</cell><cell>MNIST FLOPs</cell><cell>PARA</cell><cell>Top1</cell><cell>CIFAR-10 FLOPs</cell><cell>PARA</cell><cell>Top2</cell><cell>MultiMNIST FLOPs</cell><cell>PARA</cell></row><row><cell cols="2">ResNet_DR (with upsampling)</cell><cell></cell><cell>99.69%</cell><cell>3.861B</cell><cell cols="2">23.86M 93.26%</cell><cell>3.861B</cell><cell cols="3">23.86M 99.01% 1.008B 23.86M</cell></row><row><cell cols="3">MobileNet_DR (with upsampling)</cell><cell cols="2">99.64% 311.29M</cell><cell>2.99M</cell><cell cols="2">89.99% 311.29M</cell><cell>2.99M</cell><cell cols="2">98.24% 82.65M</cell><cell>2.99M</cell></row><row><cell cols="3">ResNet_DR (without upsampling)</cell><cell>99.59%</cell><cell>1.225B</cell><cell cols="2">23.85M 93.31%</cell><cell>1.225B</cell><cell cols="3">23.85M 99.09% 1.621B 23.85M</cell></row><row><cell cols="4">MobileNet_DR (without upsampling) 99.47%</cell><cell>26.09M</cell><cell>2.43M</cell><cell>79.83%</cell><cell>26.09M</cell><cell>2.43M</cell><cell cols="2">98.00% 41.60M</cell><cell>2.43M</cell></row><row><cell>Table 9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">This table shows the FLOPs of the original FC layers and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>that of DR.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Datasets (with upsampling)</cell><cell cols="5">ResNet-50 ResNet_DR MobileNet V2 MobileNet_DR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MNIST and CIFAR-10</cell><cell>68.18M</cell><cell>1.08M</cell><cell>1.64M</cell><cell>675.2K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MultiMNIST</cell><cell>17.84M</cell><cell>1.08M</cell><cell>1.64M</cell><cell>675.2K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Datasets (without upsampling)</cell><cell cols="5">ResNet-50 ResNet_DR MobileNet V2 MobileNet_DR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MNIST and CIFAR-10</cell><cell>17.84M</cell><cell>1.08M</cell><cell>1.64M</cell><cell>675.2K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MultiMNIST</cell><cell>38.81M</cell><cell>1.08M</cell><cell>1.64M</cell><cell>675.2K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10</head><label>10</label><figDesc>This table shows the structure of all FC layers on ResNet-50 and MobileNet V2</figDesc><table><row><cell cols="2">Models</cell><cell cols="2">ResNet-50</cell><cell>MobileNet V2</cell><cell></cell></row><row><cell>FC1</cell><cell></cell><cell>1024</cell><cell></cell><cell>512</cell><cell></cell></row><row><cell>FC2</cell><cell></cell><cell>512</cell><cell></cell><cell>256</cell><cell></cell></row><row><cell>FC3</cell><cell></cell><cell>10</cell><cell></cell><cell>128</cell><cell></cell></row><row><cell>FC4</cell><cell></cell><cell>-</cell><cell></cell><cell>10</cell><cell></cell></row><row><cell>= {</cell><cell>0 ,</cell><cell>1 , ...,</cell><cell>, ...,</cell><cell>? }, where</cell><cell>denote</cell></row><row><cell cols="2">? scaling factor.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This work focuses on the accuracy and speed of networks instead of the Decoder part</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">XnODR and XnIDR: Two Accurate and Fast Fully Connected Layers For Convolutional Neural Networks</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/jiansfoggy/CODE-SHOW/blob/master/Python/Multi_Mnist/fast_generate_multimnist.py</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank Ms. Druselle May who helped us proofread the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Provable bounds for learning some deep representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhaskara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<editor>Xing, E.P., Jebara, T.</editor>
		<meeting>the 31st International Conference on Machine Learning<address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="584" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Bahadori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Spectral capsule networks</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Xnor-net++: Improved binary neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">No routing needed between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Byerly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kalganova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dear</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2021.08.064</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2021.08.064" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">463</biblScope>
			<biblScope unit="page" from="545" to="553" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An analysis of generative adversarial networks and variants for image synthesis on mnist dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tahir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-019-08600-2</idno>
		<ptr target="https://doi.org/10.1007/s11042-019-08600-2" />
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="13725" to="13752" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02551274</idno>
		<idno>doi:10.1007/BF02551274</idno>
		<ptr target="http://dx.doi.org/10.1007/BF02551274" />
	</analytic>
	<monogr>
		<title level="m">Mathematics of Control, Signals, and Systems (MCSS)</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J.W.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3965" to="3977" />
		</imprint>
	</monogr>
	<note>URL: https</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Big neural networks waste capacity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1301.3583</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Plm: Partial label masking for imbalanced multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2739" to="2748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Asmnet: a lightweight deep neural network for face alignment and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abdollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1521" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Facial landmark points detection using knowledge distillation-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="page">103316</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Complete vector quantization of feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Floropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2019.08.003</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2019.08.003" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="page" from="55" to="63" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Compressing Deep Convolutional Networks using Vector Quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6115arXiv:1412.6115</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving the robustness of capsule networks to image affine transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7283" to="7291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving the robustness of capsule networks to image affine transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00731</idno>
		<idno>doi:10.1109/ CVPR42600.2020.00731</idno>
		<ptr target="https://doi.ieeecomputersociety.org/10.1109/CVPR42600.2020.00731" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7283" to="7291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic Pruning for Quantized Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00523arXiv:2002.00523</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Memristive residual capsnet: A hardware friendly multi-level capsule network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2022.04.088</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2022.04.088" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing URL</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fine particles (pm 2.5) questions and answers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y S D</forename></persName>
		</author>
		<ptr target="https://www.health.ny.gov/environmental/indoors/air/pmq_a.htm" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ensemble learning in CNN augmented with fully connected subnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hirata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08562arXiv:2003.08562</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fixed-point feedforward deep neural network design using weights +1, 0, and -1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
		<idno type="DOI">10.1109/SiPS.2014.6986082</idno>
		<idno>doi:10.1109/ SiPS.2014.6986082</idno>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Workshop on Signal Processing Systems (SiPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360arXiv:1602.07360</idno>
		<title level="m">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ladder capsule network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v97/jeong19b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, PMLR</title>
		<editor>Chaudhuri, K., Salakhutdinov, R.</editor>
		<meeting>the 36th International Conference on Machine Learning, PMLR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3071" to="3079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rescapsnet: Residual capsule network for data classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11063-022-10806-9</idno>
		<idno>doi:10.1007/ s11063-022-10806-9</idno>
		<ptr target="https://doi.org/10.1007/s11063-022-10806-9" />
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters URL</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Text classification using capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">376</biblScope>
			<biblScope unit="page" from="214" to="221" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked capsule autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1906.06818.pdf" />
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html" />
		<title level="m">Cifar-10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">MNIST handwritten digit database URL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Group equivariant capsule networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Libuschewski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NeurIPS</publisher>
			<biblScope unit="page" from="8858" to="8867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Wind speed prediction based on multi-variable capsnet-bilstm-mohho for wpccc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.energy.2022.123761</idno>
		<ptr target="https://doi.org/10.1016/j.energy.2022.123761" />
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">250</biblScope>
			<biblScope unit="page">123761</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV</title>
		<editor>Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural networks with few multiplications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1510.03009" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<editor>Bengio, Y., LeCun, Y.</editor>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Capsnet meets sift: A robust framework for distorted target categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2021.08.087</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2021.08.087" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">464</biblScope>
			<biblScope unit="page" from="290" to="316" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A coarse-to-fine capsule network for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2021.05.032</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2021.05.032" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">456</biblScope>
			<biblScope unit="page" from="200" to="219" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bireal net: Binarizing deep network towards real-network performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="202" to="219" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Greedy Algorithm for Quantizing Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lybrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Saab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15979arXiv:2010.15979</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient-capsnet: Capsule network with self-attention routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mazzia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Salvetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiaberge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pruning vs xnor-net: A comprehensive study of deep learning for audio classification on edge-devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohaimenuzzaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bergmeir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Meyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06128</idno>
		<ptr target="https://arxiv.org/abs/2108.06128" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Mu?oz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lyalyushkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Akhauri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kozlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10878arXiv:2112.10878</idno>
		<title level="m">Enabling NAS with Automated Super-Network Generation. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">CapsNet comparative performance evaluation for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mukhometzianov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carrillo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11195arXiv:1805.11195</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Capsule routing via variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Leontidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kollias</surname></persName>
		</author>
		<idno>1-8. URL: https:</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<idno type="DOI">10.1609/aaai.v34i04.5785</idno>
		<idno>date: 07-02-2020 Through 12-02-2020</idno>
		<title level="m">34th AAAI 2020 Accepted Paper -Flagship/Top conference with very high h-index; Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI ; Conference</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Matrix capsules with em routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E H</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Thin mobilenet: An enhanced mobilenet architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Sharkawy</surname></persName>
		</author>
		<idno type="DOI">10.1109/UEMCON47517.2019.8993089</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE 10th Annual Ubiquitous Computing, Electronics Mobile Communication Conference (UEMCON)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="280" to="0285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239arXiv:2103.17239</idno>
		<title level="m">Going deeper with Image Transformers. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06832arXiv:1906.06832</idno>
		<title level="m">Sample-Efficient Neural Architecture Search by Learning Action Space. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A dynamic routing capsnet based on increment prototype clustering for overcoming catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">http:/arxiv.org/abs/https:/ietresearch.onlinelibrary.wiley.\com/doi/pdf/10.1049/cvi2.12068</idno>
		<ptr target="https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/cvi2.12068" />
	</analytic>
	<monogr>
		<title level="j">IET Computer Vision</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Capsule Network Performance on Complex Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03480arXiv:1712.03480</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Exploring highly efficient compact neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kung</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP40778.2020.9191334</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2930" to="2934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Routing Towards Discriminative Power of Class Capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04278arXiv:2103.04278</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Reducing the dilution: An analysis of the information sensitiveness of capsule network with a practical improvement method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10588arXiv:1903.10588</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08695arXiv:2009.08695</idno>
		<title level="m">Searching for Low-Bit Weights in Quantized Neural Networks. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Image decomposition and classification through a generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Regan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2019.8802991</idno>
		<idno>doi:10.1109/ ICIP.2019.8802991</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2019-09-25" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
	<note>ICIP. Conference date</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ternary sparse xnornet for fpga implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Oiwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISNE.2018.8394728</idno>
		<idno>doi:10.1109/ ISNE.2018.8394728</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Symposium on Next Generation Electronics (ISNE)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Conversational speech transcription using context-dependent deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="2" />
			<pubPlace>Omnipress, Madison, WI, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Coca: Contrastive captioners are image-text foundation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno>ArXiv abs/2205.01917</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Bird sound detection with binarized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Zabidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">U</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Abdul Manan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A N</forename><surname>Hamzah</surname></persName>
		</author>
		<idno type="DOI">10.11113/elektrika.v21n1.349</idno>
		<ptr target="https://elektrika.utm.my/index.php/ELEKTRIKA_Journal/article/view/349" />
	</analytic>
	<monogr>
		<title level="j">ELEKTRIKA -Journal of Electrical Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="48" to="53" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Estimating the near-ground pm2.5 concentration over china based on the capsnet model during 2018-2020. Remote Sensing 14</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs14030623</idno>
		<idno>doi:10. 3390/rs14030623</idno>
		<ptr target="https://www.mdpi.com/2072-4292/14/3/623" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">An efficient agreement mechanism in capsnets by pairwise product</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">2022. A 0.02% accuracy loss voltage-mode parallel sensing scheme for rram-based xnor-net application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSII.2022.3157767</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems II: Express Briefs</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Xor-net: An efficient computation pipeline for binary neural network inference on edge devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H K</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICPADS51040.2020.00026</idno>
		<idno>doi:10.1109/ ICPADS51040.2020.00026</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="124" to="131" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
