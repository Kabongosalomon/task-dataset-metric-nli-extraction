<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VLG-Net: Video-Language Graph Matching Network for Video Grounding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Soldan</surname></persName>
							<email>mattia.soldan@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
							<email>mengmeng.xu@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sisi</forename><surname>Qu</surname></persName>
							<email>sisi.qu@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Tegner</surname></persName>
							<email>jesper.tegner@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VLG-Net: Video-Language Graph Matching Network for Video Grounding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Grounding language queries in videos aims at identifying the time interval (or moment) semantically relevant to a language query. The solution to this challenging task demands understanding videos' and queries' semantic content and the fine-grained reasoning about their multi-modal interactions. Our key idea is to recast this challenge into an algorithmic graph matching problem. Fueled by recent advances in Graph Neural Networks, we propose to leverage Graph Convolutional Networks to model video and textual information as well as their semantic alignment. To enable the mutual exchange of information across the modalities, we design a novel Video-Language Graph Matching Network (VLG-Net) to match video and query graphs. Core ingredients include representation graphs built atop video snippets and query tokens separately and used to model intra-modality relationships. A Graph Matching layer is adopted for cross-modal context modeling and multi-modal fusion. Finally, moment candidates are created using masked moment attention pooling by fusing the moment's enriched snippet features. We demonstrate superior performance over state-of-the-art grounding methods on three widely used datasets for temporal localization of moments in videos with language queries: ActivityNet-Captions, TACoS, and DiDeMo.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Temporal action understanding is at the forefront of computer vision research. Hendricks et al. <ref type="bibr" target="#b1">[2]</ref> and Gao et al. <ref type="bibr" target="#b15">[16]</ref> recently introduced the task of temporally grounding language queries in videos as a generalization of the temporal action localization task, aiming to overcome the constraint of a predefined set of actions. This novel interdisciplinary task has gained momentum within the vision and language communities for its relevance and possible applications in video retrieval <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b74">75]</ref>, video question answering <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref>, human-computer interaction <ref type="bibr" target="#b81">[82]</ref>, and video storytelling <ref type="bibr" target="#b18">[19]</ref>. Enabling this fine-grained matching of language in videos can be adopted by professional * equal contribution.</p><formula xml:id="formula_0">X (GM ) v 2 R cv?nv<label>(12)</label></formula><p>Query: "The man wiped the frisbee and then threw it again, <ref type="bibr" target="#b12">(13)</ref> and the dog caught and brought it back to the owner." <ref type="bibr" target="#b13">(14)</ref> Query </p><p>Query: "The man wiped the frisbee and then threw it again, <ref type="bibr" target="#b12">(13)</ref> and the dog caught and brought it back to the owner."</p><p>2</p><formula xml:id="formula_3">X (GM ) v 2 R cv?nv<label>(12)</label></formula><p>Query: "The man wiped the frisbee and then threw it again, <ref type="bibr" target="#b12">(13)</ref> and the dog caught and brought it back to the owner." <ref type="bibr" target="#b13">(14)</ref> Query -Moments Matching </p><formula xml:id="formula_4">X (GM ) v 2 R cv ?nv<label>(12)</label></formula><p>Query: "The man wiped the frisbee and then threw it again, <ref type="bibr" target="#b12">(13)</ref> and the dog caught and brought it back to the owner." </p><p>Query: "The man wiped the frisbee and then threw it again, <ref type="bibr" target="#b12">(13)</ref> and the dog caught and brought it back to the owner." <ref type="bibr" target="#b13">(14)</ref> Query -Moments Matching </p><p>Query: "The man wiped the frisbee and then threw it again, <ref type="bibr" target="#b12">(13)</ref> and the dog caught and brought it back to the owner." <ref type="bibr" target="#b13">(14)</ref> Query -Moments Matching </p><p>Query: "The man wiped the frisbee and then threw it again, <ref type="bibr" target="#b12">(13)</ref> and the dog caught and brought it back to the owner." <ref type="bibr" target="#b13">(14)</ref> Query -Moments Matching </p><p>Query: "The man wiped the frisbee and then threw it again, <ref type="bibr" target="#b12">(13)</ref> and the dog caught and brought it back to the owner." <ref type="bibr" target="#b13">(14)</ref> Query -Moments Matching  We regard moments and queries as sequences of snippets and tokens respectively. We employ scheme (d), which allows for fine alignment by snippet-token matching.</p><p>video content creators during the editing process. For example, video editing often requires searching through many hours of raw, unlabelled video content for specific interesting highlights. Thus, the ability to retrieve such highlights through textual queries could provide a faster experience. Natural language grounding in videos inherits challenges from temporal action localization such as context modeling and candidate moment generation <ref type="bibr" target="#b0">[1]</ref>. Semantic context is a fundamental cue necessary to boost the performance of localization methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b71">72]</ref>. To enrich video representation <ref type="bibr" target="#b1">[2]</ref> adopted global-context, which is moment independent, leading to sub-optimal performance. Conversely, a moment specific local-context, defined as a moment's temporal neighbourhood, was used in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b54">55]</ref>. In our view, non-local context merits deeper analysis, since it has the potential to identify relevant information not restricted to the temporal neighbourhood within one data modality. For example, in <ref type="figure">Fig. 1(a)</ref>, although "First throw and fetch" is not in the temporal vicinity of "Second throw and fetch", it is still semantically related with the target moment, showcasing the importance of non-local context modeling for video grounding.</p><p>Moreover and as shown by the example, the free-form nature of the language modality introduces additional challenges. A model must understand the semantic content of both videos and language queries and reason about their multi-modal interactions. Previous studies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b54">55]</ref> employ a cross-modal processing unit designed to jointly model text and visual features through simple operations such as element-wise addition, Hadamard product, and direct concatenation of a moment's representation and the query embedding. A high-level overview of this multimodal interaction scheme is presented in <ref type="figure">Fig. 1(b)</ref>. Instead, recent works such as <ref type="bibr" target="#b43">[44]</ref>, only employ the Hadamard product to fuse the multi-modal information at the query-snippet level. This scheme, depicted in <ref type="figure">Fig. 1(c)</ref>, can determine different correlations between a query and each video snippet, allowing for a finer fusion with respect to <ref type="figure">Fig. 1(b)</ref>. Motivated by the work in <ref type="bibr" target="#b71">[72]</ref>, we propose to leverage the representational capability of graphs to encode snippetsnippet, token-snippet, and token-token relations as graph edge connections. As such, we design a new architecture referred to as Video-Language Graph Matching Network (VLG-Net) which employs Graph Convolutional Networks (GCN) <ref type="bibr" target="#b56">[57]</ref>. First, representation graphs for both video and language are constructed. The video graph models each snippet as a node and takes advantage of two sets of edges to represent both local temporal relations and non-local semantic relations between video snippets. Similarly, we construct a language graph, where each node is a token, and each edge reflects token-to-token relations, e.g. syntactic dependencies <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. These modality-specific graphs are used to model local and non-local intra-modality context through graph convolutions. This sets the stage for addressing modality alignment by recasting inter-modality interactions as an algorithmic graph matching problem. Inspired by <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b70">71]</ref>, we adopt a cross-graph attention-based matching mechanism to enable the mutual exchange of information between modalities, allowing for fine-grained alignment through a specialized set of learnable edges. Unlike some methods that focus on relatively coarse query-moment or query-snippet interactions, and similar to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b78">79]</ref>, our method performs the matching operation at the level of snippets and tokens, as depicted in <ref type="figure">Fig. 1(d)</ref>. With this design, we avoid the need for heuristics of context modeling while learning a successful strategy for multi-modal fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions. (1)</head><p>We propose VLG-Net, a new deep learning pipeline that consistently adopts graph representation for modeling modality interaction and multi-modal fusion. We address the modality fusion problem by resorting to a graph-matching approach that learns snippet-token connectivity.</p><p>(2) Through extensive experiments, VLG-Net demonstrates its effectiveness in capturing modality interactions by achieving performance on par or better than state-of-the-art on three standard datasets, showing significant improvements over previously published methods in TACoS <ref type="bibr" target="#b45">[46]</ref> and DiDeMo <ref type="bibr" target="#b1">[2]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Video Grounding</head><p>Moment candidates. Previous works can be categorized into proposal-free and proposal-based methods. Proposalfree approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b76">77]</ref> aim at directly regressing the temporal boundaries of the queried moment from the multi-modal fused feature. In contrast, proposalbased methods adopt a propose-and-rank pipeline by first generating moment proposals and then ranking them according to their similarity with the textual query <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b80">81]</ref>. Similar to these approaches, VLG-Net is a proposal based approach. Moments in context. For moment context modeling, some methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref> attempt to use the memory property of LSTM cells <ref type="bibr" target="#b50">[51]</ref> to contextualize the video features. Alternatively, attention-based mechanisms <ref type="bibr" target="#b58">[59]</ref> adopted in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b59">60]</ref> can improve the aggregation of long-range semantic dependencies. Similar to <ref type="bibr" target="#b59">[60]</ref>, we argue that visual context modeling should be dynamic and query-dependent. <ref type="bibr" target="#b80">[81]</ref> claims that neighbouring proposals hold valuable context and thus apply 2D convolutions (with large kernel size) to moment representations to gather context information in the latter stages of their pipeline. Compared to <ref type="bibr" target="#b80">[81]</ref>, we delegate context gathering to earlier stages of our pipeline and only use a Multi-Layer Perceptron (MLP) network for moment score computation, reducing the overall computation. Multi-modal fusion. Moving beyond the simple scheme adopted in <ref type="bibr" target="#b15">[16]</ref>, the work of [67] devises a new crossmodality interaction scheme based on circular matrices. In <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b59">60]</ref>, frame features are concatenated with frameguided attention-pooled features from the query. Lu et al. <ref type="bibr" target="#b38">[39]</ref> take advantage of the QANet <ref type="bibr" target="#b73">[74]</ref> architecture, which is based on cross-attention and convolution operations, for multi-modal fusion. Dynamic filters generated from language features are used in <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b79">80]</ref> in order to modulate (through convolutions) the visual information based on the query content. Recently, the Hadamard product has become a popular way to fuse/gate multi-modal information <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b80">81]</ref>. In contrast to these methods, our graph matching layer specifically models local, non-local, and query-guided context, thereby exploiting the semantic neighbourhood of snippets and tokens to fuse the modalities through graph convolutions. Concurrently to our method, <ref type="bibr" target="#b32">[33]</ref> adopted attention based cross-modal graphs for fusing the video and language modality. However, opposed to our formulation, <ref type="bibr" target="#b32">[33]</ref> lacks a formal design for the graph edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graphs and Graph Neural Networks</head><p>Graphs in Videos. In various video understanding tasks, such as action recognition <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b64">65]</ref> and action localization <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b75">76]</ref>, graphs can offer extensive representational power to data sequences. For example, a video can be represented as a space-time region graph <ref type="bibr" target="#b64">[65]</ref> or as a 3D point cloud in the spatial-temporal space <ref type="bibr" target="#b35">[36]</ref>. Moreover, Zeng et al. <ref type="bibr" target="#b75">[76]</ref> define temporal action proposals as nodes to form a graph, while Xu et al. <ref type="bibr" target="#b71">[72]</ref> consider video snippets as the graph nodes. Inspired by <ref type="bibr" target="#b71">[72]</ref>, in VLG-Net, video snippets are represented as nodes in a graph and different specifically designed edges model their relationships.</p><p>Graphs in Language. In natural language processing (NLP), both sequential and non-local relations are crucial. The former is usually captured by recurrent neural networks <ref type="bibr" target="#b48">[49]</ref>, while the latter can be represented using graph neural networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b54">55]</ref>. Moreover, syntactic information has proven useful for language modeling when combined with GCNs <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41]</ref>. Driven by these findings, we use LSTMs and Syntactic Graph Convolution Networks (SyntacGCN) together to model and enrich the language features in the query.</p><p>Graph Neural Networks in Graph Matching. Graph matching is one of the core problems in graph pattern recognition, aiming to find node correspondences between different graphs <ref type="bibr" target="#b4">[5]</ref>. Given the ability of Graph Neural Networks (GNNs) to encode graph structure information, approaches leveraging GNNs have recently surfaced to address the graph matching problem <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b70">71]</ref>. For example, Li et al. <ref type="bibr" target="#b28">[29]</ref> propose a GNN-based graph matching network to represent graphs as feature vectors, which simplifies measuring their similarity. Following <ref type="bibr" target="#b28">[29]</ref>, a neighborhood matching network is introduced by [69] to match graph nodes by estimating similarities of their neighborhoods. Due to their superiority in finding consistent correspondences between sets of features, graph matching methods have been widely applied in various tasks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b70">71]</ref>. Motivated by these works, we apply graph matching to the video grounding task by specifically employing a cross-graph attention matching mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Given an untrimmed video and a language query, the video grounding task aims to localize a temporal moment in the video that matches the query. Each video-query pair has one associated ground-truth moment, defined as a temporal interval with boundary (? s , ? e ). Our method scores m candidate moments, where the k-th moment consists of start time t s,k , end time t e,k , and confidence score p k . The video stream is represented as a sequence of n v snippets V = {v i } nv i=1 , where each snippet has consecutive frames. Similarly, a language query is represented by n l tokens L = {l i } n l i=1 . The inputs to VLG-Net are n v snippet features X v ? R cv?nv and n l token features X l ? R c l ?n l extracted using pre-trained models, where c v and c l are the snippet and token feature dimensions. We describe the details of feature extraction in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">VLG-Net Architecture</head><p>Our video grounding architecture is illustrated in <ref type="figure">Fig. 2</ref>. First, we feed both the video features X v and the query embeddings X l into a stack of computation blocks. On the video path, we use 1D convolutions and GCNeXt <ref type="bibr" target="#b71">[72]</ref> blocks to enrich the visual representation with local and non-local intra-modality context. On the language path, we apply LSTM and SyntacGCN <ref type="bibr" target="#b21">[22]</ref> to aggregate temporal and syntactic context, which models the grammatical structure of the language query. The two paths converge in the graph matching layer for cross-modal context modeling and multi-modal fusion. After the graph matching layer, we apply masked moment attention pooling to produce the representations of possible moment candidates. Finally, we use an MLP to score the query-moment pair based on their representation and post-process the score through non-maximum suppression (NMS). We report top-? ranked moments as the final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Video and Language Representations</head><p>Here, we detail the set of operations performed on each modality. The stack of computation blocks of each path is specifically designed to model intra-modality context to enrich the snippet and token features. Video Representation. We add 1D positional encoding, as formulated in <ref type="bibr" target="#b11">[12]</ref>, to each input visual feature and apply 1D convolutions to map them to a desired dimension. The video is then cast as a graph, where each node represents a snippet and each edge represents a dependency between a snippet pair. We design two types of edges: (i) Ordering Edges and (ii) Semantic Edges. Static Ordering Edges connect consecutive snippets and model the temporal order. Conversely, Semantic Edges are dynamically constructed, using the k-nearest neighbors algorithm. They connect semantically similar snippets based on their current feature representations. Specifically, an ordering or semantic snippet neighborhood is determined, and its aggregated representation is computed through edge convolutions F, similar to <ref type="bibr" target="#b65">[66]</ref>. Each edge convolution employs a split-transformmerge strategy <ref type="bibr" target="#b69">[70]</ref> to increase the diversity of transformations. These graph operations (called GCNeXt) were proposed in <ref type="bibr" target="#b71">[72]</ref> to enrich video snippet representations for the purpose of temporal action localization. In our architecture, we stack b v GCNeXt blocks together and refer to the input of each block as X</p><formula xml:id="formula_10">(i) v such that X (i+1) v = GCNeXt(X (i) v ) = (1) ? F(X (i) v , A (i) o , W (i) o ) + F(X (i) v , A (i) s , W (i) s ) + X (i) v , where X (0)</formula><p>v is the output of the convolutional layer, A s are the trainable weights for the i-th GCNeXt block. We use Rectified Linear Unit (ReLU) as the activation function ?. Refer to <ref type="bibr" target="#b71">[72]</ref> for additional details about GCNeXt. The output of the last block is referred to as X</p><formula xml:id="formula_11">(bv) v</formula><p>, which is the input to the graph matching layer. Language Representation. The query token features X l are fed through an LSTM of b s layers to capture semantic information and the sequential context. Moreover, given that language follows a predefined set of grammatical rules, we set out to leverage syntactic information <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b77">78]</ref> to model grammatical inter-word relations. For this purpose, we adopt SyntacGCN, as shown in <ref type="figure">Fig. 2</ref>. Syntactic graphs are preferred over fully connected graphs, since the former's sparsity property offers more robustness against noise in language <ref type="bibr" target="#b21">[22]</ref>. Our SyntacGCN represents a query as a sparse directed graph, in which each output of the last LSTM layer, referred to as X (0) l , is viewed as a node, and each syntactic relation as an edge. The adjacency matrix A l is directly constructed from the query's syntactic dependencies <ref type="bibr" target="#b39">[40]</ref> and the graph convolution is formulated as:</p><formula xml:id="formula_12">X (i+1) l,j = ? ? ? X (i) l,j + k?N (j) ? (i) jk A l,jk W (i) l X (i) l,k ? ? , (2) where X (i)</formula><p>l,j is the j-th token feature of previous layer's output, N (j) is the syntactic neighbourhood of node j, W (i) l is the learnable weight in the i-th layer, and ? is ReLU.</p><formula xml:id="formula_13">Moreover, ? (i)</formula><p>jk is the edge weight learned from the feature of paired nodes X (i) l,j and X (i) l,k , defined as:</p><formula xml:id="formula_14">? (i) jk = SoftMax(w (i) ? ?(W (i) ? (X (i) l,j ||X (i) l,k ))),<label>(3)</label></formula><p>where w ? and W ? are learnable parameters and || denotes vector concatenation. We refer to the last output of the Syn-tacGCN as X</p><formula xml:id="formula_15">(b l ) l</formula><p>, which will be used to match the video representation X </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Video-Language Graph Matching</head><p>The enriched video representation X (bv) v and query representation X (b l ) l meet and interact in the graph matching layer. This layer models the cross-modal context and allows for multi-modal fusion. It does so by evaluating the intra-modality correlation between each video snippet and between each query token and the inter-modality correlation between each snippet-token pair. The process is shown in <ref type="figure" target="#fig_7">Fig. 3</ref>. First, we create a video-language matching graph, where each node can be either a video snippet or a query token. We include three types of edges: (i) Ordering Edge (O), (ii) Semantic Edge (S), and (iii) Matching Edge (M).</p><p>As depicted in <ref type="figure" target="#fig_7">Fig. 3</ref>, we use Ordering Edge and Semantic Edge in the video-language matching graph (as defined in Sec. 3.3), while the Matching Edge reflects the intermodality relation. Ordering Edge models the sequential nature of both modalities. For example, if an Ordering Edge links two tokens, the words corresponding to the two tokens are consecutive in the input query. Semantic Edge is used to connect graph nodes in the same modality according to their feature similarity, providing non-local dependencies over the entire graph. Importantly, Matching Edge is employed to explicitly model and learn the cross-modality interaction, to extract meaningful alignment information and learn an aggregation policy. The Matching Edge weights are referenced as B. We use Matching Edge to densely connect all possible snippet-token pairs, and set the edge weight proportional to the correlation between the matched node features. Similar to Semantic Edges, Matching Edges are dynamic and evolve in the training process.  <ref type="figure" target="#fig_7">Figure 3</ref>. The video-language matching graph. The nodes represent video snippets and query tokens. Ordering Edge models the sequential nature of both modalities. Semantic Edge connects graph nodes in the same modality according to their feature similarity. Matching Edge captures the cross-modality relationships. We apply graph convolution on the video-language graph for cross-modal context modeling and multi-modal fusion. The neighborhood is specific for the node highlighted in red.</p><p>To combine all three types of edges, we employ relation graph convolution <ref type="bibr" target="#b49">[50]</ref> on the constructed video-language matching graph. Eq. 6 shows the high level representation of the convolutions in this layer. Refer to the supplementary material for the full formulation.</p><formula xml:id="formula_16">X (GM ) = A O XW O +A S BXW S +A M ?XW M +X (4) Here, X = {X (bv) v,1 , . . . , X (bv) v,nv , X (b l ) l,1 , . . . , X (b l )</formula><p>l,n l } is the feature representation of all the nodes in the video-language matching graph. A r and W r for r ? {O, S, M} represent the binary adjacency matrix and learnable weights for each set of edges. B and ? scale the adjacency matrices A S and A M , respectively, such that ? i,j ? B and ? i,j ? ? are proportional to x i x j , We stack together all video and language node features to form X (GM ) v ? R c?nv and X (GM ) l ? R c?n l , and we pass them to the masked moment pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Masked Attention Pooling</head><p>The graph matching layer returns a new video graph and a new language graph fused with information from the other modality. Then, a masked attention pooling operation is applied to the new video graph to list the relevant sub-graph representations as candidate moments. The output of this module is denoted as Y = [y k ] m k=1 , y k ? R c , where m is the number of candidate moments, and c is the feature dimension of each moment. For efficiency purposes, the operation is implemented as a masked attention, allowing us to process each snippet feature only once, while computing each moment's representation.</p><p>We implement three different schemes, namely: (i) learnable self-attention, (ii) cross-attention, and (iii) learnable cross-attention. In (i), we obtain the unnormalized attention weights by applying a 1D convolutional layer that maps each snippet feature to a single score. In (ii) and (iii), we compute the query representation X 1 Introduction 1 Introduction </p><formula xml:id="formula_17">w 2 R nv?1 (1) W 2 R nv?m (2) M 2 R nv?m (3) Y 2 R cv?m (4) X (0) l,nl<label>(5)</label></formula><formula xml:id="formula_18">w 2 R nv?1 (1) W 2 R nv?m (2) M 2 R nv?m (3) Y 2 R cv?m (4) X (0) l,nl<label>(5)</label></formula><formula xml:id="formula_19">(9) X (GM ) v,2<label>(10)</label></formula><formula xml:id="formula_20">X (GM ) v,nv<label>(11)</label></formula><p>1 w nv</p><p>Conv. 1D and repeat m times</p><formula xml:id="formula_22">SoftMax Mask X (GM ) v,1 (9) X (GM ) v,2<label>(10)</label></formula><formula xml:id="formula_23">X (GM ) v,nv<label>(11)</label></formula><p>1 w nv</p><p>Conv. 1D and repeat m times snippet feature only once while computing each moment's representation. Specifically, we implement and ablate three different schemes, namely: (i) learnable self-attention, (ii) crossattention, and (iii) learnable cross-attention. In (i), we obtain the unnormalized attention weights by applying a 1D convolutional layer that maps each clip feature to a single score. In (ii) and (iii), we compute the sentence representation by applying self-attention pooling on top of the last SyntacGCN layer X (bl) l , we refer to this quantity as X (att) l . Configuration (iii) is depicted in <ref type="figure">Fig. 4</ref>.</p><formula xml:id="formula_25">SoftMax Mask X (GM ) v,1 (9) X (GM ) v,2<label>(10)</label></formula><formula xml:id="formula_26">X (GM ) v,nv<label>(11)</label></formula><p>Cross-attention obtains the unnormalized weights by computing the inner product between the snippet and sentence features, while learnable cross-attention concatenates each snippet feature with the sentence feature and uses a 1D convolutional layer to obtain the weights. In all cases, the unnormalized weight vector has shape w 2 R nv?1 for each video. The vector is repeated m times to obtain the matrix W 2 R nv?m , and a fixed mask M 2 R nv?m is applied to it. Similar to Songyang et al. <ref type="bibr" target="#b70">[71]</ref>, we generate moment candidates and apply a sparse sampling strategy to discard redundant moments. Therefore, the mask is generated according to the sampled moments, highlighting for each of them, which are the clips that must be taken into account when computing the moment's pooled feature. The attention scores are then obtained by applying the softmax operation. Thanks to the masking operation, clips not related to the n-th moment will not be considered. Finally, the moments' features are obtained simply as a matrix multiplication: Y = X (GM ) SoftMax(W + M ). Ablation results are reported in the experiment section (Sec 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Moment Localization</head><p>The output of the previous module is then fed to a Multi-Layer Perceptron (MLP) network to compute the score p k for each moment candidate. This scores predicts the Intersection-over-Union (IoU) of each moment with the ground truth one. For training, we supervise this process using a cross-entropy loss, shown in Eq. 5. We assign the label t k = 1 if the IoU is greater than a threshold ? and t k = 0 otherwise.</p><formula xml:id="formula_27">L = 1 m m X k=1 t k log p k + (1 t k ) log(1 p k ),<label>(5)</label></formula><p>At inference time, moment candidates are ranked according to their predicted scores and non-maximum suppression is adopted to discard highly overlapping moments. The remaining top-? moments are involved in the recall computation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>ActivityNet-Captions <ref type="bibr" target="#b20">[21]</ref> is a popular benchmark dataset for the video grounding task. It is a large-scale action understanding dataset initially collected for the task of dense captioning, but it has been recently restructured for the task of moment localization with natural language <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>. The dataset contains 20k diverse videos with about 100k sentence queries, subdivided into four splits: train, val 1, val 2, and test. The test set is withheld for competition purposes leaving the rest publicly available. See Tab. 1 for more details about the publicly available splits. Following the previous setting in <ref type="bibr" target="#b25">[26]</ref>, in this paper, we use val 1 as the validation set and val 2 as the testing set.</p><p>TACoS <ref type="bibr" target="#b38">[39]</ref> consists of 127 videos selected from the MPII Cooking Composite Activities video corpus <ref type="bibr" target="#b40">[41]</ref>. It consists of 18818 moment-query pairs of different cooking activities in the kitchen. On average, every video in TACoS contains 148 queries, some of which are annotations of very short video segments.</p><p>DiDeMo <ref type="bibr" target="#b0">[1]</ref> consists of unedited video footage from Flickr with sentences aligned to unique moments in its 10642 videos. It is split into 33008, 4180, and 4021 videolanguage pairs for training, validation, and testing, respectively. Note that moment start and end points are aligned to five-second intervals and that the maximum annotated moment length is 30 seconds. Charades-STA <ref type="bibr" target="#b10">[11]</ref> consists of 16124 video-sentences pairs resulting in the smallest dataset for the task in terms of training and testing samples. Moreover, this dataset is also characterized by the smallest vocabulary size (and average sentence length). See Tab 1 for more details. In addition, the dataset only has two splits available and lacks an official validation split, making it prone to overfitting when hyperparameters are chosen with respect to training performance. Although this dataset has been widely adopted for the task, for the reasons listed above, we choose not to evaluate our method on it. More discussion can be found in the <ref type="table">Supple</ref> snippet feature only once while computing each moment's representation. Specifically, we implement and ablate three different schemes, namely: (i) learnable self-attention, (ii) crossattention, and (iii) learnable cross-attention. In (i), we obtain the unnormalized attention weights by applying a 1D convolutional layer that maps each clip feature to a single score. In (ii) and (iii), we compute the sentence representation by applying self-attention pooling on top of the last SyntacGCN layer X (bl) l , we refer to this quantity as X (att) l . Configuration (iii) is depicted in <ref type="figure">Fig. 4</ref>.</p><p>Cross-attention obtains the unnormalized weights by computing the inner product between the snippet and sentence features, while learnable cross-attention concatenates each snippet feature with the sentence feature and uses a 1D convolutional layer to obtain the weights. In all cases, the unnormalized weight vector has shape w 2 R nv?1 for each video. The vector is repeated m times to obtain the matrix W 2 R nv?m , and a fixed mask M 2 R nv?m is applied to it. Similar to Songyang et al. <ref type="bibr" target="#b70">[71]</ref>, we generate moment candidates and apply a sparse sampling strategy to discard redundant moments. Therefore, the mask is generated according to the sampled moments, highlighting for each of them, which are the clips that must be taken into account when computing the moment's pooled feature. The attention scores are then obtained by applying the softmax operation. Thanks to the masking operation, clips not related to the n-th moment will not be considered. Finally, the moments' features are obtained simply as a matrix multiplication: Y = X (GM ) SoftMax(W + M ). Ablation results are reported in the experiment section (Sec 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Moment Localization</head><p>The output of the previous module is then fed to a Multi-Layer Perceptron (MLP) network to compute the score p k for each moment candidate. This scores predicts the Intersection-over-Union (IoU) of each moment with the ground truth one. For training, we supervise this process using a cross-entropy loss, shown in Eq. 5. We assign the label t k = 1 if the IoU is greater than a threshold ? and t k = 0 otherwise.</p><formula xml:id="formula_28">L = 1 m m X k=1 t k log p k + (1 t k ) log(1 p k ),<label>(5)</label></formula><p>At inference time, moment candidates are ranked according to their predicted scores and non-maximum suppression is adopted to discard highly overlapping moments. The remaining top-? moments are involved in the recall computation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>ActivityNet-Captions <ref type="bibr" target="#b20">[21]</ref> is a popular benchmark dataset for the video grounding task. It is a large-scale action understanding dataset initially collected for the task of dense captioning, but it has been recently restructured for the task of moment localization with natural language <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>. The dataset contains 20k diverse videos with about 100k sentence queries, subdivided into four splits: train, val 1, val 2, and test. The test set is withheld for competition purposes leaving the rest publicly available. See Tab. 1 for more details about the publicly available splits. Following the previous setting in <ref type="bibr" target="#b25">[26]</ref>, in this paper, we use val 1 as the validation set and val 2 as the testing set.</p><p>TACoS <ref type="bibr" target="#b38">[39]</ref> consists of 127 videos selected from the MPII Cooking Composite Activities video corpus <ref type="bibr" target="#b40">[41]</ref>. It consists of 18818 moment-query pairs of different cooking activities in the kitchen. On average, every video in TACoS contains 148 queries, some of which are annotations of very short video segments.</p><p>DiDeMo <ref type="bibr" target="#b0">[1]</ref> consists of unedited video footage from Flickr with sentences aligned to unique moments in its 10642 videos. It is split into 33008, 4180, and 4021 videolanguage pairs for training, validation, and testing, respectively. Note that moment start and end points are aligned to five-second intervals and that the maximum annotated moment length is 30 seconds. Charades-STA <ref type="bibr" target="#b10">[11]</ref> consists of 16124 video-sentences pairs resulting in the smallest dataset for the task in terms of training and testing samples. Moreover, this dataset is also characterized by the smallest vocabulary size (and average sentence length). See Tab 1 for more details. In addition, the dataset only has two splits available and lacks an official validation split, making it prone to overfitting when hyperparameters are chosen with respect to training performance. Although this dataset has been widely adopted for the task, for the reasons listed above, we choose not to evaluate our method on it. More discussion can be found in the <ref type="table">Supple</ref> snippet feature only once while computing each moment's representation. Specifically, we implement and ablate three different schemes, namely: (i) learnable self-attention, (ii) crossattention, and (iii) learnable cross-attention. In (i), we obtain the unnormalized attention weights by applying a 1D convolutional layer that maps each clip feature to a single score. In (ii) and (iii), we compute the sentence representation by applying self-attention pooling on top of the last SyntacGCN layer X (bl) l , we refer to this quantity as X (att) l . Configuration (iii) is depicted in <ref type="figure">Fig. 4</ref>.</p><p>Cross-attention obtains the unnormalized weights by computing the inner product between the snippet and sentence features, while learnable cross-attention concatenates each snippet feature with the sentence feature and uses a 1D convolutional layer to obtain the weights. In all cases, the unnormalized weight vector has shape w 2 R nv?1 for each video. The vector is repeated m times to obtain the matrix W 2 R nv?m , and a fixed mask M 2 R nv?m is applied to it. Similar to Songyang et al. <ref type="bibr" target="#b70">[71]</ref>, we generate moment candidates and apply a sparse sampling strategy to discard redundant moments. Therefore, the mask is generated according to the sampled moments, highlighting for each of them, which are the clips that must be taken into account when computing the moment's pooled feature. The attention scores are then obtained by applying the softmax operation. Thanks to the masking operation, clips not related to the n-th moment will not be considered. Finally, the moments' features are obtained simply as a matrix multiplication: Y = X (GM ) SoftMax(W + M ). Ablation results are reported in the experiment section (Sec 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Moment Localization</head><p>The output of the previous module is then fed to a Multi-Layer Perceptron (MLP) network to compute the score p k for each moment candidate. This scores predicts the Intersection-over-Union (IoU) of each moment with the ground truth one. For training, we supervise this process using a cross-entropy loss, shown in Eq. 5. We assign the label t k = 1 if the IoU is greater than a threshold ? and t k = 0 otherwise.</p><formula xml:id="formula_29">L = 1 m m X k=1 t k log p k + (1 t k ) log(1 p k ),<label>(5)</label></formula><p>At inference time, moment candidates are ranked according to their predicted scores and non-maximum suppression is adopted to discard highly overlapping moments.  <ref type="bibr" target="#b25">[26]</ref>, in this paper, we use val 1 as the validation set and val 2 as the testing set.</p><p>TACoS <ref type="bibr" target="#b38">[39]</ref> consists of 127 videos selected from the MPII Cooking Composite Activities video corpus <ref type="bibr" target="#b40">[41]</ref>. It consists of 18818 moment-query pairs of different cooking activities in the kitchen. On average, every video in TACoS contains 148 queries, some of which are annotations of very short video segments.</p><p>DiDeMo <ref type="bibr" target="#b0">[1]</ref> consists of unedited video footage from Flickr with sentences aligned to unique moments in its 10642 videos. It is split into 33008, 4180, and 4021 videolanguage pairs for training, validation, and testing, respectively. Note that moment start and end points are aligned to five-second intervals and that the maximum annotated moment length is 30 seconds. Charades-STA <ref type="bibr" target="#b10">[11]</ref> consists of 16124 video-sentences pairs resulting in the smallest dataset for the task in terms of training and testing samples. Moreover, this dataset is also characterized by the smallest vocabulary size (and average sentence length). See Tab 1 for more details. In addition, the dataset only has two splits available and lacks an official validation split, making it prone to overfitting when hyperparameters are chosen with respect to training performance. Although this dataset has been widely adopted for the task, for the reasons listed above, we choose not to evaluate our method on it. More discussion can be found in the Supplementary Material. The lack of validation split makes it very difficult to asses if hyper-parameters have been chosen to 6 0.8s</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21.0s</head><p>Ground truth moment Predicted moment</p><formula xml:id="formula_30">(a) (b) (c) (d)</formula><p>Tokens -Snippets (Ours)</p><formula xml:id="formula_31">X (GM ) 2 R c?nv Y 2 R c?m 0.8s</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21.0s</head><p>Ground truth moment Predicted moment</p><formula xml:id="formula_32">(a) (b) (c) (d)</formula><p>Tokens -Snippets (Ours)  . Cross-attention obtains the unnormalized weights by computing the inner product between the snippet and query feature, while learnable cross-attention concatenates each snippet feature with the query feature and uses a 1D convolutional layer to obtain the weights. Configuration (iii) is depicted in <ref type="figure" target="#fig_8">Fig. 7</ref>. In all cases, the unnormalized weight vector has shape w ? R nv?1 for each video. The vector is repeated m times to obtain the matrix W ? R nv?m , and a fixed mask M ? R nv?m is applied to it. Similar to Songyang et al. <ref type="bibr" target="#b80">[81]</ref>, we generate m moment candidates and apply a sparse sampling strategy to discard redundant moments. The value m is dataset dependent. The mask is generated according to the sampled moments, highlighting for each of them, which are the snippets that must be taken into account when computing the moment's pooled feature. The attention scores are then obtained by a softmax operation. Thanks to the masking operation, snippets not related to the n-th moment will not be considered. Finally, the moments' features are obtained simply as a matrix multiplication Y = X (GM ) SoftMax(W + M ).</p><formula xml:id="formula_33">X (GM ) 2 R c?nv Y 2 R</formula><formula xml:id="formula_34">X (GM ) v 2 R c?nv Y 2 R c?m</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Moment Localization</head><p>Each candidate moment representation, output of previous module, is endowed with an additive 2D positional embedding encoding the start and end timestamps (t s,k , t e,k with k ? [1, m]). Then each moment feature is fed to an MLP to compute the alignment confidence score p k . This score predicts the Intersection-over-Union (IoU) of each moment with the ground truth of the corresponding videoquery pair. For training, we supervise this process using a cross-entropy loss, shown in Eq. 5. Similar to <ref type="bibr" target="#b80">[81]</ref>, we assign the label t k = 1 if the IoU ? ? 2 , t k = 0 if the IoU ? ? 1 , and otherwise t k = (IoU ? ? 1 )/(? 2 ? ? 1 ).</p><formula xml:id="formula_35">L = 1 m m k=1 t k log p k + (1 ? t k ) log(1 ? p k ),<label>(5)</label></formula><p>During inference, moment candidates are ranked based on their predicted scores and NMS is used to discard highly overlapping moments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>ActivityNet-Captions <ref type="bibr" target="#b26">[27]</ref> is a popular dataset initially collected for the task of dense captioning, and recently adopted for the task of moment localization with natural language queries <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>. The dataset is subdivided into four splits: train, val 1, val 2, and test. The test set is withheld for competition purposes, while leaving the rest publicly available. Refer to <ref type="table" target="#tab_11">Table 1</ref> for details about the publicly available splits. Following the setting in <ref type="bibr" target="#b30">[31]</ref>, we use val 1 as the validation set and val 2 as the testing set.</p><p>TACoS <ref type="bibr" target="#b45">[46]</ref> consists of videos selected from the MPII Cooking Composite Activities video corpus <ref type="bibr" target="#b47">[48]</ref>. It comprises 18818 video-query pairs of different cooking activities. Each video contains an average of 148 queries, some of which are annotations of short video segments.</p><p>DiDeMo <ref type="bibr" target="#b1">[2]</ref> contains unedited video footage from Flickr with sentences aligned to unique moments in its 10642 videos. It contains more than 40k video-query pairs with coarse temporal annotations. Moment start and end points are aligned to five-second intervals and the maximum annotated moment length is 30 seconds. Concerns regarding the Charades-STA <ref type="bibr" target="#b15">[16]</ref> dataset discouraged us from evaluating our method on it. Refer to the supplementary material for a detailed discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation</head><p>Evaluation Metrics. We follow the commonly used setting in <ref type="bibr" target="#b13">[14]</ref>, where the Rank@? for IoU=? serves as our evaluation metric. For example, given a video-query pair, the result is positive if any of the top-? predictions has IoU with the ground-truth larger or equal to ?; otherwise the result is negative. We average the results across all testing samples. Following standard practice, we set ? ? {1, 5} with ? ? {0.3, 0.5, 0.7} for ActivityNet Captions, ? ? {1, 5} with ? ? {0.1, 0.3, 0.5} for TACoS, and ? ? {1, 5} with ? ? {0.5, 0.7, 1.0} for DiDeMo. Language and Video Features. After lower-case conversion and tokenization, we use the pretrained GloVe model <ref type="bibr" target="#b44">[45]</ref> to obtain the initial query embedding for every token and extract the syntactic dependencies using the Stanford CoreNLP 4.0.0 parser <ref type="bibr" target="#b39">[40]</ref>. The b s layers of LSTM with 512 hidden units are used as the query encoder. Then, the syntactic GCN encodes syntactic information of the queries and returns a new embedding with 512 dimensions. For visual features, we use pretrained C3D <ref type="bibr" target="#b57">[58]</ref> for Activi-tyNet Captions and TACoS, and VGG16 <ref type="bibr" target="#b53">[54]</ref> for DiDeMo, while holding their parameters fixed during training, as they are readily available and commonly used by state-of-the-art methods. We use 1D convolutions to project the input visual features to a fixed dimension (512), and the GCNeXt blocks' hyper-parameters are set as in <ref type="bibr" target="#b71">[72]</ref>.  <ref type="table" target="#tab_11">Table 1</ref>. Datasets statistics. We report relevant information for each datasets available for the grounding task.</p><p>Implementation details. We use Adam <ref type="bibr" target="#b25">[26]</ref> with a StepLR scheduler <ref type="bibr" target="#b37">[38]</ref>. We adopt learning rates ranging from 10 ?3 to 10 ?4 for different datasets. The number of sampled snippets n v is set to 64 for ActivityNet Captions, and 256 for TACoS, and 48 for DiDeMo. The values of (b v , b s , b l ) are equal to <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4)</ref>, (4, 5, 2), <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4)</ref>, respectively for the three datasets. In post-processing, we apply NMS with values respectively to the m predictions to filter out highly overlapping moments. Values of m for each dataset are: 1104, 3101, 505 while the NMS thresholds are: 0.5, 0.3, 0.5. We adapt BCE with logits loss to make the training process more numerically stable. The IoU thresholds (? 1 , ? 1 ) for the three datasets are: (0.7, 0.71), (0.5, 0.7), (0.69, 1.0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Art</head><p>Comparisons are carried out only against methods using the same input features as VLG-Net. In the Tables, we highlight the top-1 and top-2 performance values by bold and underline, respectively. ActivityNet Captions <ref type="table">(Table 2)</ref>. VLG-Net offers the highest performance for the tight IoU=0.7. However, it falls short against <ref type="bibr" target="#b32">[33]</ref> on the lousier IoU=0.5. In practical terms, the two methods are to be considered on par. Nonetheless, notice how tighter IoU translates to better retrieval quality in a real use-case scenario. In these terms, VLG-Net is to be preferred over the competitive <ref type="bibr" target="#b32">[33]</ref>. Finally, notice how VLG-Net achieves a significant boost against the recently released 2D-TAN <ref type="bibr" target="#b80">[81]</ref> and DRN <ref type="bibr" target="#b76">[77]</ref>. TACoS <ref type="table">(Table 3)</ref>. Our model outperforms state-of-the-art methods and achieves the highest scores for all IoU thresholds with significant improvements. In particular, VLG-Net exceeds the previous art <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b80">81]</ref> by a large margin, ranging from 7.10% to 11.52%, across all evaluation settings, showcasing the excellent design of the architecture. DiDeMo <ref type="table">(Table 4</ref>). Our proposed technique outperforms the top-ranked methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> with respect to R@1 and R@5 for IoU0.5 and 0.7 with evident increases. It also reaches the highest performance in regards to R@1 IoU1.0. For R@5 IoU1.0, VLG-Net ranks second, falling short with respect to TMN [7] by 1.32%. For completeness, we report the performances of TGN <ref type="bibr" target="#b31">[32]</ref> and TMN <ref type="bibr" target="#b6">[7]</ref>; with the caveat that their performance could not be verified as the code is not made publicly available. Hence the different colour for the corresponding rows in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>To motivate our design choices, we present two ablations that focus on relevant aspects of our method. The first ablation showcases the importance of context modeling. The second investigates VLG-Net's performance when other commonly adopted multi-modal fusion operations replace the graph matching module. In <ref type="table">Table 5</ref>, we report our best result (first column) and summarize all ablated variants. For simplicity, we evaluate on the TACoS dataset and specifically focus on its most challenging setups: R@1 IoU0.5 and R@5 IoU0.5. To promote fair comparison, we report each model's capacity in millions (M) of parameters. More ablations are reported in the supplementary material. Context Ablation. First, we investigate the impact of different context modeling strategies and compare six variants with our VLG-Net. VLG-Net NC represents a "No Context" architecture, in which we replace the GCNeXt and Syn-tacGCN modules with fully connected layers that do not model any intra-modality context. Moreover, we switch off the Ordering and Semantic Edges in the graph matching module. Although the model capacity for VLG-Net NC only drops 0.21M (1.6%), its performance degrades up to 8.82% with respect to VLG-Net. Following <ref type="bibr" target="#b1">[2]</ref>, we devise VLG-Net GM ("Global on Moments") and VLG-Net GI ("Global on Input") experiments. The first one extends VLG-Net NC by concatenating each moment feature with a global video feature after the matching operation. Instead, in VLG-Net GI , we concatenate each snippet and token feature with an average pooled version of their respective raw input features. Differently, following <ref type="bibr" target="#b15">[16]</ref>, VLG-Net LM ("Local on Moments") models local context by extending the moment's boundaries when computing the moment's features in the masked attention pooling module. The simple context modeling adopted in these architectures  </p><formula xml:id="formula_36">X (GM ) 2 R c?nv Y 2 R c?m 3</formula><p>Three people are riding a very colourful camel on the beach. 0.8s</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21.0s</head><p>Ground truth moment Predicted moment</p><formula xml:id="formula_37">(a) (b) (c) (d)</formula><p>Tokens -Snippets (Ours)</p><formula xml:id="formula_38">X (GM ) 2 R c?nv Y 2 R c?m 3</formula><p>Ordering  <ref type="table">Table 5</ref>. Ablation on context modelling and multi-modal fusion approaches. This ablation shows how the pipeline takes advantage of context modelling and our graph matching module to achieving state-of-the-art performance.</p><note type="other">Edge Semantic Edge Matching Edge Neighbourhood</note><p>Multi-modal Fusion Ablation. To evaluate our graph matching module's capabilities for multi-modality fusion, we replace it with other commonly used operations in the literature. Following <ref type="bibr" target="#b80">[81]</ref>, we use a Hadamard product between the video moment's features and an average pooled feature for the language query. We adopt several fully connected layers before and after the fusion to keep the model size close to ours. As compared to <ref type="bibr" target="#b80">[81]</ref>, we adopt the learnable masked attention pooling for generating the moment's features, which allows for interactions between query tokens and video snippets before the fusion operation. We refer to this model as VLG-Net HM ("Hadamard on Moments"). We also apply the Hadamard product at the snippet level <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b76">77]</ref> and train VLG-Net HS ("Hadamard on Snippets"). Finally, following <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b59">60]</ref>, we concatenate each snippet feature with the query feature and use linear layers for the projection in VLG-Net CS ("Concatenate on Snippets"). We can conclude that fusing the modalities at the snippet level tends to perform better. The Hadamard operation has gained quite some traction for its good performance and absence of trainable parameters, making it efficient to compute. However, we argue that the complexity of multi-modal alignment calls for more elaborate strategies for multi-modal fusion. Our graph matching module offers a perspective in this research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization</head><p>We show several qualitative grounding results from Ac-tivityNet Captions in <ref type="figure">Fig. 5</ref>. Our VLG-Net can generate precise moment boundaries that match the query well in different scenarios. Worth mentioning, our method can sometimes give predictions that are more meaningful than the ground truth annotation. As shown in <ref type="figure">Fig. 5(d)</ref>, although the ground truth only aligns to the beginning of the video, the query "Three people are riding a very colourful camel on the beach." can semantically match the whole video. In this case, our VLG-Net gives a more reasonable grounding result. Additional visualizations are reported in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper addresses the problem of text-to-video temporal grounding, where we cast the problem as an algorithmic graph matching. We propose Video-Language Graph Matching Network (VLG-Net) to match the video and language modalities. We represent each modality as graphs and explore four types of edges, Syntactic Edge, Ordering Edge, Semantic Edge, and Matching Edge, to encode local, non-local, and cross-modality relationships to align the video-query pair. Extensive experiments show that our VLG-Net can model inter-and intra-modality context, learn multi-modal fusion and surpass the current state-of-the-art performance on three widely used datasets.</p><p>outputs of the single modality stack of computational blocks. The graph matching layer models the cross-modal context and allows for multi-modal fusion. To this purpose the video-language matching graph is constructed and three types of edges are designed: (i) Ordering Edge (O), (ii) Semantic Edge (S), and (iii) Matching Edge (M).</p><p>To aggregate the information, we employ relation graph convolution <ref type="bibr" target="#b49">[50]</ref> on the constructed video-language matching graph. Eq. 6 shows the high level representation of the convolutions in this layer.</p><formula xml:id="formula_39">X (GM ) = A O XW O +A S BXW S +A M ?XW M +X (6) Here, X = {X (bv) v,1 , . . . , X (bv) v,nv , X (b l ) l,1 , . . . , X (b l )</formula><p>l,n l } is the feature representation of all the nodes in the video-language matching graph. A r and W r for r ? {O, S, M} represent the binary adjacency matrix and learnable weights for each set of edges. Specifically, B and ? scale the adjacency matrices A S and A M . Both ? i,j ? B and ? i,j ? ? are proportional to x i x j ,</p><formula xml:id="formula_40">? i,j = exp [x i x j ] A S (k,j)=1 exp [x k x j ] ,<label>(7)</label></formula><formula xml:id="formula_41">? i,j = exp [x i x j ] A M (k,j)=1 exp [x k x j ] .<label>(8)</label></formula><p>In practise, to implement GPU-memory efficient graph convolution operation, we replace the time-consuming matrix multiplication by indexing operation of tensors. Thus, the semantic and matching edge convolution can be present as</p><formula xml:id="formula_42">A S BXW S = j?N S i (? T S [? j x j ||x i ]),<label>(9)</label></formula><formula xml:id="formula_43">A M ?XW M = j?N M i (? T M [? j x j ||x i ]),<label>(10)</label></formula><p>where N * i is the neighbourhood of node i connected by edge with type * , * ? {S, M}. The || sign means concatenation of features.? S ,? M are learnable weights.</p><p>Moreover, as shown by A.2 of G-TAD <ref type="bibr" target="#b71">[72]</ref>, our ordering edge convolution, can be efficiently computed as a 1D convolution with kernel size 3.</p><formula xml:id="formula_44">A O XW O = Conv1D[X]<label>(11)</label></formula><p>Therefore, we can equivalently formulate Eq. 6 as:</p><formula xml:id="formula_45">X (GM ) = Conv1D[X] + j?N S i (? T S [? j x j ||x i ]) + j?N M i (? T M [? j x j ||x i ]) + X (12)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph matching edges ablation</head><p>We ablate the contribution of the three different types of edges designed for the graph matching module. We report in <ref type="table">Table 6</ref> the performance of VLG-Net for the TACoS dataset when each edge is removed from the architecture. As previously stated, the Ordering Edges or Semantic Edges are responsible for aggregating contextual information within the graph matching module. When removed, they lead to noticeable degradation of the performance of 2.15% and 3.77%, respectively. Conversely, as expected, when the Matching Edges are removed, the performances are severely impaired. We assist in a drop of 27.34%, showcasing the high relevance of the matching operation. Note that, the removal of the Matching Edges prevents the fusion between the modalities. Nonetheless, the two modalities still interact in the Masked Attention Pooling module through the learnable cross-attention pooling method. However, this limited interaction cannot bridge the complex semantic information between modalities. The ablation showcases the importance of designing effective operation for multi-modal fusion to achieve high performance on the grounding task. Nonetheless, we can conclude that all edges are relevant and necessary to obtain state-of-theart performance.  <ref type="table">Table 6</ref>. Ablation of different edges. We investigate the impact of edges within the graph matching layer. We report the performance of our VLG-Net when specific edges are removed, as well as our best performance for TACoS datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization graph matching attention</head><p>In <ref type="figure">Fig. 6</ref>, we plot the Matching Edge weights (before SoftMax) for two video-query pairs, where the Matching Edge weights are used to measure the similarity between video snippets and language tokens. In graph convolutions, a Matching Edge propagates more information if its weight is high, and vice versa.  <ref type="figure">Figure 6</ref>. Visualization of graph matching attention. We visualize the Matching Edge of the graph matching layer. Correspondence between video snippets and query tokens can be evaluated through the heat-map.</p><p>In <ref type="figure">Fig. 6a</ref>, we show the grounding result for a 2 minutes accordian tutorial, with associated query: "She is holding an accordian as she talks". It can be observed from the blue-yellow heat-map that high scores are assigned to the words "holding", "accordian", and "talks", which are the most discriminative tokens for the query localization. Below the heat-map, we visualize the snippets of the video. The unrelated snippets (first and last) are associated with low scores. Conversely, more relevant snippets (central ones) have higher Matching Edge weights. This entails that the algorithm is successfully correlating important language cues with relevant video cues when performing the graph matching operation.</p><p>Similarly, <ref type="figure">Fig. 6b</ref> shows the result for a 22 second camel riding video, for which the associated query is: "Three people are riding a very colorful camel on the beach." The heatmap highlights the keywords: "riding", "colorful camel", and "beach", which are relatively more informative in the query sentence. Interestingly, the word "riding" is always associated with high attention weights, and a visual inspection confirms that the action happens throughout the whole video. This showcases that our VLG-Net can successfully learn semantic video-language matching. If we focus on the first two snippets of <ref type="figure">Fig. 6b</ref>, we can see that both have associated high scores with the word "riding". However, given the smaller field of view of the first frame, only the second frame contains a more distinguishable camel. In fact, for this particular frame, we observe a high weight score for the words "colorful" and "camel". Moreover, the context of "beach" can be learned from all the last three snippets. equations mattia.soldan.ms November 2020 1 Introduction 1 Introduction</p><formula xml:id="formula_46">w 2 R nv?1 (1) W 2 R nv?m (2) M 2 R nv?m (3) Y 2 R cv?m (4) X (0) l,nl<label>(5)</label></formula><formula xml:id="formula_47">w 2 R nv?1 (1) W 2 R nv?m (2) M 2 R nv?m (3) Y 2 R cv?m (4) X (0) l,nl<label>(5)</label></formula><p>1</p><formula xml:id="formula_48">X (0) l,n l (5) w 1 (6) w 2 (7) w n v<label>(8)</label></formula><p>Conv. 1D and repeat m times SoftMax 1</p><formula xml:id="formula_49">X (0) l,n l (5) w 1 (6) w 2<label>(7)</label></formula><p>w nv <ref type="bibr" target="#b7">(8)</ref> Conv. 1D and repeat m times SoftMax Mask 1 w 2</p><p>w nv</p><p>Conv. 1D and repeat m times</p><formula xml:id="formula_52">SoftMax Mask X (GM ) v,1 (9) X (GM ) v,2<label>(10)</label></formula><formula xml:id="formula_53">X (GM ) v,nv<label>(11)</label></formula><p>1 w 2</p><p>w nv</p><p>Conv. 1D and repeat m times</p><formula xml:id="formula_56">SoftMax Mask X (GM ) v,1 (9) X (GM ) v,2<label>(10)</label></formula><formula xml:id="formula_57">X (GM ) v,nv<label>(11)</label></formula><p>1 w 2</p><p>w nv</p><p>Conv. 1D and repeat m times</p><formula xml:id="formula_60">SoftMax Mask X (GM ) v,1 (9) X (GM ) v,2<label>(10)</label></formula><formula xml:id="formula_61">X (GM ) v,nv<label>(11)</label></formula><p>1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21.0s</head><p>Ground truth moment Predicted moment (a) (b) (c) (d)</p><p>Tokens -Snippets (Ours)</p><formula xml:id="formula_62">X (GM ) 2 R c?nv Y 2 R c?m 3</formula><p>Three people are riding a very colourful 0.8s</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21.0s</head><p>Ground truth moment Predicted moment</p><formula xml:id="formula_63">(a) (b) (c) (d)</formula><p>Tokens -Snippets (Ours)</p><formula xml:id="formula_64">X (GM ) 2 R c?nv Y 2 R c?m 3</formula><p>Three people are riding a very colourful 0.8s</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21.0s</head><p>Ground truth moment Predicted moment</p><formula xml:id="formula_65">(a) (b) (c) (d)</formula><p>Tokens -Snippets (Ours)</p><formula xml:id="formula_66">X (GM ) 2 R c?nv Y 2 R c?m 3</formula><p>0.8s 21.0s Ground truth moment Predicted moment (a) (b) (c) (d) Tokens -Snippets (Ours) X (GM ) 2 R c?nv Y 2 R c?m Inner product and repeat m times Conv. 1D and repeat m times 1 Introduction 1 Introduction</p><formula xml:id="formula_67">w 2 R nv?1 (1) W 2 R nv?m (2) M 2 R nv?m (3) Y 2 R cv?m (4) X (0) l,nl<label>(5)</label></formula><formula xml:id="formula_68">w 2 R nv?1 (1) W 2 R nv?m (2) M 2 R nv?m (3) Y 2 R cv?m (4) X (0) l,nl<label>(5)</label></formula><p>1 l,n l w 1</p><formula xml:id="formula_69">(6) w 2 (7) w n v<label>(8)</label></formula><p>Conv. 1D and repeat m times SoftMax 1</p><formula xml:id="formula_70">w 2 R nv?1 (1) W 2 R nv?m (2) M 2 R nv?m (3) Y 2 R cv?m (4) X (0) l,n l (5) w 1 (6) w 2<label>(7)</label></formula><p>w nv</p><p>Conv. 1D and repeat m times</p><formula xml:id="formula_72">SoftMax Mask 1 M 2 R nv?m (3) Y 2 R cv?m<label>(4)</label></formula><formula xml:id="formula_73">X (0) l,nl<label>(5)</label></formula><formula xml:id="formula_74">w 1 (6) w 2 (7) w nv<label>(8)</label></formula><p>Conv. 1D and repeat m times</p><formula xml:id="formula_75">SoftMax Mask X (GM ) v,1<label>(9)</label></formula><formula xml:id="formula_76">X (GM ) v,2<label>(10)</label></formula><formula xml:id="formula_77">X (GM ) v,nv<label>(11)</label></formula><formula xml:id="formula_78">1 W 2 R nv?m (2) M 2 R nv?m (3) Y 2 R cv?m<label>(4)</label></formula><formula xml:id="formula_79">X (0) l,nl (5) w 1 (6) w 2 (7) w nv<label>(8)</label></formula><p>Conv. 1D and repeat m times</p><formula xml:id="formula_80">SoftMax Mask X (GM ) v,1<label>(9)</label></formula><formula xml:id="formula_81">X (GM ) v,2<label>(10)</label></formula><formula xml:id="formula_82">X (GM ) v,nv<label>(11)</label></formula><formula xml:id="formula_83">1 W 2 R nv?m (2) M 2 R nv?m (3) Y 2 R cv?m<label>(4)</label></formula><formula xml:id="formula_84">X (0) l,nl (5) w 1 (6) w 2 (7) w nv<label>(8)</label></formula><p>Conv. 1D and repeat m times snippet feature only once while computing each moment's representation. Specifically, we implement and ablate three different schemes, namely: (i) learnable self-attention, (ii) crossattention, and (iii) learnable cross-attention. In (i), we obtain the unnormalized attention weights by applying a 1D convolutional layer that maps each clip feature to a single score. In (ii) and (iii), we compute the sentence representation by applying self-attention pooling on top of the last SyntacGCN layer X (bl) l , we refer to this quantity as X (att) l . Configuration (iii) is depicted in <ref type="figure">Fig. 4</ref>.</p><formula xml:id="formula_85">SoftMax Mask X (GM ) v,1<label>(9)</label></formula><formula xml:id="formula_86">X (GM ) v,2<label>(10)</label></formula><formula xml:id="formula_87">X (GM ) v,nv<label>(11)</label></formula><p>Cross-attention obtains the unnormalized weights by computing the inner product between the snippet and sentence features, while learnable cross-attention concatenates each snippet feature with the sentence feature and uses a 1D convolutional layer to obtain the weights. In all cases, the unnormalized weight vector has shape w 2 R nv?1 for each video. The vector is repeated m times to obtain the matrix W 2 R nv?m , and a fixed mask M 2 R nv?m is applied to it. Similar to Songyang et al. <ref type="bibr" target="#b70">[71]</ref>, we generate moment candidates and apply a sparse sampling strategy to discard redundant moments. Therefore, the mask is generated according to the sampled moments, highlighting for each of them, which are the clips that must be taken into account when computing the moment's pooled feature. The attention scores are then obtained by applying the softmax operation. Thanks to the masking operation, clips not related to the n-th moment will not be considered. Finally, the moments' features are obtained simply as a matrix multiplication: Y = X (GM ) SoftMax(W + M ). Ablation results are reported in the experiment section (Sec 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Moment Localization</head><p>The output of the previous module is then fed to a Multi-Layer Perceptron (MLP) network to compute the score p k for each moment candidate. This scores predicts the Intersection-over-Union (IoU) of each moment with the ground truth one. For training, we supervise this process using a cross-entropy loss, shown in Eq. 5. We assign the label t k = 1 if the IoU is greater than a threshold ? and t k = 0 otherwise.</p><formula xml:id="formula_88">L = 1 m m X k=1 t k log p k + (1 t k ) log(1 p k ),<label>(5)</label></formula><p>At inference time, moment candidates are ranked according to their predicted scores and non-maximum suppression is adopted to discard highly overlapping moments. The remaining top-? moments are involved in the recall computation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>ActivityNet-Captions <ref type="bibr" target="#b20">[21]</ref> is a popular benchmark dataset for the video grounding task. It is a large-scale action understanding dataset initially collected for the task of dense captioning, but it has been recently restructured for the task of moment localization with natural language <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>. The dataset contains 20k diverse videos with about 100k sentence queries, subdivided into four splits: train, val 1, val 2, and test. The test set is withheld for competition purposes leaving the rest publicly available. See Tab. 1 for more details about the publicly available splits. Following the previous setting in <ref type="bibr" target="#b25">[26]</ref>, in this paper, we use val 1 as the validation set and val 2 as the testing set.</p><p>TACoS <ref type="bibr" target="#b38">[39]</ref> consists of 127 videos selected from the MPII Cooking Composite Activities video corpus <ref type="bibr" target="#b40">[41]</ref>. It consists of 18818 moment-query pairs of different cooking activities in the kitchen. On average, every video in TACoS contains 148 queries, some of which are annotations of very short video segments.</p><p>DiDeMo <ref type="bibr" target="#b0">[1]</ref> consists of unedited video footage from Flickr with sentences aligned to unique moments in its 10642 videos. It is split into 33008, 4180, and 4021 videolanguage pairs for training, validation, and testing, respectively. Note that moment start and end points are aligned to five-second intervals and that the maximum annotated moment length is 30 seconds. Charades-STA <ref type="bibr" target="#b10">[11]</ref> consists of 16124 video-sentences pairs resulting in the smallest dataset for the task in terms of training and testing samples. Moreover, this dataset is also characterized by the smallest vocabulary size (and average sentence length). See Tab 1 for more details. In addition, the dataset only has two splits available and lacks an official validation split, making it prone to overfitting when hyperparameters are chosen with respect to training performance. Although this dataset has been widely adopted for the task, for the reasons listed above, we choose not to evaluate our method on it. More discussion can be found in the <ref type="table">Supple</ref> snippet feature only once while computing each moment's representation. Specifically, we implement and ablate three different schemes, namely: (i) learnable self-attention, (ii) crossattention, and (iii) learnable cross-attention. In (i), we obtain the unnormalized attention weights by applying a 1D convolutional layer that maps each clip feature to a single score. In (ii) and (iii), we compute the sentence representation by applying self-attention pooling on top of the last SyntacGCN layer X (bl) l , we refer to this quantity as X (att) l . Configuration (iii) is depicted in <ref type="figure">Fig. 4</ref>.</p><p>Cross-attention obtains the unnormalized weights by computing the inner product between the snippet and sentence features, while learnable cross-attention concatenates each snippet feature with the sentence feature and uses a 1D convolutional layer to obtain the weights. In all cases, the unnormalized weight vector has shape w 2 R nv?1 for each video. The vector is repeated m times to obtain the matrix W 2 R nv?m , and a fixed mask M 2 R nv?m is applied to it. Similar to Songyang et al. <ref type="bibr" target="#b70">[71]</ref>, we generate moment candidates and apply a sparse sampling strategy to discard redundant moments. Therefore, the mask is generated according to the sampled moments, highlighting for each of them, which are the clips that must be taken into account when computing the moment's pooled feature. The attention scores are then obtained by applying the softmax operation. Thanks to the masking operation, clips not related to the n-th moment will not be considered. Finally, the moments' features are obtained simply as a matrix multiplication: Y = X (GM ) SoftMax(W + M ). Ablation results are reported in the experiment section (Sec 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Moment Localization</head><p>The output of the previous module is then fed to a Multi-Layer Perceptron (MLP) network to compute the score p k for each moment candidate. This scores predicts the Intersection-over-Union (IoU) of each moment with the ground truth one. For training, we supervise this process using a cross-entropy loss, shown in Eq. 5. We assign the label t k = 1 if the IoU is greater than a threshold ? and t k = 0 otherwise.</p><formula xml:id="formula_89">L = 1 m m X k=1 t k log p k + (1 t k ) log(1 p k ),<label>(5)</label></formula><p>At inference time, moment candidates are ranked according to their predicted scores and non-maximum suppression is adopted to discard highly overlapping moments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>ActivityNet-Captions <ref type="bibr" target="#b20">[21]</ref> is a popular benchmark dataset for the video grounding task. It is a large-scale action understanding dataset initially collected for the task of dense captioning, but it has been recently restructured for the task of moment localization with natural language <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>. The dataset contains 20k diverse videos with about 100k sentence queries, subdivided into four splits: train, val 1, val 2, and test. The test set is withheld for competition purposes leaving the rest publicly available. See Tab. 1 for more details about the publicly available splits. Following the previous setting in <ref type="bibr" target="#b25">[26]</ref>, in this paper, we use val 1 as the validation set and val 2 as the testing set.</p><p>TACoS <ref type="bibr" target="#b38">[39]</ref> consists of 127 videos selected from the MPII Cooking Composite Activities video corpus <ref type="bibr" target="#b40">[41]</ref>. It consists of 18818 moment-query pairs of different cooking activities in the kitchen. On average, every video in TACoS contains 148 queries, some of which are annotations of very short video segments.</p><p>DiDeMo <ref type="bibr" target="#b0">[1]</ref> consists of unedited video footage from Flickr with sentences aligned to unique moments in its 10642 videos. It is split into 33008, 4180, and 4021 videolanguage pairs for training, validation, and testing, respectively. Note that moment start and end points are aligned to five-second intervals and that the maximum annotated moment length is 30 seconds. Charades-STA <ref type="bibr" target="#b10">[11]</ref> consists of 16124 video-sentences pairs resulting in the smallest dataset for the task in terms of training and testing samples. Moreover, this dataset is also characterized by the smallest vocabulary size (and average sentence length). See Tab 1 for more details. In addition, the dataset only has two splits available and lacks an official validation split, making it prone to overfitting when hyperparameters are chosen with respect to training performance. Although this dataset has been widely adopted for the task, for the reasons listed above, we choose not to evaluate our method on it. More discussion can be found in the <ref type="table">Supple</ref> snippet feature only once while computing each moment's representation. Specifically, we implement and ablate three different schemes, namely: (i) learnable self-attention, (ii) crossattention, and (iii) learnable cross-attention. In (i), we obtain the unnormalized attention weights by applying a 1D convolutional layer that maps each clip feature to a single score. In (ii) and (iii), we compute the sentence representation by applying self-attention pooling on top of the last SyntacGCN layer X (bl) l , we refer to this quantity as X (att) l . Configuration (iii) is depicted in <ref type="figure">Fig. 4</ref>.</p><p>Cross-attention obtains the unnormalized weights by computing the inner product between the snippet and sentence features, while learnable cross-attention concatenates each snippet feature with the sentence feature and uses a 1D convolutional layer to obtain the weights. In all cases, the unnormalized weight vector has shape w 2 R nv?1 for each video. The vector is repeated m times to obtain the matrix W 2 R nv?m , and a fixed mask M 2 R nv?m is applied to it. Similar to Songyang et al. <ref type="bibr" target="#b70">[71]</ref>, we generate moment candidates and apply a sparse sampling strategy to discard redundant moments. Therefore, the mask is generated according to the sampled moments, highlighting for each of them, which are the clips that must be taken into account when computing the moment's pooled feature. The attention scores are then obtained by applying the softmax operation. Thanks to the masking operation, clips not related to the n-th moment will not be considered. Finally, the moments' features are obtained simply as a matrix multiplication: Y = X (GM ) SoftMax(W + M ). Ablation results are reported in the experiment section (Sec 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Moment Localization</head><p>The output of the previous module is then fed to a Multi-Layer Perceptron (MLP) network to compute the score p k for each moment candidate. This scores predicts the Intersection-over-Union (IoU) of each moment with the ground truth one. For training, we supervise this process using a cross-entropy loss, shown in Eq. 5. We assign the label t k = 1 if the IoU is greater than a threshold ? and t k = 0 otherwise.</p><formula xml:id="formula_90">L = 1 m m X k=1 t k log p k + (1 t k ) log(1 p k ),<label>(5)</label></formula><p>At inference time, moment candidates are ranked according to their predicted scores and non-maximum suppression is adopted to discard highly overlapping moments. </p><formula xml:id="formula_91">w 2 R nv?1 (1) W 2 R nv?m (2) M 2 R nv?m (3) Y 2 R cv?m (4) X (0) l,nl<label>(5)</label></formula><formula xml:id="formula_92">w 1 (6) w 2<label>(7)</label></formula><p>w nv</p><p>Conv. 1D and repeat m times snippet feature only once while computing each moment's representation. Specifically, we implement and ablate three different schemes, namely: (i) learnable self-attention, (ii) crossattention, and (iii) learnable cross-attention. In (i), we obtain the unnormalized attention weights by applying a 1D convolutional layer that maps each clip feature to a single score. In (ii) and (iii), we compute the sentence representation by applying self-attention pooling on top of the last SyntacGCN layer X (bl) l , we refer to this quantity as X (att) l . Configuration (iii) is depicted in <ref type="figure">Fig. 4</ref>.</p><formula xml:id="formula_94">SoftMax Mask X (GM ) v,1 (9) X (GM ) v,2<label>(10)</label></formula><formula xml:id="formula_95">X (GM ) v,nv<label>(11)</label></formula><p>Cross-attention obtains the unnormalized weights by computing the inner product between the snippet and sentence features, while learnable cross-attention concatenates each snippet feature with the sentence feature and uses a 1D convolutional layer to obtain the weights. In all cases, the unnormalized weight vector has shape w 2 R nv?1 for each video. The vector is repeated m times to obtain the matrix W 2 R nv?m , and a fixed mask M 2 R nv?m is applied to it. Similar to Songyang et al. <ref type="bibr" target="#b70">[71]</ref>, we generate moment candidates and apply a sparse sampling strategy to discard redundant moments. Therefore, the mask is generated according to the sampled moments, highlighting for each of them, which are the clips that must be taken into account when computing the moment's pooled feature. The attention scores are then obtained by applying the softmax operation. Thanks to the masking operation, clips not related to the n-th moment will not be considered. Finally, the moments' features are obtained simply as a matrix multiplication: Y = X (GM ) SoftMax(W + M ). Ablation results are reported in the experiment section (Sec 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Moment Localization</head><p>The output of the previous module is then fed to a Multi-Layer Perceptron (MLP) network to compute the score p k for each moment candidate. This scores predicts the Intersection-over-Union (IoU) of each moment with the ground truth one. For training, we supervise this process using a cross-entropy loss, shown in Eq. 5. We assign the label t k = 1 if the IoU is greater than a threshold ? and t k = 0 otherwise.</p><formula xml:id="formula_96">L = 1 m m X k=1 t k log p k + (1 t k ) log(1 p k ),<label>(5)</label></formula><p>At inference time, moment candidates are ranked according to their predicted scores and non-maximum suppression is adopted to discard highly overlapping moments. . Configuration (iii) is depicted in <ref type="figure">Fig. 4</ref>.</p><p>Cross-attention obtains the unnormalized weights by computing the inner product between the snippet and sentence features, while learnable cross-attention concatenates each snippet feature with the sentence feature and uses a 1D convolutional layer to obtain the weights. In all cases, the unnormalized weight vector has shape w 2 R nv?1 for each video. The vector is repeated m times to obtain the matrix W 2 R nv?m , and a fixed mask M 2 R nv?m is applied to it. Similar to Songyang et al. <ref type="bibr" target="#b70">[71]</ref>, we generate moment candidates and apply a sparse sampling strategy to discard redundant moments. Therefore, the mask is generated according to the sampled moments, highlighting for each of them, which are the clips that must be taken into account when computing the moment's pooled feature. The attention scores are then obtained by applying the softmax operation. Thanks to the masking operation, clips not related to the n-th moment will not be considered. Finally, the moments' features are obtained simply as a matrix multiplication: Y = X (GM ) SoftMax(W + M ). Ablation results are reported in the experiment section (Sec 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Moment Localization</head><p>The output of the previous module is then fed to a Multi-Layer Perceptron (MLP) network to compute the score p k for each moment candidate. This scores predicts the Intersection-over-Union (IoU) of each moment with the ground truth one. For training, we supervise this process using a cross-entropy loss, shown in Eq. 5. We assign the label t k = 1 if the IoU is greater than a threshold ? and t k = 0 otherwise.</p><formula xml:id="formula_97">L = 1 m m X k=1 t k log p k + (1 t k ) log(1 p k ),<label>(5)</label></formula><p>At inference time, moment candidates are ranked according to their predicted scores and non-maximum suppression is adopted to discard highly overlapping moments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>ActivityNet-Captions <ref type="bibr" target="#b20">[21]</ref> is a popular benchmark dataset for the video grounding task. It is a large-scale action understanding dataset initially collected for the task of dense captioning, but it has been recently restructured for the task of moment localization with natural language <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>. The dataset contains 20k diverse videos with about 100k sentence queries, subdivided into four splits: train, val 1, val 2, and test. The test set is withheld for competition purposes leaving the rest publicly available. See Tab. 1 for more details about the publicly available splits. Following the previous setting in <ref type="bibr" target="#b25">[26]</ref>, in this paper, we use val 1 as the validation set and val 2 as the testing set.</p><p>TACoS <ref type="bibr" target="#b38">[39]</ref> consists of 127 videos selected from the MPII Cooking Composite Activities video corpus <ref type="bibr" target="#b40">[41]</ref>. It consists of 18818 moment-query pairs of different cooking activities in the kitchen. On average, every video in TACoS contains 148 queries, some of which are annotations of very short video segments.</p><p>DiDeMo <ref type="bibr" target="#b0">[1]</ref> consists of unedited video footage from Flickr with sentences aligned to unique moments in its 10642 videos. It is split into 33008, 4180, and 4021 videolanguage pairs for training, validation, and testing, respectively. Note that moment start and end points are aligned to five-second intervals and that the maximum annotated moment length is 30 seconds. Charades-STA <ref type="bibr" target="#b10">[11]</ref> consists of 16124 video-sentences pairs resulting in the smallest dataset for the task in terms of training and testing samples. Moreover, this dataset is also characterized by the smallest vocabulary size (and average sentence length). See Tab 1 for more details. In addition, the dataset only has two splits available and lacks an official validation split, making it prone to overfitting when hyperparameters are chosen with respect to training performance. Although this dataset has been widely adopted for the task, for the reasons listed above, we choose not to evaluate our method on it. More discussion can be found in the Supplementary Material. The lack of validation split makes it very difficult to asses if hyper-parameters have been chosen to   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation of Masked Attention Pooling</head><p>As presented in the main paper, three different implementations of attention for moment pooling operation have been tested. They differ for inputs and operations to achieve the attention scores. Learnable self-attention ( <ref type="figure" target="#fig_8">Fig. 7a)</ref>, only relies on the fused features of video and language modalities, which are the output of the graph matching layer, while the cross-attention and learnable cross-attention configurations ( <ref type="figure" target="#fig_8">Fig. 7b and 7c</ref>) also involve a global sentence representation X (att) l in the process. (See Sec. 3.5 of the paper for more details.) We compare the performances of the three different implementations in Tab 7.</p><p>Following the ablation settings in our main paper, we focus on R@1 IoU0.5 and R@5 IoU0.5 for TACoS dataset. We find that the cross-attention setup leads to the lowest performance. Conversely the learnable cross-attention configuration instead, obtains the best performance. Therefore we adopt this configuration as default in the main paper.  <ref type="table">Table 7</ref>. Ablation of masked attention pooling implementations. The experimental results show that the cross-attention setup leads to sub-optimal performance. Instead, the learnable crossattention configuration obtains the best performance.</p><p>Interestingly we notice that the learnable self-attention setup can achieve relatively high performance. This can be motivated by the intuition that our graph matching layer can effectively fuse the video and language modalities, and by relying on those enriched features only, can we obtain a good representation of the moment's feature. However, involving a global language representation for guiding the moment creation from the enriched snippets features has been shown to yield the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Charades-STA</head><p>Based on the results obtained from Activitynet-Caption, TACoS, and DiDeMo, our method can theoretically achieve state-of-the-art performance in the Charades-STA dataset. However, we choose not to evaluate VLG-Net on this dataset because of the following observations.</p><p>(1) This dataset is characterized by the smallest vocabulary size and shortest language annotation with respect all others datasets (see Tab. 8 and Tab. 9) For example, its vocabulary contains 43% less unique words with respect to TACoS <ref type="bibr" target="#b45">[46]</ref>, 83% with respect to DiDeMo <ref type="bibr" target="#b1">[2]</ref>, and 92% with respect to Activity-Captions <ref type="bibr" target="#b26">[27]</ref>. This fact can potentially hamper the development of successful methods and reduce the applicability to a real-world scenario where users might use a richer vocabulary when querying for moments. Given the great importance of the language for the task at hand, it's diversity in terms of unique tokens' number, and sentence lengths are important factors. This suggests that Charades-STA is less favourable for evaluating the video-language grounding task.</p><p>(2) Charades-STA has the smallest number of videoquery pairs (16124) with respect to all other datasets (See Tab 8). As deep learning methods benefit from a large   <ref type="table">Table 9</ref>. Language annotations statistics. We report average length (measured in number of tokens) and standard deviation for queries in each dataset. Statistics are computed considering every split for each dataset.</p><p>amount of annotated data, the reduced number of training/testing samples makes the dataset less suited for deeplearning approaches.</p><p>(3) Most importantly, Charades-STA lacks an official validation split. In machine learning applications, the validation set is mandatory for hyper-parameters search, while the test set is adopted for evaluating the generalization capabilities of a given method to previously unseen data. Given the absence of a validation set, nor a widely accepted procedure for selecting the best models during the development phase, some might use the test set for tuning the hyperparameters, therefore, harming the measurement of generalization performance. The goal of research is to develop tailor-made solutions for specific problems rather than finding the hyper-parameters that can fit the test set best. A conservative researcher could attempt at using the training set (or part of it) as a synthetic validation split. However, this could lead the model to overfit on the specific set of samples. Other methods could be potentially applied (e.g. cross-validation), yet no previous work mentioned the adoption of such techniques.</p><p>For all these reasons we can conclude that, despite the popularity of Charades-STA as benchmark for the language grounding in video task, we decide not to evaluate our method on it.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 Figure 1 .</head><label>31</label><figDesc>Temporal video grounding task and multi-modality interaction schemes. (a) A video grounding example showcasing the importance of fine-grained semantic understanding and proper context modeling. (b,c,d) Approaches for multi-modal interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>s</head><label></label><figDesc>are the adjacency matrices of Ordering Edges and Semantic Edges, respectively, and W (i) o and W (i)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>repeat m times 3 Figure 4 .</head><label>34</label><figDesc>Masked attention pooling. Sequence of operations for the learnable cross-attention configuration. Inputs are video nodes X (GM ) v from the graph matching layer and the query embedding X (att) l computed through self-attention pooling atop the graph matching output. The output Y represents all moment candidates. applying self-attention pooling atop the graph matching output X (GM ) l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Query: "She is holding an accordian as she talks." high low (b) (b) Query: "Three people are riding a very colorful camel on the beach."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3 (</head><label>3</label><figDesc>c) Learnable cross-attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Masked attention pooling. Inputs are video nodes X (GM ) v from the graph matching layer and the query embedding X (att) l computed through self-attention pooling atop the graph matching output. The output Y represents all moment candidates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Query: "The man wiped the frisbee and then threw it and the dog caught and brought it back to th</figDesc><table><row><cell>X (GM )</cell><cell></cell></row><row><cell></cell><cell>X (GM ) v</cell><cell>2 R cv?nv</cell></row><row><cell></cell><cell>Query -Moments Matching</cell></row><row><cell cols="2">Query -Moments Matching Query -Snippets Matching Query -Snippets Matching</cell></row><row><cell cols="2">Tokens -Snippets Matching (ours) Tokens -Snippets Matching (ours)</cell></row><row><cell>First throw and fetch</cell><cell>First throw and fetch</cell></row><row><cell>Frisbee polishing</cell><cell>Frisbee polishing</cell></row><row><cell cols="2">Second throw and fetch Second throw and fetch</cell></row><row><cell>Query grounding</cell><cell></cell></row><row><cell>39.4s</cell><cell>Query grounding</cell></row><row><cell>63.5s</cell><cell>39.4s</cell></row><row><cell></cell><cell>63.5s</cell></row><row><cell></cell><cell>2</cell></row><row><cell></cell><cell></cell><cell>2</cell></row></table><note>v 2 R cv?nv</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The man wiped the frisbee and then threw it again, and the dog caught and brought it back to the owner." The man wiped the frisbee and then threw it again, ( and the dog caught and brought it back to the owner." (</figDesc><table><row><cell>0.0s 11.3s</cell><cell>0.0s 11.3s</cell></row><row><cell>Wiping</cell><cell>Wiping</cell></row><row><cell>Video Snippet</cell><cell cols="2">0.0s 11.3s Video Snippet</cell></row><row><cell>Language Token</cell><cell cols="2">Wiping Language Token</cell></row><row><cell>Ordering Edge</cell><cell cols="2">Video Snippet Ordering Edge</cell></row><row><cell>Semantic Edge</cell><cell cols="2">Language Token Semantic Edge</cell></row><row><cell>Matching Edge</cell><cell cols="2">Ordering Edge Matching Edge</cell></row><row><cell>Neighbourhood</cell><cell cols="2">Semantic Edge Neighbourhood</cell></row><row><cell cols="3">Matching Edge Three people are riding a very colourful camel on the be Three people are riding a very colourful camel on the beach.</cell></row><row><cell></cell><cell></cell><cell>Neighbourhood</cell></row><row><cell></cell><cell>0.8s</cell><cell>Three people are riding a very colourful camel</cell></row><row><cell></cell><cell>21.0s</cell><cell>0.8s</cell></row><row><cell cols="3">21.0s Ground truth moment Predicted moment (a) (b) (c) (d) Tokens -Snippets (Ours) X (GM ) v 2 R cv?nv X (GM ) v Ground truth moment 2 R cv?nv Predicted moment Query: "Query -Moments Matching Query -Snippets Matching (a) (b) (c) (d) Query: "Query -Moments Matching Tokens -Snippets (Ours)</cell><cell>(</cell></row><row><cell cols="3">Tokens -Snippets Matching (ours) Query -Snippets Matching</cell></row><row><cell cols="3">Query -Snippets Matching Tokens -Snippets Matching (ours)</cell></row><row><cell>Tokens -Snippets Matching (ours)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>3</cell></row><row><cell></cell><cell></cell><cell>3</cell></row><row><cell></cell><cell></cell><cell>2</cell></row><row><cell></cell><cell></cell><cell>2</cell></row><row><cell>2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>2021 Submission #509. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.</head><label></label><figDesc></figDesc><table><row><cell>CVPR #509</cell><cell>CVPR</cell><cell>C #</cell></row><row><cell>540</cell><cell></cell><cell></cell></row><row><cell>541</cell><cell></cell><cell></cell></row><row><cell>542</cell><cell></cell><cell></cell></row><row><cell>543</cell><cell></cell><cell></cell></row><row><cell>544</cell><cell></cell><cell></cell></row><row><cell>545</cell><cell></cell><cell></cell></row><row><cell>546</cell><cell></cell><cell></cell></row><row><cell>547</cell><cell></cell><cell></cell></row><row><cell>548</cell><cell></cell><cell></cell></row><row><cell>549</cell><cell></cell><cell></cell></row><row><cell>550</cell><cell></cell><cell></cell></row><row><cell>551</cell><cell></cell><cell></cell></row><row><cell>552 553</cell><cell>1</cell><cell></cell></row><row><cell>554</cell><cell></cell><cell></cell></row><row><cell>555</cell><cell></cell><cell></cell></row><row><cell>556</cell><cell></cell><cell></cell></row><row><cell>557</cell><cell></cell><cell></cell></row><row><cell>558</cell><cell></cell><cell></cell></row><row><cell>559</cell><cell></cell><cell></cell></row><row><cell>560</cell><cell></cell><cell></cell></row><row><cell>561</cell><cell></cell><cell></cell></row><row><cell>562</cell><cell></cell><cell></cell></row><row><cell>563</cell><cell></cell><cell></cell></row><row><cell>564</cell><cell></cell><cell></cell></row><row><cell>565</cell><cell></cell><cell></cell></row><row><cell>566</cell><cell></cell><cell></cell></row><row><cell>567</cell><cell></cell><cell></cell></row><row><cell>568</cell><cell></cell><cell></cell></row><row><cell>569</cell><cell></cell><cell></cell></row><row><cell>570</cell><cell></cell><cell></cell></row><row><cell>571</cell><cell></cell><cell></cell></row><row><cell>572</cell><cell></cell><cell></cell></row><row><cell>573</cell><cell></cell><cell></cell></row><row><cell>574</cell><cell></cell><cell></cell></row><row><cell>575</cell><cell></cell><cell></cell></row><row><cell>576</cell><cell></cell><cell></cell></row><row><cell>577</cell><cell></cell><cell></cell></row><row><cell>578</cell><cell></cell><cell></cell></row><row><cell>579</cell><cell></cell><cell></cell></row><row><cell>580</cell><cell></cell><cell></cell></row><row><cell>581</cell><cell></cell><cell></cell></row><row><cell>582</cell><cell></cell><cell></cell></row><row><cell>583</cell><cell></cell><cell></cell></row><row><cell>584</cell><cell></cell><cell></cell></row><row><cell>585</cell><cell></cell><cell></cell></row><row><cell>586</cell><cell></cell><cell></cell></row><row><cell>587</cell><cell></cell><cell></cell></row><row><cell>588</cell><cell></cell><cell></cell></row><row><cell>589</cell><cell></cell><cell></cell></row><row><cell>590</cell><cell></cell><cell></cell></row><row><cell>591</cell><cell></cell><cell></cell></row><row><cell>592</cell><cell></cell><cell></cell></row><row><cell>593</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 1 .</head><label>1</label><figDesc>Video-language grounding dataset statistics definition of the metric used and details about the training strategy are presented in Sec. 4.2</figDesc><table><row><cell>Dataset</cell><cell>Num. Videos</cell><cell cols="3">Video-Sentence pairs train val test</cell><cell>Vocab. Size</cell></row><row><cell cols="6">Activitynet-Captions [21] 14926 37421 17505 17031 15406 TACoS [39] 127 10146 4589 4083 2255 DiDeMo [1] 10642 33005 4180 4021 7523</cell></row><row><cell>Charades-STA [11]</cell><cell>6670</cell><cell>12404</cell><cell>0</cell><cell>3720</cell><cell>1289</cell></row></table><note>The temporal boundaries (t s,k , t e,k ) associated with the top-? moments are used to calculate the Intersection- over-Union (IoU) with the ground-truth video moments (? s , ? e ) to determine the alignment performance. A formal</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 1 .</head><label>1</label><figDesc>Video-language grounding dataset statistics definition of the metric used and details about the training strategy are presented in Sec. 4.2</figDesc><table><row><cell>Dataset</cell><cell>Num. Videos</cell><cell cols="3">Video-Sentence pairs train val test</cell><cell>Vocab. Size</cell></row><row><cell cols="6">Activitynet-Captions [21] 14926 37421 17505 17031 15406 TACoS [39] 127 10146 4589 4083 2255 DiDeMo [1] 10642 33005 4180 4021 7523</cell></row><row><cell>Charades-STA [11]</cell><cell>6670</cell><cell>12404</cell><cell>0</cell><cell>3720</cell><cell>1289</cell></row></table><note>The temporal boundaries (t s,k , t e,k ) associated with the top-? moments are used to calculate the Intersection- over-Union (IoU) with the ground-truth video moments (? s , ? e ) to determine the alignment performance. A formal</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 1 .</head><label>1</label><figDesc>The remaining top-? moments are involved in the recall computation. The temporal boundaries (t s,k , t e,k ) associated with the top-? moments are used to calculate the Intersectionover-Union (IoU) with the ground-truth video moments (? s , ? e ) to determine the alignment performance. A formal Video-language grounding dataset statistics definition of the metric used and details about the training strategy are presented in Sec. 4.2 Captions [21] is a popular benchmark dataset for the video grounding task. It is a large-scale action understanding dataset initially collected for the task of dense captioning, but it has been recently restructured for the task of moment localization with natural language<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>. The dataset contains 20k diverse videos with about 100k sentence queries, subdivided into four splits: train, val 1, val 2, and test. The test set is withheld for competition purposes leaving the rest publicly available. See Tab. 1 for more details about the publicly available splits. Following the previous setting in</figDesc><table><row><cell>Dataset</cell><cell>Num. Videos</cell><cell cols="3">Video-Sentence pairs train val test</cell><cell>Vocab. Size</cell></row><row><cell cols="6">Activitynet-Captions [21] 14926 37421 17505 17031 15406 TACoS [39] 127 10146 4589 4083 2255 DiDeMo [1] 10642 33005 4180 4021 7523</cell></row><row><cell>Charades-STA [11]</cell><cell>6670</cell><cell>12404</cell><cell>0</cell><cell>3720</cell><cell>1289</cell></row><row><cell>4. Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1. Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ActivityNet-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>c?m eighbourhood hree people are riding a very colourful camel on the beach.</figDesc><table><row><cell></cell><cell>2 R c?m Ground truth moment Predicted moment (a) (b) (c) (d) Tokens -Snippets (Ours)</cell><cell>(GM ) 2 R c?nv</cell><cell>okens -Snippets (Ours)</cell><cell>) (b) (c) (d)</cell><cell>redicted moment</cell><cell>round truth moment</cell><cell>1.0s</cell><cell>.8s</cell></row><row><cell>3</cell><cell>Inner product and repeat m times</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 4 .Table 2 .</head><label>42</label><figDesc>IoU0.7 IoU0.5 IoU0.7 MCN [2] 21.36 6.43 53.23 29.70 CTRL [16] 29.01 10.34 59.17 37.54 TGN [7] 27.93 ? 44.20 ? ACRN [32] 31.67 11.25 60.34 38.57 CMIN [31] 44.62 24.48 69.66 52.96 TAN (P) [81] 44.51 26.54 77.13 61.96 2D-TAN (C) [81] 44.05 27.38 76.65 62.26 DRN [77] 45.45 24.36 77.97 50.30 CSMGAN [33] 49.11 29.15 77.43 59.63 VLG-Net 46.32 29.82 77.15 63.33 State-of-the-art comparison on ActivityNet Captions. We report the results at different Recall@? and different IoU thresholds. VLG-Net reaches the highest scores for IoU0.7 for both R@1 and R@5.</figDesc><table><row><cell>R@1 IoU0.5 ABLR [73] 36.79 TripNet [43] 32.19 13.93 ? PMI [53] 38.28 17.83 2D-</cell><cell>? ? ?</cell><cell>R@5</cell><cell>? ? ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>IoU0.3 IoU0.5 IoU0.1 IoU0.3 IoU0.5 24.32 18.32 13.30 48.73 36.69 25.42 MCF [68] 25.84 18.64 12.53 52.96 37.13 24.73 TGN [7] 41.87 21.77 18.90 53.40 39.06 31.02 ACRN [34] 24.22 19.52 14.62 47.42 34.97 24.88 ROLE [35] 20.37 15.38 9.94 45.45 31.17 20.13 VAL [56] 25.74 19.76 14.74 51.87 38.55 26.52 ACL-K [18] 31.64 24.17 20.01 57.85 42.15 30.66 CMIN [31] 36.68 27.33 19.57 64.93 43.35 28.53 SM-RL [64] 26.51 20.25 15.95 50.01 38.47 27.84</figDesc><table><row><cell cols="2">IoU0.1 MCN [2] 14.42 CTRL [16]</cell><cell>R@1 ?</cell><cell>5.58 37.35</cell><cell>R@5 ?</cell><cell>10.33</cell></row><row><cell cols="6">23.13 17.07 11.92 46.52 32.90 20.86 31.15 ? 18.24 53.51 28.11 ? ? 23.95 19.17 ? ? ? 2D-TAN (P) [81] 47.59 37.29 25.32 70.31 57.81 45.04 SLTA [24] SAP [8] TripNet [43] 2D-TAN (C) [81] 46.44 35.22 25.19 74.43 56.94 44.21 DRN [77] ? ? 23.17 ? 33.36 ? CSMGAN [33] 42.74 33.90 27.09 68.97 53.98 41.22</cell></row><row><cell>VLG-Net</cell><cell cols="5">57.21 45.46 34.19 81.80 70.38 56.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 3 .Table 4</head><label>34</label><figDesc>He takes off the glasses and applies contact lenses. Comparison among different methods for multi-modal relationships modellingThe men then begin arm wrestling one another while other watch. Comparison among different methods for multi-modal relationships modellingThe men then begin arm wrestling one another while other watch. Comparison among different methods for multi-modal relationships modellingThe men then begin arm wrestling one another while other watch. Comparison among different methods for multi-modal relationships modellingThe men then begin arm wrestling one another while other watch. Comparison among different methods for multi-modal relationships modellingThe men then begin arm wrestling one another while other watch.20.4s 22.0s57.5s 58.7sHe takes off the glasses and applies contact lenses. Comparison among different methods for multi-modal relationships modellingThe men then begin arm wrestling one another while other watch.20.4s 22.0s57.5s 58.7sHe takes off the glasses and applies contact lenses. Comparison among different methods for multi-modal relationships modellingThe men then begin arm wrestling one another while other watch.He takes off the glasses and applies contact lenses. Comparison among different methods for multi-modal relationships modellingThe men then begin arm wrestling one another while other watch.20.4s 22.0s57.5s 58.7sHe takes off the glasses and applies contact lenses. Comparison among different methods for multi-modal relationships modellingThe men then begin arm wrestling one another while other watch. Comparison among different methods for multi-modal relationships modellingThe men then begin arm wrestling one another while other watch.20.4s 22.0s57.5s 58.7sHe takes off the glasses and applies contact lenses. Comparison among different methods for multi-modal relationships modellingThe men then begin arm wrestling one another while other watch.20.4s 22.0s57.5s 58.7sHe takes off the glasses and applies contact lenses. Comparison among different methods for multi-modal relationships modellingThe men then begin arm wrestling one another while other watch.20.4s 22.0s57.5s 58.7sHe takes off the glasses and applies contact lenses.</figDesc><table><row><cell>R@1 IoU0.5 IoU0.7 IoU1.0 IoU0.5 IoU0.7 IoU1.0 R@5</cell></row><row><cell>MCN [2] TMN [32] TGN [7] ACRN [34] ROLE [35] CSMGAN [33] 29.44 19.16 ? ? ? ? ? ? 27.44 16.65 29.40 15.68 VLG-Net 33.35 25.57 25.57 88.86 71.72 71.65 13.10 ? 44.82 ? 18.71 ? 72.97 ? 24.28 ? 71.43 ? ? 69.43 29.45 ? ? 70.72 33.08 ? ? 70.77 41.61 ?</cell></row><row><cell>GC-NeXt ones. In this variant, only temporal dependencies are modeled. VLG-Net S ("Semantic Context Only") does not model temporal dependencies but just the semantic ones. Both models' performance surpasses 30% for R@1 IoU0.5, which indicates that the introduction of GCNeXt and Syn-tacGCN layers can boost the performance. Our final ar-chitecture takes advantage of both modules, achieving the best results. These ablations demonstrate that temporal and semantic context are complementary and showcase the ben-efits of the proposed context modeling strategy.</cell></row></table><note>State-of-the-art comparison on TACoS. Our model out- performs all previous methods achieving significantly higher per- formance with great margins on all metrics.. State-of-the-art comparison on DiDeMo. Our proposed model outperforms the top ranked method ROLE and ACRN with respect to IoU0.5 and 0.7 for R@1 and R@5 with clear margins. It also reaches the highest performance in regards to R@1 IoU1.0.allows them to improve their performance with respect to VLG-Net NC up to 2.32%. Nonetheless, they fall short of VLG-Net by 6.5 ? 7.4%. Note that VLG-Net GM and VLG- Net GI have a larger model capacity, 0.07M and 0.41M, re- spectively, compared to VLG-Net. VLG-Net T ("Temporal Context Only") is as VLG-Net, where we remove each Se- mantic Edge and replace the SyntacGCN layers with</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>Three people are riding a very colourful camel on the beach.</figDesc><table><row><cell>Ordering Edge</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Semantic Edge</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Matching Edge</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Neighbourhood</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Three people are riding a very colourful camel on the beach.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.8s</cell><cell></cell><cell></cell><cell></cell><cell>0.8s</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>21.0s</cell><cell></cell><cell></cell><cell></cell><cell>21.0s</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ground truth moment</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ground truth moment</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Predicted moment</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Predicted moment</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a) (b) (c) (d)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a) (b) (c) (d)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tokens -Snippets (Ours)</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Tokens -Snippets (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">Figure 5. Qualitative Results. Examples of grounding results, we compare ground truth annotations (box) and predicted temporal end-points (arrow). See Section 4.5 for more details.</cell></row><row><cell></cell><cell>VLG-Net</cell><cell cols="9">Context Ablation VLG-Net NC VLG-Net GM VLG-Net LM VLG-Net GI VLG-Net T VLG-Net S VLG-Net HM VLG-Net HS VLG-Net CS Multi-modal Fusion Ablation</cell></row><row><cell>R@1 IoU0.5</cell><cell>34.19</cell><cell>25.37</cell><cell>27.12</cell><cell>26.82</cell><cell>27.69</cell><cell>30.34</cell><cell>30.94</cell><cell>27.27</cell><cell>27.54</cell><cell>28.39</cell></row><row><cell>R@5 IoU0.5</cell><cell>56.56</cell><cell>48.24</cell><cell>48.56</cell><cell>47.11</cell><cell>49.39</cell><cell>51.84</cell><cell>53.14</cell><cell>49.99</cell><cell>48.54</cell><cell>51.31</cell></row><row><cell cols="2"># of Parameters (M) 13.29</cell><cell>13.08</cell><cell>13.35</cell><cell>13.08</cell><cell>13.70</cell><cell>13.22</cell><cell>13.22</cell><cell>13.28</cell><cell>13.28</cell><cell>13.28</cell></row><row><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head></head><label></label><figDesc>The remaining top-? moments are involved in the recall computation. The temporal boundaries (t s,k , t e,k ) associated with the top-? moments are used to calculate the Intersectionover-Union (IoU) with the ground-truth video moments (? s , ? e ) to determine the alignment performance. A formal The test set is withheld for competition purposes leaving the rest publicly available. See Tab. 1 for more details about the publicly available splits. Following the previous setting in<ref type="bibr" target="#b25">[26]</ref>, in this paper, we use val 1 as the validation set and val 2 as the testing set.TACoS<ref type="bibr" target="#b38">[39]</ref> consists of 127 videos selected from the MPII Cooking Composite Activities video corpus<ref type="bibr" target="#b40">[41]</ref>. It consists of 18818 moment-query pairs of different cooking activities in the kitchen. On average, every video in TACoS contains 148 queries, some of which are annotations of very short video segments.DiDeMo<ref type="bibr" target="#b0">[1]</ref> consists of unedited video footage from Flickr with sentences aligned to unique moments in its 10642 videos. It is split into 33008, 4180, and 4021 videolanguage pairs for training, validation, and testing, respectively. Note that moment start and end points are aligned to five-second intervals and that the maximum annotated moment length is 30 seconds. Charades-STA<ref type="bibr" target="#b10">[11]</ref> consists of 16124 video-sentences pairs resulting in the smallest dataset for the task in terms of training and testing samples. Moreover, this dataset is also characterized by the smallest vocabulary size (and average sentence length). See Tab 1 for more details. In addition, the dataset only has two splits available and lacks an official validation split, making it prone to overfitting when hyperparameters are chosen with respect to training performance. Although this dataset has been widely adopted for the task, for the reasons listed above, we choose not to evaluate our method on it. More discussion can be found in the Supplementary Material. The lack of validation split makes it very difficult to asses if hyper-parameters have been chosen to</figDesc><table><row><cell></cell><cell>Semantic Edge</cell><cell>Language Token Language Token</cell></row><row><cell></cell><cell>Matching Edge</cell><cell>Ordering Edge Ordering Edge</cell></row><row><cell>1 Introduction</cell><cell cols="2">Neighbourhood Three people are riding a very colourful camel on the beach. Semantic Edge Matching Edge Semantic Edge Matching Edge</cell></row><row><cell></cell><cell>0.8s</cell><cell>Neighbourhood Neighbourhood</cell></row><row><cell></cell><cell>21.0s</cell><cell>Three people are riding a very colou Three people are riding a very colou</cell></row><row><cell></cell><cell cols="2">Dataset Activitynet-Captions [21] 14926 37421 17505 17031 15406 Num. Video-Sentence pairs Vocab. Videos train val test Size TACoS [39] 127 10146 4589 4083 2255 DiDeMo [1] 10642 33005 4180 4021 7523 Charades-STA [11] 6670 12404 0 3720 1289 Table 1. Video-language grounding dataset statistics definition of the metric used and details about the training strategy are presented in Sec. 4.2 4. Experiments 4.1. Datasets ActivityNet-Captions [21] is a popular benchmark dataset for the video grounding task. It is a large-scale action un-3 and test. 6 tence queries, subdivided into four splits: train, val 1, val 2, dataset contains 20k diverse videos with about 100k sen-of moment localization with natural language [3, 26]. The captioning, but it has been recently restructured for the task derstanding dataset initially collected for the task of dense Ground truth moment Predicted moment (a) (b) (c) (d) Tokens -Snippets (Ours) X (GM ) 2 R c?nv Y 2 R c?m 0.8s 21.0s Ground truth moment Predicted moment (a) (b) (c) (d) Tokens -Snippets (Ours) X (GM ) 2 R c?nv Y 2 R c?m 0.8s 21.0s Ground truth moment Predicted moment (a) (b) (c) (d) Tokens -Snippets (Ours) X (GM ) 2 R c?nv Y 2 R c?m Neighbourhood 0.8s 21.0s Ground truth moment Predicted moment (a) (b) (c) (d) Tokens -Snippets (Ours) X (GM ) 2 R c?nv Y 2 R c?m Three people are riding a very colourful camel on the beach. Inner product and repeat m times Conv. 1D and repeat m times</cell><cell>Matching Edge</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>Table 1 .</head><label>1</label><figDesc>Video-language grounding dataset statistics definition of the metric used and details about the training strategy are presented in Sec. 4.2</figDesc><table><row><cell>Dataset</cell><cell>Num. Videos</cell><cell cols="3">Video-Sentence pairs train val test</cell><cell>Vocab. Size</cell></row><row><cell cols="6">Activitynet-Captions [21] 14926 37421 17505 17031 15406 TACoS [39] 127 10146 4589 4083 2255 DiDeMo [1] 10642 33005 4180 4021 7523</cell></row><row><cell>Charades-STA [11]</cell><cell>6670</cell><cell>12404</cell><cell>0</cell><cell>3720</cell><cell>1289</cell></row></table><note>The re- maining top-? moments are involved in the recall compu- tation. The temporal boundaries (t s,k , t e,k ) associated with the top-? moments are used to calculate the Intersection- over-Union (IoU) with the ground-truth video moments (? s , ? e ) to determine the alignment performance. A formal</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37"><head>Table 8 .</head><label>8</label><figDesc>Datasets statistics. Same as Table 1 in main paper, reported in Supplementary Material for completeness.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Sentence's lengths Avg. Std.</cell></row><row><cell cols="2">Activitynet-Captions [27] 14.4 9.4 TACoS [46] DiDeMo [2] 8.0</cell><cell>6.5 5.4 3.4</cell></row><row><cell>Charades-STA [16]</cell><cell>7.2</cell><cell>1.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">(a) Learnable self-attention. equations mattia.soldan.ms November 2020</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(GM ) v,1 (9) X (GM ) v,2 (10) X (GM ) v,nv(11)1 mattia.soldan.ms November 2020</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material Formulation of Video-Language Graph Matching</head><p>In this section, we provide a detailed overview and formulation of the video-language graph matching. This inputs to this layer are the enriched video representation X (bv) v and query representation X 1 Introduction 1 Introduction</p><p>1 l,n l w 1 <ref type="bibr" target="#b5">(6)</ref> w 2 <ref type="bibr" target="#b6">(7)</ref> w n v</p><p>Conv. 1D and repeat m times SoftMax 1 equations mattia.soldan.ms November 2020 1 Introduction 1 Introduction</p><p>Conv. 1D and repeat m times 1 Introduction </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>ActivityNet-Captions <ref type="bibr" target="#b20">[21]</ref> is a popular benchmark dataset for the video grounding task. It is a large-scale action understanding dataset initially collected for the task of dense captioning, but it has been recently restructured for the task of moment localization with natural language <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>. The dataset contains 20k diverse videos with about 100k sentence queries, subdivided into four splits: train, val 1, val 2, and test. The test set is withheld for competition purposes leaving the rest publicly available. See Tab. 1 for more details about the publicly available splits. Following the previous setting in <ref type="bibr" target="#b25">[26]</ref>, in this paper, we use val 1 as the validation set and val 2 as the testing set.</p><p>TACoS <ref type="bibr" target="#b38">[39]</ref> consists of 127 videos selected from the MPII Cooking Composite Activities video corpus <ref type="bibr" target="#b40">[41]</ref>. It consists of 18818 moment-query pairs of different cooking activities in the kitchen. On average, every video in TACoS contains 148 queries, some of which are annotations of very short video segments.</p><p>DiDeMo <ref type="bibr" target="#b0">[1]</ref> consists of unedited video footage from Flickr with sentences aligned to unique moments in its 10642 videos. It is split into 33008, 4180, and 4021 videolanguage pairs for training, validation, and testing, respectively. Note that moment start and end points are aligned to five-second intervals and that the maximum annotated moment length is 30 seconds. Charades-STA <ref type="bibr" target="#b10">[11]</ref> consists of 16124 video-sentences pairs resulting in the smallest dataset for the task in terms of training and testing samples. Moreover, this dataset is also characterized by the smallest vocabulary size (and average sentence length). See Tab 1 for more details. In addition, the dataset only has two splits available and lacks an official validation split, making it prone to overfitting when hyperparameters are chosen with respect to training performance. Although this dataset has been widely adopted for the task, snippet feature only once while computing each moment's representation. Specifically, we implement and ablate three different schemes, namely: (i) learnable self-attention, (ii) crossattention, and (iii) learnable cross-attention. In (i), we obtain the unnormalized attention weights by applying a 1D convolutional layer that maps each clip feature to a single score. In (ii) and (iii), we compute the sentence representation by applying self-attention pooling on top of the last SyntacGCN layer X (bl) l , we refer to this quantity as X (att) l . Configuration (iii) is depicted in <ref type="figure">Fig. 4</ref>.</p><p>Cross-attention obtains the unnormalized weights by computing the inner product between the snippet and sentence features, while learnable cross-attention concatenates each snippet feature with the sentence feature and uses a 1D convolutional layer to obtain the weights. In all cases, the unnormalized weight vector has shape w 2 R nv?1 for each video. The vector is repeated m times to obtain the matrix W 2 R nv?m , and a fixed mask M 2 R nv?m is applied to it. Similar to Songyang et al. <ref type="bibr" target="#b70">[71]</ref>, we generate moment candidates and apply a sparse sampling strategy to discard redundant moments. Therefore, the mask is generated according to the sampled moments, highlighting for each of them, which are the clips that must be taken into account when computing the moment's pooled feature. The attention scores are then obtained by applying the softmax operation. Thanks to the masking operation, clips not related to the n-th moment will not be considered. Finally, the moments' features are obtained simply as a matrix multiplication: Y = X (GM ) SoftMax(W + M ). Ablation results are reported in the experiment section (Sec 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Moment Localization</head><p>The output of the previous module is then fed to a Multi-Layer Perceptron (MLP) network to compute the score p k for each moment candidate. This scores predicts the Intersection-over-Union (IoU) of each moment with the ground truth one. For training, we supervise this process using a cross-entropy loss, shown in Eq. 5. We assign the label t k = 1 if the IoU is greater than a threshold ? and t k = 0 otherwise.</p><p>At inference time, moment candidates are ranked according to their predicted scores and non-maximum suppression is adopted to discard highly overlapping moments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>ActivityNet-Captions <ref type="bibr" target="#b20">[21]</ref> is a popular benchmark dataset for the video grounding task. It is a large-scale action understanding dataset initially collected for the task of dense captioning, but it has been recently restructured for the task of moment localization with natural language <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>. The dataset contains 20k diverse videos with about 100k sentence queries, subdivided into four splits: train, val 1, val 2, and test. The test set is withheld for competition purposes leaving the rest publicly available. See Tab. 1 for more details about the publicly available splits. Following the previous setting in <ref type="bibr" target="#b25">[26]</ref>, in this paper, we use val 1 as the validation set and val 2 as the testing set.</p><p>TACoS <ref type="bibr" target="#b38">[39]</ref> consists of 127 videos selected from the MPII Cooking Composite Activities video corpus <ref type="bibr" target="#b40">[41]</ref>. snippet feature only once while computing each moment's representation. Specifically, we implement and ablate three different schemes, namely: (i) learnable self-attention, (ii) crossattention, and (iii) learnable cross-attention. In (i), we obtain the unnormalized attention weights by applying a 1D convolutional layer that maps each clip feature to a single score. In (ii) and (iii), we compute the sentence representation by applying self-attention pooling on top of the last SyntacGCN layer X (bl) l , we refer to this quantity as X (att) l</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diagnosing error in temporal action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Localizing Moments in Video With Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Diego Marcheggiani, and Khalil Sima&apos;an. Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04675</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Graphto-sequence learning using gated graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09835</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tib?rio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1048" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Temporally grounding natural sentence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 conference on empirical methods in natural language processing</title>
		<meeting>the 2018 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Temporally Grounding Natural Sentence in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic Proposal for Activity Localization in Videos via Sentence Query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph-based global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Shuicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical Visual-Textual Graph for Temporal Activity Localization via Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shaoxiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Yu-Gang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal Context Network for Activity Localization in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan Qiu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dual encoding for zero-example video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TALL: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TALL: Temporal Activity Localization via Language Query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Jiyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhenheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)<address><addrLine>Nevatia, Ram</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MAC: Mining Activity Concepts for Language-Based Temporal Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MAC: Mining Activity Concepts for Language-based Temporal Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhou</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A dataset for telling the stories of social media videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spandana</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ExCL: Extractive Clip Localization Using Natural Language Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuva</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Location-Aware Graph Convolutional Networks for Video Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Aligned Dual Channel Graph Convolutional Network for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingbao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jielong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmeng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Ho-Fung Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multilevel Language and Vision Integration for Text-to-Clip Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Xu Huijuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sigal</forename><surname>Kun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sclaroff</forename><surname>Leonid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saenko</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross-Modal Video Moment Retrieval with Spatial and Language-Temporal Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 on International Conference on Multimedia Retrieval (ICMR)</title>
		<meeting>the 2019 on International Conference on Multimedia Retrieval (ICMR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual-semantic graph matching for visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingtao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4041" to="4050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dense-Captioning Events in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">TVQA: Localized, Compositional Video Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Graph matching networks for learning the similarity of graph structured objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenjie</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dullien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12787</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BSN: Boundary Sensitive Network for Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Moment Retrieval via Cross-Modal Interaction Networks With Query Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal Modular Networks for Retrieving Complex Compositional Activities in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Jointly cross-and self-modal graph attention network for query-based moment localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daizong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichuan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4070" to="4078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attentive Moment Retrieval in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cross-Modal Moment Localization in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Multimedia</title>
		<meeting>the 26th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning Video Representations from Correspondence Proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning cross-modal context graph for visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11645" to="11652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">SGDR: Stochastic Gradient Descent with warm Restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chujie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chilie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations</title>
		<meeting>52nd annual meeting of the association for computational linguistics: system demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04826</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tripping through time: Efficient Localization of Activities in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meera</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Local-Global Video-Text Interactions for Temporal Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwan</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Grounding Action Descriptions in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominikus</forename><surname>Wetzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Proposal-free Temporal Moment Localization of a Natural-Language Query in Video using Guided Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marrese-Taylor</forename><surname>Rodriguez Cristian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saleh Fatemeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gould</forename><surname>Hongdong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sikandar Amin, Manfred Pinkal, and Bernt Schiele. Script data for attribute-based recognition of composite activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
		<respStmt>
			<orgName>California Univ San Diego La Jolla Inst for Cognitive Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hochreiter</forename><surname>Sepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schmidhuber</forename><surname>J?rgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Find and Focus: Retrieve and Localize Video Events with Natural Language Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">VAL: Visual-Attention Action Localizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Wen-Huang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Multimedia Information Processing (PCM)</title>
		<meeting>the Advances in Multimedia Information Processing (PCM)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Richang Hong</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">VAL: Visual-Attention Action Localizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Multimedia Information Processing (PCM)</title>
		<meeting>the Advances in Multimedia Information Processing (PCM)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Temporally Grounding Language Queries in Videos by Contextual Boundary-aware Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning combinatorial embedding networks for deep graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3056" to="3065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Cross-modal scene graph matching for relationship-aware image-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1508" to="1517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Languagedriven Temporal Activity Localization: A Semantic Matching Reinforcement Learning Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Dynamic Graph CNN for Learning on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multi-modal Circulant Fusion for Video-to-Language and Backward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Multi-modal Circulant Fusion for Video-to-Language and Backward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Neighborhood matching network for entity alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05607</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Cross-lingual knowledge graph alignment via graph matching neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11605</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Sub-Graph Localization for Temporal Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. G-Tad</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">To Find Where You Talk: Temporal Sentence Localization in Video with Attention Based Location Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yitian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Wenwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Combining Local Convolution with Global Self-Attention for Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qanet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03252</idno>
		<title level="m">Graph Convolutional Networks for Temporal Action Localization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Dense Regression Network for Video Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Graph Convolution over Pruned Dependency Trees Improves Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Cross-modal interaction networks for query-based moment retrieval in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Xiyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yuan-Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Man</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learning 2D Temporal Adjacent Networks for Moment Localization with Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Songyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Houwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Jianlong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Vision-Language Navigation With Self-Supervised Auxiliary Reasoning Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengda</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
