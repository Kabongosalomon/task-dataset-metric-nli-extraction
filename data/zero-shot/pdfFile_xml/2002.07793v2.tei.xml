<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MAST: A Memory-Augmented Self-Supervised Tracker</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lai</surname></persName>
							<email>zlai@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MAST: A Memory-Augmented Self-Supervised Tracker</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent interest in self-supervised dense tracking has yielded rapid progress, but performance still remains far from supervised methods. We propose a dense tracking model trained on videos without any annotations that surpasses previous self-supervised methods on existing benchmarks by a significant margin (+15%), and achieves performance comparable to supervised methods. In this paper, we first reassess the traditional choices used for self-supervised training and reconstruction loss by conducting thorough experiments that finally elucidate the optimal choices. Second, we further improve on existing methods by augmenting our architecture with a crucial memory component. Third, we benchmark on large-scale semi-supervised video object segmentation (aka. dense tracking), and propose a new metric: generalizability. Our first two contributions yield a self-supervised network that for the first time is competitive with supervised methods on standard evaluation metrics of dense tracking. When measuring generalizability, we show self-supervised approaches are actually superior to the majority of supervised methods. We believe this new generalizability metric can better capture the real-world use-cases for dense tracking, and will spur new interest in this research direction. Our code will be released at https://github.com/zlai0/MAST.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Although the working mechanisms of the human visual system remain somewhat obscure at the level of neurophysiology, it is a consensus that tracking objects is a fundamental ability that a baby starts developing at two to three months of age <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b57">58]</ref>. Similarly, in computer vision systems, tracking plays key roles in many applications ranging from autonomous driving to video surveillance.</p><p>Given arbitrary objects defined in the first frame, a tracking algorithm aims to relocate the same object throughout the entire video sequence. In the literature, tracking can be cast into two categories: the first is Visual Object Tracking (VOT) <ref type="bibr" target="#b34">[35]</ref>, where the goal is to relocalize objects  <ref type="figure">Figure 1</ref>: Comparison with other recent works on the DAVIS-2017 benchmarks, i.e. dense tracking or semi-supervised video segmentation given the first frame annotation. The proposed approach significantly outperforms the other self-supervised approaches, and even comparable to approaches trained with heavy supervision on ImageNet, COCO, Pascal, DAVIS, Youtube-VOS. In the x-axis, we only count pixel-wise segmentation. Notation: CINM <ref type="bibr" target="#b2">[3]</ref>, OSVOS <ref type="bibr" target="#b5">[6]</ref>, AGAME <ref type="bibr" target="#b27">[28]</ref>, VOSwL <ref type="bibr" target="#b30">[31]</ref>, FAVOS <ref type="bibr" target="#b7">[8]</ref>, mgPFF <ref type="bibr" target="#b32">[33]</ref>, CorrFlow <ref type="bibr" target="#b36">[37]</ref>, DyeNet <ref type="bibr" target="#b38">[39]</ref>, PReMVOS <ref type="bibr" target="#b40">[41]</ref>. OSVOS-S <ref type="bibr" target="#b41">[42]</ref>, RGMP <ref type="bibr" target="#b43">[44]</ref>, RVOS <ref type="bibr" target="#b53">[54]</ref>, FEELVOS <ref type="bibr" target="#b55">[56]</ref>, OnAVOS <ref type="bibr" target="#b56">[57]</ref>, Video Colorization <ref type="bibr" target="#b58">[59]</ref>, SiamMask <ref type="bibr" target="#b61">[61]</ref>, CycleTime <ref type="bibr" target="#b64">[64]</ref>, RANet <ref type="bibr" target="#b65">[65]</ref>, OSMN <ref type="bibr" target="#b73">[73]</ref>, with bounding boxes throughout the video; the other aims for more fine-grained tracking, i.e. relocalize the objects with pixel-level segmentation masks, also known as Semisupervised Video Object Segmentation (Semi-VOS) <ref type="bibr" target="#b47">[48]</ref>.</p><p>In this paper, we focus on the latter case, and will refer to it interchangeably with dense tracking from here on. In order to train such dense tracking systems, most recent approaches rely on supervised training with extensive human annotations (see <ref type="figure">Figure 1</ref>). For instance, an Ima-geNet <ref type="bibr" target="#b9">[10]</ref> pre-trained ResNet <ref type="bibr" target="#b17">[18]</ref> is typically adopted as a feature encoder, and further fine-tuned on images or video frames annotated with fine-grained, pixelwise segmentation masks, e.g. COCO <ref type="bibr" target="#b39">[40]</ref>, Pascal <ref type="bibr" target="#b12">[13]</ref>, DAVIS <ref type="bibr" target="#b47">[48]</ref> and YouTube-VOS <ref type="bibr" target="#b71">[71]</ref>. Despite their success, this top-down training scheme seems counter-intuitive when considering the development of the human visual system, as infants can track and follow slow-moving objects before they are able to map objects to semantic meanings. With this evidence, it For all examples, the mask of the 0th frame is given, and the task is to track the objects along with the video. Our self-supervised tracking model is able to deal with challenging scenarios, such as large camera motion, occlusion and disocclusion, large deformation and scale variation.</p><p>is unlikely the case that humans develop their tracking ability in a top-down manner (supervised by semantics), at least not at the early-stage development of the visual system. In contrast to the aforementioned approaches based on heavy supervision, self-supervised methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b64">64]</ref> have recently been introduced, leading to more neurophysiologically intuitive directions. While not requiring any labeled data, the performance of these methods is still far from that of supervised methods ( <ref type="figure">Figure 1</ref>).</p><p>We continue in the vein of self-supervised methods and propose an improved tracker, which we call Memory-Augmented Self-Supervised Tracker (MAST). Similar to previous self-supervised methods, our model performs tracking by learning a feature representation that enables robust pixel-wise correspondences between frames; it then propagates a given segmentation mask to subsequent frames based on the correspondences. We make three main contributions: first, we reassess the traditional choices used for self-supervised training and reconstruction loss by conducting thorough experiments to finally determine the optimal choices. Second, to resolve the challenge of tracker drift (i.e. as the object changes appearance or becomes occluded, each subsequent prediction becomes less accurate if propagated only from recent frames), we further improve on existing methods by augmenting our architecture with a crucial memory component. We design a coarse-to-fine approach that is necessary to efficiently access the memory bank: a two-step attention mechanism first coarsely searches for candidate windows, and then computes finegrained matching. We conduct experiments to analyze our choice of memory frames, showing that both short-and long-term memory are crucial for good performance. Third, we benchmark on large-scale video segmentation datasets and propose a new metric, i.e. generalizability, with the goal of measuring the performance gap between tracking seen and unseen categories, which we believe better captures the real-world use-cases for category-agnostic tracking.</p><p>The result of the first two contributions is a selfsupervised network that surpasses all existing approaches by a significant margin on DAVIS-2017 (15%) and YouTube-VOS (17%) benchmarks, making it competitive with supervised methods for the first time. Our results show that a strong representation for tracking can be learned without using any semantic annotations, echoing the earlystage development of the human visual system. Beyond significantly narrowing the gap with supervised methods on the existing metrics, we also demonstrate the superiority of self-supervised approaches over supervised methods on generalizability. On the unseen categories of YouTube-VOS benchmark, we surpass PreMVOS <ref type="bibr" target="#b40">[41]</ref>, the 2018 challenge winner algorithm trained on massive segmentation datasets. Furthermore, when we analyze the drop in performance between seen and unseen categories, we show that our method (along with other self-supervised methods) has a significantly smaller generalization gap than supervised methods. These results show that contrary to the popular belief that self-supervised methods are not yet useful due to their weaker performance, their greater generalization capability (due to not being at risk of overfitting to labels) is actually a more desirable quality when being deployed in real-world settings, where the domain gap can be significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Dense tracking (aka. semi-supervised video segmentation) has typically been approached in one of two ways: propagation-based or detection/segmentation-based. The former approaches formulate the dense tracking task as a mask propagation problem from the first frame to the consecutive frames. To leverage the temporal consistency between two adjacent frames, many propagation-based methods often try to establish dense correspondences with optical flow or metric learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b55">56]</ref>. However, computing optical flow remains a challenging, yet unsolved problem. Our method relaxes the constraint of optical flow's one-to-one brightness constancy constraint and spatial smoothness, allowing each query pixel to potentially build correspondence with multiple reference pixels. On the other hand, detection/segmentation-based approaches address the tracking task with sophisticated detection or segmentation networks, but since these models are usually not class-agnostic during training, they often have to be fine-tuned on the first frame of the target video during inference <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>, whereas our method requires no fine-tuning.</p><p>Self-supervised learning on videos has generated fruitful research in recent years. Due to the abundance of online data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b68">68]</ref>, various ideas have been explored to learn representations by exploiting the spatio-temporal information in videos. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b66">66]</ref> exploit spatio-temporal ordering for learning video representations. Recently, Han et al. <ref type="bibr" target="#b16">[17]</ref> learn strong video representations for action recognition by self-supervised contrastive learning on raw videos. Of more relevance, <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b58">59]</ref> have recently leveraged the natural temporal coherency of color in videos, to train a network for tracking and correspondence related tasks. We discuss these works in more detail in Section 3.1. In this work, we propose to augment the self-supervised tracking algorithms with a differentiable memory module. We also rectify some flaws in their training process.</p><p>Memory-augmented models refer to the computational architecture that has access to a memory repository for prediction. Such models typically involve an internal memory implicitly updated in a recurrent process, e.g. LSTM <ref type="bibr" target="#b18">[19]</ref> and GRU <ref type="bibr" target="#b8">[9]</ref>, or an explicit memory that can be read or written with an attention-based procedure <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b70">70]</ref>. Memory models have been used for many applications, including reading comprehension <ref type="bibr" target="#b50">[51]</ref>, summarization <ref type="bibr" target="#b49">[50]</ref>, tracking <ref type="bibr" target="#b69">[69]</ref>, video understanding <ref type="bibr" target="#b6">[7]</ref>, and image and video captioning <ref type="bibr" target="#b70">[70,</ref><ref type="bibr" target="#b74">74]</ref>. In dense visual tracking, the popular memory-augmented models treat key frames as memory <ref type="bibr" target="#b44">[45]</ref>, and use attention mechanisms to read from the memory. Despite its effectiveness, the process of computing attention either does not scale to multiple frames or is unable to process high-resolution frames, due to the computational bottleneck in hardware, e.g. physical memory. In this work, we propose a scalable way to process high-resolution information in a coarse-to-fine manner. The model enables dynamic localization of salient regions, and fine-grained processing is only required for a small fraction of the memory bank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The proposed dense tracking system, MAST (Memory-Augmented Self-Supervised Tracker), is a conceptually simple model for dense tracking that can be trained with self-supervised learning, i.e. zero manual annotation is required during training, and an object mask is only required for the first frame during inference. In Section 3.1, we provide relevant background of previous self-supervised dense tracking algorithms, and terminologies that will be used in later sections. Next, in Section 3.2, we pinpoint weaknesses in these works and propose improvements to the training signals. Finally, in Section 3.3, we propose memory augmentation as an extension to existing self-supervised trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background</head><p>In this section, we review previous papers that are closely related to this work <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b58">59]</ref>. In general, the goal of selfsupervised tracking is to learn feature representations that enable robust correspondence matching. During training, a proxy task is posed as reconstructing a target frame (I t ) by linearly combining pixels from a reference frame (I t?1 ), with the weights measuring the strength of correspondence between pixels.</p><p>Specifically, a triplet ({Q t , K t , V t }) exists for each input frame I t , referring to Query, Key, and Value. In order to reconstruct a pixel i in the t-th frame (? i t ), an Attention mechanism is used for copying pixels from a subset of previous frames in the original sequence. This procedure is formalized as:</p><formula xml:id="formula_0">? i t = j A ij t V j t?1 (1) A ij t = exp Q i t , K j t?1 p exp Q i t , K p t?1<label>(2)</label></formula><p>where ?, ? refers to the dot product between two vectors, query (Q) and key (K) are feature representations computed by passing the target frame I t to a Siamese ConvNet ?(?; ?), i.e. Q t = K t = ?(I t ; ?), A t is the affinity matrix representing the feature similarity between pixel I i t and I j t?1 , value (V) is the raw reference frame (I t?1 ) during the training stage, and instance segmentation mask during inference, achieving reconstruction or dense tracking respectively.</p><p>A key element in self-supervised learning is to set the proper information bottleneck, or the choice of what input information to withhold for learning the desired feature representation and avoiding trivial solutions. For example, in the reconstruction-by-copying task, an obvious shortcut is that the pixel in I t can learn to match any pixel in I t?1 with the exact same color, yet not necessarily correspond to the same object. To circumvent such learning shortcuts, Vondrick et al. <ref type="bibr" target="#b58">[59]</ref> intentionally drop the color information from the input frames. Lai and Xie <ref type="bibr" target="#b36">[37]</ref> further show that a simple channel dropout can be more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Improved Reconstruction Objective</head><p>In this section, we reassess the choices made in previous self-supervised dense tracking works and provide intuition for our optimal choices, which we empirically support in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Decorrelated Color Space</head><p>Extensive experiments in the human visual system have shown that colors can be seen as combinations of the primary colors, namely red (R), green (G) and blue (B). For this reason, most of the cameras and emissive color displays represent pixels as a triplet of intensities: (R, G, B) ? R 3 . However, a disadvantage of the RGB representation is that the channels tend to be extremely correlated <ref type="bibr" target="#b48">[49]</ref>, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. In this case, the channel dropout proposed in <ref type="bibr" target="#b36">[37]</ref> is unlikely to behave as an effective information bottleneck, since the dropped channel can almost always be determined by one of the remaining channels.  : Correlation between channels of RGB and Lab colorspace. We randomly take 100, 000 pixels from 65 frames in a sequence (snowboard) in the DAVIS dataset and plot the relative relationships between RGB channels. This phenomena generally holds for all natural images <ref type="bibr" target="#b48">[49]</ref>, due to the fact that all of the channels include a representation of brightness. Values are normalized for visualization purposes.</p><p>To remedy this limitation, we hypothesize that dropout in the decorrelated representations (e.g. Lab) would force the model to learn invariances suitable for self-supervised dense tracking; i.e. if the model cannot predict the missing channel from the observed channels, it is forced to learn a more robust representation rather than relying on local color information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Classification vs. Regression</head><p>In the recent literature on colorization and generative models <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b75">75]</ref>, colors were quantized into discrete classes and treated as a multinomial distribution, since generating images or predicting colors from grayscale images is usually a non-deterministic problem; e.g. the color of a car can reasonably be red or white. However, this convention is suboptimal for self-supervised learning of correspondences, as we are not trying to generate colors for each pixel, but rather, estimate a precise relocation of pixels in the reference frames. More importantly, quantizing the colors leads to an information loss that can be crucial for learning highquality correspondences.</p><p>We conjecture that directly optimizing a regression loss between the reconstructed frame (? t ) and real frame (I t ) will provide more discriminative training signals. In this work, the objective L is defined as the Huber Loss:</p><formula xml:id="formula_1">L = 1 n i z i<label>(3)</label></formula><p>where</p><formula xml:id="formula_2">z i = 0.5(? i 3.3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Memory-Augmented Tracking</head><p>So far we have discussed the straightforward attentionbased mechanism for propagating a mask from a single previous frame. However, as predictions are made recursively, errors caused by object occlusion and disocclusion tend to accumulate and eventually degrade the subsequent predictions. To resolve this issue, we propose an attentionbased tracker that efficiently makes use of multiple reference frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Multi-frame tracker</head><p>An overview of our tracking model is shown in <ref type="figure">Figure 4</ref>. To summarize the tracking process: given the present frame and multiple past frames (memory bank) as input, we first compute the query (Q) for the present frame and keys (K) for all frames in memory. Here, we follow the general procedure in previous works as described in Section 3.1, where K and Q are computed from a shared-weight feature extractor and V is equal to the input frame (during training) or object mask (during testing). The computed affinity between Q and all the keys (K) in memory is then used to make a prediction for each query pixel depending on V. Note we don't put any weights on the reference frames, as this should be encoded in the affinity matrix (e.g. when a target and reference frame are dis-similar, the corresponding similarity value will be naturally low; thus the reference label will have less contribution to the labeling of a target pixel).</p><p>The decision of which pixels to include in K is crucial for good performance. Including all pixels previously seen is far too computationally expensive due to the quadratic explosion of the affinity matrix (e.g. the network of <ref type="bibr" target="#b36">[37]</ref> produces affinity matrices with more than 1 billion elements  <ref type="figure">Figure 4</ref>: Structure of MAST. The current frame is used to compute query to attend and retrieve from memory (key &amp; value). During training, we use raw video frame as value for self-supervision. Once the encoder is trained, we use instance mask as value. See Section 3.3 for details. for 480p videos). To reduce computation, <ref type="bibr" target="#b36">[37]</ref> exploit temporal smoothness in videos and apply restricted attention, only computing the affinity with pixels in a ROI around the query pixel location. However, the temporal smoothness assumption holds only for temporally close frames.</p><p>To efficiently process temporally distant frames, we propose a two-step attention mechanism. The first stage involves coarse pixel-matching with the frames in the memory bank to determine which ROIs are likely to contain good matches with the query pixel. In the second stage, we extract the ROIs and compute fine-grained pixel matching, as described in Section 3.1. Overall, the process can be summarized in Algorithm 1. for each of the reference frames 3: Compute similarity matrix A ij t = Q j , R i t between target frame Q and each ROI. 4: Output: pixel's label is determined by aggregating the labels of the ROI pixels (weighted by its affinity score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">ROI Localization</head><p>The goal of ROI localization is to estimate the candidate windows non-locally from memory banks. As shown in <ref type="figure" target="#fig_5">Figure 5</ref>, this can be achieved through restricted attention with varying dilation rate. Intuitively, for short-term memory (temporally close frames), dilation is not required since spatial-temporal coherence naturally exists in videos; thus ROI localization becomes restricted attention (similar to <ref type="bibr" target="#b36">[37]</ref>). However, for long-term memory, we aim to account for the fact that objects can potentially appear anywhere in the reference frames. We unify both scenarios into a single framework for learning ROI localization. Formally, for the query pixel i in I t , to localize the ROI from frame (I t?N ), we first compute in parallel H i t?N,x,y , the similarity heatmap between i and all candidate pixels in the dilated window:</p><formula xml:id="formula_3">H i t?N,x,y = sof tmax(Q i t ? im2col(K i t?N , ? t?N )) (5)</formula><p>where ? t?N refers to the dilation rate for window sampling in frame I t?N , and im2col refers to an operation that transforms the input feature map into a matrix based on dilation rate. Specifically, in our experiments, the dilation rate is proportional to the temporal distance between the present frame and the past frames in the memory bank, i.e. ? t?N ? N . We use ? t?N = (t ? N )/15 .</p><p>The center coordinates for ROIs can be then computed via a soft-argmax operation:</p><formula xml:id="formula_4">P i x,y = x,y H i x,y * C<label>(6)</label></formula><p>where P i x,y is the estimated center location of the candidate window in frame I t?N for query pixel I i t , and C refers to the grid coordinates (x, y) corresponding to the pixels in the window from im2col. The resampled Key (K i t?N ) for pixel I i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Details</head><p>Training: For fair comparison, we adopt as our feature encoder the same architecture (ResNet18) as <ref type="bibr" target="#b36">[37]</ref> in all experiments (as shown in Supplementary Material). The network produces feature embeddings with a spatial resolution 1/4 of the original image. The model is trained in a completely self-supervised manner, meaning the model is initialized with random weights, and we do not use any information other than raw video sequences. We report main results on two training datasets: OxUvA <ref type="bibr" target="#b51">[52]</ref> and YouTube-VOS (both raw videos only). We report the first for fair comparison with the state-of-the-art method <ref type="bibr" target="#b36">[37]</ref> and the second for maximum performance. As pre-processing, we resize all frames to 256 ? 256 ? 3. In all of our experiments, we use I 0 , I 5 (only if the index for the current frame is larger than 5) as long term memory, and I t?5 , I t?3 , I t?1 as short term memory. Empirically, we find the choice of frame number has small impact on performance, but using both long and short term memory is essential (See appendix). During training, we first pretrain the network with a pair of input frames, i.e. one reference frame and one target frame are fed as inputs. One of the color channels is randomly dropped with probability p = 0.5. We train our model endto-end using a batch size of 24 for 1M iterations with the Adam optimizer. The initial learning rate is set to 1e-3, and halved after 0.4M, 0.6M and 0.8M iterations. We then finetune the model using multiple reference frames (our full memory-augmented model) with a small learning rate of 2e-5 for another 1M iterations. As discussed in Section 3.2.2, the model is trained with a photometric loss between the reconstruction and the true frame. Inference: We use the trained feature encoder to compute the affinity matrix between pixels in the target frame and those in the reference frames. The affinity matrix is then used to propagate the desired pixel-level entities, such as instance masks in the dense tracking case, as described in Algorithm 1. Image Feature Alignment: Due to memory constraints, the supervision signals in previous methods were all defined on bilinearly downsampled images. As shown in <ref type="figure">Figure 6(a)</ref>, this introduces a misalignment between strided convolution layers and images from na?ve bilinear downsampling. We handle this spatial misalignment between feature embedding and image by directly sampling at the strided convolution centers. This seemingly minor change actually brings significant improvement to the downstream tracking task <ref type="table" target="#tab_5">(Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We benchmark our model on two public benchmarks: DAVIS-2017 <ref type="bibr" target="#b47">[48]</ref> and the current largest video segmentation dataset, YouTube-VOS <ref type="bibr" target="#b71">[71]</ref>. The former contains 150  <ref type="figure">Figure 6</ref>: Image Feature Alignment. We fix the misalignment between strided convolution layers (with kernel centered at red circle) and images from na?ve bilinear downsampling (with kernel centered at blue dot) by sampling directly at the strided convolution centers.</p><p>HD videos with over 30K manual instance segmentations, and the latter has over 4000 HD videos of 90 semantic categories, totalling over 190k instance segmentations. For both datasets, we benchmark the proposed self-supervised learning architecture (MAST) on the official semi-supervised video segmentation setting (aka. dense tracking), where a ground truth instance segmentation mask is given for the first frame, and the objective is to propagate the mask to subsequent frames. In Section 5.1, we report performance of our full model and several ablated models on the DAVIS benchmark. Next, in Section 5.2, we analyze the generalizability of our model by benchmarking on the large-scale YouTube-VOS dataset. Standard evaluation metrics. We use region similarity (J ) and contour accuracy (F) to evaluate the tracked instance masks <ref type="bibr" target="#b46">[47]</ref>. Generalizability metrics To demonstrate the generalizability of tracking algorithms in category-agnostic scenarios, i.e. the categories in training set and testing set are disjoint, YouTube-VOS also explicitly benchmarks the performances on unseen categories. We therefore evaluate a generalization gap in Section 5.2.1, which is defined as the average performance difference between seen and unseen object classes:</p><p>Gen. Gap = (J seen ? J unseen ) + (F seen ? F unseen ) 2</p><p>Note, the proposed metric aims to explicitly penalize the case where the performance on seen outperforms unseen by large margins, while at the same time provide a reward when the performance on unseen categories is higher than on seen ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Video Segmentation on DAVIS-2017</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Main results</head><p>In  J &amp;F). Second, despite using only ResNet18 as the feature encoder, our model trained with self-supervised learning can still surpass supervised approaches that use heavier architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Ablation Studies</head><p>To examine the effects of different components, we conduct a series of ablation studies by removing one component at a time. All models are trained on OxUvA (except for the analysis on different datasets), and evaluated on DAVIS-2017 semi-supervised video segmentation (aka. dense tracking) without any finetuning.</p><p>Choice of color spaces. As shown in <ref type="table" target="#tab_3">Table 2</ref>, we perform different experiments with input frames transformed into different color spaces, e.g. RGB, Lab or HSV. We find that the MAST model trained with Lab color space always outperforms the other color spaces, validating our conjecture that dropout in a decorrelated color space leads to better feature representations for self-supervised dense tracking, as explained in Section 3.2.1. Additionally, we compare our default setting with a model trained with cross-color space matching task (shown in <ref type="table" target="#tab_4">Table 3</ref>). That means to use a different color space for the input and the training objective, e.g. input frames are in RGB, and loss function is defined in Lab color space. Interestingly, the performance drops significantly, we hypothesis this can attribute to the fact that all RGB channels include a representation of brightness, making it highly correlate to the luminance in Lab, therefore acting as a weak information bottleneck.</p><p>Loss functions. As a variation of our training procedure, we experiment with different loss functions: cross entropy loss on the quantized colors, and photometric loss with Huber loss. As shown in <ref type="table" target="#tab_3">Table 2</ref>, regression with real-valued photometric loss surpasses classification significantly, validating our conjecture that the information loss during color quantization results in inferior representations for self-supervised tracking (as explained in Section 3.2), due to less discriminative training signals.       We conjecture that our model gains from higher quality videos and larger object classes in these datasets.</p><p>Image feature alignment. To evaluate the alignment module proposed for aligning features with the original image, we compare it to direct bilinear image downsampling used by CorrFlow <ref type="bibr" target="#b36">[37]</ref>. The result in <ref type="table" target="#tab_5">Table 4</ref> shows that our approach achieves about 2.2% higher performance.</p><p>Dynamic memory by exploiting more frames. We compare our default network with variants that have only short term memory or long term memory. Results are shown in <ref type="table" target="#tab_6">Table 5</ref>. While both short term memory and long term memory alone can make reasonable predictions, the combined model achieves the highest performance. The qualitative predictions <ref type="figure" target="#fig_7">(Figures 10 and 7)</ref> also confirm that the improvements come from reduced tracker drift. For instance, when severe occlusion occurs, our model is able to attend and retrieve high-resolution information from frames that are temporally distant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Youtube Video Object Segmentation</head><p>We also evaluate the MAST model on the Youtube-VOS validation split (474 videos with 91 object categories). As no other self-supervised methods have been tested on the benchmark, we directly compare our results with supervised methods. As shown in <ref type="table">Table 8</ref>, our method outperforms the other self-supervised learning approaches by a significant margin (64.2 vs. 46.6), and even achieves comparable performance to many heavily supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Generalizability</head><p>As another metric for evaluating category-agonostic tracking, the YouTube-VOS dataset conveniently has separate measures for seen and unseen object categories. We can therefore estimate testing performance on out-of-  <ref type="table">Table 8</ref>: Video segmentation results on Youtube-VOS dataset. Higher values are better. According to the evaluation protocol of the benchmark, we report performance separated into "seen" and "unseen" classes ("Seen" with respect to training set). ? indicates results based on our reimplementation. The first-and second-best results on the unseen category are highlighted in red and blue, respectively. distribution samples to gauge the model's generalizability to more challenging, unseen, real-world scenarios. As seen from the last two columns, we rank second amongst all algorithms in unseen objects. In these unseen classes, we are even 3.9% higher than the DAVIS 2018 and YouTube-VOS 2018 video segmentation challenge winner, PreMVOS <ref type="bibr" target="#b40">[41]</ref>, a complex algorithm trained with multiple large manually labeled datasets. For fair comparison, we train our model only on the YouTube-VOS training set. We also re-train two most relevant self-supervised methods in the same manner as baselines. Even learning from only a subset of all classes, our model generalizes well to unseen classes, with a generalization gap (i.e. the performance difference between seen and unseen objects) near zero (0.4). This gap is much smaller than any of the baselines (avg = 11.5), suggesting a unique advantage to most other algorithms trained with labels. By training on large amounts of unlabeled videos, we learn an effective tracking representation without the need for any human annotations. This means that the learned network is not limited to a specific set of object categories (i.e. those in the training set), but is more likely to be a "universal feature representation" for tracking. Indeed, the only supervised algorithm that is comparable to our method in generalizability is OSVOS (2.7 vs. 0.4). However, OSVOS uses the first image from the testing sequence to perform costly domain adaptation, e.g. one-shot fine-tuning. In contrast, our algorithm requires no fine-tuning, which further demonstrates its zero-shot generalization capability.</p><p>Note our model also has a smaller generalization gap compared to other self-supervised methods as well. This further attests to the robustness of its learned features, suggesting that our improved reconstruction objective is highly effective in capturing general features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In summary, we present a memory-augmented selfsupervised model that enables accurate and generalizable pixel-level tracking. The algorithm is trained without any semantic annotation, and surpasses previous self-supervised methods on existing benchmarks by a significant margin, narrowing the gap with supervised methods. On unseen object categories, our model actually outperforms all but one existing methods that are trained with heavy supervision. As computation power grows and more high quality videos become available, we believe that self-supervised learning algorithms can serve as a strong competitor to their supervised counterparts for their flexibility and generalizability. <ref type="figure">Figure 8</ref>: Optimal memory size: Here, we test a changing memory size of n + m: n short term memory and m long term memory, where n and m grow alternatively. The performance of our model initially increases as the number of frames in memory grows, eventually plateauing at 5 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS2017 J &amp; F (Mean)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Analysis by attributes</head><p>We provide a more detailed accuracy list broken down by video attributes provided by the DAVIS benchmark <ref type="bibr" target="#b46">[47]</ref> (listed in <ref type="table" target="#tab_1">Table 10</ref>). The attributes illustrate the difficulties associated with each video sequence. <ref type="figure">Figure 9</ref> contains the accuracies categorized by attribute. Several trends emerge: first, MAST outperforms all other self-supervised and unsupervised models by a large margin in all attributes. This shows that our model is robust to various challenges in dense tracking. Second, MAST obtains significant gains on occlusion-related video sequences (e.g. OOC, OV), suggesting that memory-augmentation is a key enabler for high-quality tracking: retrieving occluded objects from pre-vious frames is very difficult without memory augmentation. Third, in videos involving background clutter, i.e. background and foreground share similar colors, MAST obtains a relatively small improvement over previous methods. We conjecture this bottleneck could be caused by a shared photometric loss; thus a different loss type (e.g. based on texture consistency) could further improve the result.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. YouTube-VOS 2019 dataset</head><p>We also evaluate MAST and two other self-supervised methods on YouTube-VOS 2019 validation dataset. The numerical results are reported in <ref type="table" target="#tab_1">Table 11</ref>. Augmenting on the 2018 version, the 2019 version contains more videos and object instances. We observe similar trend as reported in the main paper (i.e. significant improvement and lower generalization gap).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. More qualitative results</head><p>As shown in <ref type="figure" target="#fig_10">Figure 10</ref>, we provide more qualitative results exhibiting some of difficulties in the tracking task. These difficulties include tracking multiple similar objects (multi-instance tracking often fails by conflating similar objects), large camera shake (objects may have motion blur), inferring unseen object pose of objects, and so on. As shown in the figure, MAST handles these difficulties well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS-2017 Accuracy (J)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identity</head><p>Optical Flow Colorization CorrFlow MAST (Ours) <ref type="figure">Figure 9</ref>: Accuracy broken down by attribute: MAST outperforms previous self-supervised methods by a significant margin on all attributes, demonstrating the robustness of our model. Input frame Propagated frames </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Train once, test on multiple datasets: Qualitative results from our self-supervised dense tracking model on DAVIS-2017 and YouTube-VOS dataset. The number on the top left refers to the frame number in the video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) RGB scatter plot (b) Lab scatter plot</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>Figure 3: Correlation between channels of RGB and Lab colorspace. We randomly take 100, 000 pixels from 65 frames in a sequence (snowboard) in the DAVIS dataset and plot the relative relationships between RGB channels. This phenomena generally holds for all natural images [49], due to the fact that all of the channels include a representation of brightness. Values are normalized for visualization purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 MAST 1 :</head><label>11</label><figDesc>Choose m reference frames Q1, Q2, ...Qm 2: Localize ROI R1, R2, ...Rm according to 3.3.2 (Eq. 5 and 6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>ROI Localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Our method vs. previous self-supervised methods. Other methods show systematic errors in handling occlusions. Row 1: The dancer undergoes large self-occlusion. Row 2: The dog is repeatedly occluded by poles. Row 3: Three women reappear after being occluded by the man in the foreground.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>More qualitative results from our self-supervised dense tracking model on the YouTube-VOS dataset. The number on the top left refers to the frame number in the video. Row 1: Tracking multiple similar objects with scale change. Row 2: Occlusions and out-of-scene objects (hand, bottle, and cup). Row 3: Large camera shake. Row 4: Small object with fine details. Row 5: Inferring unseen pose of the deer; out-of-scene object (hand).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>or M Key Value Enc.</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Past frames (Memory)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Key</cell><cell cols="2">Value</cell><cell>Key</cell><cell cols="2">Value</cell><cell>Query</cell><cell>K K K</cell><cell>V V V</cell></row><row><cell>training</cell><cell>testing</cell><cell>training</cell><cell>testing</cell><cell></cell><cell>training</cell><cell>testing</cell></row><row><cell></cell><cell>Enc.</cell><cell></cell><cell></cell><cell>Enc.</cell><cell></cell><cell></cell><cell>Enc.</cell><cell>Attend &amp; Retrieve</cell></row><row><cell></cell><cell></cell><cell>I I or M</cell><cell></cell><cell></cell><cell>I I or M</cell><cell></cell><cell>I</cell><cell>training</cell><cell>testing</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Target frame</cell><cell>Prediction (Image or mask)</cell></row></table><note>Q PresentI: Image M: Mask Enc.: Shared-weight encoderI I</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 ,</head><label>1</label><figDesc>VOC Y=YouTube-VOS. For size of datasets, we report (length of raw videos) for self-supervised methods and (#image-level annotations, #pixel-level annotations) for supervised methods. denotes concurrent work. ? denotes highest results reported after original publication. Higher values are better.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Supervised</cell><cell>Dataset (Size)</cell><cell cols="5">J &amp;F (Mean) ? J (Mean) ? J (Recall) ? F(Mean) ? F(Recall) ?</cell></row><row><cell>Vid. Color. [59]</cell><cell>ResNet-18</cell><cell></cell><cell>Kinetics (800 hours)</cell><cell>34.0</cell><cell>34.6</cell><cell>34.1</cell><cell>32.7</cell><cell>26.8</cell></row><row><cell cols="2">CycleTime  ? [64] ResNet-50</cell><cell></cell><cell>VLOG (344 hours)</cell><cell>48.7</cell><cell>46.4</cell><cell>50.0</cell><cell>50.0</cell><cell>48.0</cell></row><row><cell>CorrFlow  ? [37]</cell><cell>ResNet-18</cell><cell></cell><cell>OxUvA (14 hours)</cell><cell>50.3</cell><cell>48.4</cell><cell>53.2</cell><cell>52.2</cell><cell>56.0</cell></row><row><cell>UVC [72]</cell><cell>ResNet-18</cell><cell></cell><cell>Kinetics (800 hours)</cell><cell>59.5</cell><cell>57.7</cell><cell>68.3</cell><cell>61.3</cell><cell>69.8</cell></row><row><cell>MAST (Ours)</cell><cell>ResNet-18</cell><cell></cell><cell>OxUvA (14 hours)</cell><cell>63.7</cell><cell>61.2</cell><cell>73.2</cell><cell>66.3</cell><cell>78.3</cell></row><row><cell>MAST (Ours)</cell><cell>ResNet-18</cell><cell></cell><cell>YT-VOS (5.58 hours)</cell><cell>65.5</cell><cell>63.3</cell><cell>73.2</cell><cell>67.6</cell><cell>77.7</cell></row><row><cell>ImageNet [18]</cell><cell>ResNet-50</cell><cell></cell><cell>I (1.28M, 0)</cell><cell>49.7</cell><cell>50.3</cell><cell>-</cell><cell>49.0</cell><cell>-</cell></row><row><cell>OSMN [73]</cell><cell>VGG-16</cell><cell></cell><cell>ICD (1.28M, 227k)</cell><cell>54.8</cell><cell>52.5</cell><cell>60.9</cell><cell>57.1</cell><cell>66.1</cell></row><row><cell>SiamMask [61]</cell><cell>ResNet-50</cell><cell></cell><cell>IVCY (1.28M, 2.7M)</cell><cell>56.4</cell><cell>54.3</cell><cell>62.8</cell><cell>58.5</cell><cell>67.5</cell></row><row><cell>OSVOS [6]</cell><cell>VGG-16</cell><cell></cell><cell>ID (1.28M, 10k)</cell><cell>60.3</cell><cell>56.6</cell><cell>63.8</cell><cell>63.9</cell><cell>73.8</cell></row><row><cell>OnAVOS [57]</cell><cell>ResNet-38</cell><cell></cell><cell>ICPD (1.28M, 517k)</cell><cell>65.4</cell><cell>61.6</cell><cell>67.4</cell><cell>69.1</cell><cell>75.4</cell></row><row><cell>OSVOS-S [42]</cell><cell>VGG-16</cell><cell></cell><cell>IPD (1.28M, 17k)</cell><cell>68.0</cell><cell>64.7</cell><cell>74.2</cell><cell>71.3</cell><cell>80.7</cell></row><row><cell cols="2">FEELVOS [56] Xception-65</cell><cell></cell><cell>ICDY (1.28M, 663k)</cell><cell>71.5</cell><cell>69.1</cell><cell>79.1</cell><cell>74.0</cell><cell>83.8</cell></row><row><cell cols="2">PReMVOS [41] ResNet-101</cell><cell></cell><cell>ICDPM (1.28M, 527k)</cell><cell>77.8</cell><cell>73.9</cell><cell>83.1</cell><cell>81.8</cell><cell>88.9</cell></row><row><cell>STM [45]</cell><cell>ResNet-50</cell><cell></cell><cell>IDY (1.28M, 164k)</cell><cell>81.8</cell><cell>79.2</cell><cell>-</cell><cell>84.3</cell><cell>-</cell></row><row><cell cols="9">Table 1: Video segmentation results on DAVIS-2017 validation set. Dataset notations: I=ImageNet, V = ImageNet-VID, C=COCO, D=DAVIS,</cell></row><row><cell cols="2">M=Mapillary, P=PASCAL-lab-coat Video colorization</cell><cell>TimeCycle</cell><cell>CorrFlow</cell><cell cols="2">OSVOS (Supervised)</cell><cell>Ours</cell><cell></cell><cell>Ground Truth</cell></row><row><cell>breakdance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>No prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>drift-chicane</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>No prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>libby</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>we compare MAST with previous approaches on the DAVIS-2017 benchmark. Two phenomena can be observed: first, our proposed model clearly outperforms all other self-supervised methods, surpassing previous state-of- the-art CorrFlow by a significant margin (65.5 vs 50.3 on</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Training colorspaces and loss: Our final model trained with Lab colorspace with regression loss outperforms all other models on dense tracking task. Higher values are better.</figDesc><table><row><cell cols="4">Input Loss J (Mean) F (Mean)</cell></row><row><cell>Lab</cell><cell>RGB</cell><cell>48.2</cell><cell>52.0</cell></row><row><cell>RGB</cell><cell>Lab</cell><cell>46.8</cell><cell>49.9</cell></row><row><cell>Lab</cell><cell>Lab</cell><cell>61.2</cell><cell>66.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Cross color space matching vs.</figDesc><table><row><cell>sin-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Image</figDesc><table><row><cell>-Feature alignment: Using the</cell></row><row><cell>improved Image-Feature alignment implemen-</cell></row><row><cell>tation improves the results. Higher values are</cell></row><row><cell>better.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Memory</figDesc><table><row><cell cols="3">Propagation J (Mean) F (Mean)</cell></row><row><cell>Soft</cell><cell>57.0</cell><cell>61.7</cell></row><row><cell>Hard</cell><cell>61.2</cell><cell>66.3</cell></row><row><cell></cell><cell>+4.2</cell><cell>+4.6</cell></row><row><cell>length: Removing either</cell><cell></cell><cell></cell></row><row><cell>long term or short term memory results in a</cell><cell></cell><cell></cell></row><row><cell>performance drop.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Soft vs.</figDesc><table><row><cell>Dataset</cell><cell cols="2">J (Mean) F (Mean)</cell></row><row><cell>OxUvA</cell><cell>61.2</cell><cell>66.3</cell></row><row><cell>ImageNet VID</cell><cell>60.0</cell><cell>63.9</cell></row><row><cell>YouTube-VOS (w/o anno.)</cell><cell>63.3</cell><cell>67.6</cell></row><row><cell>hard propagation: Quantiz-</cell><cell></cell><cell></cell></row><row><cell>ing class probability of each pixel (hard prop-</cell><cell></cell><cell></cell></row><row><cell>agation) shows large gains over propagating</cell><cell></cell><cell></cell></row><row><cell>probility distribution (soft propagation).</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Training dataset: All datasets provide reasonable performance, with O and Y slightly superior.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>List of video attributes provided in the DAVIS benchmark. We break down the validation accuracy according to the attribute list.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Video segmentation results on Youtube-VOS 2019 dataset. Higher values are better. ? indicates results based on our reimplementation.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t ? I i t ) 2 , if |? i t ? I i t | &lt; 1 |? i t ? I i t | ? 0.5, otherwise(4)where? i t ? R 3 refers to RGB or Lab, normalized to the range [-1,1] in the reconstructed frame that is copied from pixels in the reference frame I t?1 , and I t is the real frame at time point t.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t can be extracted with a bilinear sampler<ref type="bibr" target="#b22">[23]</ref>. With all the candidate Keys dynamically sampled from different reference frames of the memory bank, we compute fine-grained matching scores only with these localized Keys, resembling a restricted attention in a non-local manner. Therefore, with the proposed design, the memory-augmented model can efficiently access high-resolution information for correspondence matching, without incurring large physical memory costs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors would like to thank Andrew Zisserman for helpful discussions, Olivia Wiles, Shangzhe Wu, Sophia Koepke and Tengda Han for proofreading. Financial support for this project is provided by EPSRC Seebibyte Grant EP/M013774/1. Erika Lu is funded by the Oxford-Google DeepMind Graduate Scholarship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Network architecture</head><p>In the same way as CorrFlow <ref type="bibr" target="#b36">[37]</ref>, we use a modified ResNet-18 <ref type="bibr" target="#b17">[18]</ref> architecture. Details of the network are illustrated in <ref type="table">Table 9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Optimal memory size</head><p>In <ref type="figure">Figure 8</ref>, we explicitly show the effectiveness of increasing the number of reference frames in the memory bank, and confirm that a 5-frame memory is optimal for our task. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cnn in mrf: Video object segmentation via inference in a cnn-based higherorder spatio-temporal mrf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual responses in the newborn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berry Brazelton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Louise</forename><surname>Scholl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Robey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pediatrics</title>
		<imprint>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Oneshot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-Term Feature Banks for Detailed Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feichtenhofer</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Haoqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kr?henb?hl</forename><surname>Philipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girshick</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vighnesh</forename><surname>Birodkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2009/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2009 (VOC2009) Results</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Geometry guided convolutional neural networks for self-supervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Workshop on Large-scale Holistic Video Understanding, ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Maskrnn: Instance level video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning visual groups from co-occurrences in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Conditional image generation for learning the structure of visual objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning image representations tied to ego-motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Slow and steady feature analysis: higher order temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Self-supervised spatiotemporal feature learning by video geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11387</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A generative appearance model for end-to-end video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Brissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Lucid data dreaming for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09554</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02646</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video object segmentation with language referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multigrid predictive filter flow for unsupervised learning on videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Smooth-pursuit eye movements in the newborn infant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><forename type="middle">P</forename><surname>Kremenitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><forename type="middle">G</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Kurtzberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Dowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child Development</title>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A novel performance evaluation methodology for single-target trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ale?</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luka?ehovin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-supervised learning for video correspondence flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Premvos: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shuffle and learn: Unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by referenceguided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Adhikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer graphics and applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Endto-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Long-term tracking in the wild: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rvos: Endto-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreu</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rvos: Endto-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreu</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Eyehand coordination in the newborn. Developmental Psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Von Hofsten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unsupervised deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Ranet: Ranking attention network for fast video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Self-supervised learning of a facial attribute embedding from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sophia Koepke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">X2face: A network for controlling face generation using images, audio, and pose codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sophia Koepke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Xizhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yujie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Jifeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yichen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Youtubevos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Joint-task self-supervised learning for temporal correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xueting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Sifei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Mello Shalini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xiaolong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kautz</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Ming-Hsuan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
