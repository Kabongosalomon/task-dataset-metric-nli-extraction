<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense anomaly detection by robust learning on synthetic negative data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Grci?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing Unska 3</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<postCode>10000</postCode>
									<settlement>Zagreb</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandi?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing Unska 3</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<postCode>10000</postCode>
									<settlement>Zagreb</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoran</forename><surname>Kalafati?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing Unska 3</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<postCode>10000</postCode>
									<settlement>Zagreb</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sini?a?egvi?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing Unska 3</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<postCode>10000</postCode>
									<settlement>Zagreb</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dense anomaly detection by robust learning on synthetic negative data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dense anomaly detection</term>
					<term>Semantic segmentation</term>
					<term>Normalizing flow</term>
					<term>out-of-distribution detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Standard machine learning is unable to accommodate inputs which do not belong to the training distribution. The resulting models often give rise to confident incorrect predictions which may lead to devastating consequences. This problem is especially demanding in the context of dense prediction since input images may be only partially anomalous. Previous work has addressed dense anomaly detection by discriminative training on mixed-content images. We extend this approach with synthetic negative patches which simultaneously achieve high inlier likelihood and uniform discriminative prediction. We generate synthetic negatives with normalizing flows due to their outstanding distribution coverage and capability to generate samples at different resolutions. We also propose to detect anomalies according to a principled information-theoretic criterion which can be consistently applied through training and inference. The resulting models set the new state of the art on standard benchmarks and datasets in spite of minimal computational overhead and refraining from auxiliary negative data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>level of driving assistance. Further advances in the field require improved perception of the environment around the ego-vehicle. Some of the related tasks are scene parsing <ref type="bibr" target="#b0">[1]</ref>, reconstruction <ref type="bibr" target="#b1">[2]</ref>, and semantic forecasting <ref type="bibr" target="#b2">[3]</ref>. The basic form of scene understanding classifies every pixel into one of the K predefined classes, which corresponds to semantic segmentation <ref type="bibr" target="#b0">[1]</ref>.</p><p>Recent semantic segmentation approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref> are based on deep learning. A deep model for semantic segmentation maps the input RGB image x 3?H?W into the corresponding prediction y K?H?W where K denotes the number of known classes. Typically, the model parameters ? are obtained by gradient optimization of a supervised discriminative objective based on maximum likelihood. Recent approaches may produce high-fidelity segmentations of large images in real time even when inferring on a modest GPU <ref type="bibr" target="#b3">[4]</ref>. However, standard learning is susceptible to overconfidence in incorrect predictions <ref type="bibr" target="#b4">[5]</ref>, which may make the model unusable in presence of domain shift <ref type="bibr" target="#b5">[6]</ref>. This poses a huge threat for deep models deployed in the real world, especially in the field of autonomous driving.</p><p>We study the ability of deep models for natural image understanding to deal with out-of-distribution input. We desire to correctly segment the scene while simultaneously detecting anomalous objects which are unlike any scenery from the training dataset. Integration of dense anomaly detection and standard dense prediction results in dense open-set recognition. Such capability is especially important in road-driving scenarios where missdetection of anomalous obstacles could result in serious consequences.</p><p>Previous approaches to dense anomaly detection rely on image resynthesis <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, Bayesian modeling <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, recognition in the latent space <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, or auxiliary negative training data <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. However, all these approaches have significant shortcomings. Image resynthesis and Bayesian approaches require extraordinary computational resources while being unable to deliver competitive performance. In any case, these methods are not suitable for real-time applications. Recognition in latent space <ref type="bibr" target="#b11">[12]</ref> may be sensitive to feature collapse <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> due to relying on a pretrained classifier. Reliance on auxiliary negative data may introduce several kinds of undesired bias such as overoptimistic performance on some test images. Moreover, appropriate negative data may be unavailable in some application areas.</p><p>This work addresses dense anomaly detection by encouraging a standard dense prediction model to emit uniform predictions in outliers <ref type="bibr" target="#b17">[18]</ref>. We propose to perform the training on mixed-content images <ref type="bibr" target="#b14">[15]</ref> which we craft by pasting synthetic negatives into inlier training images. The generative model for synthetic negatives is jointly trained to generate patches which give rise to uniform discriminative prediction and to assign high likelihood in inliers <ref type="bibr" target="#b17">[18]</ref>. We argue that normalizing flows are better than GANs for the task at hand due to much better distribution coverage and more stable training. Additionally, normalizing flows can generate samples of variable spatial dimensions <ref type="bibr" target="#b18">[19]</ref> which makes them especially suitable for mimicking anomalous objects of varying size. This paper proposes two major improvements over our preliminary report <ref type="bibr" target="#b19">[20]</ref>. First, we show that Jensen-Shannon divergence is a criterion of choice both for training and inference. This is more principled than previous approaches which optimize KL divergence and perform inference according to max-softmax <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14]</ref>. Second, we propose two stage training in order to discourage overfitting of the discriminative model to synthetic outliers. We refer to the consolidated method as NFlowJS. NFlowJS achieves state-ofthe-art performance on standard benchmarks for dense anomaly detection, despite abstaining from auxiliary negative data <ref type="bibr" target="#b14">[15]</ref>, image resynthesis <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref> and Bayesian modelling <ref type="bibr" target="#b9">[10]</ref>. NFlowJS can accurately detect anomalies at three times larger distances than the previous best result. Finally, NFlowJS has a very low overhead over the standard discriminative model, making it especially suitable for real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>There are several computer vision tasks which require anomaly detection. In the simplest case we are interested whether a given image is an inlier or an outlier <ref type="bibr" target="#b20">[21]</ref> (section 2.1). In practice, this often has to be integrated with the primary recognition task <ref type="bibr" target="#b21">[22]</ref>. The resulting open-set classifiers produce predictions over a set of K known classes and one unknown class (section 2.2). Things get more complicated in the case of dense prediction where we have to deal with outlier objects in inlier scenes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8]</ref> </p><formula xml:id="formula_0">(section 2.3).</formula><p>The simplest approach to anomaly detection is to look at the likelihood produced by a generative model fitted to the training data. However, today we know that such approach is neither simple nor straight-forward <ref type="bibr" target="#b22">[23]</ref>. Still, generative models can be useful in less obvious approaches to anomaly detection <ref type="bibr" target="#b17">[18]</ref>. Generative flows are especially interesting in this context due to outstanding distribution coverage and capability to generate samples at different resolutions (subsection 2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Out-of-distribution detection</head><p>Out-of-distribution (OOD) detection <ref type="bibr" target="#b20">[21]</ref> is a binary classification task which discriminates inliers from outliers. In-distribution samples, also known as inliers, are generated by the same generative process as the training data. Contrary, outliers are generated by a process which is disjoint from the training distribution <ref type="bibr" target="#b22">[23]</ref>. OOD detection is typically carried out according to scoring function s ? : [0, 1] 3?H?W ? R which assigns the scalar score to each test sample. We detect anomalies by thresholding the anomaly score.</p><p>Early OOD detectors utilize maximal softmax probability <ref type="bibr" target="#b20">[21]</ref>. This scoring function can be further improved by utilizing anti-adversarial perturbations <ref type="bibr" target="#b23">[24]</ref> or auxiliary negative data <ref type="bibr" target="#b17">[18]</ref>. However, the former require additional processing while the latter may be unsuitable for atypical domains which are not represented in any of the large image datasets (e.g: medical, astronomical, and microscopic imagery). Additionally, we can not get rid of the doubt that experimental performance may be biased by the particular negative dataset.</p><p>There are several prior approaches to replacing real negatives with synthetic ones <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>. A seminal approach <ref type="bibr" target="#b17">[18]</ref> proposes cooperative training of a generative adversarial network and a standard classifier. The classifier loss requires uniform predictions in generated samples and thus encourages the generator to yield samples at the distribution border. This idea can be carried out without a separate generative model, by leveraging Langevin sampling <ref type="bibr" target="#b24">[25]</ref>. However, adapting these approaches for dense prediction is neither straight-forward nor trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Open-set recognition</head><p>Open-set recognition integrates OOD detection with the standard classification <ref type="bibr" target="#b21">[22]</ref>. These models discourage excessive generalization for K known classes and thus attempt to distinguish them from the remaining visual content of the open world. Many approaches address this goal by rejecting known classes in input samples which are recognized as outliers <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>A major problem for this task and OOD detection in general is that outliers and inliers may be indistinguishable in the feature space <ref type="bibr" target="#b16">[17]</ref>. Feature collapse <ref type="bibr" target="#b15">[16]</ref> can be alleviated by forcing the model to learn more informative features. This can be implemented by supplying auxiliary self-supervised loss <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref> as well as by training on negative data which can be sourced from real datasets <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b14">15]</ref> or sampled from generative models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Dense open-set recognition</head><p>Dense open-set recognition has a great potential for improving visual perception in road-driving scenarios. The resulting models strive to detect unknown hazards while correctly segmenting the rest of the scene.</p><p>A principled Bayesian approach to dense anomaly detection measures epistemic uncertainty <ref type="bibr" target="#b9">[10]</ref>. However, the assumption that MC dropout corresponds to Bayesian model sampling may not be satisfied in practice. Anomalies can also be detected by estimating the likelihood in feature space <ref type="bibr" target="#b11">[12]</ref>, however that approach is vulnerable to feature collapse <ref type="bibr" target="#b15">[16]</ref>.</p><p>Several approaches to dense open-set recognition train on auxiliary negative data. This idea requires two implementation details <ref type="bibr" target="#b14">[15]</ref>. First, the training has to be conducted on mixed-content images obtained by pasting negative patches into positive training examples. Second, the negative dataset should be as broad as possible (eg. ImageNet, ADE20k or COCO) in order to cover a large portion of the background distribution. This will likely introduce some inlier content into negative images, however the resulting noise can be alleviated by careful batch composition. The training can be implemented through a separate OOD head <ref type="bibr" target="#b14">[15]</ref> or by maximizing softmax entropy in negative pixels <ref type="bibr" target="#b12">[13]</ref>. Recent work shows that anomaly detector can also be trained on instance classes of an auxiliary semantic segmentation dataset <ref type="bibr" target="#b12">[13]</ref>.</p><p>Another line of work resynthesizes the input scene with a conditional generative model <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref>. Dissimilarity between the input and the resyntesized image gives rise to competitive OOD detection. Still, resynthesis requires significant computational overhead which limits real-world applications. A related approach <ref type="bibr" target="#b30">[31]</ref> utilizes a parallel upsampling path for input reconstruction. This improves inference speed with respect to generative resynthesis approaches, but fails to correctly recreate cluttered scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Generative modeling</head><p>The main goal of generative modeling is to approximate implicit distribution of the training data with a statistical model. Early approaches consider unnormalized distributions <ref type="bibr" target="#b31">[32]</ref> which require complex training procedures based on MCMC sample generation. Alternatively, the distribution can be autoregressively factorized <ref type="bibr" target="#b32">[33]</ref>, which allows likelihood estimation and powerful but slow sample generation. Both of these two approaches are unsuitable for generating synthetic negatives due to slow sampling.</p><p>VAEs <ref type="bibr" target="#b33">[34]</ref> use a factorized variational approximation which allows to learn a lower bound of the likelihood. However, such training requires storing both the encoder and the decoder in the GPU memory. This makes VAEs unsuitable for joint training with a dense prediction model, since the latter requires large batches and large crops <ref type="bibr" target="#b34">[35]</ref>. Orthogonally, GANs <ref type="bibr" target="#b35">[36]</ref> ignore the factorization of the likelihood. Instead, the generator network learns to mimic the dataset samples by competing in a minimax game. However, the produced samples do not span the entire support of the training distribution <ref type="bibr" target="#b36">[37]</ref> which makes GANs unsuitable for our task.</p><p>Contrary to previous approaches, normalizing flows <ref type="bibr" target="#b18">[19]</ref> model the likelihood by a bijective mapping towards a predefined latent distribution p(z), typically a fully factorized Gaussian. Given a diffeomorphism f ? , the likelihood is defined according to the change of variables formula:</p><formula xml:id="formula_1">p ? (x) = p(z) det ?z ?x , z = f ? (x).<label>(1)</label></formula><p>This approach requires computation of the Jacobian determinant (det ?z ?x ). Therefore, a great emphasis is placed on the design of transformations with tractable determinant computation and efficient inverse. This setup is further improved by introducing skip connections which increase capacity and improve convergence speed <ref type="bibr" target="#b37">[38]</ref>.</p><p>A trained normalizing flow f ? can be sampled in two steps. First, we sample the latent distribution to obtain the latent tensor z. Second, we recover the sample through the inverse transformation x = f ?1 ? (z). Since f is a bijection, both the latent representation and the generated image have the same dimensionality (R 3?H?W ? [0, 1] 3?H?W ). This property is especially useful for generating synthetic negatives since it allows to train and sample the model on different resolutions <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dense anomaly detection with NFlowJS</head><p>We train dense anomaly detection on mixed-content images <ref type="bibr" target="#b14">[15]</ref> obtained by pasting artificial negatives into regular training images. We propose to generate such negatives by a jointly trained normalizing flow (section 3.1). We train our models to recognize outliers according to a principled information-theoretic criterion (section 3.2), and use the same criterion to express the anomaly score which we use for inference (section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generating anomalies by a jointly trained normalizing flow</head><p>We train our discriminative model by minimizing cross-entropy over inliers (s ij = 0) and maximizing prediction uncertainty in pasted negatives (s ij = 1) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29]</ref>:</p><formula xml:id="formula_2">L(?) = H,W i,j ?(1 ? s ij ) ? ln p ? (y ij |x ) + ? ? s ij ? L ij neg (?).<label>(2)</label></formula><p>We sample a randomly sized negative patch x ? from a jointly trained normalizing flow f ? . We assemble a mixed-content training image x by pasting x ? atop an inlier training image x + :</p><formula xml:id="formula_3">x = (1 ? s) ? x + + pad(x ? , s) where x ? = f ?1 ? (z), z ? N (0, I).<label>(3)</label></formula><p>The negative patch is pasted on a random location. The outlier mask s has ones at the randomly selected pasting locations and zeros elsewhere. The synthetic outlier x ? is zero-padded in order to allow pasting by addition. We jointly train the normalizing flow alongside the primary discriminative model (cf. <ref type="figure" target="#fig_0">Figure 1</ref>) in order to satisfy two opposing criteria. First, the generated samples should yield uniform predictive distribution at the output of the discriminative model. This moves the generative distribution away from the inliers. Second, the normalizing flow should maximize the likelihood of inlier patches. This moves the generative distribution towards the inliers. Thus, such training encourages generation of synthetic samples at the boundary of the training distribution and incorporates outlier awareness within the primary discriminative model <ref type="bibr" target="#b17">[18]</ref>. The joint training procedure minimizes the following loss:</p><formula xml:id="formula_4">L(?, ?) = L gen (?) + H,W i,j (1 ? s ij ) ? L ij cls (?) + ? ? s ij ? L ij neg (?, ?).<label>(4)</label></formula><p>L gen is the negative log-likelihood of the inlier patch which we replace by the synthetic outlier. L cls is the standard discriminative loss. We scrutinize L neg in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Robust loss in negative pixels</head><p>The loss L neg has been often designed as KL-divergence between the uniform distribution and the model's predictive distribution <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. However, synthetic anomalies may contain parts which look alike chunks of inlier scenes. Discriminative predictions in such pixels are confident, which is strongly penalized by the KL-divergence. Such loss teaches the discriminative model to reduce the confidence in inlier content, and therefore triggers frequent false-positive responses of anomaly detector. Hence, we search the set of f-divergences for a more robust loss function. <ref type="figure" target="#fig_1">Figure 2</ref> (left) shows loss values for various f-divergences in the twoclass setup. We observe that Jensen-Shannon divergence mildly penalizes high confidence predictions, which makes it a suitable candidate for our loss. <ref type="figure" target="#fig_1">Figure 2</ref> (right) shows a histogram of per-pixel losses which shows that JSdivergence offers robust penalisation in a realistic setup. Note that other f-divergences such as Pearson or Hellinger divergence are even more rigorous than the KL-divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference with divergence-based scoring</head><p>Contrary to all existing designs, we propose to perform inference by aligning the scoring function with the training objective. <ref type="figure">Figure 3</ref> illustrates the resulting dense open-set recognition method. The input image is fed into the discriminative model. The produced logits are fed into two branches. The top branch delivers closed-set predictions through arg-max. The bottom branch recovers the dense anomaly map through temperature scaling, softmax and JS divergence with respect to the uniform distribution. The two branches are fused into the final open-set segmentation map. The anomaly map overrides the closed-set prediction wherever the anomaly score exceeds a dataset-wide threshold.</p><p>We use temperature scaling <ref type="bibr" target="#b4">[5]</ref> since it reduces the relative anomaly score of distributions with two dominant logits compared to distributions with homogeneous non-maximum logits. This discourages false positive OOD responses at semantic borders. We use the same temperature T=2 in all experiments which compare our performance with respect to previous methods. Note that our inference is very fast since we use our generative model only to simulate anomalies during training. This is different from image resynthesis where the generative model has to be used during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setup</head><p>This section describes our experimental setup for dense anomaly detection. We review the employed datasets, introduce performance metrics, and describe details of the training procedure.  <ref type="figure">Figure 3</ref>: Proposed open-set inference. First, we perform inference with the primary dense prediction model. We recover the OOD map according to our divergence-based score (JSD). Closed-set predictions are overridden (white pixels in the final open-set output) wherever the OOD score exceeds the threshold ?. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense prediction model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmarks and Datasets</head><p>Benchmarking anomaly detection performance in road driving scenarios has experienced significant progress in recent years. Early approach <ref type="bibr" target="#b14">[15]</ref> pasted anomalies at a random position atop a road driving scene (cf. <ref type="figure">Figure 4, left)</ref>. This was further improved by a more advanced pasting strategy which adapts shadows and lighting <ref type="bibr" target="#b11">[12]</ref> (cf. <ref type="figure">Figure 4</ref>, center). Recent datasets avoid object pasting. Instead, they use real images in which the anomalies are fully-aligned with the environment <ref type="bibr" target="#b38">[39]</ref> (cf. <ref type="figure">Figure 4, right)</ref>. In parallel, significant effort has been invested into artificial datasets by leveraging simulated environments <ref type="bibr" target="#b39">[40]</ref>. A simulated environment provides more freedom in insertion and manipulation of anomalies.</p><p>WD-Pascal 1 <ref type="bibr" target="#b14">[15]</ref> is an anomaly detection dataset created by pasting Pascal VOC objects into WildDash <ref type="bibr" target="#b5">[6]</ref> images. WildDash images are challenging to segment due to difficult scenarios they capture. The resulting dataset al- <ref type="figure">Figure 4</ref>: Advancement of dense anomaly detection datasets through time. Early work pastes objects to random locations <ref type="bibr" target="#b14">[15]</ref>. This was further improved by carefully choosing pasting locations and postprocessing <ref type="bibr" target="#b11">[12]</ref>. Recent work ensures that anomalies match the environment by selecting adequate real-world scenes <ref type="bibr" target="#b38">[39]</ref>. Best viewed in color.</p><p>lows to evaluate anomaly detection in demanding conditions. However, the random pasting policy disturbs the scene as shown in <ref type="figure">Figure 4</ref> (left). Consequently, there is a possibility that such anomalies may be easier to detect.</p><p>Fishyscapes <ref type="bibr" target="#b11">[12]</ref> evaluates model's ability to detect anomalies in urban driving scenarios. The benchmark consists of two datasets: FS LostAnd-Found and FS Static. FS LostAndFound is a small subset of original Lo-stAndFound <ref type="bibr" target="#b40">[41]</ref> which contains small objects (e.g. toys, boxes or car parts that could fall off) on the roadway. FS Static contains Cityscapes validation images overlaid with Pascal VOC objects. The objects are positioned to mimic real world and further postprocessed to obtain smoother anomaly injection into the scene.</p><p>SegmentMeIfYouCan (SMIYC) <ref type="bibr" target="#b38">[39]</ref> quantifies dense anomaly detection performance in multiple setups. The benchmark consists of three datasets: AnomalyTrack, ObstacleTrack and LostAndFound <ref type="bibr" target="#b40">[41]</ref>. AnomalyTrack provides large anomalous objects which are fully aligned with the environment. For instance, they have a leopard in the middle of a dirt road as shown in <ref type="figure">Fig.  4 (right)</ref>. LostAndFound <ref type="bibr" target="#b40">[41]</ref> tests detection of small hazardous objects (e.g. boxes, toys, car parts, etc.) in urban scenes. Finally, ObstacleTrack tests detection of small objects on various road types. Inconsistent road surfaces can trick the detector and increase the false positive rate. Consequently, SMIYC provides a solid notion on anomaly segmentation performance of a model deployed in the wild.</p><p>StreetHazards <ref type="bibr" target="#b39">[40]</ref> is a synthetic dataset created with the CARLA game engine. The dataset captures simulated urban environment with carefully inserted anomalous objects (e.g. a horse carriage or a helicopter parked on the road). Simulating anomalies in virtual environments is appealing due to high flexibility in positioning and appearance of anomalies and low cost of data accumulation. Unfortunately, there is a notable quality mismatch between the simulated environment and the real world. Still, this approach has a great potential for evaluating various forms of anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics</head><p>We measure anomaly segmentation performance using average precision (AP) and false-positive rate at true-positive rate of 95% (FPR 95 ) <ref type="bibr" target="#b11">[12]</ref>. AP is well suited for measuring the anomaly detection performance since it emphasizes the detection of the minority class. A perfect classifier would have AP equal to one. Likewise, FPR 95 is significant for real-world applications since high false-positive rates would require a large number of human interventions in practical deployments and therefore severely diminish the practical value of an autonomous system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation details</head><p>All our models are based on the Ladder-DenseNet (LDN) architecture due to its memory efficiency and near real-time performance on 2MPix <ref type="bibr" target="#b34">[35]</ref>. However, our framework can accommodate any other dense prediction architecture. All our experiments consist of two training stages. In both stages we utilize Cityscapes, Vistas and Wilddash 2 <ref type="bibr" target="#b5">[6]</ref>. These three datasets contain 25 231 images. The images are resized to 1024 pixels (shorter edge), randomly flipped with the probability of 0.5, randomly resized in the interval [0.5, 2], and randomly cropped to 768 ? 768 pixels. We optimize our models with Adam optimizer. In the first stage we train for 25 epochs without synthetic anomalies. We use batch size 16 as validated in previous work <ref type="bibr" target="#b34">[35]</ref>. The starting learning rate is set to 10 ?4 for the feature extractor and 4 ? 10 ?4 for the upsampling path. The learning rate is annealed according to a cosine schedule to the minimal value of 10 ?7 which would have been reached in the 50th epoch. In the second stage we train for 15 epochs on mixed content images, as described in Section 3.1. In this stage we use batch size 12 since this is the maximum which could fit to one GPU with 24 GB RAM. Note that we use gradient checkpointing <ref type="bibr" target="#b34">[35]</ref>. The initial learning rate is set to 1 ? 10 ?5 for the upsampling path and 2.5 ? 10 ?6 for the backbone. Once more the learning rate is decayed according to the cosine schedule to the value of 10 ?7 . We set the hyperparameter ? to 3 ? 10 ?2 . This value is chosen in a way that the performance on the primary segmentation task is not reduced.</p><p>Our normalizing flow generates rectangular anomalous patches with random spatial dimensions from the range <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">216]</ref>. The flow is pretrained on random 64 ? 64 crops from Vistas. We train the flow with the Adamax optimizer with learning rate set to 10 ?6 .</p><p>In the case of WD-Pascal, we train our model only on Vistas in order to achieve a fair comparison with the previous work <ref type="bibr" target="#b14">[15]</ref>. In the case of Street-Hazards, we train on the corresponding train subset for 80 epochs on inlier images and 40 epochs on mixed content images. In the case of Fishyscapes, we train exclusively on Cityscapes. We train for 150 epochs during stage 1 (inliers) and 50 epochs during stage 2 (mixed content). All other hyperparameters are kept constant across all experiments. Each experiment lasts for approximately 38 hours on a single NVIDIA RTX A5000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental evaluation</head><p>We evaluate our method on WD-Pascal, Fishyscapes, SegmentMeIfY-ouCan, and StreetHazards. We compare our performance with respect to contemporary methods which do not require the negative dataset nor image resynthesis. Still, we list all methods in our tables, so we can discuss our method in a broader context. We also analyze the sensitivity of our method with respect to the distance of the anomaly from the camera. Finally, we measure the computational overhead of our method with respect to the baseline segmentation model, visualize our synthetic anomalies and ablate the proposed contributions. <ref type="table">Table 1</ref> presents performance on WD-Pascal over 50 runs <ref type="bibr" target="#b14">[15]</ref>. The bottom section compares our method with early approaches: MC dropout <ref type="bibr" target="#b9">[10]</ref>, ODIN <ref type="bibr" target="#b23">[24]</ref>, and max-softmax <ref type="bibr" target="#b20">[21]</ref>. These approaches are not competitive with the current state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Performance evaluation on WD-Pascal</head><p>The top section shows that training with auxiliary negative data can significantly improve the performance. However, our method closes the performance gap. It outperforms all other methods in FPR95 and AUROC metrics while achieving competitive AP. <ref type="table">Table 2</ref> presents performance evaluation on the Fishyscapes benchmark. Our synthetic negative data succeed to tie-up the score with negative training Method Aux data AP ? FPR 95 ? AUROC ? OOD head <ref type="bibr" target="#b14">[15]</ref> 34.9 ? 6.8 40.9 ? 3.9 88.8 ? 1.6 Max softmax <ref type="bibr" target="#b14">[15]</ref> 33.8 ? 5.1 35.5 ? 3.4 91.1 ? 1.0 Void classifer <ref type="bibr" target="#b11">[12]</ref> 25.6 ? 5.5 44.2 ? 4.7 87.7 ? 1.7 MC Dropout 0.2 <ref type="bibr" target="#b9">[10]</ref> 9.7 ? 1.2 41.1 ? 3.7 86.0 ? 1.2 MC Dropout 0.5 <ref type="bibr" target="#b9">[10]</ref> 8.2 ? 1.1 58.2 ? 5.6 80.3 ? 1.6 ODIN <ref type="bibr" target="#b23">[24]</ref> 6.0 ? 0.5 53.7 ? 7.0 79.9 ? 1.5 Max softmax <ref type="bibr" target="#b20">[21]</ref> 5.0 ? 0.5 48.8 ? 4.7 78.7 ? 1.5 NFlowJS (ours)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance evaluation on the Fishyscapes benchmark</head><p>30.2 ? 4.1 32.3 ? 5.9 92.3 ? 1.3 <ref type="table">Table 1</ref>: Performance evaluation on WD-Pascal <ref type="bibr" target="#b14">[15]</ref>. Our method outperforms all other methods in FPR 95 and AUROC while achieving competitive AP.</p><p>data on FS LostAndFound. Our method outperforms all previous methods in terms of FPR 95 while achieving the second best AP, by slightly underperforming only with respect to SynBoost which trains on real negative data and precludes real-time inference. The last section of the table shows a variant of our method which focuses on the ground during inference (GF stands for ground focus). We define the ground as a convex hull which covers all pixels predicted as the class road or the class sidewalk. This change entails a huge improvement in both metrics.</p><p>In the case of FS Static dataset, our method achieves the best FPR 95 and the second best AP among the methods which do not train on auxiliary data. Training on auxiliary negative data may result in overoptimistic performance evaluation due to possible alignment between negative training data and actual anomalies in test images. <ref type="table" target="#tab_3">Table 3</ref> presents performance evaluation on SMIYC benchmark. Our method outperforms all previous methods which use image resynthesis <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7]</ref>, partial image reconstruction <ref type="bibr" target="#b30">[31]</ref> or auxiliary data utilization <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance evaluation on SegmentMeIfYouCan</head><p>Our method achieves very low FPR 95 (less than 1%) on ObstacleTrack and LostAndFound-noKnown. This is especially important for real-world applications where a high incidence of false positives may make the anomaly detection useless. Note that ObstacleTrack includes small anomalies in front of a variety of road surfaces, which makes it extremely hard not to misclassify road parts as anomalies. Moreover, this dataset includes low-visibility images captured at dusk and other challenging evaluation setups. More detailed performance evaluation on SMIYC benchmark is available in Appendix A.  <ref type="table">Table 2</ref>: Results on the Fishyscapes benchmark. On FS L&amp;F our method yields the best FPR 95 and the second best AP. On FS Static our method achieves the best FPR 95 and the second best AP among methods which do not use the negative data. <ref type="figure" target="#fig_2">Figure 5</ref> shows the performance of the proposed method on two sequences of images from SMIYC LostAndFound dataset. Road groundtruth is designated in gray and the predicted anomaly in yellow. The top sequence contains an anomalous object which changes position through time. The bottom sequence contains multiple anomalous objects. Our method succeeds to detect a toy car and cardboard boxes even though no such objects were present during the training. <ref type="figure" target="#fig_3">Figure 6</ref> shows performance of our method in two night scenes with overexposure due to car headlights. Both images belong to the ObstacleTrack test set. We show our dense anomaly score in the second column. The third column shows the fused open-set segmentation map with anomalous pixels designated in white. We use the OOD threshold which gives less than one percent FPR on the corresponding validation set. <ref type="table">Table 4</ref> presents performance evaluation on StreetHazards. To the best of our knowledge, our method strongly outperforms all previous work. In particular, our method is better than methods based on conditional random fields <ref type="bibr" target="#b39">[40]</ref>, image resynthesis <ref type="bibr" target="#b29">[30]</ref> and ensemble of networks formed by sampling the weights distribution accumulated during the training <ref type="bibr" target="#b42">[43]</ref>.   <ref type="table">Table 4</ref>: Performance on the StreetHazards <ref type="bibr" target="#b39">[40]</ref> dataset. Our method outperforms all other methods for dense anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Performance evaluation on StreetHazards</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Sensitivity of anomaly detection to distance from the camera</head><p>We analyze sensitivity of anomaly detection to distance from the camera on the LostAndFound dataset which is included both in Fishyscapes and SMIYC. The test set consists of 1203 images with the corresponding pixellevel disparity maps based on which we compute the distances from the ego-vehicle. Due to limitations in the available disparity, we perform our analysis in range between 5 and 60 meters from the camera. <ref type="figure" target="#fig_4">Figure 7</ref> (left) shows histograms of inlier and outlier pixels. The figure shows that 62.5% of anomalous pixels are closer than 15 meters. Hence, the usual metrics are biased towards closer ranges. Indeed, as we further demonstrate, many methods fail to detect anomalies at longer distances.</p><p>We compare our method with the max-logit and max-softmax <ref type="bibr" target="#b20">[21]</ref> baselines, ODIN <ref type="bibr" target="#b23">[24]</ref>, SynBoost <ref type="bibr" target="#b6">[7]</ref> and OOD head <ref type="bibr" target="#b14">[15]</ref> which trains on noisy negative data. <ref type="table">Table 5</ref> shows that these approaches produce high false positive rates, even at small ranges, which makes them practically useless.  <ref type="table">Table 5</ref>: Analysis of FPR at TPR of 95% at various distances from camera. Our method is a great candidate for real-world applications due to very low FPR rates at all distances. <ref type="figure" target="#fig_4">Figure 7</ref> (right) stratifies the relative performance of our method with respect to distance of the anomalies. Our method overcomes all previous work by a wide margin at all distances except at close range (5-10m) where we perform comparably with SynBoost <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Inference speed</head><p>A convenient dense anomaly detector should not drastically increase already heavy computational burden of semantic segmentation. Hence, we measure computational overhead of our method and compare it with other approaches. We measure the inference speed on NVIDIA RTX 3090 for 1024 ? 2048 inputs. <ref type="table">Table 6</ref> shows that SynBoost <ref type="bibr" target="#b6">[7]</ref> and SynthCP <ref type="bibr" target="#b29">[30]</ref>  are not applicable for real-time inference since they require more than one second. The baseline model LDN-121 <ref type="bibr" target="#b34">[35]</ref> achieves near real-time inference for two megapixel images (46.5 ms, 21.5 FPS). ODIN <ref type="bibr" target="#b23">[24]</ref> requires an additional forward-backward pass in order to recover the gradients of the loss with respect to the image. This results in a 3-fold slow-down with respect to the baseline. Similarly, MC Dropout <ref type="bibr" target="#b9">[10]</ref> requires K forward passes for prediction with K MC samples. This results in 45.8 ms overhead when only 2 MC samples are used. Contrary, our approach increases the inference time for only 7.8 ms with respect to the baseline while outperforming all previous approaches. Note that the results for SynthCP are taken from <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Resynth. Infer. Time (ms) Overhead (ms) FPS SynthCP <ref type="bibr" target="#b29">[30]</ref> 146.9 -6.8 SynBoost <ref type="bibr" target="#b6">[7]</ref> 1055.5 -&lt; 1 LDN-121 (Base) <ref type="bibr" target="#b34">[35]</ref> 46.5 -21.5 Base + ODIN <ref type="bibr" target="#b23">[24]</ref> 195.6 149.  <ref type="table">Table 6</ref>: Comparison of the inference speed for selected dense prediction approaches. Our method requires only 7.8ms overhead to produce the dense anomaly map, while the contemporary methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b9">10]</ref> are not applicable for real-time inference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Visualization of synthetic outliers</head><p>Our normalizing flow models are able to generate samples at multiple resolutions with the same model. The generated samples have a limited variety when compared to a typical negative dataset such as ImageNet. Still, training with them greatly reduces overconfidence since the model is explicitly trained to discriminate between the known and the unknown. <ref type="figure" target="#fig_5">Figure 8</ref> shows samples of a normalizing flow which was jointly trained as described in section 3.2. <ref type="table" target="#tab_7">Table 7</ref> analyzes the impact of the loss function (L neg ) and the anomaly score (s ? ) in negative pixels on large (AnomalyTrack val) and small anomalies (ObstacleTrack val). We separately validated the modulation factor ? for each choice of the negative loss, as well as the temperature parameter. We use T=10 for max-softmax and T=2 for divergence-based scoring functions. We report average performance over last three epochs. Row 1 shows the standard setting where the loss function is KL divergence between the uniform distribution and the softmax output <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14]</ref> while the anomaly score is maxsoftmax. Row 2 features the KL divergence both as the loss function and the anomaly score. Row 3 features the reversed KL divergence. Minimizing the reversed divergence between the uniform distribution and the softmax distribution is equivalent to maximizing the softmax entropy. Rows 4 and 5 feature the JS divergence. We observe that the reverse KL divergence outperforms the standard setup in 3 out of 4 metrics. JS divergence substantially outperforms all alternatives both as the loss function (JSD-MSP vs KL-MSP) and as the anomaly score (JSD-JSD vs JSD-MSP and RKL-RKL). We explain this advantage with robust response in synthetic outliers which resemble inliers, as well as with improved consistency during training and scoring (cf. sections 3.2 and 3.3). <ref type="table">Table 8</ref> exhibits the impact of pre-training to anomaly detection performance. Row 1 shows the performance when neither generative nor discriminative model are trained prior to the joint training (Section 3.1). In this case, we jointly train both models from their respective initializations. Row 2 reveals that classifier pre-training improves the anomaly detection. Introducing the synthetic anomalies only after the initial training stage prevents overfitting. Row 3 shows that pre-training both models yields even better performance due to better quality of the generated anomalies. We also note   <ref type="table">Table 8</ref>: Impact of model pre-training to anomaly detection performance. It is beneficial to pre-train both models prior to joint training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">Impacts of the loss function and anomaly score</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9.">Impact of pre-training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.10</head><p>. Impact of temperature scaling <ref type="table">Table 9</ref> shows the impact of softmax recalibration to anomaly detection. The table shows the performance on SMIYC val with temperatures T=1, T=1.5 and T=2. We observe that temperature scaling significantly improves divergence-based scoring.  <ref type="table">Table 9</ref>: Impact of the softmax recalibration on dense anomaly detection. Temperature scaling improves our divergence-based anomaly detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented a novel method for dense anomaly detection and openset recognition. Our method trains on mixed-content images obtained by pasting synthetic negative patches into training images. We produce synthetic negatives by sampling a generative model which is jointly trained to maximize the likelihood and to produce uniform predictions at the far end of the discriminative model. Such collaborative learning leads to conservative unknown-aware predictions which are suitable for anomaly detection and open-set recognition.</p><p>We extend the previous work with the following contributions. First, we replace the adversarial generative model (GAN) with a normalizing flow. We believe that the resulting improvement is caused by better coverage of the training distribution. Second, we adapt the collaborative training setup for dense prediction. Generative flows are especially well-suited for this task due to straightforward generation at different resolutions. Third, we propose to prevent overfitting of the dicriminative model to synthetic anomalies by pre-training the normalizing flow and the discriminative model prior to joint training. Fourth, we propose to use JS divergence as a robust criterion for training a discriminative model with synthetic anomalies. We also show that the same criterion can be used as a principled and competitive replacement for ad-hoc scoring functions such as max-softmax.</p><p>We have evaluated the proposed method on standard benchmarks and datasets for dense anomaly detection. The results indicate state of the art performance at 6 out of 7 evaluation datasets, substantial advantage with respect to all previous approaches that do not train on real negative data, as well as a very low overhead with respect to the dense prediction baseline. Consequently, the proposed method is appropriate for real-time applications. Suitable avenues for future work include allowing our method to train on real negative data, and extending it to other dense prediction tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proposed training setup. The normalizing flow generates the synthetic negative patch x ? which we paste atop the raw inlier image x + . The resulting mixedcontent image x is fed to the dense classifier which is trained to discriminate inlier pixels (L cls ) and to raise uniform predictions in negative pixels (L neg ). This formulation enables gradient flow from L neg to the normalizing flow while maximizing the likelihood of inlier patches (L gen ). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>F-divergence towards uniform distribution in the two-class setting (left). Jensen-Shannon offers the most robust response. Histograms of ?L neg in synthetic negatives at the beginning of joint training in our setup (right). The modulation factors ? have been separately validated for each of the three choices of L neg . Jensen-Shannon divergence produces a more uniform learning signal than other f-divergences. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Anomaly detection on LostAndFound dataset. Our method can detect anomalies at different distances from the camera (top) as well as multiple anomalies in one image (bottom). Road groundtruth is designated in gray and the predicted anomaly in yellow. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Anomaly detection in two night scenes from SMIYC-ObstacleTrack. The first column shows input images. The second column shows our dense anomaly scores. The third column shows the fused open-set segmentation map. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Left: Histogram of LostAndFound test set pixels. Right: Analysis of FPR at TPR of 95% at various distances from ego-vehicle. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Samples of our generative model DenseFlow-25-6 which is jointly trained with our open-set segmentation model as proposed in section 3.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Anomaly detection performance on SMIYC. We outperform all other methods. Our method has low FPR which makes it especially suitable for real-world applications.</figDesc><table><row><cell>Method</cell><cell cols="4">Resyn. mIoU ? AP ? FPR 95 ? AUROC ?</cell></row><row><cell>SynthCP, t=1 [30]</cell><cell>-</cell><cell>8.1</cell><cell>46.0</cell><cell>81.9</cell></row><row><cell>SynthCP, t=0.999 [30]</cell><cell>-</cell><cell>9.3</cell><cell>28.4</cell><cell>88.5</cell></row><row><cell>Max softmax [21]</cell><cell>56.2</cell><cell>7.3</cell><cell>30.8</cell><cell>89.0</cell></row><row><cell>Dropout [10][30]</cell><cell>-</cell><cell>7.5</cell><cell>79.4</cell><cell>69.9</cell></row><row><cell>TRADI [43]</cell><cell>-</cell><cell>7.2</cell><cell>25.3</cell><cell>89.2</cell></row><row><cell>MSP + CRF [40]</cell><cell>-</cell><cell>6.5</cell><cell>29.9</cell><cell>88.1</cell></row><row><cell>S0+H [20]</cell><cell>-</cell><cell>12.7</cell><cell>25.2</cell><cell>91.7</cell></row><row><cell>NFlowJS (ours)</cell><cell>56.8</cell><cell>28.4</cell><cell>14.9</cell><cell>95.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Analysis of the loss function in negative pixels. The loss function based on JS-divergence outperforms the standard KL-divergence loss (row 4 vs row 1). Detecting anomalies using JSD scoring outperforms the standard ad hoc criterion (row 5 vs row 4).that utilizing RealNVP [19] instead of DenseFlow [38] reduces the sample quality and decreases the anomaly detection performance to 61.6% AP on AnomalyTrack val and 94.9% AP on ObstacleTrack val. ? 1.2 27.8 ? 2.1 90.5 ? 0.3 1.0 ? 0.1 2 61.4 ? 0.8 21.7 ? 1.3 94.9 ? 0.1 0.1 ? 0.1 3 63.3 ? 0.6 19.8 ? 0.8 95.8 ? 0.2 0.1 ? 0.0</figDesc><table><row><cell>#</cell><cell>Classif. pretrain. pretrain. Flow</cell><cell>AnomalyTrack-val AP FPR 95</cell><cell>ObstacleTrack-val AP FPR 95</cell></row><row><cell>1</cell><cell></cell><cell>56.9</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>? 0.5 40.0 ? 0.8 92.6 ? 0.3 1.1 ? 0.1 2 T=1.5 62.7 ? 0.6 23.7 ? 0.9 95.3 ? 0.2 0.2 ? 0.0 3 T=2 63.3 ? 0.6 19.8 ? 0.8 95.8 ? 0.2 0.1 ? 0.0</figDesc><table><row><cell cols="2"># Temperature</cell><cell>AnomalyTrack-val AP FPR 95</cell><cell>ObstacleTrack-val AP FPR 95</cell></row><row><cell>1</cell><cell>T=1</cell><cell>59.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>AP ? FPR 95 ? sIoU-gt ? PPV ? F1 ?Table A.12: Anomaly detection performance on SMIYC LostAndFound.</figDesc><table><row><cell>Aux Img data rsyn. SynBoost [7]</cell><cell cols="2">Pixel Level 81.7 4.6</cell><cell cols="3">Component Level 36.8 72.3 48.7</cell></row><row><cell>Void Classifier [12]</cell><cell>4.8</cell><cell>47.0</cell><cell>1.8</cell><cell>35.1</cell><cell>1.9</cell></row><row><cell>Maxim. Ent. [13]</cell><cell>77.9</cell><cell>9.7</cell><cell>45.9</cell><cell>63.1</cell><cell>49.9</cell></row><row><cell>Image Resyn. [8]</cell><cell>57.1</cell><cell>8.8</cell><cell>27.2</cell><cell>30.7</cell><cell>19.2</cell></row><row><cell>Road Inpaint [9]</cell><cell>82.9</cell><cell>35.8</cell><cell>49.2</cell><cell>60.7</cell><cell>52.3</cell></row><row><cell>Max softmax [21]</cell><cell>30.1</cell><cell>33.2</cell><cell>14.2</cell><cell>62.2</cell><cell>10.3</cell></row><row><cell>MC Dropout [10]</cell><cell>36.8</cell><cell>35.6</cell><cell>17.4</cell><cell>34.7</cell><cell>13.0</cell></row><row><cell>ODIN [24]</cell><cell>52.9</cell><cell>30.0</cell><cell>39.8</cell><cell>49.3</cell><cell>34.5</cell></row><row><cell>Embed. Dens. [12]</cell><cell>61.7</cell><cell>10.4</cell><cell>37.8</cell><cell>35.2</cell><cell>27.6</cell></row><row><cell>JSRNet [31]</cell><cell>74.2</cell><cell>6.6</cell><cell>34.3</cell><cell>45.9</cell><cell>36.0</cell></row><row><cell>NFlowJS (ours)</cell><cell>89.3</cell><cell>0.7</cell><cell>54.6</cell><cell>59.7</cell><cell>61.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/pb-brainiac/semseg_od</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Jakob Verbeek for suggesting normalizing flows as a suitable tool for generating synthetic anomalies. This work has been supported by Croatian Science Foundation (grant IP-2020-02-5851 ADEPT), European Regional Development Fund and Gideon Brothers ltd (KK.01.2.1.02.0119 A-Unit).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Extended SMIYC results</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting future instance segmentation by forecasting convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient semantic segmentation with pyramidal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2020.107611</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107611</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th International Conference on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murschitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steininger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Dominguez</surname></persName>
		</author>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Wilddash -creating hazard-aware benchmarks</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pixel-wise anomaly detection in complex driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Biase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detecting the unexpected via image resynthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Nakka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Detecting road obstacles by erasing them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<idno>CoRR abs/2012.13633</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The fishyscapes benchmark: Measuring blind spots in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3119" to="3135" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Entropy maximization and meta classification for out-of-distribution detection in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gottschalk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simultaneous semantic segmentation and outlier detection in presence of domain shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-33676-9_3</idno>
	</analytic>
	<monogr>
		<title level="m">41st DAGM German Conference, DAGM GCPR 2019</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jesson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Key</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11409</idno>
		<title level="m">On feature collapse and deep kernel learning for single forward pass uncertainty</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wigington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<title level="m">Generative-discriminative feature representations for open-set recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition, CVPR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training confidence-calibrated classifiers for detecting out-of-distribution samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dense open-set recognition with synthetic outliers generated by real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grci?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?egvi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-ofdistribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De Rezende Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2012.256</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding failures in out-ofdistribution detection with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">38th International Conference on Machine Learning, ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Revealing distributional vulnerability of explicit discriminators by implicit generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR abs/2108.09976</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Towards open set deep networks</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hybrid models for open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th European Conference on Computer Vision ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with self-supervised learning and adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reducing network agnostophobia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dhamija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>G?nther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Synthesize then compare: Detecting failures and anomalies for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Road anomaly detection by partial image reconstruction with segmentation coupling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>?ipka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chumerin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">O</forename><surname>Reino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twelth International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient ladder-style densenets for semantic segmentation of large images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Generative adversarial networks</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive density estimation for generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Densely connected normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grci?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Grubi?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?egvi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Segmentmeifyoucan: A benchmark for anomaly segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rottmann</surname></persName>
		</author>
		<idno>CoRR abs/2104.14812</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Scaling out-of-distribution detection for real-world settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11132</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lost and found: detecting small road hazards for self-driving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems, IROS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Standardized max logits: A simple yet effective approach for identifying unexpected road obstacles in urban-scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Franchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aldea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dubuisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bloch</surname></persName>
		</author>
		<title level="m">TRADI: tracking deep neural network weight distributions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>16th European Conference on Computer Vision, ECCV</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<title level="m">Biography Matej Grci? received an M.Sc.degree from the Faculty of Electrical Engineering and Computing in Zagreb. He finished the master study program in Computer Science in 2020. He is pursuing his Ph.D. degree at Uni-ZG FER. His research interests include generative modeling and open-world recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">He is currently a full professor at Uni-ZG FER. His research interests focus on deep convolutional architectures for classification, object detection and tracking. Sini?a?egvi? received a Ph.D. degree in computer science from the University of</title>
		<imprint>
			<pubPlace>Zagreb, Croatia; Zagreb, Croatia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Petra Bevandi? received an M.Sc.degree from the Faculty of Electrical Engineering and Computing in Zagreb</orgName>
		</respStmt>
	</monogr>
	<note>He was a post-doctoral researcher at IRISA Rennes and also at TU Graz. He is currently a full professor at Uni-ZG FER. His research interests focus on deep convolutional architectures for classification and dense prediction</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">10: Anomaly detection performance on SMIYC AnomalyTrack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Table</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">11: Anomaly detection performance on SMIYC ObstacleTrack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Table</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
