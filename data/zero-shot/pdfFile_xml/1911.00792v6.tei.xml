<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Algorithm for Routing Capsules in All Domains with Sample Applications in Vision and Language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">A</forename><surname>Heinsen</surname></persName>
							<email>franz@glassroom.com</email>
						</author>
						<title level="a" type="main">An Algorithm for Routing Capsules in All Domains with Sample Applications in Vision and Language</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Building on recent work on capsule networks, we propose a new, general-purpose form of "routing by agreement" that activates output capsules in a layer as a function of their net benefit to use and net cost to ignore input capsules from earlier layers. To illustrate the usefulness of our routing algorithm, we present two capsule networks that apply it in different domains: vision and language. 1 The first network achieves new state-of-the-art accuracy of 99.1% on the smallNORB visual recognition task with fewer parameters and an order of magnitude less training than previous capsule models, and we find evidence that it learns to perform a form of "reverse graphics." The second network achieves new state-of-the-art accuracies on the root sentences of the Stanford Sentiment Treebank: 58.5% on fine-grained and 95.6% on binary labels with a single-task model that routes frozen embeddings from a pretrained transformer as capsules. In both domains, we train with the same regime.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Capsule networks with routing by agreement can be more effective than convolutional neural networks for segmenting highly overlapping images <ref type="bibr" target="#b23">(Sabour et al., 2017)</ref> and for generalizing to different poses of objects embedded in images and resisting white-box adversarial image attacks <ref type="bibr" target="#b16">(Hinton et al., 2018)</ref>, typically requiring fewer parameters but more training and computation.</p><p>A capsule is a group of neurons whose outputs represent different properties of the same entity in different contexts. Routing by agreement is an iterative form of clustering in which a capsule detects an entity by looking for agreement among votes from input capsules that have already detected parts of the entity in a previous layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Routing Algorithm</head><p>Here, we propose a new, general-purpose form of "EM routing" <ref type="bibr" target="#b16">(Hinton et al., 2018)</ref> that uses the expectation-maximization (EM) algorithm to cluster similar votes from input capsules to output capsules. Each output capsule iteratively maximizes the probability of input votes assigned to it, given its probabilistic model.</p><p>Our EM routing algorithm has multiple differences compared to previous ones. The most significant difference is that we compute each output capsule's activation by applying a logistic function to the difference between a net benefit to use and a net cost to ignore (i.e., not use) each input capsule. We compute the share of each input capsule used or ignored by each output capsule in a new procedure we call the D-Step, executed between the E-Step and M-Step of each EM iteration. We are motivated by the intuitive notion that "output capsules should benefit from the input data they use, and lose benefits from any input data they ignore,"</p><p>as they maximize the probability of votes from those input capsules they use in each EM iteration.</p><p>We simultaneously (a) optimize the entire layer for a training objective, (b) iteratively maximize the probability of input capsule votes each output capsule uses, and (c) iteratively maximize the probability of net input capsule benefits less costs in service of (a) and <ref type="bibr">(b)</ref>. We like to think of this mechanism as finding "the combination of net benefits and costs that produces greater profit," or, more colorfully, maximizing "bang per bit." Another significant difference of our routing algorithm, compared to previous ones, is that it accepts variable-size inputs, such as sequences of contextualized token embeddings in natural language applications. A contextualized token embedding is a special case of a capsule, one whose neuron outputs represent different properties of the same token id in different contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample Applications in Two Domains</head><p>For comparison with prior work on capsule networks, we evaluate our routing algorithm on the smallNORB visual recognition task <ref type="bibr" target="#b17">(LeCun et al., 2004)</ref>, in which objects must be recognized from stereo images with varying azimuths, elevations, and lighting conditions. We construct a capsule network that routes pixel convolutions as capsules to achieve new state-of-the-art accuracy of 99.1% on this task. Compared to the previous state of the art <ref type="bibr" target="#b16">(Hinton et al., 2018)</ref>, our smallNORB model has approximately 272,000 instead of 310,000 parameters, trains in 50 instead of 300 epochs, and accepts as input non-downsampled pairs of 96?96 images, instead of 32?32 downsampled crops that are nine times smaller. Also, we do not average over multiple crops per image to compute test accuracy. <ref type="figure" target="#fig_0">Fig. 1</ref> shows how our model compares to prior state-of-the-art models on two criteria.</p><p>We find evidence that our smallNORB model Our model  Figure 2: Test set accuracy and publication year of models that have achieved state-of-the-art results on SST-5/R root sentence classification.</p><p>learns to encode pose solely from pixel data and classification labels, i.e., it learns its own form of "reverse graphics" without us explicitly having to optimize for it. See the visualization in <ref type="bibr">Fig. 4 (p. 9</ref>) and the 24 plots and corresponding captions in Supp. <ref type="bibr">Fig. 6 (p. 14)</ref> and Supp. <ref type="bibr">Fig. 7 (p. 15)</ref>. For illustration of the general-purpose nature of our routing algorithm, we also evaluate it on a natural language task: classifying the root sentences of the Stanford Sentiment Treebank  into fine-grained (SST-5/R) and binary (SST-2/R) labels. (We add the "/R" designation to distinguish these root-sentence tasks from classification of phrases in the parse trees, because we have seen research that does not.)</p><p>For evaluation of our algorithm on SST-5/R and SST-2/R, we construct a capsule network with only approximately 140,000 parameters that routes frozen token embeddings from a pretrained transformer <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref> as capsules. In our implementation, we use a GPT-2 large <ref type="bibr" target="#b21">(Radford et al., 2018)</ref> as the pretrained transformer, the largest such model publicly available at the time of writing. Our SST model achieves new state-ofthe-art test set accuracy of 58.5% on SST-5/R, and new state-of-the-art test set accuracy for singletask models of 95.6% on SST-2/R. <ref type="figure">Fig. 2</ref> shows how our SST model compares to prior state-ofthe-art models on SST-5/R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation</head><p>As we show here, we can achieve state-of-the-art results in more than one domain by stacking one or more layers of our routing algorithm atop, or between, blocks of non-iterative layers (e.g., convolutional, self-attention, LSTM). Our motivation is to develop universal, composable learning algorithms that can adapt to any application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notation</head><p>We use tensor notation, with all operations performed element-wise (Hadamard), implicitly assuming conventional broadcasting for any missing indexes. This notation is both more succint than linear algebra's and more natural to implement with frameworks like PyTorch and Tensor-Flow. For extra clarity, we do not use implicit (Einstein) summations nor raised (contravariant) indexes. See Tab. 1 for examples of our notation and their implementation in PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Implementation in PyTorch</p><formula xml:id="formula_0">Aij + B ijk A.unsqueeze(-1) + B AijB ijk A.unsqueeze(-1) * B or einsum("ij,ijk-&gt;ijk", A, B) i AijB ijk einsum("ij,ijk-&gt;jk", A, B)</formula><p>ij AijB ijk einsum("ij,ijk-&gt;k", A, B) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Routing Algorithm</head><p>For brevity, we assume familiarity with both capsule networks and the matrix EM routing algorithm proposed by <ref type="bibr" target="#b16">Hinton et al. (2018)</ref>, so we focus our discussion here mainly on those aspects of our work that are new. Also, while our algorithm generalizes to any probabilistic model that can be used in an expectation-maximization loop, we restrict our discussion here only to one case: a multidimensional Gaussian model. As shown in Alg. 1, for a given sample, our algorithm dynamically routes n (inp) input capsules to n (out) output capsules, where n (inp) is specified in advance if samples are of fixed size or left unspecified if samples are of variable size. Input capsules are tensors of size d (cov) ? d <ref type="bibr">(inp)</ref> , and output capsules are tensors of size d (cov) ? d <ref type="bibr">(out)</ref> .</p><p>Per sample, we accept as input two tensors: scores and capsules (a <ref type="bibr">(inp)</ref> i , ? <ref type="bibr">(inp)</ref> icd ). We return as output three tensors: scores, capsules, and variances (a (out) j , ? <ref type="bibr">(out)</ref> jch , (? (out) jch ) 2 ). The indexes are i = (1, 2, . . . , n (inp) ), j = (1, 2, . . . , n (out) ), <ref type="bibr">1, 2, . . . , d (out)</ref> ).</p><formula xml:id="formula_1">c = (1, 2, . . . , d (cov) ), d = (1, 2, . . . , d (inp) ), h = (</formula><p>Intuitively, we can think of n (inp) as the number of detectable parts, n (out) as the number of detectable entities, each consisting of or associated with one or more parts, d (cov) as the dimension of the covector space, or dual, of the spaces in which parts and entities have properties, and d (inp) and d (out) as the dimensions of part and entity properties, respectively, in those spaces.</p><p>For example, if we wanted to detect, say, dogs and cats embedded in images, from 64 detectable animal parts, the values of n (inp) , n (out) , d (cov) , d (inp) , and d (out) would be 64, 2, 4, 4, and 4, respectively. Each of the 64 input capsules and 2 output capsules would be a 4 ? 4 matrix capable of representing the spatial relationship between the viewer of an image and objects embedded in the image.</p><p>In other domains, the dimensions d (cov) of the dual space, d (inp) of part properties, and d (out) of entity properties may be very different.</p><p>When n (inp) is unspecified, the number of detectable parts is variable. In that case, it might be desirable for input capsules themselves to have properties that represent their type and/or position. This is commonly done, for example, in language models, which add relative or absolute position information to token embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Votes</head><p>Before starting the routing loop, we compute votes from each input to each output capsule,</p><formula xml:id="formula_2">V ijch = d W ijdh ? (inp) icd + B ijch , if n (inp) fixed d W jdh ? (inp) icd + B jch , otherwise</formula><p>(1) where V ijch is a tensor of votes computed from the ith input capsule to the jth output capsule for each component ch of the output capsule, and W ijdh and B ijch (or W jdh and B jch , if n (inp) is unspecified) are parameters. We perform tensor contraction on index d and compute element-wise Algorithm 1: Our Routing Algorithm. Per sample, we route n (inp) input capsules of shape d (cov) ? d (inp) to n (out) output capsules of shape d (cov) ? d (out) , where n (inp) is the number of detectable entity parts and n (out) is the number of detectable entities. All tensor operations shown are element-wise (i.e., there are no Einstein summations) and implicitly assume conventional broadcasting for missing indexes. f is the logistic function. Superscript text in parenthesis, such as (inp) and (out) , denotes labels used for disambiguation. Tensor indexes are i = (1, 2, . . . , n (inp) ), j = (1, 2, . . . , n (out) ), c = (1, 2, . . . , d (cov) ), d = (1, 2, . . . , d (inp) ), h = (1, 2, . . . , d (out) ).</p><formula xml:id="formula_3">Input: (a (inp) i , ? (inp) icd ). Output: (a (out) j , ? (out) jch , (? (out) jch ) 2 ). 1 V ijch ?? d W ijdh ? (inp) icd + B ijch , if n (inp) fixed d W jdh ? (inp) icd + B jch , otherwise ; 2 for n (iters) iterations do 3 begin E-Step 4 if on first iteration then 5 R ij ?? 1 n (out) ; 6 else 7 P ij ?? 1 ch 2? ? (out) jch 2 exp ? ch V ijch ?? (out) jch 2 2 ? (out) jch 2 ; 8 R ij ?? f a (out) j P ij j f a (out) j P ij ; 9 end 10 end 11 begin D-Step 12 D (use) ij ?? f (a (inp) i )R ij ; 13 D (ign) ij ?? f (a (inp) i ) ? D (use) ij ; 14 end 15 begin M-Step 16 a (out) j ?? i ? (use) ij D (use) ij ? i ? (ign) ij D (ign) ij , if n (inp) fixed i ? (use) j D (use) ij ? i ? (ign) j D (ign) ij , otherwise ; 17 ? (out) jch ?? i D (use) ij V ijch i D (use) ij ; 18 (? (out) jch ) 2 ?? i D (use) ij V ijch ?? (out) jch 2 i D (use) ij</formula><p>; 19 end operations, as indicated, along indexes i, j, c, and h, with conventional broadcasting implicitly assumed for any missing dimensions. For each input capsule i, we obtain a different jch slice of votes for the jth output capsule, breaking symmetry. Intuitively, the computation of V ijch in (1) can be understood as n (inp) ? n (out) simultaneous matrix-matrix multiplications, each applying a d (out) ?d (inp) linear transformation to a d (inp) ?d (cov) transposed input capsule, followed by addition of biases and then another transposition, to obtain</p><formula xml:id="formula_4">n (inp) ? n (out) votes of size d (cov) ? d (out) .</formula><p>When n (inp) is left unspecified, we remove index i from the parameters used to compute V ijch , so we reduce their size by a factor of n (inp) . In this case, the computation of V ijch applies n (out) simultaneous matrix-matrix multiplications to each input capsule, followed by addition of n (out) corresponding ch biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Adapting to Variable-Size Outputs</head><p>A trivial adaptation of our algorithm, which we do not show, is to allow both n (inp) and n (out) to be unspecified, resulting in a variable number of input capsules voting for an equal number of output capsules. We can do this by removing indexes ij from W ijdh , reducing its size by a factor of n (inp) ?n <ref type="bibr">(out)</ref> . In that case, the computation of V ijch would apply the same linear transformation to all input capsules, and would have to break symmetry via other means (e.g., by adding different biases).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Routing Loop</head><p>Unlike previous versions of EM routing, which on each iteration compute first an M-Step and then an E-Step procedure, our algorithm computes these procedures in the conventional order: first the E-Step and then the M-Step. This ordering removes the final, superfluous E-Step from the loop, and also, we believe, simplifies exposition.</p><p>We also introduce a new procedure, which we call the D-Step, between the E-Step and M-Step. The D-Step computes the share of each input capsule's data used or ignored by each output capsule, for subsequent use in the computation of output scores a (out) j . These computations, described in 3.4 and 3.5, represent our most significant departure from previous forms of EM routing.</p><p>Another difference is that in our algorithm, a <ref type="bibr">(inp)</ref> i and a (out) j are "pre-activation" scores in the interval [??, ?] on which we apply logistic functions as needed, at the last minute as it were, to compute activations. This trivial modification facilitates more flexible use of the "raw" values of a <ref type="bibr">(inp)</ref> i and a (out) j by subsequent layers and/or objective functions, with more numerical stability. For example, a subsequent layer can apply a Softmax function to the output scores a (out) j to induce a distribution over output capsule activations.</p><p>In the following subsections, we describe the computations performed by the E-Step, D-Step, and M-Step on each iteration, in order of execution, emphasizing those computations which are new or different with respect to previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">E-Step</head><p>At the start of each iteration, for each sample, our E-Step computes an n (inp) ? n (out) tensor R ij of routing probabilities for assigning each input capsule i to each output capsule j,</p><formula xml:id="formula_5">R ij = ? ? ? ? ? ? ? ? ? 1 n (out) , on first iteration f a (out) j P ij j f a (out) j P ij , otherwise (2)</formula><p>where f is the logistic function, a (out) j is the jth output score computed in the previous iteration's M-Step, and P ij stands for</p><formula xml:id="formula_6">ch P V ijch V (out) jch ? N ? (out) jch , (? (out) jch ) 2 ,</formula><p>(3) the products of the probability densities of input capsule i's votes for output capsule j's ch components, given output capsule j's Gaussian model, updated in the previous iteration's M-Step, as in other forms of EM routing, except that in our case votes have two indexes, ch, instead of one. See Alg. 1 for computation. Informally, we can think of P ij as "the probability of votes from input capsule i given capsule j's probabilistic model."</p><p>In our implementation, for numerical stability, we compute R ij after the first iteration by applying a Softmax function to simplified log-sums, instead of using the second equation in (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">D-Step</head><p>At the beginning of each D-Step, we multiply the assigned routing probabilities R ij by logistic function activations of input scores, which act as gates, to obtain D <ref type="bibr">(use)</ref> ij , the share of data used from each input capsule i to update each output capsule j's Gaussian model,</p><formula xml:id="formula_7">D (use) ij = f a (inp) i R ij ,<label>(4)</label></formula><p>where f is the logistic function and a <ref type="bibr">(inp)</ref> i is the input score associated with input capsule i. Except for the "last-minute" application of the logistic function, (4) is the same as its corresponding equation in previous forms of EM routing. However, our notation explicitly differentiates between R ij , the routing probabilities, and D <ref type="bibr">(use)</ref> ij , the share of capsule i's data used by capsule j.</p><p>Each row of routing probabilities R ij adds up to 1, and f maps [??, ?] to [0, 1]; therefore,</p><formula xml:id="formula_8">0 ? D (use) ij ? f a (inp) i ? 1,<label>(5)</label></formula><p>that is, D <ref type="bibr">(use)</ref> ij has values that range from 0 ("completely ignore input capsule i in output capsule j's model") to 1 ("use the whole of input capsule i in output capsule j's model"), but never exceeds f (a (inp) i ) ("how much of input capsule i can we use among all output capsules?").</p><p>Given these relationships, we can compute the share of data ignored (i.e., not used) D (ign) ij from each input capsule i by each output capsule j,</p><formula xml:id="formula_9">D (ign) ij = f a (inp) i ? D (use) ij ,<label>(6)</label></formula><p>such that for each input and output capsule pair ij the two shares, D (use) ij and D <ref type="bibr">(ign)</ref> ij , plus the data that is "gated off" by logistic activation of the corresponding input score, 1 ? f (a (inp) i ), add up to 1, or the whole input capsule,</p><formula xml:id="formula_10">D (use) ij + D (ign) ij + 1 ? f a (inp) i = 1,<label>(7)</label></formula><p>accounting for all input data. Output capsules are thus in a competition with each other to use "more valuable bits," and ignore "less valuable bits," of input data. Each output capsule can improve its use-ignore shares only at the expense of other output capsules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">M-Step</head><p>The M-Step computes updated output scores a (out) j and weighted means and variances ? <ref type="bibr">(out)</ref> jch and (? (out) jch ) 2 , respectively, to maximize the probability that each output capsule j's Gaussian model would generate the votes computed from each input capsule i used by j. We discuss these computations in the subsections that follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Output Scores</head><p>Previous forms of EM routing use the minimum description length principle to derive approximations of the cost to activate and the cost not to activate each output capsule j, and compute output activations by applying a logistic function to the difference between those approximations. Such costs must be approximated because the only known method for accurately computing them would require inverting n (inp) ?n (out) vote-computation matrices, which is impractical. See <ref type="bibr" target="#b16">Hinton et al. (2018)</ref> for details.</p><p>We use a different approach, motivated by the intuitive notion that "output capsules should benefit from the input data they use, and lose benefits from any input data they ignore,"</p><p>as they maximize the probability of votes from those input capsules they use in each iteration.</p><p>For each output capsule j, we compute output score a (out) j as the difference of a net benefit to use and a net cost to ignore input capsule data,</p><formula xml:id="formula_11">a (out) j = ? ? ? ? ? ? ? ? ? ? ? i ? (use) ij D (use) ij ? i ? (ign) ij D (ign) ij , if n (inp) fixed i ? (use) j D (use) ij ? i ? (ign) j D (ign) ij , otherwise (8) where D (use) ij and D (ign)</formula><p>ij are the shares of input capsule i's data used and ignored by output capsule j, computed in <ref type="formula" target="#formula_7">(4)</ref> and <ref type="formula" target="#formula_9">(6)</ref>, respectively, and ? <ref type="bibr">(use)</ref> ij and ? (ign) ij (or ? (use) j and ? (ign) j , if n (inp) is unspecified) are parameters representing each output capsule's net benefit to use and net cost to ignore input capsule data, respectively. The adjective "net" denotes that ? (use) and ? (ign) can have positive ("credits") or negative ("debits") values.</p><p>For certain tasks, we may want the net cost of ignoring each input capsule to be equal to the net benefit we lose from not using it. We can accomplish this trivially by setting ? (ign) = ? (use) for all ij, making them one and the same parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Output Capsule Probabilistic Models</head><p>At the end of each M-Step, we compute updated means ? <ref type="bibr">(out)</ref> jch and variances (? (out) jch ) 2 of every output capsule j's Gaussian model, weighted by D <ref type="bibr">(use)</ref> ij , the amount of data the output capsule uses from each input capsule i. See Alg. 1 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Motivation and Intuitition</head><p>In the next iteration's E-Step, when we activate a (out) j by applying to it a logistic function f in <ref type="formula">(2)</ref>, we induce in each output capsule a distribution (f (a (out) j ), 1 ? f (a (out) j )) over a quantity equal to (a) the output capsule's net benefits from those input capsules it uses, less (b) its net costs (or lost benefits) from those input capsules it ignores.</p><p>When we recompute routing probabilities R ij in (2), we weight P ij , the probability of votes from input capsules each output capsule uses, by f (a (out) j ), the probability of net benefits less costs from using those input capsules, jointly maximizing both probabilities in the EM loop for optimization of a training objective specified elsewhere.</p><p>Informally, we can think of this multi-faceted mechanism as finding, for each output capsule, "the combination of net benefits and costs that produces greater profit," where "greater profit" stands for maximizing input vote probabilities at each output capsule and optimizing the whole layer for another objective. We prefer to think of it as maximizing "bang per bit."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4">Relationship to Description Length</head><p>There is an interesting connection between our activation mechanism and that used by previous forms of EM routing: All else remaining equal, at each output capsule, using more data from an input capsule is associated with greater description length, and using less data from the input capsule is associated with the opposite-by definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Sample Application: smallNORB</head><p>The smallNORB dataset <ref type="bibr" target="#b17">(LeCun et al., 2004)</ref> has grayscale stereo 96?96 images of five classes of toys: airplanes, cars, trucks, humans, and animals. There are 10 toys in each class, five selected for the training set and five for the test set. Each toy is photographed stereographically at 18 different azimuths (0-340 degrees), 9 different elevations, and 6 lighting conditions, such that the training and test sets each contain 24,300 pairs of images. Supp. <ref type="figure">Fig. 9</ref> shows samples from each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture</head><p>The architecture of our smallNORB model is described in detail in <ref type="figure" target="#fig_1">Fig. 3</ref>. At a high level of abstraction, we can think of the model as doing two things: First, it applies a sequence of standard convolutions to detect 64 toy parts and their 4 ? 4 poses (spatial relationships to the viewer of the image) in multiple possible locations in the image (steps (a) through (d)) in <ref type="figure" target="#fig_1">Fig. 3)</ref>. Then, the model applies two layers of our routing algorithm, one to detect 64 larger toy parts and their poses, and another to detect five categories of toys and their poses ( <ref type="figure" target="#fig_1">Fig. 3(e)</ref>). The routing layers are meant to induce the standard convolutions to learn to recognize toy parts and their poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Use of Variable-Size Inputs</head><p>To keep the number of parameters small, we leave n (inp) unspecified in the first routing layer, so it accepts a variable number of input capsules without regard for their location in the image. To counteract this loss of location information, we stack input images with two tensors of coordinate values evenly spaced from -1.0 to 1.0, one horizontally and one vertically, as shown in <ref type="figure" target="#fig_1">Fig. 3(b)</ref>.</p><p>Besides reducing the number of parameters in the first routing layer (by a factor of 64m n , as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>), our decision to accept a variable number of capsules in that first routing layer makes our model capable of accepting images of variable size, limited only by memory, though we do not make use of this capability here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Initialization and Training</head><p>We initialize all convolutions with Kaiming normal initialization <ref type="bibr" target="#b15">(He et al., 2015)</ref> and the two routing layers as follows: Normal initialization scaled by 1 d (inp) for the multilinear weights that compute votes, zeros for the bias parameters, and zeros for the net benefit and cost parameters. We train the model for 50 epochs, with a batch size of 20, using RAdam <ref type="bibr" target="#b18">(Liu et al., 2019)</ref> for optimization via stochastic gradient descent. We use a single-cycle hyperparameter scheduling policy in which learning rate r and first momentum ? 1 start at (r = 10 ?5 , ? 1 = 0.999), each change linearly to (r = 5 ? 10 ?4 , ? 1 = 0.9 ? 0.999) over the first 10% of training iterations, and then return to their respective starting values with a cosine shape over the remaining iterations.</p><p>During training, we add 16 pixels of padding on each side to each pair of images and randomly crop them to 96 ? 96 size. We do not use any other image processing, nor any metadata, nor any additional data in training.</p><p>For regularization, we use mixup <ref type="bibr" target="#b28">(Zhang et al., 2017)</ref> with Beta distribution parameters (0.2, 0.2), inducing the iterative EM clustering algorithms in We apply six 3?3 convolutions, each with 64 output channels and alternating strides of 1 and 2. Each convolution is preceded by batch normalization and followed by a Swish activation <ref type="bibr" target="#b22">(Ramachandran et al., 2017)</ref> with constant ? = 1. The last convolution outputs a tensor of shape 64?m ?n . (d) We compute a (inp) and ? (inp) by applying two 1?1 convolutions, with 64 and 1024 output channels, respectively, and reshape them as shown. Both convolutions are preceded by batch normalization. After reshaping, a (inp) consists of 64m n input scores, representing possible presence or absence of 64 toy parts in m n image locations. ? (inp) consists of 64m n slices of shape 4 ? 4, each representing a pose for one of 64 parts in m n locations. (e) We apply two layers of our routing algorithm; the first one routes a variable number of input capsules to 64 output capsules, each representing a larger toy part with a 4 ? 4 pose; the second one routes those capsules to five capsules, each representing a type of toy with a 4 ? 4 pose. For prediction, we apply a Softmax to a (out) .</p><p>our routing layers to learn to distinguish samples that have been mixed together.</p><p>The objective function is a Cross Entropy loss, computed on Softmax activations of the output scores of the final routing layer's five capsules. Supp. <ref type="figure">Fig. 8</ref> shows validation loss and accuracy after each epoch of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Tab. 2 shows test set accuracy, number of parameters, and number of epochs to train our small-NORB model, and how it compares to prior models that have achieved state-of-the-art results without using any metadata or additional data.</p><p>Compared to the previous state of the art (Hinton et al., 2018), our model has fewer parameters, trains in an order of magnitude fewer epochs, and accepts full-size unprocessed images instead of downsampled, cropped ones. We do not use multiple crops per image to compute test accuracy.</p><p>Compared to the best-performing conventional CNN on this benchmark <ref type="bibr" target="#b12">(Cire?an et al., 2011)</ref>, our model has an order of magnitude fewer parameters and is trained with minimal data augmentation (only cropping), whereas the CNN is trained with  additional stereo pairs of images created using different filters and affine distortions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>Besides superior test set performance, we find evidence that our smallNORB model learns to perform its own form of "reverse graphics" without explicitly optimizing for it, solely from pixel data and classification labels. The model learns to use all four pose vectors jointly to represent poses, albeit in a way that feels quite alien compared to the typical human approach (e.g., a 3 ? 3 rotation ma- trix inside a 4 ? 4 matrix with translation data). The visualization in <ref type="figure" target="#fig_2">Fig. 4</ref> shows multidimensional scaling (MDS) representations in R 2 of the trajectories of an activated class capsule's pose vectors as we feed test images of an object in the class with varying elevations to our model. We can see that the four pose vectors that constitute the class capsule jointly move and eventually seem to "flip" as we change viewpoint elevation. The same visualization for other objects in the dataset, and for varying azimuth, look qualitatively similar.</p><p>We also analyze quantitatively the behavior of pose vectors as we vary azimuth and elevation for every category and instance of toy in the dataset, and find that pose vectors behave in ways that are consistent with variation in azimuth and elevation. See the 24 plots and their captions in Supp. <ref type="figure">Fig. 6</ref> and Supp. <ref type="figure">Fig. 7</ref> for details.</p><p>Much more work remains to be done to under-stand and quantify our routing algorithm's ability to learn "reverse graphics." However, we think such work falls outside the scope of this paper, given that we also evaluate our routing algorithm in another domain, natural language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Sample Application: SST</head><p>The Stanford Sentiment Treebank (SST)  consists of 11,855 sentences extracted from movie reviews, parsed into trees with 215,154 unique phrases. The root sentences are split into training (8,544), validation (1,101) and test (2,210) sets, each with its own subset of the parse trees. The fine-grained root sentence classification task (SST-5/R) involves selecting one of five labels (very negative, negative, neutral, positive, very positive) for each root sentence in the test set. The binary root sentence classification task (SST-2/R) involves selecting one of two labels (negative, positive) after removing all neutral samples from the dataset, leaving 9,613 root sentences, split into training (6,920), validation <ref type="formula" target="#formula_10">(872)</ref>, and test (1,821) sets, each with their own subset of the parse trees. We chose SST, and SST-5/R in particular, for three reasons: First, its size is small enough to sidestep certain challenges to scaling EM routing (e.g., see <ref type="bibr" target="#b11">Barham and Isard (2019)</ref>). Second, since this dataset's release in 2013, no model has come close to human performance on SST-5/R, as measured by accuracy on its labels, which were assigned by human beings. Finally, we suspect SST-5/R has remained challenging because it is less susceptible than other benchmarks to the "Clever Hans" effect, in which seemingly impressive performance is explained by exploitation of spurious statistical cues in the data.</p><p>The Clever Hans effect has been documented in multiple natural language datasets, for example, by <ref type="bibr" target="#b19">McCoy et al. (2019)</ref> and <ref type="bibr" target="#b20">Niven and Kao (2019)</ref>. Models have become so good at recognizing patterns in natural language that human beings are now finding it difficult to design benchmarks that are free of spurious statistical cues.</p><p>Several qualities, we think, make it challenging for machines to find and exploit spurious statistical cues in SST-5/R: First, its labels map to sentiments that transition into each other in complicated ways (e.g., the boundary between neutral and positive sentences). Second, the dataset is small (e.g., only 2,210 test sentences). Third, For each sample, the input is a tensor of transformer embeddings of shape n ? l ? m, where n is the number of tokens, l is the number of transformer layers, and m is the embedding size. (b) We element-wise add to the input tensor a depth-of-layer parameter of shape l ? m. (c) We apply a linear transformation from m to 64, followed by a Swish activation with constant ? = 1 and layer normalization, obtaining a tensor of shape n ? l ? 64. (d) We reshape the tensor as shown to obtain ? (inp) , consisting of ln input capsules of size 1?64. We compute a (inp) ?? log( x 1?x ) from a mask x of length nl with ones and zeros indicating, respectively, which embeddings correspond to tokens and which correspond to any padding necessary to group samples in batches, obtaining logits that are equal to ? for tokens, ?? for padding, and values in between for any tokens and padding that get combined by mixup regularization in training. (e) We apply two layers of our routing algorithm; the first one routes a variable number of capsules in ? (inp) to 64 capsules of shape 1?2; the second one routes those capsules to five or two capsules of equal shape, each representing a classification label in SST-5/R or SST-2/R. For prediction, we apply a Softmax to output scores a (out) . the samples exhibit a variety of complex syntactic phenomena (e.g., nested negations). Finally, the samples exhibit diverse linguistic constructions (e.g., idiosyncratic movie fan idioms).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Architecture</head><p>The architecture of our SST model resembles that of our smallNORB model. We show and describe it in detail in <ref type="figure" target="#fig_3">Fig. 5</ref>. At a high level of abstraction, we can think of our SST model as doing two things: First, it applies a nonlinear transformation to every embedding from a pretrained transformer, mapping each one to a vector with 64 elements indicating present or absence of "sentiment features" (steps (a) through (d)) in <ref type="figure" target="#fig_3">Fig. 5)</ref>. Then, the model applies two layers of our routing algorithm, one to detect 64 composite parts, and another to detect classification labels, five for SST-5/R and two for SST-2/R ( <ref type="figure" target="#fig_3">Fig. 5(e)</ref>). The routing layers are meant to induce the nonlinear transformation to learn to recognize useful "sentiment parts."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Use of Variable-Size Inputs</head><p>The number of tokens in sentences is variable, so we leave n (inp) unspecified in the first routing layer of our SST model. This first routing layer accepts any number of input capsules without regard for their position in the sequence or the depth of the transformer layer from which they originate.</p><p>Transformer embeddings incorporate information about their position in a sequence, but not about layer depth. To counteract the loss of depth information, we add a "depth-of-layer" parameter to the input tensor, as shown in <ref type="bibr">Fig. 5(b)</ref>. In this parameter, each transformer layer has a corresponding vector slice, which we add element-wise to every embedding in the input tensor originating from that transformer layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Use of GPT-2</head><p>In our implementation, we use a GPT-2 large <ref type="bibr" target="#b21">(Radford et al., 2018)</ref> as the pretrained transformer. We chose this model mainly because it was the largest one publicly available at the time of writing, and also because we like the simplicity of its training objective: it is trained only to predict the next subword token in approximately 40GB of text.</p><p>GPT-2 large has approximately 774 million parameters and outputs 37 layers (36 hidden plus one visible) of dimension 1280. If a sentence contains, say, 10 tokens, this GPT-2 model transforms it into 37 sequences, each with 10 embeddings of size 1280. We concatenate them into an input tensor of shape 10 ? 37 ? 1280 for our SST model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Initialization and Training</head><p>We initialize the routing layers in the same manner as for the smallNORB model, the depth embeddings with zeros, and the linear transformations with Kaiming normal.</p><p>The training regime is the same as for the small-NORB model, except that we train the SST model for only 3 epochs. We use as training data all unique token sequences in the parse trees of the training split. Supp. <ref type="figure">Fig. 8</ref> shows loss and accuracy on the root-sentence validation set after each epoch of training for both SST-5/R and SST-2/R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on SST-5/R</head><p>As Tab. 3 shows, we achieve new state-of-theart test set accuracy of 58.5% on SST-5/R, a significant improvement (2.3 percentage points) over previous state-of-the-art test set accuracy. <ref type="table">Table 3</ref>: State-of-the-art test set accuracy on SST-5/R since its publication is associated with both new architectures and new pretraining methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results on SST-2/R</head><p>Changing only the final number of capsules to two, the same model achieves test set accuracy of 95.6% on SST-2/R, a new state-of-the-art performance for single-task models. The previous state of the art was achieved by BERT <ref type="bibr" target="#b14">(Devlin et al., 2018)</ref>. Current state-of-the-art test set accuracy for multi-task models or ensembles is 96.8%, by an ensemble of XLNet models trained on multiple GLUE tasks <ref type="bibr" target="#b27">(Yang et al., 2019)</ref>. See Tab. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Analysis</head><p>We are surprised to see a such a large improvement over the state of the art on SST-5/R and  <ref type="bibr" target="#b18">(Liu et al., 2019)</ref>  <ref type="table">(single model)</ref> 95.6 MT-DNN <ref type="bibr">(Liu et al., 2019) (ensemble)</ref> 96.5 XLNet <ref type="bibr">(Yang et al., 2019) (ensemble)</ref> 96.8</p><p>Single-task models: BCN+Char/CoVe <ref type="bibr" target="#b7">(McCann et al., 2017)</ref> 90.3 Block-sparse LSTM <ref type="bibr" target="#b5">(Radford et al., 2017)</ref> 93.2 BERT <ref type="bibr" target="#b14">(Devlin et al., 2018)</ref> 94.9 Our Model/GPT-2large <ref type="bibr">(non-finetuned)</ref> 95.6 <ref type="table">Table 4</ref>: Recent state-of-the-art test set accuracy on SST-2/R by multi-task models or ensembles, and by single-task models like ours.</p><p>also new state-of-the-art performance on SST-2/R compared to previous single-task models, considering that (a) we do not finetune the transformer, (b) our SST model resembles the model we use for a visual task, and (c) we train the SST model with the same regime, changing only the number of epochs. (By implication, greater accuracy might be possible with transformer finetuning and more careful tweaking of model and training regime.)</p><p>We also note that progress on SST-5/R has been remarkably steady, year after year, since its publication in 2013, as AI researchers have devised new architectures that exploit new pretraining mechanisms (Tab. 3). Our results represent a continuation of this long-term trend. As of yet, no model has come close to accurately modeling the labeling decisions of human beings on SST-5/R. Performance on the binary task, SST-2/R, whose labels lack the subtlety of the fine-grained ones, has been close to human baseline for several years now.</p><p>Finally, our successful use of a capsule network to route embeddings from a pretrained transformer links two areas of AI research that have been largely independent from each other: capsule networks with routing by agreement, used mainly for visual tasks, and transformers with self-attention, used mainly for sequence tasks.</p><p>6 Related Work <ref type="bibr" target="#b16">Hinton et al. (2018)</ref> proposed the first form of EM routing and showed that capsule networks using it to route matrix capsules can generalize to different poses of objects in images and resist whitebox adversarial attacks better than conventional CNNs. Their "related work" section compares capsule networks to other efforts for improving the ability of visual recognition models to deal effectively with viewpoint variations. <ref type="bibr" target="#b23">Sabour et al. (2017)</ref> showed that capsule networks with an earlier form of routing by agreement, operating on vector capsules, can be more effective than conventional CNNs for segmenting highly overlapping images of digits. <ref type="bibr" target="#b11">Barham and Isard (2019)</ref> showed that currently it can be challenging to scale capsule networks to large datasets and output spaces in some circumstances due to current software (e.g., PyTorch, TensorFlow) and hardware (e.g., GPUs, TPUs) systems, which are highly optimized for a fairly small set of computational kernels, in a way that is tightly coupled with memory hardware, leading to poor performance on non-standard workloads, including basic operations on capsules. <ref type="bibr" target="#b13">Coenen et al. (2019)</ref> found evidence that BERT, and possibly other transformer architectures, learn to embed sequences of natural language as trees. Their work inspired us to wonder if capsule networks might be able to recognize such "language trees" in different "poses," analogously to the way in which capsule networks can recognize different poses of objects embedded in images. <ref type="bibr" target="#b26">Vaswani et al. (2017)</ref> proposed transformer models using query-key-value dot-product attention, and showed that such models can be more effective than prior methods for modeling sequences. Our routing algorithm can be seen as a new kind of attention mechanism in which output capsules "compete with each other for the attention of input capsules," with each output capsule seeing a different set of input capsule votes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future Work</head><p>Capsule networks are a recent innovation, and our routing algorithm is still more recent. Its behavior and properties are not yet widely or fully understood. As current challenges to scaling, such as those studied by <ref type="bibr" target="#b11">Barham and Isard (2019)</ref>, are overcome, we think it would make sense to conduct more comprehensive evaluations and ablation studies of our algorithm in multiple domains.</p><p>We are also intrigued about using our routing algorithm for natural language modeling. At present this seems impractical, due in part to the computational complexity of the algorithm. 2 A more tractable alternative in the near future might be to intersperse layers of our routing algorithm between blocks of query-key-value self-attention.</p><p>Another possible avenue for future research involves experimenting with probabilistic models other than a multidimensional Gaussian in output capsules. While our limited experiments show that a multidimensional Gaussian works remarkably well, we harbor some doubts about its effectiveness with capsules of much greater size.</p><p>Finally, we naturally wonder about using nonprobabilistic clustering in our routing algorithm, k-means being the most obvious choice, given its relationship to EM and its proven effectiveness at dealing with large-scale data in other settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Building on recent work on capsule networks, we propose a new, general-purpose form of routing by agreement that computes output capsule activations as a logistic function of net benefits to use less net costs to ignore input capsules. To make the computation of net benefits and costs possible, we introduce a new step in the EM loop, the D-Step, that computes the share of data used and ignored from each input capsule by each output capsule, accounting for all input capsule data. We construct our routing algorithm to accept variable-size inputs, such as sequences, which also proves useful for keeping the number of model parameters small in applications for which it is otherwise not necessary. We also explain how to adapt the algorithm for variable-size outputs. Finally, our algorithm uses "pre-activation" scores to which we apply logistic functions as needed, facilitating more flexible use by subsequent layers and/or objective functions, with more numerical stability.</p><p>We illustrate the usefulness of our routing algorithm with two capsule networks that apply it in different domains, vision and language. Both networks achieve state-of-the-art performance in their respective domains after training with the same regime, thereby showing that adding one or more layers of our routing algorithm can produce stateof-the-art results in more than one domain, without requiring tuning. Our motivation is to develop universal, composable learning algorithms. Our work is but a small step in this direction. layer of the model alone would have to compute and hold in memory the equivalent of 3 ? 10 4 simultaneous EM loops, each on a different set of input votes per output capsule.  <ref type="figure">Figure 6</ref>: For each toy category and instance in the test set, we feed our smallNORB model a sequence of images with varying azimuth, while keeping everything else constant, and plot the change in each pose vector of the activated capsule. The mean change is shown in dark blue. (a) The first row of plots show each pose vector's Euclidean distance to its initial value, divided by the norm of the initial value, as azimuth varies from 0 to 340 degrees. We can see that the pose vector tends to move away from and then back close to its initial value, consistent with rotation. (b) The second row shows the norm of all pose vectors, divided by their initial norms. We can see that pose vector norms tend to stay close to the initial norm, consistent with rotation. (c) The third row shows cosine similarity between pose vectors and their initial value. We can see that the angle tends to increase and then decrease, consistent with rotation.  <ref type="figure">Figure 7</ref>: For each toy category and instance in the test set, we feed our smallNORB model a sequence of images with varying elevation, while keeping everything else constant, and plot the change in each pose vector of the activated capsule. The mean change is shown in dark blue. (a) The first row of plots show each pose vector's Euclidean distance to its initial value, divided by the norm of the initial value, as elevation varies through nine levels (from near flat to looking from above). We can see that the pose vector moves away from its initial value, consistent with the change in elevation. (b) The second row shows the norm of all pose vectors, divided by their initial norms. We can see that pose vector norms tend to stay close to the initial norm, consistent with rotation due to the change in elevation. (c) The third row shows cosine similarity between each pose vector and its initial value. We can see that the angle tends to increase but not decrease back to its original value, consistent with the change in elevation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Test set accuracy and number of parameters of models that have achieved state-of-the-art results on smallNORB visual recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Our smallNORB model. (a) We stack each pair of images with (b) coordinate values evenly spaced from -1.0 to 1.0, horizontally and vertically, creating an input tensor of shape 4 ? m ? n, where m = n = 96 for unmodified images at test time. (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Multidimensional scaling (MDS) representations in R 2 of the trajectories of an activated class capsule's four pose vectors, each of size d (out) = 4, as we feed test images of an object in the class with varying elevations to our trained smallNORB model. For each image, the R 2 coordinates are plotted as four connected vertices, each vertex corresponding to a pose vector, preserving as much as possible the pairwise distances between pose vectors from all images. Circles indicate the activated capsule's first pose vector for the first and last image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Our SST model. (a)  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Mean validation loss and accuracy after each epoch of training. Shaded area denotes standard deviation of batches, with 20 samples each. Note: smallNORB dataset does not have a validation split. Sample smallNORB stereographic images of one toy in each of five toy categories. For each toy we show 18 image-pair samples of varying azimuth while keeping elevation and lighting constant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Examples of our notation and their im- plementation in PyTorch. In all examples, A has shape d 1 ? d 2 and B has shape d 1 ? d 2 ? d 3 .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test set accuracy, parameters, and number of training epochs of models with state-of-theart performance on smallNORB.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In both domains, we use the same routing code, available at https://github.com/glassroom/heinsen routing along with pretrained models and replication instructions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Consider: if we wanted to use our routing algorithm to predict the next capsule in a natural language sequence, over a dictionary of typical size, say, 3?10 4 subword ids, the final</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Russell T. Klophaus for his feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rntn/None (socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cnn/Word2vec (kim</surname></persName>
		</author>
		<idno>48.0</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Para-Vec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikolov</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wieting</surname></persName>
		</author>
		<title level="m">LSTM/on PP2B</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Dmn/Glove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">51</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Radford</surname></persName>
		</author>
		<title level="m">ByteLSTM/82M reviews</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">52</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Looks</surname></persName>
		</author>
		<idno>CT-LSTM/word2vec</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bcn+char/Cove (mccann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Bcn/Elmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">54</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subilstm+char/Cove</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">56</biblScope>
			<pubPlace>Brahma</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Our Model/GPT-2large (non-finetuned)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Machine learning systems are stuck in a rut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Hot Topics in Operating Systems</title>
		<meeting>the Workshop on Hot Topics in Operating Systems<address><addrLine>New York, NY, USA, HotOS</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="177" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">High-performance neural networks for visual object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">C</forename><surname>Cire?an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1102.0183</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Visualizing and measuring the geometry of BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><forename type="middle">B</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CoRR abs/1906.02715</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<title level="m">Matrix capsules with em routing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA, CVPR&apos;04</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<title level="m">On the variance of the adaptive learning rate and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3428" to="3448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Probing neural network comprehension of natural language arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Niven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
		</author>
		<idno>abs/1907.07355</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1710.05941</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1710.09829</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Citeseer</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>abs/1706.03762</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1906.08237</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">mixup: Beyond empirical risk minimization. CoRR abs/1710.09412. List of Supplementary Figures ? Fig. 6: Change in pose vectors by azimuth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">7: Change in pose vectors by elevation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Fig</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Fig</surname></persName>
		</author>
		<title level="m">Validation losses and accuracies</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Fig</surname></persName>
		</author>
		<title level="m">Sample smallNORB images</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
