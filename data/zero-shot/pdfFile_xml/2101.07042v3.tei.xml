<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLASTER: Clustering with Reinforcement Learning for Zero-Shot Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyank</forename><forename type="middle">N</forename><surname>Gowda</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CLASTER: Clustering with Reinforcement Learning for Zero-Shot Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Zero-Shot</term>
					<term>Clustering</term>
					<term>Action Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Zero-Shot action recognition is the task of recognizing action classes without visual examples. The problem can be seen as learning a representation on seen classes which generalizes well to instances of unseen classes, without losing discriminability between classes. Neural networks are able to model highly complex boundaries between visual classes, which explains their success as supervised models. However, in Zero-Shot learning, these highly specialized class boundaries may overfit to the seen classes and not transfer well from seen to unseen classes. We propose a novel cluster-based representation, which regularizes the learning process, yielding a representation that generalizes well to instances from unseen classes. We optimize the clustering using reinforcement learning, which we observe is critical. We call the proposed method CLASTER and observe that it consistently outperforms the state-of-the-art in all standard Zero-Shot video datasets, including UCF101, HMDB51 and Olympic Sports; both in the standard Zero-Shot evaluation and the generalized Zero-Shot learning. We see improvements of up to 11.9% over SOTA. Project Page: https://sites.google.com/view/claster-zsl/home</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Research on action recognition in videos has made rapid progress in the last years, with models becoming more accurate and even some datasets becoming saturated. Much of this progress has depended on large scale training sets. However, it is often not practical to collect thousands of video samples for a new class. This idea has led to research in the Zero-Shot learning (ZSL) domain, where training occurs in a set of seen classes, and testing occurs in a set of unseen classes. In particular, in the case of video ZSL, each class label is typically enriched with semantic embeddings. These embeddings are sometimes manually annotated, by providing attributes of the class, and other times computed automatically using language models of the class name or class description. At test time the semantic embedding of the predicted seen class is used to search for a nearest neighbor in the space of semantic embeddings of unseen classes.</p><p>While ZSL is potentially a very useful technology, this standard pipeline poses a fundamental representation challenge. Neural networks have proven extraor- <ref type="figure">Fig. 1</ref>. Left: Learning curve for the seen classes. Right: Learning curve for the unseen classes. The clustering-based representation avoids overfitting, which in the case of seen classes means that the gap between validation and training accuracy is smaller than in the vanilla representation. This regularization effect improves the validation accuracy in unseen classes.</p><p>dinarily powerful at learning complex discriminative functions of classes with many modes. In other words, instances of the same class can be very different and still be projected by the neural network to the same category. While this works well in supervised training, it can be a problem in Zero-Shot recognition, where the highly specialized discriminative function might not transfer well to instances of unseen classes. In this work, we adress this representation problem using three main ideas.</p><p>First, we turn to clustering, and use the centroids of the clusters to represent a video. We argue that centroids are more robust to outliers, and thus help regularize the representation, avoiding overfitting to the space of seen classes. <ref type="figure" target="#fig_4">Figure 6</ref> shows that the gap between training and validation accuracy is smaller when using clustering in seen classes (left). As a result, the learned representation is more general, which significantly improves accuracy in unseen classes (right).</p><p>Second, our representation is a combination of a visual and a semantic representation. The standard practice at training time is to use a visual representation, and learn a mapping to the semantic representation. Instead, we use both cues, which we show yields a better representation. This is not surprising, since both visual and semantic information can complement each other.</p><p>Third, we use the signal from classification as direct supervision for clustering, by using Reinforcement Learning (RL). Specifically, we use the REINFORCE algorithm to directly update the cluster centroids. This optimization improves the clustering significantly and leads to less noisy and more compact representations for unseen classes.</p><p>These three pieces are essential to learn a robust, generalizable representation of videos for Zero-Shot action recognition, as we show in the ablation study. Crucially, none of them have been used in the context of Zero-Shot or action recognition. They are simple, yet fundamental and we hope they will be useful to anyone in the Zero-Shot and action recognition communities.</p><p>We call the proposed method CLASTER, for CLustering with reinforcement learning for Action recognition in zero-ShoT lEaRning, and show that it significantly outperforms all existing methods across all standard Zero-Shot action recognition datasets and tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Fully Supervised Action Recognition. This is the most widely studied setting in action recognition, where there is a large amount of samples at training time and the label spaces are the same at training and testing time. A thorough survey is beyond our scope, but as we make use of these in the backbone of our model, we mention some of the most widely used work. The seminal work of Simonyan and Zisserman <ref type="bibr" target="#b38">[39]</ref> introduced the now standard two-stream deep learning framework, which combines spatial and temporal information. Spatio-temporal CNNs <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b4">5]</ref> are also widely used as backbones for many applications, including this work. More recently, research has incorporated attention <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b13">14]</ref> and leveraged the multi-modal nature of videos <ref type="bibr" target="#b1">[2]</ref>. In this work, we use the widely used I3D <ref type="bibr" target="#b4">[5]</ref>.</p><p>Zero-Shot Learning. Early approaches followed the idea of learning semantic classifiers for seen classes and then classifying the visual patterns by predicting semantic descriptions and comparing them with descriptions of unseen classes. In this space, Lampert et al. <ref type="bibr" target="#b20">[21]</ref> propose attribute prediction, using the posterior of each semantic description. The SJE model <ref type="bibr" target="#b0">[1]</ref> uses multiple compatibility functions to construct a joint embedding space. ESZSL <ref type="bibr" target="#b36">[37]</ref> uses a Frobenius norm regularizer to learn an embedding space. Repurposing these methods for action classification is not trivial. In videos, there are additional challenges: action labels need more complex representations than objects and hence give rise to more complex manual annotations.</p><p>ZSL for Action Recognition. Early work <ref type="bibr" target="#b35">[36]</ref> was restricted to cooking activities, using script data to transfer to unseen classes. Gan et al. <ref type="bibr" target="#b10">[11]</ref> consider each action class as a domain, and address semantic representation identification as a multi-source domain generalization problem. Manually specified semantic representations are simple and effective <ref type="bibr" target="#b48">[49]</ref> but labor-intensive to annotate. To overcome this, the use of label embeddings has proven popular, as only category names are needed. Some approaches use common embedding space between class labels and video features <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45]</ref>, pairwise relationships between classes <ref type="bibr" target="#b8">[9]</ref>, error-correcting codes <ref type="bibr" target="#b33">[34]</ref>, inter-class relationships <ref type="bibr" target="#b9">[10]</ref>, out-of-distribution detectors <ref type="bibr" target="#b25">[26]</ref>, and Graph Neural networks <ref type="bibr" target="#b12">[13]</ref>. In contrast, we are learning to Overview of CLASTER. We map the semantic embedding a(yi) to the space of visual features xi, and concatenate both to obtain a visual-semantic representation. We cluster these visual-semantic representations with K-means to obtain initial cluster centroids. Each video is represented as the sum of the visual-semantic representation and the centroid clusters, weighted by their distance (see Sec. 3.3 and <ref type="figure" target="#fig_1">Fig. 3</ref>). This is used as input for classification (Sec 3.4 and Eq. 3). Based on the classification result, we send a reward to optimize the cluster centroids using REINFORCE (Sec. <ref type="bibr">3.5)</ref>. At test time, we first perform classification on the seen classes and then do a nearest neighbor (NN) search to predict the unseen class.</p><p>optimize centroids of visual semantic representations that generalize better to unseen classes.</p><p>Reinforcement Learning for Zero-Shot Learning. RL for ZSL in images was introduced by Liu et al. <ref type="bibr" target="#b22">[23]</ref> by using a combination of ontology and RL. In Zero-Shot text classification, Ye et al. <ref type="bibr" target="#b46">[47]</ref> propose a self-training method to leverage unlabeled data. RL has also been used in the Zero-Shot setting for task generalization <ref type="bibr" target="#b31">[32]</ref>, active learning <ref type="bibr" target="#b6">[7]</ref>, and video object segmentation <ref type="bibr" target="#b15">[16]</ref>. To the best of our knowledge, there is no previous work using RL for optimizing centroids in Zero-Shot recognition.</p><p>Deep Approaches to Centroid Learning for Classification. Since our approach learns cluster centroids using RL, it is related to the popular cluster learning strategy for classification called Vector of Locally Aggregated Descriptors (VLAD) <ref type="bibr" target="#b2">[3]</ref>. The more recent NetVLAD <ref type="bibr" target="#b2">[3]</ref> leverages neural networks which helps outperform the standard VLAD by a wide margin. ActionVLAD <ref type="bibr" target="#b14">[15]</ref> aggregates NetVLAD over time to obtain descriptors for videos. ActionVLAD uses clusters that correspond to spatial locations in a video while we use joint visual semantic embeddings for the entire video. In general, VLAD uses residuals with respect to cluster centroids as representation while CLASTER uses a weighting of the centroids. The proposed CLASTER outperforms NetVLAD by a large margin on both HMDB51 and UCF101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CLASTER</head><p>We now describe the proposed CLASTER, which leverages clustering of visual and semantic features for video action recognition and optimizes the clustering with RL. <ref type="figure" target="#fig_0">Figure 2</ref> shows an overview of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Let S be the training set of seen classes. S is composed of tuples (x, y, a(y)), where x represents the spatio-temporal features of a video, y represents the class label in the set of Y S seen class labels, and a(y) denotes the categoryspecific semantic representation of class y. These semantic representations are either manually annotated or computed using a language-based embedding of the category name, such as word2vec <ref type="bibr" target="#b27">[28]</ref> or sentence2vec <ref type="bibr" target="#b32">[33]</ref>.</p><p>Let U be the set of pairs (u, a(u)), where u is a class in the set of unseen classes Y U and a(u) are the corresponding semantic representations. The seen classes Y S and the unseen classes Y U do not overlap.</p><p>In the Zero-Shot Learning (ZSL) setting, given an input video the task is to predict a class label in the unseen classes, as f ZSL : X ? Y U . In the related generalized Zero-Shot learning (GZSL) setting, given an input video, the task is to predict a class label in the union of the seen and unseen classes, as f GZSL :</p><formula xml:id="formula_0">X ? Y S ? Y U .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual-Semantic Representation</head><p>Given video i, we compute visual features x i and a semantic embedding a(y i ) of their class y i (see Sec. 4 for details). The goal is to map both to the same space, so that they have the same dimensionality and magnitude, and therefore they will have a similar weight during clustering. We learn this mapping with a simple multi-layer perceptron (MLP) <ref type="bibr" target="#b47">[48]</ref>, trained with a least-square loss. This loss minimizes the distance between x i and the output from the MLP, which we call a (y). Finally, we concatenate x i and a (y) to obtain the visual-semantic representations that will be clustered. The result is a representation which is not only aware of the visual information but also the semantic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CLASTER Representation</head><p>We now detail how we represent videos using the proposed CLASTER representation, which leverages clustering as a form of regularization. In other words, a representation w.r.t centroids is more robust to outliers, which is helpful since all instances of the unseen classes are effectively outliers w.r.t. the training distribution.</p><p>We initialize the clustering of the training set S using K-means <ref type="bibr" target="#b7">[8]</ref>. Each resulting cluster j has a centroid c j , that is the average of all visual-semantic samples in that particular cluster. The CLASTER representation of a given video is the sum of the visual-semantic representation and the centroids, weighted by the inverse of the distance, such that closer clusters will have more weight. <ref type="figure" target="#fig_1">Figure 3</ref> shows this process in detail.</p><p>Specifically, given video i, we compute the visual representation x i . We estimate the semantic vector ? i using an MLP. This is a necessary step, as during test time we do not have any semantic information. Concatenating the visual x i and semantic ? i we obtain the intermediate representation ? i , which is in the same space as the cluster centroids.</p><p>We compute the Euclidean distance d i,j between the visual-semantic point ? i and each cluster j, which we refer to as d i,j . We take the inverse 1/d i,j and normalize them using their maximum and minimum values, such that they are between 0 and 1. We refer to these normalized values as ? i,j , and they are used as the weights of each cluster centroid in the final CLASTER representation ? i :</p><formula xml:id="formula_1">? i = ? i + k j=1 ? i,j c j .</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function</head><p>Given the CLASTER representation ? i we predict a seen class using a simple MLP, V . Instead of the vanilla softmax function, we use semantic softmax <ref type="bibr" target="#b17">[18]</ref>, which includes the semantic information a(y i ) and thus can transfer better to Zero-Shot classes:?</p><formula xml:id="formula_2">i = e a(yi) T V (?i) S j=1 e a(yj ) T V (?i) .<label>(2)</label></formula><p>The output? i is a vector with a probability distribution over the S seen classes. We train the classifier, which minimizes the cross-entropy loss with a regularization term:</p><formula xml:id="formula_3">min W N i=1 L(x i ) + ? W ,<label>(3)</label></formula><p>where W refers to all weights in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Optimization with Reinforcement Learning</head><p>As there is no ground truth for clustering centroids, we use the classification accuracy as supervision. This makes the problem non-differentiable, and therefore we cannot use traditional gradient descent. Instead, we use RL to optimize the cluster centroids. For this, we compute two variables that will determine each centroid update: the reward, which measures whether the classification is correct and will determine the direction of the update, and the classification score, which measures how far the prediction is from the correct answer and will determine the magnitude of the update. Given the probabilistic prediction? i and the one-hot representation of the ground truth class y i , we compute the classification score as the dot product of the two: z i = y i?i . To obtain the reward, we check if the maximum of? i and y i lie in the same index:</p><formula xml:id="formula_4">r = 1 if arg max? i = arg max y i ?1 otherwise<label>(4)</label></formula><p>This essentially gives a positive reward if the model has predicted a correct classification and a negative reward if the classification was incorrect. This formulation is inspired by Likas <ref type="bibr" target="#b21">[22]</ref>, which was originally proposed for a different domain and the problem of competitive learning.</p><p>For each data point ? i we only update the closest cluster centroid c j . We compute the update ?c j using the REINFORCE <ref type="bibr" target="#b21">[22]</ref> algorithm as:</p><formula xml:id="formula_5">?c j = ? r (z i ? p j ) (? i ? c j ).<label>(5)</label></formula><p>For further details on this derivation, please see the Supplementary Material as well as Likas <ref type="bibr" target="#b21">[22]</ref>. The main difference between our model and Likas' is that we do not consider cluster updates to be Bernoulli units. Instead, we modify the cluster centroid with the classification score z i , which is continuous in the range between 0 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation Details</head><p>Visual features. We use RGB and flow features extracted from the Mixed 5c layer of an I3D network pre-trained on the Kinetics <ref type="bibr" target="#b4">[5]</ref> dataset. The Mixed 5c output of the flow network is averaged across the temporal dimension and pooled by four in the spatial dimension and then flattened to a vector of size 4096. We then concatenate the two.</p><p>Network architecture. The MLP that maps the semantic features to visual features consists of two fully-connected (FC) layers and a ReLU. The MLP in the CLASTER Representation module, which maps the visual feature to the semantic space is a two-layer FC network, whose output after concatenation with the video feature has the same dimensions as the cluster representatives. The size of the FC layers is 8192 each. The final classification MLP (represented as a classification block in <ref type="figure" target="#fig_0">Figure 2</ref>) consists of two convolutional layers and two FC layers, where the last layer equals the number of seen classes in the dataset we are looking at. All the modules are trained with the Adam optimizer with a learning rate of 0.0001 and weight decay of 0.0005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of clusters.</head><p>Since the number of clusters is a hyperparameter, we evaluate the effect of the number of clusters on the UCF101 dataset for videos and choose 6 after the average performance stablizes as can be seen in the supplementary material. We then use the same number for the HMDB51 and Olympics datasets.</p><p>RL optimization. We use 10,000 iterations and the learning rate ? is fixed to 0.1 for the first 1000 iterations, 0.01 for the next 1000 iterations and then drop it to 0.001 for the remaining iterations.</p><p>Semantic embeddings. We experiment with three types of embeddings as semantic representations of the classes. We have human-annotated semantic representations for UCF101 and the Olympic sports dataset of sizes 40 and 115 respectively. HMDB51 does not have such annotations. Instead, we use a skipgram model trained on the news corpus provided by Google to generate word2vec embeddings. Using action classes as input, we obtain a vector representation of 300 dimensions. Some class labels contain multiple words. In those cases, we use the average of the word2vec embeddings. We also use sentence2vec embeddings, trained on Wikipedia. These can be obtained for both single words and multi-word expressions. The elaborate descriptions are taken from <ref type="bibr" target="#b5">[6]</ref> and only evaluated for fair comparison to them.</p><p>For the elaborative descriptions, we follow ER <ref type="bibr" target="#b5">[6]</ref> and use the provided embeddings in their codebase.</p><p>Rectification of the Semantic Embedding Sometimes, in ZSL, certain data points tend to appear as nearest-neighbor of many other points in the projection space. This is referred to as the hubness problem <ref type="bibr" target="#b37">[38]</ref>. We avoid this problem using semantic rectification <ref type="bibr" target="#b23">[24]</ref>, where the class representation is modified by averaging the output generated by the projection network, which in our case is the penultimate layer of the classification MLP. Specifically, for the unseen classes, we perform rectification by first using the MLP trained on the seen classes to project the semantic embedding to the visual space. We add the average of projected semantic embeddings from the k-nearest neighbors of the seen classes, specifically as follows:</p><formula xml:id="formula_6">a(y i ) = a (y i ) + 1 k n?N cos (a (y i ), n) ? n,<label>(6)</label></formula><p>where a (y) refers to the embedding after projection to the visual space, cos(a, n) refers to the cosine similarity between a and n, the operator ? refers to the dot product and N refers to the k-nearest neighbors of a (y ui ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nearest Neighbor Search</head><p>At test time in ZSL, given a test video, we predict a seen class and compute or retrieve its semantic representation. After rectification, we find the nearest neighbor in the set of unseen classes. In the GZSL task, class predictions may be of seen or unseen classes. Thus, we first use a bias detector <ref type="bibr" target="#b11">[12]</ref> which helps us detect if the video belongs to the seen or unseen class. If it belongs to a seen class, we predict the class directly from our model, else we proceed as in ZSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Analysis</head><p>In this section, we look at the qualitative and quantitative performance of the proposed model. We first describe the experimental settings, and then show an ablation study, that explores the contribution of each component. We then compare the proposed method to the state-of-the-art in the ZSL and GZSL tasks, and give analytical insights into the advantages of CLASTER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We choose the Olympic Sports <ref type="bibr" target="#b30">[31]</ref>, HMDB-51 <ref type="bibr" target="#b19">[20]</ref> and UCF-101 <ref type="bibr" target="#b39">[40]</ref>, so that we can compare to recent state-of-the-art models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref>. We follow the commonly used 50/50 splits of Xu et al. <ref type="bibr" target="#b44">[45]</ref>, where 50 percent are seen classes and 50 are unseen classes. Similar to previous approaches <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b18">19]</ref>, we report average accuracy and standard deviation over 10 independent runs. We report results on the split proposed by <ref type="bibr" target="#b43">[44]</ref>, in the standard inductive setting. We also report on the recently introduced TruZe <ref type="bibr" target="#b16">[17]</ref>. This split accounts for the fact that some classes present on the dataset used for pre-training (Kinetics <ref type="bibr" target="#b4">[5]</ref>) overlap with some of the unseen classes in the datasets used in the Zero-Shot setting, therefore breaking the premise that those classes have not been seen. <ref type="table" target="#tab_0">Table 1</ref> shows the impact of using the different components of CLASTER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>Impact of Clustering. We consider several baselines. First, omitting clustering, which is the equivalent of setting ? i = ? i , in Eq. 1. This is, ignoring the cluster centroids in the representation. This is referred to in <ref type="table" target="#tab_0">Table 1</ref> as "No clustering". Second, we use random clustering, which is assigning each instance to a random cluster. Finally, we use the standard K-means. We observe that using clusters is beneficial, but only if they are meaningful, as in the case of K-means.</p><p>Impact of using a visual-semantic representation. We compare to the standard representation, which only includes visual information, and keep everything the same. This is, clustering and classification are done using only the visual features. This is referred to in the table as "CLASTER w/o SE". We observe that there is a very wide gap between using and not using the semantic features at training time. This effect is present across all datasets, suggesting it is a general improvement in the feature learning. We also show a comparison of aggregation strategies and interaction between visual and semantic features in the supplementary material.</p><p>Impact of different optimization choices. We make cluster centroids learnable parameters and use the standard SGD to optimize them ('CLASTER w/o RL") We also test the use of the related work of NetVLAD to optimize the cluster ("CLASTER w/ NetVLAD"). We see that the proposed model outperforms NetVLAD by an average of 4.7% and the CLASTER w/o RL by 7.3% on the UCF101 dataset. A possible reason for this difference is that the loss is backpropagated through multiple parts of the model before reaching the centroids. However, with RL the centroids are directly updated using the reward signal. Section 5.6 explores how the clusters change after the RL optimization. In a nutshell, the RL optimization essentially makes the clusters cleaner, moving most instances in a class to the same cluster. <ref type="table" target="#tab_1">Table 2</ref> shows the comparison between CLASTER and several state-of-the-art methods: the out-of-distribution detector method (OD) <ref type="bibr" target="#b25">[26]</ref>, a generative approach to Zero-Shot action recognition (GGM) <ref type="bibr" target="#b29">[30]</ref>, the evaluation of output embeddings (SJE) <ref type="bibr" target="#b0">[1]</ref>, the feature generating networks (WGAN) <ref type="bibr" target="#b42">[43]</ref>, the endto-end training for realistic applications approach (E2E) <ref type="bibr" target="#b3">[4]</ref>, the inverse autoregressive flow (IAF) based generative model, bi-directional adversarial GAN(Bidir GAN) <ref type="bibr" target="#b28">[29]</ref> and prototype sampling graph neural network (PS-GNN) <ref type="bibr" target="#b12">[13]</ref>. To make results directly comparable, we use the same backbone across all of them, which is the I3D <ref type="bibr" target="#b4">[5]</ref> pre-trained on Kinetics. We observe that the proposed CLASTER consistently outperforms all other state-of-the-art methods across all datasets. The improvements are significant: up to 3.5% on HMDB51 and 13.5% on UCF101 with manual semantic embedding. We also measure the impact of different semantic embeddings, including using sentence2vec instead of word2vec. We show that sentence2vec significantly improves over using word2vec, especially on UCF101 and HMDB51. Combination of embeddings resulted in average improvements of 0.3%, 0.8% and 0.9% over the individual best performing embedding of CLASTER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on ZSL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results on GZSL</head><p>We now compare to the same approaches in the GZSL task in <ref type="table">Table 3</ref>, the reported results are the harmonic mean of the seen and unseen class accuracies. Here CLASTER outperforms all previous methods across different modalities. We obtain an improvement on average of 2.6% and 5% over the next best performing method on the Olympics dataset using manual representations and word2vec respectively. We obtain an average improvement of 6.3% over the next best performing model on the HMDB51 dataset using word2vec. We obtain an improvement on average performance by 1.5% and 4.8% over the next best performing model on the UCF101 dataset using manual representations and word2vec respectively. Similarly to ZSL, we show generalized performance improvements using sentence2vec. We also report results on the combination of embeddings. We see an improvement of 0.3%, 0.6% and 0.4% over the individual best embedding for CLASTER. The seen and unseen accuracies are shown in the Supplemental Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results on TruZe</head><p>We also evaluate on the more challenging TruZe split. The proposed UCF101 and HMDB51 splits have 70/31 and 29/22 classes (represented as training/testing). We compare to WGAN <ref type="bibr" target="#b42">[43]</ref>, OD <ref type="bibr" target="#b25">[26]</ref> and E2E <ref type="bibr" target="#b3">[4]</ref> on both ZSL and GZSL scenarios. Results are shown in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Analysis of the RL optimization</head><p>We analyze how optimizing with RL affects clustering on the UCF101 training set. <ref type="figure" target="#fig_2">Figure 4</ref> shows the t-SNE <ref type="bibr" target="#b24">[25]</ref> visualization. Each point is a video instance in the unseen classes, and each color is a class label. As it can be seen, the RL optimization makes videos of the same class appear closer together.</p><p>We also do a quantitative analysis of the clustering. For each class in the training set, we measure the distribution of clusters that they belong to, visualized in the <ref type="figure" target="#fig_3">Fig 5.</ref> We observe that after the RL optimization, the clustering  <ref type="table">Table 3</ref>. Results on GZSL. SE: semantic embedding, M: manual representation, W: word2vec embedding, S: sentence2vec, C: combination of embeddings. The seen and unseen class accuracies are listed in the supplementary material.</p><p>becomes "cleaner". This is, most instances in a class belong to a dominant cluster. This effect can be measured using the purity of the cluster:</p><formula xml:id="formula_7">P urity = 1 N k i=1 max j |c i ? t j | ,<label>(7)</label></formula><p>where N is the number of data points (video instances), k is the number of clusters, c i is a cluster in the set of clusters, and t j is the class which has the maximum count for cluster c i . Poor clustering results in purity values close to 0, and a perfect clustering will return a purity of 1. Using K-means, the purity is 0.77, while optimizing the clusters with RL results in a purity of 0.89.  <ref type="table">Table 4</ref>. Results on TruZe. For ZSL, we report the mean class accuracy and for GZSL, we report the harmonic mean of seen and unseen class accuracies. All approaches use sen2vec annotations as the form of semantic embedding.  We can see that the clusters are a lot "cleaner" after the optimization by RL.</p><p>Finally, we observe another interesting side effect of clustering. Some of the most commonly confused classes before clustering (e.g. "Baby crawling" vs. "Mopping floor", "Breaststroke" vs. "front crawl", "Rowing vs. front crawl") are assigned to different clusters after RL, resolving confusion. This suggests that clusters are also used as a means to differentiate between similar classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Regularization Effect of Clustering</head><p>In the main paper, we showed the regularization effect that clustering had when using 6, 10 and 51 clusters in comparison to no clusters. Here, we look at the same effect with 20 and 35 clusters as well. We see consistent improvements of over 15% in accuracy for the unseen classes using the proposed CLASTER representation compared to no clustering.</p><p>In addition, we show that using only 35% of the data of seen classes for training also benefits from clustering on the unseen classes. This can be seen in <ref type="figure">Figure 7</ref> While in the case of seen classes, using no clustering has the highest validation accuracy, at test time for the unseen classes, clustering leads to the best results. There are a few interesting points to note here. First, no clustering results in clear overfitting. The training accuracy reaches over 80% while the validation accuracy reaches a peak of 46% before dropping. However, using clustering results in the training and validation curves to be really close to each other. Another interesting point is that when there is a limited number of samples, having more clusters results in better performance at test time. This was not the case when we had all samples for the seen classes. When having all samples at training time, the number of clusters resulted in the same average accuracy as can be seen in Section 9. <ref type="figure">Fig. 7</ref>. Left: Learning curve for the seen classes using 35% of the data. Right: Learning curve for the unseen classes. The clustering-based representation avoids overfitting, which in the case of seen classes means that the gap between validation and training accuracy is smaller than in the vanilla representation. This regularization effect improves the accuracy in unseen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Statistical Significance</head><p>We consider the dependent t-test for paired samples. This test is utilized in the case of dependent samples, in our case different model performances on the same random data split. This is a case of a paired difference test. This is calculated as shown in Eq 8.</p><formula xml:id="formula_8">t =X D ? ? 0 s D / ? n<label>(8)</label></formula><p>WhereX D is the average of the difference between all pairs and s D is the standard deviation of the difference between all pairs. The constant ? 0 is zero in case we wish to test if the average of the difference is different; n represents the number of samples, n = 10 in our case. The comparisons can be seen in <ref type="table">Table 5</ref>. The lower the value of 'p', higher the significance.</p><p>As we can see, our results are statistically significant in comparison to both OD <ref type="bibr" target="#b25">[26]</ref> and WGAN <ref type="bibr" target="#b42">[43]</ref> in both ZSL and GZSL. We also see that our results are statistically significant for both HMDB51 and Olympics in comparison to E2E <ref type="bibr" target="#b3">[4]</ref>. In GZSL, OD <ref type="bibr" target="#b25">[26]</ref> also achieves results that are significantly different in comparison to WGAN <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Average of Differences in Performance for Same Splits</head><p>Since the performance of the model varies for each random split (as witnessed by the standard deviation values), we average the difference in performance between CLASTER, OD, WGAN and E2E on the same splits. We believe that this gives us a better metric to check the performance of CLASTER with the other approaches. The results are depicted in <ref type="table">Table 6</ref>. <ref type="figure">Fig. 8</ref>. Effect of using different number of clusters. The green line represents the standard deviation. The reported accuracy is on the UCF101 dataset. As can be seen, the average cluster accuracy increases till about 6 clusters and then remains more or less constant. The vertical lines correspond to the standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Number of Clusters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pairs</head><p>Dataset t-value Statistical significance(p&lt;0.05) Type CLASTER and OD <ref type="bibr" target="#b25">[26]</ref> UCF101 -15.77 Significant, p&lt;0.00001 ZSL CLASTER and WGAN <ref type="bibr" target="#b42">[43]</ref> UCF101 -9.08</p><p>Significant, p&lt;0.00001 ZSL CLASTER and E2E <ref type="bibr" target="#b3">[4]</ref> UCF101 -0.67 Not Significant, p = 0.26 ZSL OD <ref type="bibr" target="#b25">[26]</ref> and WGAN <ref type="bibr" target="#b42">[43]</ref> UCF101 -1.70 Not Significant, p=0.12278 ZSL CLASTER and OD <ref type="bibr" target="#b25">[26]</ref> HMDB51 -4.33 Significant, p=0.00189 ZSL CLASTER and WGAN <ref type="bibr" target="#b42">[43]</ref> HMDB51 -5.54</p><p>Significant, p=0.00036 ZSL CLASTER and E2E <ref type="bibr" target="#b3">[4]</ref> HMDB51 -3.77 Significant, p = 0.00219 ZSL OD <ref type="bibr" target="#b25">[26]</ref> and WGAN <ref type="bibr" target="#b42">[43]</ref> HMDB51 -3.71 Significant, p=0.00483 ZSL CLASTER and OD <ref type="bibr" target="#b25">[26]</ref> Olympics -9.06 Significant, p&lt;0.00001 ZSL CLASTER and WGAN <ref type="bibr" target="#b42">[43]</ref> Olympics -11.73</p><p>Significant, p&lt;0.00001 ZSL CLASTER and E2E <ref type="bibr" target="#b3">[4]</ref> Olympics -2.72 Significant, p = 0.012 ZSL OD <ref type="bibr" target="#b25">[26]</ref> and WGAN <ref type="bibr" target="#b42">[43]</ref> Olympics -2.47 Significant, p=0.03547 ZSL CLASTER and OD <ref type="bibr" target="#b25">[26]</ref> UCF101 -4.51 Significant, p=0.00148 GZSL CLASTER and WGAN <ref type="bibr" target="#b42">[43]</ref> UCF101 -5. <ref type="bibr" target="#b48">49</ref> Significant, p=0.00039 GZSL OD <ref type="bibr" target="#b25">[26]</ref> and WGAN <ref type="bibr" target="#b42">[43]</ref> UCF101 -3.16 Significant, p=0.01144 GZSL CLASTER and OD <ref type="bibr" target="#b25">[26]</ref> HMDB51 -5.08 Significant, p=0.00066 GZSL CLASTER and WGAN [43] HMDB51 -7.51</p><p>Significant, p=0.00004 GZSL OD <ref type="bibr" target="#b25">[26]</ref> and WGAN <ref type="bibr" target="#b42">[43]</ref> HMDB51 -5.27 Significant, p=0.00051 GZSL CLASTER and OD <ref type="bibr" target="#b25">[26]</ref> Olympics -5.79 Significant, p=0.00026 GZSL CLASTER and WGAN <ref type="bibr" target="#b42">[43]</ref> Olympics -8. <ref type="bibr" target="#b38">39</ref> Significant, p=0.00002 GZSL OD <ref type="bibr" target="#b25">[26]</ref> and WGAN <ref type="bibr" target="#b42">[43]</ref> Olympics -6.22 Significant, p=0.00014 GZSL <ref type="table">Table 5</ref>. Comparison of the t-test for different pairs of models on the same random split. Lower the value of 'p', higher the significance. As we can see, our results are statistically significant in comparison to both OD <ref type="bibr" target="#b25">[26]</ref> and WGAN <ref type="bibr" target="#b42">[43]</ref> in both ZSL and GZSL. For GZSL, OD <ref type="bibr" target="#b25">[26]</ref> also achieves results that are significant in comparison to WGAN <ref type="bibr" target="#b42">[43]</ref>.  <ref type="table">Table 6</ref>. Comparing the average of the difference in performance for recent stateof-the-art approaches in zero-shot and generalized zero-shot action recognition on the same splits. All results were computed using sen2vec as the embedding. We can see that we outperform recent approaches in every scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>We test using different number of clusters on the UCF-101 dataset and show the results in <ref type="figure">Figure 8</ref>. These are for 5 runs on random splits. As we can see, the average accuracy increases until 6 clusters, and after that remains more or less constant. Thus, we use 6 clusters and continue with the same number for both HMDB51 and Olympics. For images, similarly, we used 5 random splits of CUB and found the performance stabilizes after having 9 clusters and use the same number of clusters for the other image datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Comparison of aggregation strategies and interaction between visual and semantic features</head><p>We compare the method with and without semantic features in <ref type="table" target="#tab_0">Table 1</ref> of the main paper. Below, in <ref type="table">Table 7</ref> we show other aggregation options such as averaging and dot product. All results are using ED as semantic embedding.  <ref type="table">Table 7</ref>. Results on different aggregation options for the semantic and visual embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Seen and Unseen Class Performance for GZSL</head><p>In order to better analyze performance of the model on GZSL, we report the average seen and unseen accuracies along with their harmonic mean. The results using different embeddings and on the UCF101, HMDB51 and Olympics datasets are reported in <ref type="table">Table 8</ref>. The reported results are on the same splits for fair comparison <ref type="bibr" target="#b16">[17]</ref>.  <ref type="table">Table 8</ref>. Seen and unseen accuracies for CLASTER on different datasets using different embeddings. 'E' corresponds to the type of embedding used, wherein 'A', 'W', 'S' and 'C' refers to manual annotations, word2vec, sen2vec and combination of the embeddings respectively. 'u', 's' and 'H' corresponds to average unseen accuracy, average seen accuracy and the harmonic mean of the two. All the reported results are on the same splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Conclusion</head><p>Zero-Shot action recognition is the task of recognizing action classes without any visual examples. The challenge is to map the knowledge of seen classes at training time to that of novel unseen classes at test time. We propose a novel model that learns clustering-based representation of visual-semantic features, optimized with RL. We observe that all three of these components are essential. The clustering helps regularizing, and avoids overfitting to the seen classes. The visual-semantic representation helps improve the representation. And the RL yields better, cleaner clusters. The results is remarkable improvements across datasets and tasks over all previous state-of-the-art, up to 11.9% absolute improvement on HMDB51 for GZSL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of CLASTER. We map the semantic embedding a(yi) to the space of visual features xi, and concatenate both to obtain a visual-semantic representation. We cluster these visual-semantic representations with K-means to obtain initial cluster centroids. Each video is represented as the sum of the visual-semantic representation and the centroid clusters, weighted by their distance (see Sec. 3.3 and Fig. 3). This is used as input for classification (Sec 3.4 and Eq. 3). Based on the classification result, we send a reward to optimize the cluster centroids using REINFORCE (Sec. 3.5). At test time, we first perform classification on the seen classes and then do a nearest neighbor (NN) search to predict the unseen class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The proposed CLASTER Representation in detail (seeFig. 2for the overview of the full method). The visual feature is mapped to match the space of the visualsemantic cluster centroids with an MLP and concatenation. Based on the distances to the cluster centroids the final representation ? is a weighted representation of the centroids, more robust to the out-of-distribution instances of the unseen test classes. Details in Sec. 3.3 and Eq. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>CLASTER improves the representation and clustering in unseen classes. The figure shows t-SNE<ref type="bibr" target="#b24">[25]</ref> of video instances, where each color corresponds to a unique unseen class label. The RL optimization improves the representation by making it more compact: in (b) instances of the same class, i.e. same color, are together and there are less outliers for each class compared to (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Analysis of how RL optimization changes the cluster to which an instance belongs. The frequencies are represented as percentages of instances in each cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Left: Learning curve for the seen classes. Right: Accuracy curve for the unseen classes. The clustering-based representation avoids overfitting, which in the case of seen classes means that the gap between validation and training accuracy is smaller than in the vanilla representation. This regularization effect improves the accuracy in unseen classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results of the ablation study of different components of CLASTER ZSL.</figDesc><table><row><cell>Component</cell><cell>HMDB51 Olympics</cell><cell>UCF101</cell></row><row><cell>No clustering</cell><cell cols="2">25.6 ? 2.8 57.7 ? 3.1 31.6 ? 4.6</cell></row><row><cell cols="3">Random clustering (K=6) 20.2 ? 4.2 55.4 ? 3.1 24.1 ? 6.3</cell></row><row><cell>K-means (K=6)</cell><cell cols="2">27.9 ? 3.7 58.6 ? 3.5 35.3 ? 3.9</cell></row><row><cell>CLASTER w/o SE</cell><cell cols="2">27.5 ? 3.8 55.9 ? 2.9 39.4 ? 4.4</cell></row><row><cell>CLASTER w/o RL</cell><cell cols="2">30.1 ? 3.4 60.5 ? 1.9 39.1 ? 3.2</cell></row><row><cell cols="3">CLASTER w/ NetVLAD 33.2 ? 2.8 62.6 ? 4.1 41.7 ? 3.8</cell></row><row><cell>CLASTER</cell><cell cols="2">36.8 ? 4.2 63.5 ? 4.4 46.4 ? 5.1</cell></row><row><cell cols="3">The study shows the effect of clustering, using visual-semantic representations, and</cell></row><row><cell cols="3">optimizing with different methods. All three components show a wide improvement</cell></row><row><cell cols="3">over the various baselines, suggesting that they are indeed complementary to improve</cell></row><row><cell>the final representation.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>CLASTER (ours) W 63.8 ? 5.7 36.6 ? 4.6 46.7 ? 5.4 CLASTER (ours) S 64.2 ? 3.3 41.8 ? 2.1 50.2 ? 3.8 CLASTER (ours) C 67.7 ? 2.7 42.6 ? 2.6 52.7 ? 2.2 CLASTER (ours) ED 68.4 ? 4.1 43.2 ? 1.9 53.9 ? 2.5 Results on ZSL. SE: semantic embedding, M: manual representation, W: word2vec embedding, S: sentence2vec, C: Combination of embeddings. The proposed CLASTER outperforms previous state-of-the-art across tasks and datasets.</figDesc><table><row><cell>Method</cell><cell>SE Olympics</cell><cell>HMDB51</cell><cell>UCF101</cell></row><row><cell>SJE [1]</cell><cell>M 47.5 ? 14.8</cell><cell>-</cell><cell>12.0 ? 1.2</cell></row><row><cell>Bi-Dir GAN [29]</cell><cell>M 53.2 ? 10.5</cell><cell>-</cell><cell>24.7 ? 3.7</cell></row><row><cell>IAF [29]</cell><cell>M 54.9 ? 11.7</cell><cell>-</cell><cell>26.1 ? 2.9</cell></row><row><cell>GGM [30]</cell><cell>M 57.9 ? 14.1</cell><cell>-</cell><cell>24.5 ? 2.9</cell></row><row><cell>OD [26]</cell><cell>M 65.9 ? 8.1</cell><cell>-</cell><cell>38.3 ? 3.0</cell></row><row><cell>WGAN [43]</cell><cell>M 64.7 ? 7.5</cell><cell>-</cell><cell>37.5 ? 3.1</cell></row><row><cell cols="2">CLASTER (ours) M 67.4 ? 7.8</cell><cell>-</cell><cell>51.8 ? 2.8</cell></row><row><cell>SJE [1]</cell><cell>W 28.6 ? 4.9</cell><cell>13.3 ? 2.4</cell><cell>9.9 ? 1.4</cell></row><row><cell>IAF [29]</cell><cell>W 39.8 ? 11.6</cell><cell>19.2 ? 3.7</cell><cell>22.2 ? 2.7</cell></row><row><cell>Bi-Dir GAN [29]</cell><cell>W 40.2 ? 10.6</cell><cell>21.3 ? 3.2</cell><cell>21.8 ? 3.6</cell></row><row><cell>GGM [30]</cell><cell>W 41.3 ? 11.4</cell><cell>20.7 ? 3.1</cell><cell>20.3 ? 1.9</cell></row><row><cell>WGAN [43]</cell><cell>W 47.1 ? 6.4</cell><cell>29.1 ? 3.8</cell><cell>25.8 ? 3.2</cell></row><row><cell>OD [26]</cell><cell>W 50.5 ? 6.9</cell><cell>30.2 ? 2.7</cell><cell>26.9 ? 2.8</cell></row><row><cell>PS-GNN [13]</cell><cell>W 61.8 ? 6.8</cell><cell>32.6 ? 2.9</cell><cell>43.0 ? 4.9</cell></row><row><cell>E2E [4]*</cell><cell>W 61.4 ? 5.5</cell><cell>33.1 ? 3.4</cell><cell>46.2 ? 3.8</cell></row><row><cell>ER [6]</cell><cell>ED 60.2 ? 8.9</cell><cell>35.3 ? 4.6</cell><cell>51.8 ? 2.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>CLASTER (ours) W 58.1 ? 2.4 42.4 ? 3.6 42.1 ? 2.6 CLASTER (ours) S 58.7 ? 3.1 47.4 ? 2.8 48.3 ? 3.1 CLASTER (ours) C 69.1 ? 5.4 48.0 ? 2.4 51.3 ? 3.5</figDesc><table><row><cell>Method</cell><cell>SE Olympics</cell><cell>HMDB51</cell><cell>UCF101</cell></row><row><cell>Bi-Dir GAN [29]</cell><cell>M 44.2 ? 11.2</cell><cell>-</cell><cell>22.7 ? 2.5</cell></row><row><cell>IAF [29]</cell><cell>M 48.4 ? 7.0</cell><cell>-</cell><cell>25.9 ? 2.6</cell></row><row><cell>GGM [30]</cell><cell>M 52.4 ? 12.2</cell><cell>-</cell><cell>23.7 ? 1.2</cell></row><row><cell>WGAN [43]</cell><cell>M 59.9 ? 5.3</cell><cell>-</cell><cell>44.4 ? 3.0</cell></row><row><cell>OD[26]</cell><cell>M 66.2 ? 6.3</cell><cell>-</cell><cell>49.4 ? 2.4</cell></row><row><cell cols="2">CLASTER (ours) M 68.8 ? 6.6</cell><cell>-</cell><cell>50.9 ? 3.2</cell></row><row><cell>IAF [29]</cell><cell cols="3">W 30.2 ? 11.1 15.6 ? 2.2 20.2 ? 2.6</cell></row><row><cell>Bi-Dir GAN [29]</cell><cell>W 32.2 ? 10.5</cell><cell cols="2">7.5 ? 2.4 17.2 ? 2.3</cell></row><row><cell>SJE [1]</cell><cell cols="2">W 32.5 ? 6.7 10.5 ? 2.4</cell><cell>8.9 ? 2.2</cell></row><row><cell>GGM[30]</cell><cell cols="3">W 42.2 ? 10.2 20.1 ? 2.1 17.5 ? 2.2</cell></row><row><cell>WGAN [43]</cell><cell cols="3">W 46.1 ? 3.7 32.7 ? 3.4 32.4 ? 3.3</cell></row><row><cell>PS-GNN [13]</cell><cell cols="3">W 52.9 ? 6.2 24.2 ? 3.3 35.1 ? 4.6</cell></row><row><cell>OD [26]</cell><cell cols="3">W 53.1 ? 3.6 36.1 ? 2.2 37.3 ? 2.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Setting Olympics HMDB51 UCF101 Ours and WGAN [43] ZSL 17.5 ? 4.5 7.0 ?3.8 17.4 ? 5.7 Ours and OD [26] ZSL 13.6 ? 4.5 2.4 ? 1.6 14.3 ? 2.7 Ours and E2E [4] ZSL 2.6 ? 2.8 3.7 ? 2.8 0.4 ? 1.8 Ours and WGAN [43] GZSL 11.2 ? 4.0 9.3 ? 3.7 8.1 ? 4.4 Ours and OD [26] GZSL 4.6 ? 2.4 5.2 ? 3.1 2.7 ? 1.8</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>WGAN [43] W 35.4 65.6 46.0 23.1 55.1 32.5 20.6 73.9 32.2 OD [26] W 41.3 72.5 52.6 25.9 55.8 35.4 25.3 74.1 37.7 CLASTER W 49.2 71.1 58.1 35.5 52.8 42.4 30.4 68.9 42.1 WGAN [43] S 36.1 66.2 46.7 28.6 57.8 38.2 27.5 74.7 40.2 OD [26] S 42.9 73.5 54.1 33.4 57.8 42.3 32.7 75.9 45.7 CLASTER S 49.9 71.3 58.7 42.7 53.2 47.4 36.9 69.8 48.3 CLASTER C 66.8 71.6 69.1 43.7 53.3 48.0 40.8 69.3 51.3</figDesc><table><row><cell>Model</cell><cell cols="3">E Olympics</cell><cell cols="3">HMDB51</cell><cell>UCF-101</cell></row><row><cell></cell><cell>u</cell><cell>s</cell><cell cols="2">H u</cell><cell>s</cell><cell cols="2">H u</cell><cell>s</cell><cell>H</cell></row><row><cell cols="5">WGAN [43] A 50.8 71.4 59.4 -</cell><cell>-</cell><cell cols="2">-30.4 83.6 44.6</cell></row><row><cell cols="5">OD [26] A 61.8 71.1 66.1 -</cell><cell>-</cell><cell cols="2">-36.2 76.1 49.1</cell></row><row><cell cols="5">CLASTER A 66.2 71.7 68.8 -</cell><cell>-</cell><cell cols="2">-40.2 69.4 50.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2927" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Self-supervised learning by cross-modal audio-video clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12667</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5297" to="5307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking zero-shot video classification: End-to-end training for realistic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chalupka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4613" to="4623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Elaborative rehearsal for zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13638" to="13647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08635</idno>
		<title level="m">Learning what data to learn</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cluster analysis of multivariate data: efficiency versus interpretability of classifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Forgy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">biometrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="768" to="769" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Concepts not alone: Exploring pairwise relationships for zero-shot video activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring semantic inter-class relationships (sir) for zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning attributes equals multi-source domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">I know the relationships: Zero-shot action recognition via two-stream graph convolutional networks and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8303" to="8311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to model relationships for zero-shot video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="34" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Actionvlad: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Alba: Reinforcement learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eustratiadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13039</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A new split for evaluating true zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13029</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic softmax loss for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="page" from="369" to="375" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for zeroshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2452" to="2460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A reinforcement learning approach to online clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Likas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1932" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Combining ontology and reinforcement learning for zero-shot classification. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="42" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zero-shot learning via attribute regression and class prototype rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="637" to="648" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Out-of-distribution detection for generalized zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9985" to="9993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatial-aware object embeddings for zero-shot localization and classification of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4443" to="4452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Zero-shot learning for action recognition using synthesized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="page" from="117" to="130" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A generative approach to zero-shot and few-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arulkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="372" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="392" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Zero-shot task generalization with multi-task deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2661" to="2670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised learning of sentence embeddings using compositional n-gram features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="528" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Zero-shot action recognition with error-correcting output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2833" to="2842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Script data for attribute-based recognition of composite activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2152" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ridge regression, hubness, and zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shigeto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Feature generating networks for zeroshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5542" to="5551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Zero-shot learning-the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4582" to="4591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transductive zero-shot action recognition by word-vector embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="333" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-task zero-shot action recognition with prioritised data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Zero-shot text classification via reinforced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3014" to="3024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards universal representation for unseen action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9436" to="9445" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
