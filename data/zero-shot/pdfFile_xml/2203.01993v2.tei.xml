<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Polarity Sampling: Quality and Diversity Control of Pre-Trained Generative Networks via Singular Values</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Imtiaz Humayun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Rice University ?</orgName>
								<orgName type="institution" key="instit2">AI Research ?</orgName>
								<address>
									<region>Meta</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Balestriero</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Rice University ?</orgName>
								<orgName type="institution" key="instit2">AI Research ?</orgName>
								<address>
									<region>Meta</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Baraniuk</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Rice University ?</orgName>
								<orgName type="institution" key="instit2">AI Research ?</orgName>
								<address>
									<region>Meta</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Polarity Sampling: Quality and Diversity Control of Pre-Trained Generative Networks via Singular Values</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Polarity Sampling, a theoretically justified plugand-play method for controlling the generation quality and diversity of any pre-trained deep generative network (DGN). Leveraging the fact that DGNs are, or can be approximated by, continuous piecewise affine splines, we derive the analytical DGN output space distribution as a function of the product of the DGN's Jacobian singular values raised to a power ?. We dub ? the polarity parameter and prove that ? focuses the DGN sampling on the modes (? &lt; 0) or anti-modes (? &gt; 0) of the DGN outputspace probability distribution. We demonstrate that nonzero polarity values achieve a better precision-recall (qualitydiversity) Pareto frontier than standard methods, such as truncation, for a number of state-of-the-art DGNs. We also present quantitative and qualitative results on the improvement of overall generation quality (e.g., in terms of the Fr?chet Inception Distance) for a number of state-of-theart DGNs, including StyleGAN3, BigGAN-deep, NVAE, for different conditional and unconditional image generation tasks. In particular, Polarity Sampling redefines the stateof-the-art for StyleGAN2 on the FFHQ Dataset to FID 2.57, StyleGAN2 on the LSUN Car Dataset to FID 2.27 and Style-GAN3 on the AFHQv2 Dataset to FID 3.95. Colab Demo.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Generative Networks (DGNs) have emerged as the go-to framework for generative modeling of highdimensional datasets, such as natural images. Within the realm of DGNs, different frameworks can be used to produce an approximation of the data distribution, e.g., Generative Adversarial Networks (GANs) <ref type="bibr" target="#b17">[18]</ref>, Variational Au-toEncoders (VAEs) <ref type="bibr" target="#b30">[31]</ref> or flow-based models <ref type="bibr" target="#b39">[40]</ref>. But despite the different training settings and losses that each of these frameworks aim to minimize, the evaluation metric of choice that is used to characterize the overall quality of generation is the Fr?chet Inception Distance (FID) <ref type="bibr" target="#b21">[22]</ref>. * equal contribution precision (quality)</p><formula xml:id="formula_0">recall (diversity) ? 0 ? &lt; 0 ? &gt; 0 ? 0 Figure 1.</formula><p>First row: Evolution of generation quality and diversity for varying truncation <ref type="bibr" target="#b28">[29]</ref> ? and polarity ?. Polarity Sampling achieves a better Pareto trade-off than truncation, e.g., polarity can be used to achieve a specified recall at higher precision or a specified precision at higher recall, compared to truncation. For additional Pareto examples, see <ref type="figure">Fig. 3</ref>. Second, Third, and Fourth row: Samples obtained from BigGAN-deep on Golden Retriever, Tiger and House Finch classes of Imagenet with samples of greater quality (? &lt; 0) and greater diversity (? &gt; 0). For examples with LSUN <ref type="bibr" target="#b53">[54]</ref>, see <ref type="figure" target="#fig_2">Fig. 4</ref>.</p><p>The FID is obtained by taking the Fr?chet Distance in the InceptionV3 <ref type="bibr" target="#b47">[48]</ref> embedding space between two distributions; the distributions are usually taken to be the training dataset and samples from a DGN trained on the dataset.</p><p>It has been established in prior work <ref type="bibr" target="#b44">[45]</ref> that FID nonlinearly combines measures of quality and diversity of the samples, which has inspired further research into disentanglement of these quantities as precision and recall <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b44">45]</ref> metrics respectively. Recent state-of-the-art DGNs such as BigGAN <ref type="bibr" target="#b7">[8]</ref>, StyleGAN2/3 <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>, and NVAE <ref type="bibr" target="#b52">[53]</ref>, have reached FIDs nearly as low as one could obtain when comparing subsets of real data with themselves. This has led to the deployment of DGNs in a variety of applications, such as real-world high-quality content generation and dataaugmentation. However, it is clear that, depending on the domain of application, generating samples from the best FID model could be suboptimal. For example, realistic content generation might benefit more from high-quality (precision) samples, while data-augmentation might benefit more from samples of high-diversity (recall), even if in each case, the overall FID slightly diminishes <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref>. Therefore, a number of state-of-the-art DGNs have introduced a controllable parameter to trade-off between the precision and recall of the generated samples, e.g., truncated latent space sampling <ref type="bibr" target="#b7">[8]</ref>, interpolating truncation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. However, these methods do not always work "out-of-thebox" <ref type="bibr" target="#b7">[8]</ref>, e.g., BigGAN requires orthogonal regularization of the DGN's parameters during training. These methods also lack a clear theoretical understanding which can limit their deployment for sensitive applications.</p><p>In this paper, we propose a principled solution to control the quality (precision) and diversity (recall) of DGN samples that does not require retraining nor specific conditioning of model training. Our method, termed Polarity Sampling, builds on our previous work on the analytical form of the learned DGN sample distribution <ref type="bibr" target="#b23">[24]</ref> and introduces a new hyperparameter, that we dub the polarity ? ? R, that adapts the latent space distribution for posttraining control. The polarity parameter provably forces the latent distribution to concentrate on the modes of the DGN distribution, i.e., regions of high probability (? &lt; 0), or on the anti-modes, i.e., regions of lowprobability (? &gt; 0); with ? = 0 recovering the original DGN distribution. The Polarity Sampling process depends only on the top singular values of the DGN's output Jacobian matrices evaluated at each input sample and can be implemented to perform online sampling. A crucial benefit of Polarity Sampling lies in its theoretical derivation from the analytical DGN data distribution <ref type="bibr" target="#b23">[24]</ref> where the product of the DGN Jacobian matrices singular values -raised to the power ? -provably controls the DGN samples distribution as desired. See <ref type="figure">Fig. 1</ref> for an initial example of Polarity Sampling in action.</p><p>Our main contributions are as follows: [C1] We first provide the theoretical derivation of Polarity Sampling based on the singular values of the generator Ja-cobian matrix. We provide pseudocode for Polarity Sampling and an approximation scheme to control its computational complexity as desired (Sec. 3).</p><p>[C2] We demonstrate on a range of DGNs and datasets that Polarity Sampling not only enables one to move on the precision-recall Pareto frontier (Sec. 4.1), i.e., it controls the quality and diversity efficiently, but it also reaches improved FID scores for each model (Sec. 4.2).</p><p>[C3] We leverage the fact that negative Polarity Sampling provides access to the modes of the learned DGN distribution, which enables us to explore several timely and important questions regarding DGNs. We provide visualization of the modes of trained GANs and VAEs (Sec. 5.1) and assess the perceptual smoothness around the modes (Sec. 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep Generative Networks as Piecewise-Linear Mappings. In most DGN settings, once training has been completed, sampling new data points is performed by first sampling latent space samples z i ? R K from a latent space distribution z i ? p z and then processing those samples throughout a DGN G :</p><formula xml:id="formula_1">R K ? R D to obtain the sample x i G(z i ), ?i.</formula><p>One recent line of research that we will rely on through our study consists in formulating DGNs as Continuous Piecewise Affine (CPA) mappings <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35]</ref>, that be expressed as</p><formula xml:id="formula_2">G(z) = ??? (A ? z + b ? )1 {z??} ,<label>(1)</label></formula><p>where ? is the input space partition induced by the DGN architecture, ? is a partition-region where z resides, and A ? , b ? are the corresponding slope and offset parameters. The CPA formulation of Eq. (1) either represents the exact DGN mapping, when the nonlinearities are CPA e.g. (leaky-)ReLU, max-pooling, or represents a first-order approximation of the DGN mapping. For more background on CPA networks, see <ref type="bibr" target="#b3">[4]</ref>. The key result from <ref type="bibr" target="#b11">[12]</ref> that we will leverage is that Eq. (1) is either exact, or can be made close enough to the true mapping G, to be considered exact for practical purposes.</p><p>Post-Training Improvement of a DGN's Latent Distribution. The idea that the training-time latent distribution p z might be suboptimal for test-time evaluation has led to multiple research directions to improve the quality of samples post-training. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b48">49]</ref> proposed to optimize the samples z ? p z based on a Wasserstein discriminator, leading to the Discriminator Optimal Transport (DOT) method. That is, after sampling a latent vector z, the latter is repeatedly updated such that the produced datum has greater quality. <ref type="bibr" target="#b49">[50]</ref> proposes to simply remove the samples that produce data out of the true data manifold. This can be viewed as a binary rejection decision of any new sample z ? p z . <ref type="bibr" target="#b1">[2]</ref> were the first to formally introduce rejection sampling based on a discriminator providing a quality estimate used for the rejection sampling of candidate vectors z ? p z . Replacing rejection sampling with the Metropolis-Hasting algorithm <ref type="bibr" target="#b20">[21]</ref> led to the method of <ref type="bibr" target="#b51">[52]</ref>, coined MH-GAN. An improvement made by <ref type="bibr" target="#b18">[19]</ref> was to use the Sampling-Importance-Resampling (SIR) algorithm <ref type="bibr" target="#b42">[43]</ref>. <ref type="bibr" target="#b25">[26]</ref> proposes latentRS which consists in training a WGAN-GP <ref type="bibr" target="#b19">[20]</ref> on top of any given DGN to learn an improved latent space distribution producing higher-quality samples. <ref type="bibr" target="#b25">[26]</ref> also proposes laten-tRS+GA, where the generated samples from that learned distribution are further improved through gradient ascent.</p><p>Truncation of the Latent Distribution. Latent space truncation was introduced for high-resolution face image generation by <ref type="bibr" target="#b32">[33]</ref> as a method of removing generated artifacts. The authors employed a latent prior of z ? U[?1, 1] during training and z ? U[?0.5, 0.5] for qualitative improvement during evaluation. The "truncation trick" was formally introduced by <ref type="bibr" target="#b7">[8]</ref> where the authors propose resampling latents z if they exceed a specified threshold for truncation. The authors also use weight orthogonalization during training to make truncation amenable. Style-based architectures <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> introduce a linear interpolation based truncation in the style-space, which is also designed to converge to the average of the dataset <ref type="bibr" target="#b28">[29]</ref>. Ablations for truncation in style-based generators are provided in <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Introducing The Polarity Parameter From First Principles</head><p>In this section, we introduce Polarity Sampling, a method that enables us to control the generation quality and diversity of DGNs. We will proceed by first expressing the analytical form of DGNs' output distribution (Sec. 3.1), and parametrizing the latent space distribution by the singular values of its Jacobian matrix and our polarity parameter (Sec. 3.2). We provide pseudo-code and an approximation strategy that enables fast sampling (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Analytical Output-Space Density Distribution</head><p>Given a DGN G, samples are obtained by sampling G(z) with a given latent space distribution, as in z ? p z . This produces samples that will lie on the image of G, the distribution of which is subject to p z , the DGN latent space partition ? and per-region affine parameters A ? , b ? . We denote the DGN output space distribution as p G . Under an injective DGN mapping assumption (g(z) = g(z ) =? z = z ) (which holds for various architectures, see, e.g., <ref type="bibr" target="#b40">[41]</ref>) it is possible to obtain the analytical form of the DGN output distribution by p G <ref type="bibr" target="#b23">[24]</ref>. For a reason that will become clear in the next section, we focus here on the case z ? U (D)</p><p>i.e., using a Uniform latent space distribution over the domain D. Leveraging the Moore-Penrose pseudo inverse <ref type="bibr" target="#b50">[51]</ref> A ? (A T A) ?1 A T , we obtain the following. Theorem 1. For z ? U (D), the probability density p G (x) is given by</p><formula xml:id="formula_3">p G (x) ? ??? det(A T ? A ? ) ? 1 2 1 {A ? ? (x?b?)???D} , (2)</formula><p>where det is the pseudo-determinant, i.e., the product of the nonzero eigenvalues of A T ? A ? . (Proof in Appendix B.1.) Note that one can also view det(A T ? A ? ) 1/2 as the product of the nonzero singular values of A ? . Theorem 1 is crucial to our development since it demonstrates that the probability of a sample x = g(z) is proportional to the change in volume (det(A T ? A ? ) 1/2 ) produced by the coordinate system A ? of the region ? in which z lies in (recall Eq. <ref type="formula" target="#formula_2">(1)</ref>). If a region ? ? ? has a slope matrix A ? that contracts the space (det(A T ? A ? ) &lt; 1) then the output density on that region -mapped to the output space region {A ? u + b ? : u ? ?} -is increased, as opposed to other regions that either do not contract the space as much, or even expand it (det(A T ? A ? ) &gt; 1). Hence, the concentration of samples in each output space region depends on how that region's slope matrix contracts or expands the space, relative to all other regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Controlling the Density Concentration with a Single Parameter</head><p>From Theorem 1 we can directly obtain an explicit parametrization of p z that enables us to control the distribution of samples in the output space, i.e., to control p G . In fact, note that one can sample from the mode of the DGN distribution by employing z ? U (? * ), ? * = arg min ??? det(A T ? A ? ). Alternatively, one can sample from the region of lowest probability, i.e., the anti-mode, by employing z ? U (? * ), ? * = arg max ??? det(A T ? A ? ). This directly leads to our Polarity Sampling method that adapts the latent space distribution based on the per-region pseudo-determinants.</p><formula xml:id="formula_4">Corollary 1. The latent space distribution p ? (z) ? ??? det(A T ? A ? ) ? 2 1 {z??} ,<label>(3)</label></formula><p>where ? ? R is the polarity parameter, produces the DGN output distribution</p><formula xml:id="formula_5">p G (x) ? ??? det(A T ? A ? ) ??1 2 1 {A ? ? (x?b?)???D} ,<label>(4)</label></formula><p>which falls back to the standard DGN distribution for ? = 0, to sampling of the mode(s) for ? ? ?? and to sampling of the anti-mode(s) for ? ? ?. (Proof in Appendix B.2.)</p><p>Polarity Sampling consists of using the latent space distribution Eq. (3) with a polarity parameter ?, that is either negative, concentrating the samples toward the mode(s) of the DGN distribution p G , positive, concentrating the samples towards the anti-modes(s) of the DGN distribution p G or zero, which removes the effect of polarity. Note that Polarity Sampling changes the output density in a continuous fashion. Its practical effect, as we will see in Sec. 4.1, is to control the quality and diversity of the obtained samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Approximation and Implementation</head><p>We now provide the details and pseudocode for the Polarity Sampling procedure that implements Corollary 1.</p><p>Computing the A ? Matrix. The per-region slope matrix as in Eq. (1), can be obtained given any DGN by first sampling a latent vector z ? ?, and then obtaining the Jacobian matrix of the DGN A ? = J z G(z), ?z ? ?. This has the benefit of directly employing automatic differentiation libraries and thus does not require any exhaustive implementation nor derivation. Computing J z G(z) of a generator is not uncommon in practice, e.g., it is employed during path length regularization of StyleGAN2 <ref type="bibr" target="#b29">[30]</ref>. Discovering the Regions ? ? ?. As per Eq. (3), we need to obtain the singular values of A ? (see next paragraph) for each region ? ? ?. This is often a complicated task, especially for state-of-the-art DGNs that can have a partition ? whose number of regions grows with the architecture depth and width <ref type="bibr" target="#b35">[36]</ref>. Furthermore, checking if z ? ? requires one to solve a linear program <ref type="bibr" target="#b14">[15]</ref>, which is expensive. As a result, we develop an approximation that consists of sampling many z ? U (D) vectors from the latent space (hence our uniform prior assumption in Corollary 1), and computing their corresponding matrices A ?(z) . This way, we are guaranteed that A ?(z) corresponds to the slope of the region ? in which z falls in, removing the need to check whether z ? ?. We do so over N samples obtained uniformly from the DGN latent space (based on the original latent space domain). Selection of N can impact performance as this exploration needs to discover as many regions from ? as possible. <ref type="bibr" target="#b16">[17]</ref>. However, not all singular values might be relevant, e.g., the smallest singular values that are nearly constant across regions ? can be omitted without altering Corollary 1. Hence, we employ only the top-k singular values of A ? to speed up singular value computation to O(Dk 2 ), details provided in Appendix A.3. (Further approximation could be employed if needed, e.g., power iteration <ref type="bibr" target="#b38">[39]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Singular Value Computation. Computing the singular values of</head><formula xml:id="formula_6">A ? is an O(min(K, D) 3 ) operation</formula><p>While the required number of latent space samples N and the number of top singular values k might seem to be a limitation of Polarity Sampling, we have found in practice that N and k for state-of-the-art DGNs can be set at Algorithm 1 Polarity Sampling procedure with polarity ?; online version and 2D examples in Appendix. Algorithm 2 and <ref type="figure" target="#fig_8">Fig. 11</ref>. For implementation details, see Sec. 3.3. We summarize how to obtain S samples using the above steps in the pseudocode given in Algorithm 1 and provide an efficient solution to reduce the memory requirement incurred when computing the large matrix A ? in Appendix A.4. We also provide an implementation that enables online sampling in Algorithm 2 (Appendix A.1). It is also possible to control the DGN prior p z with respect to a different space than the data-space e.g. inception-space, or with a different input space than the latent-space e.g. stylespace in StyleGAN2/3. This incurs no changes in Algorithm 1 except that the DGN is now considered to be either a subset of the original one, or to be composed with a VGG/InceptionV3 network. We provide the implementation details for style-space, VGG-space, and Inception-space in Appendix A.5. In those cases, the partition ? and the per-region mapping parameters A ? , b ? are the ones of the corresponding sub-network or composition of networks (recall Eq. <ref type="formula" target="#formula_2">(1)</ref>). Polarity Sampling adapts the DGN prior distribution to obtain the modes or anti-modes with respect to the considered output spaces.</p><formula xml:id="formula_7">Input: K &gt; 0, S &gt; 0, N S, G, D, ? ? R Z, S, R ? [], [], [] for n = 1, . . . , N do z ? U (D) ? = SingularValues(J z G(z), decreasing = True) Z.append(z) S.append(? K k=1 log(?[k] + )) for n = 1, . . . , S do i ? Categorical(prob = softmax(S)) R.append(Z[i]) Output: R</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Controlling Precision, Recall, and FID via Polarity</head><p>We now provide empirical validation of Polarity Sampling with an extensive array of experiments. Since calculation of distribution metrics such as FID, precision, and recall are sensitive to image processing nuances, we use each model's original code repository except for BigGAN-deep on ImageNet <ref type="bibr" target="#b12">[13]</ref>, for which we use the evaluation pipeline specified for ADM <ref type="bibr" target="#b13">[14]</ref>. For NVAE (trained on colored-MNIST [1]), we use a modified version of the StyleGAN3 evaluation pipeline. Precision and recall metrics are all based on the implementation of <ref type="bibr" target="#b31">[32]</ref>. Metrics in Tab. 2 are calculated for 50K training samples to be able to compare with existing latent reweighing methods. For all other results, the metrics are calculated using min{N D , 100K} training samples, where N D is the number of samples in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Polarity Efficiently Parametrizes the Precision-Recall Pareto Frontier</head><p>As we have discussed above, Polarity Sampling can explicitly sample from the modes or anti-modes of any learned DGN distribution. Since the DGN is trained to fit the training distribution, sampling from the modes and anti-modes correspond to sampling from regions of the data manifold that are approximated better/worse by the DGN . Therefore, Polarity Sampling is an efficient parameterization of the trade-off between precision and recall of generation <ref type="bibr" target="#b31">[32]</ref> since regions with higher precision are regions where the manifold approximation is more accurate.</p><p>As experimental proof, we provide in <ref type="figure">Fig. 3</ref> the precision-recall trade-off when sweeping polarity, and compare it with truncation <ref type="bibr" target="#b28">[29]</ref> for pretrained StyleGAN{2,3} architectures. We see that Polarity Sampling offers a competitive alternative to truncation for controlling the precision-recall trade-off of DGNs across datasets and models. For any given precision, the ? parameter allows us to reach greater recall than what is possible via latent space truncation <ref type="bibr" target="#b28">[29]</ref>. And conversely, for any given recall, it is possible to reach a higher precision than what can be attained using latent space truncation. We see that diversity collapses rapidly for latent truncation compared to Polarity Sampling, across all architectures, which is a major limitation. In addition to that, controlling both truncation and polarity allows us to further extend the Pareto frontier for all of our experiments. Recall <ref type="figure">Figure 3</ref>. Pareto frontier of the precision-recall metrics can be obtained solely by varying the polarity parameter, for any given truncation level. We depict here six different models and datasets. Results for additional models and datasets are provided in <ref type="figure">Fig. 1</ref> and <ref type="figure">Fig. 8</ref>.</p><p>Apart from the results presented here, we also see that polarity can be used to effectively control the precisionrecall trade-off for BigGAN-deep <ref type="bibr" target="#b7">[8]</ref> and ProGAN <ref type="bibr" target="#b26">[27]</ref>. ProGAN unlike BigGAN and StyleGAN, is not compatible with truncation based methods, i.e., latent space truncation has negligible effect on precision-recall. Hence, polarity offers a great benefit over those existing solutions: Polarity Sampling can be applied regardless of training or controllability factors that are preset in the DGN design. We provide additional results in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Polarity Improves Any DGN's FID</head><p>We saw in Sec. 4.1 that polarity can be used to control quality versus diversity in a meaningful and controllable manner. In this section, we connect the effect of polarity with FID. Recall that the FID metric nonlinearly combines quality and diversity <ref type="bibr" target="#b44">[45]</ref> into a distribution distance measure. Since polarity allows us to control the output distri- bution of the DGN, an indirect result of polarity is the reduction of FID by matching the inception embedding distribution of the DGN with that of the training set distribution.</p><formula xml:id="formula_8">(modes ?) ? = ?2 ? = ?1 ? = ?0.5 ? = ?0.2 (baseline) ? = 0 ? = 0.2 ? = 0.5 ? = 1 (? anti-modes) ? = 2 LSUN Cars LSUN Cats LSUN Church</formula><p>Recall that ? = 0 recovers the baseline DGN sampling; for all the state-of-the-art methods in question, we reach lower (better) FID by using a nonzero polarity. In Tab. 1, we compare Polarity Sampling with state-of-the-art solutions that propose to improve FID by learning novel DGN latent space distributions, as were discussed in Sec. 2. We see that for a StyleGAN2 pre-trained on the LSUN church <ref type="bibr" target="#b53">[54]</ref> dataset, by increasing the diversity (? = 0.2) of the VGG embedding distribution, Polarity Sampling surpasses the FID of methods reported in literature that post-hoc improves quality of generation.</p><p>In Tab. 2, we present for LSUN {Church, Car, Cat} <ref type="bibr" target="#b53">[54]</ref>, ImageNet <ref type="bibr" target="#b12">[13]</ref>, FFHQ <ref type="bibr" target="#b28">[29]</ref>, and AFHQv2 <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref> improved FID obtained solely by changing the polarity ? of a state-ofthe-art DGN. This implies that Polarity Sampling provides an efficient solution to adapt the DGN latent space.</p><p>We observe that, given any specific setting, ? = 0 always improves a model's FID. We see that in a case specific manner, both positive and negative ? improves the FID.For  StyleGAN2-F trained on FFHQ, increasing the diversity of the inception space embedding distribution helps reach a new state-of-the-art FID. By increasing the precision of StyleGAN3-T via Polarity Sampling in the Vgg space, we are able to surpass the FID of baseline StyleGAN2-F <ref type="bibr" target="#b27">[28]</ref>. We observe that controlling the polarity of the InceptionV3 embedding distribution of StyleGAN2-F gives the most significant gains in terms of FID. This is due to the fact that the  <ref type="table">Table 2</ref>. ? Paper reported metrics. We observe that moving away from ? = 0, Polarity Sampling improves FID across models and datasets, empirically validating that the top singular values of a DGN's Jacobian matrices contain meaningful information to improve the overall quality of generation .</p><p>Frechet distance between real and generated distributions is directly affected while performing Polarity Sampling in the Inception space. We provide generated samples in <ref type="figure" target="#fig_2">Fig. 4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">New Insights into DGN Distributions</head><p>In Sec. 4 we demonstrated that Polarity Sampling is a practical method to manipulate DGN output distributions to control their quality and diversity. We now demonstrate that Polarity Sampling has more foundational theoretical applications as well. In particular, we dive into several timely questions regarding DGNs that can be probed using our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Are GAN/VAE Modes Training Samples?</head><p>Mode collapse <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b46">47]</ref> has complicated GAN training for many years. It consists of the entire DGN collapsing to generate a few different samples or modes. For VAEs, modes can be expected to be related to the modes of the empirical dataset distribution, as reconstruction is part of the objective. But this might not be the case with GANs e.g., the modes can correspond to parts of the space where the discriminator is the least good at differentiating between true and fake samples. There has been no reported methods in literature that allows us to observe the modes of a trained GAN. Existing visualization techniques focus on finding the role of each DGN unit <ref type="bibr" target="#b5">[6]</ref> or finding images that GANs cannot generate <ref type="bibr" target="#b6">[7]</ref>. Using Polarity Sampling, we can visualize the modes of DGNs for the first time. In <ref type="figure" target="#fig_4">Fig. 5</ref>, we present samples from the modes of BigGAN-deep trained on Ima-geNet, StyleGAN3 trained on AFHQv2, and NVAE trained on colored-MNIST. We observe that BigGAN modes tend to reproduce the unique features of the class, removing the background and focusing more on the object that the class is assigned to. AFHQv2 modes on the other hand, focus on younger animal faces and smoother textures. NVAE mode sampling predominately produce the digit '1' which corresponds to the dataset mode (digit with the least intra-class  <ref type="figure">Figure 6</ref>. Distribution of l2 distance to 3 training set nearest neighbors at 32 ? 32 resolution, for 1000 generated samples from LSUN Church StyleGAN2 (left) and colored-MNIST NVAE (right). Samples closer to the modes (? &lt; 0) have a significant shift in the distribution closer to the training samples for NVAE, while for StyleGAN2 the distribution shift is minimal with significant overlap. This behavior is expected as VAE models are encouraged to position their modes on the training samples, as opposed to GANs whose modes depend on the discriminator. variation). We also provide in <ref type="figure">Fig. 6</ref> the distribution of the l 2 distances between generated samples and their 3 nearest training samples for modal (? = ?5) and anti-modal (? = 1) polarity. We see that even after reducing the polarity, StyleGAN2 nearest neighbor distributions have overlap whereas for NVAE the modes move significantly closer to the training samples. In Appendix. <ref type="figure" target="#fig_4">Fig. 15</ref> we observe a similar effect for WGAN and NVAE trained on MNIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Perceptual Path Length Around Modes</head><p>Perceptual Path Length (PPL) is the distance between the Vgg space image of two latent space points. It has previously been proposed as a measure of perceptual distance <ref type="bibr" target="#b29">[30]</ref>. In <ref type="figure" target="#fig_5">Fig. 7</ref>, we report the PPL of a StyleGAN2-F trained on FFHQ, for an interpolation step of length 10 ?4 between endpoints from the latent/style space. We sample points using Polarity Sampling varying ? ? [1, ?1], es- sentially measuring the PPL for regions of the data manifold with increasing density as we increase ?. We see that for negative values of polarity, we have significantly lower PPL compared to positive polarity or even baseline sampling (? = 0). This result shows that for StyleGAN2, there are smoother perceptual transitions closer to modes. While truncation also reduces the PPL, it essentially does so by sampling points closer to the style space mean <ref type="bibr" target="#b28">[29]</ref>, see Appendix C.5 for comparisons. Polarity Sampling in the Vgg space, can be used to directly sample from Vgg modes, making it the first method that can be used to explicitly sample regions that are perceptually smoother. It can therefore be used to develop sophisticated interpolation methods where, the interpolation is done along a high-likelihood path on a feature space manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have proposed a new parameterization of the DGN prior p z in terms of a single parameter -the polarity ?to force the DGN samples to be concentrated on the distribution modes or anti-modes (Sec. 3). As a byproduct, for a range of DGNs, we improve the state-of-the-art FID performance. On the theoretical side, Polarity Sampling's guarantee that it samples from the modes of a DGN enabled us to explore some timely open questions, including the relation between distribution modes and training samples (Sec. 5.1), and the effect of going from mode to anti-mode generation on the perceptual path length (Sec. 5.2). We show that Polarity sampling can also be performed on feature space distributions of classifiers appended with a generator, which can be possibly used for fair attribute generation, out-ofdistribution synthetic data generation and much more.</p><p>We provide the following supplementary materials (SMs) as support of our theoretical and empirical claims. This SM is organized as follows.</p><p>Appendix A provides all the implementation details. We first provide the pseudocode for the online sampling algorithm in Appendix A.1 that allows for polarity sampling to be performed rapidly for real-time applications. Along it, we provide further details on the effect of the two hyperparameters of polarity sampling that are N and k, namely the number of samples used to estimate the per-region singular values of A ? for as many ? i.e. N of them, and the number k of top-singular values to utilize (Appendix A.2). We then describe the computation times we observed on our hardware/software (Appendix A.3).</p><p>Appendix B provides the proofs for Theorem 1 and Corollary 1. Appendix C supports our claims with additional experiments on various dataset and models. First, Appendix C.1 studies how polarity sampling can help under distribution shift between the training distribution and a target distribution, this is done through colored-MNIST and NVAE. Then Appendix C.2 proposes to study the effect of polarity sampling on ProGAN which is crucial as ProGAN does not allow for truncation based control of its samples.</p><p>We conclude with Appendix D and Appendix E that provide descriptions of the datasets and additional qualitative samples from the empirical experiments performed in the main part of the paper, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details and Online Sampling Solution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Online Algorithm</head><p>One important aspect of polarity sampling, as summarized in Algorithm 1 is the need to first sample the DGN latent space to obtain the top singular values of as many per-region slope matrices A ? as possible. This might seem as a bottleneck if one wants to repeatedly apply polarity sampling on a same DGN. However, this is to provide an estimate of the DGN per-region change of variables, and as the DGN is not retrained nor fine-tuned, it only needs to be done once. Furthermore, this allows for an online sampling algorithm that we provide in Algorithm 2. In short, one first perform this task of estimating as many per-region top singular values as possible, once this is completed, only sampling of latent vectors z and rejection sampling based on the corresponding A ? matrix is done online, A ? of the sampled z being obtained easily via A ? = J G(z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Online Rejection Sampling Algorithm</head><p>Input: Latent space domain, D; Generator G; N change of volume scalars {? 1 , ? 2 , ..., ? N }; Number of singular values K;</p><formula xml:id="formula_9">while True do z ? U (D) ? ? U [0, 1] A = J G (z) ? z = K k=1 K-SingularV alues(A, K) if ? ? z ? ? z + N i=1 ? ? i ? ? then x ? G(z) return x A.2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Effect of N and k</head><p>One important aspect of our algorithm comes from the two hyper-parameters N and k. They represent respectively the number of latent space samples to use to estimate as much A ? as possible (recall Algorithm 1), and the number of top singular values to compute. Both represent a tradeoff between exact polarity sampling, and computation complexity. We argue that in practice, N ? 150K and k ? 100 is enough to obtain a good estimate of the polarity sampling distribution (Eq. (4)). To demonstrate that, we first provide an ablation study of the number of N and k used for polarity sampling in Tab. 3 and Tab. 4. We also present a visual inspection of the impact of N and k on the precision and recall in <ref type="figure">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Computation Times and Employed Software/Hardware</head><p>All the experiments were run on a Quadro RTX 8000 GPU, which has 48 GB of high-speed GDDR6 memory and 576 Tensor cores. For the software details we refer the reader to the provided codebase. In short, we employed TF2 (2.4 at the time of writing), all the usual Python scientific libraries such as NumPy and PyTorch. We employed the official repositories of the various models we employed with official pre-trained weights. As a note, most of the architectures can not be run on GPUs with less or equal to 12 GB of memory.  <ref type="table">Table 4</ref>. Ablation of K and its effect on the best FID, Precision and Recall values that can be obtained by a StyleGAN2 (? = 1) on the FFHQ dataset. We vary the polarity in the inception space for FFHQ dataset</p><p>We report here the Jacobian computation times for Tensorflow 2.5 with CUDA 11 and Cudnn 8 on an NVIDIA Titan RTX GPU. For StyleGAN2 pixel space, 5.03s/it; Style-GAN2 style-space, 1.12s/it; BigGAN 5.95s/it; ProgGAN 3.02s/it. For NVAE on Torch 1.6 it takes 20.3s/it. Singular value calculation for StyleGAN2 pixel space takes .005s/it, StyleGAN2 style space .008s/it, BigGAN .001s/it, Prog-GAN .004s/it and NVAE .02s/it on NumPy. According to this, for StyleGAN2-e, N=250,000 requires 14 days to obtain. This only needs to be done once, and it is also possible to perform online sampling once it is calculated. The time required for this is relatively small compared to the training time required for only one set of hyperparameters, which is 35 days and 11 hours 1 . We have added pseudocode for MaGNET sampling and online sampling in Appendix G.</p><p>Computational Complexity. We are computing the topk singular values of the D ?K Jacobian matrix. This can be performed in O(DKk + Dk 2 ). In fact, one has to project k K-dimensional vectors onto the D ? K Jacobian's matrix: O(DKk) and then perform QR-decomposition of the D?k matrix: O(Dk 2 ). Then, k D-dimensional vectors are pro-1 https://github.com/NVlabs/stylegan2 jected onto the transpose of the Jacobian matrix: O(DKk) followed by their QR-decomposition: O(Kk 2 ), dominated by O(Dk 2 ) (full SVD runs in O(DK 2 )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Reducing Memory Requirements</head><p>The core of polarity sampling relies on computing the top-singular values of the possibly large matrix A ? for a variety of regions ? ? ?, discovered through latent space sampling (recall Algorithm 1). One challenge for state-ofthe-art DGNs lies in the size of the matrices A ? . Multiple solutions exist, such as computing the top singular values through block power iterations. Doing so, the matrices A ? do not need to be computed entirely, only the matrix-matrix product A ? W and A T ? V needs to be performed repeatedly (interleaved with QR decompositions). After many iterations, W estimate the top right-singular vectors of A ? , and V the corresponding top left-singular vectors from which the singular values can be obtained. However, we found this solution to remain computationally extensive, and found that in practice, a simpler approximation that we now describe provided sufficiently accurate estimates.</p><p>Instead of the above iterative estimation, one can instead compute the top-singular values of W A ? with W a semiorthogonal matrix of shape D ?D with D &lt; D (recall that A ? is of shape D ? K). Doing so, we are now focusing on the singular values of A ? whose left-singular vectors are not orthogonal with the right singular vectors of W . While this possibly incurs an approximation error, we found that the above was sufficient to provide polarity sampling and adequate precision-recall control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Applying Polarity Sampling in Style, VGG and Inception Space</head><p>We call the ambient space of the images the pixelspace, because each dimension in this space corresponds to individual pixels of the images. Apart from controlling the density of the pixel-space manifold, polarity can also be used to control the density of the style-space manifold for style based architectures such as StyleGAN{1,2,3} <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. We also extend the idea of intermediate manifolds to feature space manifolds such as VGG or Incep-tionV3 space, which can be assumed continuous mappings of the pixel space to the corresponding models' bottleneck embedding space. In <ref type="figure">Fig. 8</ref>-left we present comparisons between Style, Pixel, VGG and Inception space precisionrecall curves for StyleGAN2-F FFHQ with ? = 1, topk = 30 and ? = [?2, 2]. We see that the VGG and In-ceptionV3 curves trace almost identically. This is expected behavior since both these feature spaces correspond to perceptual features, therefore the transform they induce on the pixel space distribution is almost identical. On the other hand, the pixel space distribution saturates at high polarity at almost equal values. The point of equal precision and recall for both the Inception and VGG spaces, occurs at a polarity of 0.1. It's clear from the figures that feature space polarity changes have a larger effect on precision and recall compared to pixel-space and style-space has the least effect on precision and recall. This could be due to the number of density transforms the style-space distribution undergoes until the VGG space, where precision and recall is calculated. In <ref type="figure">Fig. 8-right</ref> we present the polarity characteristics for StyleGAN2-E, StyleGAN2-F and Style-GAN3. For each model, we choose the best space w.r.t the pareto frontier, VGG and Inception space for StyleGAN2-E and StyleGAN2-F, and pixel-space for StyleGAN3. Notice that StyleGAN3 exceeds the recall of the other two models for negative polarity, while matching the precision for StyleGAN2-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proofs</head><p>The proofs of the two main claims of the paper heavily rely on the spline form of the DGN input-output mapping from Eq. (1). For more background on the form of the latent space partition ?, the per-region affine mappings A ? , b ? , ?? ? ? and further discussion on how to deal with DGN including smooth activation functions, we refer the reader to <ref type="bibr" target="#b3">[4]</ref>, and in particular to <ref type="bibr" target="#b23">[24]</ref> for DGN specific results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Proof of Theorem 1</head><p>Proof. We will be doing the change of variables z =</p><formula xml:id="formula_10">(A T ? A ? ) ?1 A T ? (x ? b ? ) A ? ? (x ? b ? )</formula><p>, also notice that J G ?1 (x) = A ? . First, we know that P G(z) (x ? w) = P z (z ? G ?1 (w)) = G ?1 (w) p z (z)dz which is well defined based on our full rank assumptions. We then proceed by</p><formula xml:id="formula_11">P G (x ? w) = ??? ??w p z (G ?1 (x)) ? det(J G ?1 (x) T J G ?1 (x))dx = ??? ??w p z (G ?1 (x)) ? det((A + ? ) T A + ? )dx = ??? ??w p z (G ?1 (x)) 1 det(A T ? A ? ) dx,</formula><p>where the second to third equality follows by noticing that ? i (A ? ) = (? i (A)) ?1 which can be showed easily be replacing A ? with its SVD and unrolling the product of matrices. Now considering a uniform latent distribution case on a bounded domain U in the DGN latent space we obtain by substitution in the above result</p><formula xml:id="formula_12">p G (x) = ??? 1 x?? det(A T ? A ? ) ? 1 2 V ol(U ) ,<label>(5)</label></formula><p>leading to the desired result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Proof of Corollary 1</head><p>Proof. The proof of this result largely relies on Theorem 1. Taking back our previous result, we know that</p><formula xml:id="formula_13">p G (x) = ??? p z (G ?1 (x))1 {G ?1 (x)??} 1 det(A T ? A ? )</formula><p>dx.</p><p>(6) However, recall that polarity sampling leverages the prior probability given by</p><formula xml:id="formula_14">p ? (z) = 1 ? ??? det(A T ? A ? ) ? 2 1 {z??} ,<label>(7)</label></formula><p>which, after replacing G ?1 (x) with its corresponding z becomes</p><formula xml:id="formula_15">p G (x) = ??? 1 ? det(A T ? A ? ) ? 2 det(A T ? A ? ) 1 {z?? } dx,<label>(8)</label></formula><p>and simplifies to</p><formula xml:id="formula_16">p G (x) = ??? 1 ? det(A T ? A ? ) ??1 2 1 {z?? } dx,<label>(9)</label></formula><p>leading to the desired result. Note that when ? = 1 then the density is uniform onto the DGN manifold, when ? = 0, one recovers the original DGN density onto the manifold, and in the extreme cases, only the region with highest or lowest probability would be sampled i.e. the modes or antimodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extra experiments C.1. Polarity Helps Under Distribution Shift</head><p>A last benefit of polarity sampling is to adapt a sampling distribution to a reference distribution that suffered a distribution shift. For example, this could occur when training a DGN on a training set, and using it for content generation with a slightly different type target samples. In fact, as long as the distribution shift remains reachable by the model i.e. in the support of p G , altering the value of ? will help to shift the sampling distribution, possible to better match the target one. In all generality, there is no guarantee that any benefit would happen for ? = 0, however, for the particular case where the distribution shift only changes the way the samples are distributed (on the same domain), we observe in <ref type="figure" target="#fig_2">Fig. 14</ref>  experimental setting, we took the colored-MNIST dataset and the NVAE DGN model <ref type="bibr" target="#b52">[53]</ref> and produce a training set with a Gaussian hue distribution favoring blue and two test set, one with same hue distribution and one with uniform hue distribution. We observe that ? can provide a beneficial distribution shift to go from the biased-hue samples to the uniform-hue one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. ProGAN Polarity Sweep</head><p>Previously in Sec. 4.1, we have drawn note to the fact that ProGAN <ref type="bibr" target="#b26">[27]</ref>, an architecture which is widely used, but does not incorporate truncation, can also be controlled via polarity sampling. In Tab. 5 we present precision-recall characteristics for polarity sweep on ProGAN. As control, we also perform latent space truncation as in <ref type="bibr" target="#b7">[8]</ref> by sampling a truncated gaussian distribution, parameterized by its support [??, ?]. We change ? between [10 ?10 , 10] and notice that for ? smaller than 10 ?4 , the generator collapses to 0 precision and recall. Other than that, it maintains a precision of 0.72 and recall of 0.34. Using polarity sweep, we also exceed the baseline FID on CelebAHQ 1024x1024 attained by ProGAN; polarity of ?.01 in pixel-space reduces the FID from 7.37 to 7.28.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. FID for truncated models</head><p>While the FID improvement for some of the methods we present are not significantly large, we see that for truncated models, i.e., models with ? &lt; 1, ? &gt; 0 provides significant FID improvements by increasing diversity, e.g., for StyleGAN2-FFHQ with ? = {.9, .7, .5}, increasing ? &gt; 0 improves FID by {.69, 8.11, 11.1} points. Such truncation is commonly used in practice for qualitative experiments, making polarity sampling particularly relevant in such settings. Since truncation reduces the range of the generator and polarity increases the diversity of sampling within the range, both can be combined to achieve greater FID improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. NVAE Negative Log-Likelihood for Varying ?</head><p>To validate the effect of ? on the likelihood of generated samples, we estimate the negative log-likelihood of samples generated via an NVAE trained on colored-MNIST while varying ?.</p><p>We generate 5000 samples each for ? = {?5, ?1, ?.5, ?.1, 0, .1, .5, 1, 5} which yields negative log-likelihood values of {3.0, 3.2, 3.5, 3.8, 3.9, 4.1, 4.3, 4.6, 5.1}?10 ?2 bits/dim. This shows that decreasing ? &lt; 0 samples high-likelihood points while increasing ? &gt; 0 samples lower-likelihood points compared to standard sampling (? = 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. Perceptual Path Length for Constant Latent Shifts</head><p>In Sec. 5.2 we present the PPL variation for a 10 ?4 interpolation step from a latent space point towards another random latent space point. To evaluate the perceptual smoothness around regions of the latent space, we also calculate the PPL for paths of length .3 starting from individual latent space points towards random directions. We see that for both StyleGAN2-FFHQ and BigGAN-Imagenet, PPL decreases monotonically with ? &lt; 0 whereas first de- <ref type="figure">Figure 10</ref>. Modes for BigGAN-deep trained on Imagenet, and conditioned on a specific class: "pug" (top left), "lion" (top right), "cheeseburger" (bottom left) and "pomerian" (bottom right). We observe that the modes correspond to nearly aligned faces with little to no background. Variation of colors and sizes can be seen across the modes. The same observation can be made for the cheeseburger, nearly no background is present, and the shape is consistent to a typical cheeseburger "template". See <ref type="figure">Fig. 19</ref> for additional classes. creases and then increases for ? &lt; 1.      . For ? = ?5 we see that both distributions have a peak around 5. For WGAN the distribution has a significantly longer tail compared to NVAE, indicating that the WGAN modes don't necessarily coincide with training points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Dataset Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. colored-MNIST</head><p>We perform controlled experiments on NVAE <ref type="bibr" target="#b52">[53]</ref> by training on datasets with and without controllable distribution shifts. To control the shift, we colorize MNIST with hue ranging [0, ?] by 1) uniformly sampling the hue and 2)  <ref type="table">Table 5</ref>. FID, Precision and Recall metrics of ProGAN <ref type="bibr" target="#b26">[27]</ref> with polarity sweep in the pixel space.</p><p>sampling the hue for each image from a truncated normal distribution, with a truncation scale of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. LSUN Dataset</head><p>We use the LSUN dataset <ref type="bibr" target="#b53">[54]</ref> available at the official website 2 . We preprocess the dataset using the StyleGAN2 repository 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. AFHQv2 and FFHQ</head><p>We use the version 2 of AFHQ that was released prepackaged with StyleGAN3 for our experiments. For FFHQ we use also use TFRecords provided with Style-GAN2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. License</head><p>The majority of Polarity-Sampling is licensed under CC-BY-NC, however portions of the project are available under separate license terms: NVAE, StyleGAN2 and StyleGAN3 are licensed under the NVIDIA license; guided-diffusion is licensed under the MIT license. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative results</head><p>We provide in the following pages, Figs. 16 to 19 that correspond to LSUN Cats, LSUN Cars, AFHQv2 samples with varying ? values and 800 Imagenet modes. For LSUN Cars and LSUN Cats we draw comparisons between varying truncation and varying polarity independently.   5} and ? = .9 in pixel-space. As we move right from baseline (middle column) we see an increase in texture diversity of images, whereas, moving left, we see images with smoother textures. <ref type="figure">Figure 19</ref>. Depiction of a single mode (large negative ?) for each class of the first 800 Imagenet classes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIDFigure 2 .</head><label>2</label><figDesc>Effect of Polarity Sampling on FID of a StyleGAN2-F model pretrained on FFHQ for varying number of top-k singular values (left) and varying number of latent space samples N used to obtain per-region slope matrix A? singular values (right) (recall Sec. 3.3 and Algorithm 1). The trend in FIDs to evaluate the impact of ? stabilizes when using around k = 40 singular values and N ?200,000 latent space samples. For the effect of k and N on precision and recall, see Fig. 9. N ? 200K, k ? [30, 100]. We conduct a careful ablation study and demonstrate the impact of different choices for N and k in Fig. 2 and Tabs. 3 and 4 in Appendix A.2. Computation times and software/hardware details are provided in Appendix A.3. To reduce round-off errors that can occur for extreme values of ?, we compute the product of singular values in log-space, as shown in Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Curated samples of cars and cats for Polarity Sampling in style-space, and church for Polarity Sampling in pixel-space. (Qualitative comparison with truncation sweep inFig. 10and nearest training samples inFig. 12in the Appendix.) None of the images correspond to training samples, as we discuss in Sec. 5.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>LSUN</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Modes for BigGAN-deep, StyleGAN3-T and NVAE obtained via ? 0 Polarity Sampling. This is, to the best of our knowledge, the first visualization of the modes of DGNs in pixel space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Distribution of PPL for StyleGAN2-F trained on FFHQ with varying Polarity Sampling (in VGG space) setting (? given in the legend) for endpoints in the input latent space (left) and endpoints in style-space (right). The means of the distributions (PPL score) are provided as markers on the horizontal axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>that ? = 0 can provide benefit. To control the Top: Precision Recall tradeoff for polarity sweep on VGG, Inception and Pixel space distributions. Bottom: BigGANdeep Imagenet pareto curves obtained for a few classes, in red is the baseline while each scatter point can be reached by varying truncation and ?. Calculated with 1300 real and generated sam-Effect of Polarity Sampling on Precision (top) and Recall (bottom) of a StyleGAN2-F model pretrained on FFHQ for varying number of top-k singular values (right) and varying number of latent space samples N (left) used to obtain per-region slope matrix A? singular values (recall Sec. 3.3 and Algorithm 1). The trend in metrics stabilizes when using around N ?300,000 latent space samples. Increasing the number of top-k singular values to use, amplifies the effect of polarity, saturating at around k = 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>For StyleGAN2-FFHQ we acquire PPL = {281, 316} for ? = {0.1, 0} and PPL = {274, 271} for ? = {?2, ?10}. For BigGAN-GoldenRetreiver we get PPL 35.9 for ? = 0 and 0.20 for ? = ?2. This possibly indicates that decreasing truncation might not always lead to perceptually smoother regions whereas decreasing polarity does.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>Polarity sweep for a WGAN trained on 2D toy datasets with 4 gaussians (top row), 4 gaussians with triangular domain (middle row) and two circles (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>Modes for a VAE trained on colored-MNIST with 8 nearest neighbors. The leftmost column for each figure contains generated samples. Notice the higher prevalence of digit 1. Due to low pixel variations, digit 1 samples have high density on the manifold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 .Figure 14 .</head><label>1314</label><figDesc>Singular value distribution and Marchenko Pastur distribution fit for StyleGAN2-FFHQ (left) and BigGAN-Imagenet (middle). Log-sigma distribution for StyleGAN2-FFHQ with varying ? (right). FID, precision and recall for an NVAE trained on colored-MNIST with hue bias. Metrics are calculated for a test dataset with hue distribution Left: identical to training and Right: uniformly distributed across digit classes. Polarity allows adapting the DGN output distribution to balance possible distribution shifts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 .</head><label>15</label><figDesc>Distribution of l2 distance to 3 MNIST training set nearest neighbors, for 1000 generated MNIST samples from WGAN (left) and NVAE (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 .</head><label>16</label><figDesc>Uncurated samples of LSUN Cats using (top) ? = {?1, ?.5, ?.2, ?.1, 0}, ? = .8 and (bottom) ? = {.7, .73, .75, .77, .8}; both representing regions with roughly an equal span of recall score on Fig. 3. Notice the significant precision of the left-most columns of top compared to the left-most of bottom, where at equal diversity, top has significantly higher precision score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 17 .</head><label>17</label><figDesc>Uncurated samples of LSUN Cars using (top) ? = {?.1, ?.075, ?.05, ?.025, 0}, ? = .7 and (bottom) ? = {.5, .55, .6, .65, .7}; both representing regions with roughly an equal span of precision score on Fig. 3 Notice the significant diversity in (top) especially in the leftmost columns, where the recall score is significantly higher than that of the leftmost column of bottom left, with equal precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 18 .</head><label>18</label><figDesc>Uncurated samples of AFHQv2 using ? = {?2, ?1, ?.5, ?.2, 0, .01, .1, .2, .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Church 256?256</cell></row><row><cell cols="4">StyleGAN2 variant FID ? Prec ? Recall ?</cell></row><row><cell>Standard SIR  ? [43] DOT  ? [49] latentRS  ? [26] latentRS+GA  ? [26] ?-sampling 0.2</cell><cell>6.29 7.36 6.85 6.31 6.27 6.02</cell><cell>.60 .61 .67 .63 .73 .57</cell><cell>.51 .58 .48 .58 .43 .53</cell></row></table><note>. Comparison of Polarity Sampling with latent reweight- ing techniques from literature. FID, Precision and Recall is cal- culated using 50,000 samples.? Metrics reported from papers due to unavailability of code.? Precision-recall is calculated with 1024 samples only.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.yf.io/p/lsun 3 https://github.com/NVlabs/stylegan2 4 https://github.com/openai/guided-diffusion/blob/main/LICENSE</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Humayun and Baraniuk were supported by NSF grants CCF-1911094, IIS-1838177, and IIS-1730574; ONR grants N00014-18-12571, N00014-20-1-2534, and MURI N00014-20-1-2787; AFOSR grant FA9550-22-1-0060; and a Vannevar Bush Faculty Fellowship, ONR grant N00014-18-1-2047.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>Codes available at github.com/AhmedImtiazPrio/magnet-polarity Google Colab demo bit.ly/polarity-samp</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06758</idno>
		<title level="m">Discriminator rejection sampling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A spline theory of deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="374" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mad max: Affine spline insights into deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mggan: Solving mode collapse using manifold-guided training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duhyeon</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2347" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10597</idno>
		<title level="m">Gan dissection: Visualizing and understanding generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Seeing what a gan cannot generate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4502" to="4511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Instance-conditioned GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marlene</forename><surname>Careil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Your gan is secretly an energy-based model and you should use discriminator driven latent sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Paull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06060</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8188" to="8197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingrid</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">A</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Dym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shira</forename><surname>Faigenbaum-Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kung-Ching</forename><surname>Kovalsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josiah</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
		<idno>abs/2107.13191</idno>
		<title level="m">Guergana Petrova, and Barak Sober. Neural network approximation of refinable functions</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05233</idno>
		<title level="m">Diffusion models beat gans on image synthesis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep neural networks as 0-1 mixed integer linear programs: A feasibility study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Fischetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Jo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.06174</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Doom level generation using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Giacomello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pier</forename><forename type="middle">Luca</forename><surname>Lanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Loiacono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Games, Entertainment, Media Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="316" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Singular value decomposition and least squares solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linear algebra</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1971" />
			<biblScope unit="page" from="134" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial networks. Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bias correction of learned generative models using likelihood-free importance weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09531</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<title level="m">Improved training of wasserstein gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Monte carlo sampling methods using markov chains and their applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Keith</forename><surname>Hastings</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 30</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11239</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Magnet: Uniform sampling from deep generative network manifolds without retraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Imtiaz Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2022</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Crash data augmentation using variational autoencoder</title>
	</analytic>
	<monogr>
		<title level="j">Accident Analysis &amp; Prevention</title>
		<editor>Zubayer Islam, Mohamed Abdel-Aty, Qing Cai, and Jinghui Yuan</editor>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">105950</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Latent reweighting, an almost free improvement for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Issenhuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Tanielian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Mary</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.09803</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12423</idno>
		<title level="m">Aliasfree generative adversarial networks</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Improved precision and recall metric for assessing generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Kynk??nniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06991</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marchesi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00082</idno>
		<title level="m">Megapixel size image creation using generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02163</idno>
		<title level="m">Unrolled generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Mont?far</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.1869</idno>
		<title level="m">On the number of linear regions of deep neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sharp bounds for the number of regions of maxout networks and vertices of minkowski sums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Mont?far</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08135,2021.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03841</idno>
		<title level="m">Generating images with sparse representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09672</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A power method for the structured singular value</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Packard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Waveglow: A flow-based generative network for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3617" to="3621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ivan Dokmani?, and Maarten de Hoop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Puthawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konik</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Lassas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08464,2020.3</idno>
	</analytic>
	<monogr>
		<title level="m">Globally injective relu networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14866" to="14876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Using the sir algorithm to simulate posterior distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donald B Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian statistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Image super-resolution via iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07636</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00035</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.01007</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Projected gans converge faster</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Veegan: Reducing mode collapse in gans using implicit variational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lazar</forename><surname>Valkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Michael U Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3310" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Tanaka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06832</idno>
		<title level="m">Discriminator optimal transport</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning disconnected manifolds: a no gan&apos;s land</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Tanielian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Issenhuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elvis</forename><surname>Dohmatob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?mie</forename><surname>Mary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9418" to="9427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Numerical linear algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Trefethen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Bau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">50</biblScope>
			<pubPlace>Siam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Metropolis-hastings generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunus</forename><surname>Saatchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6345" to="6353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03898</idno>
		<title level="m">Nvae: A deep hierarchical variational autoencoder</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
