<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimizing the Neural Architecture of Reinforcement Learning Agents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mazyavkina</surname></persName>
							<email>[n.mazyavkina@skoltech.ru</email>
							<affiliation key="aff0">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moustafa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Trofimov</surname></persName>
							<email>ilya.trofimov@skoltech.ru</email>
							<affiliation key="aff0">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burnaev</surname></persName>
							<email>e.burnaev]@skoltech.ru</email>
							<affiliation key="aff0">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimizing the Neural Architecture of Reinforcement Learning Agents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>AutoML</term>
					<term>Neural Architecture Search</term>
					<term>Reinforcement Learn- ing</term>
					<term>Atari</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reinforcement learning (RL) enjoyed significant progress over the last years. One of the most important steps forward was the wide application of neural networks. However, architectures of these neural networks are quite simple and typically are constructed manually. In this work, we study recently proposed neural architecture search (NAS) methods for optimizing the architecture of RL agents. We create two search spaces for the neural architectures and test two NAS methods: Efficient Neural Architecture Search (ENAS) and Single-Path One-Shot (SPOS). Next, we carry out experiments on the Atari benchmark and conclude that modern NAS methods find architectures of RL agents outperforming a manually selected one.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the last several years, deep learning (DL) has experienced enormous growth in popularity among the researchers from both the academia and the industry. Moreover, each of the separate tasks solved by the DL methods requires its own approach, one of the most important aspects of which is the choice of the neural network's (NN) architecture. In this case, it is essential to demonstrate good expertise and experience in the problem's field. However, even then, the chosen architecture may not give any acceptable results until various heuristics and tricks will be applied to its construction. This motivated the emergence of the neural architecture search (NAS) field, which focuses on automating the ways to find the optimal architecture for the specific tasks.</p><p>Another family of methods that has been gaining popularity is reinforcement learning (RL) and deep reinforcement learning (deep RL), in particular. It consolidates a vast collection of machine learning methods, designed to solve a variety of Markov-Decision-Process-like problems. Over the last several years the successes of RL and deep RL has been frequently demonstrated by the research community: from better-than-human performance in ATARI <ref type="bibr" target="#b26">[27]</ref>, DOTA 2 <ref type="bibr" target="#b23">[24]</ref>, Go <ref type="bibr" target="#b32">[33]</ref> to robotic manipulation <ref type="bibr" target="#b13">[14]</ref>. In deep RL, neural networks are usually equal contribution arXiv:2011.14632v3 [cs.LG] 28 Apr 2021 used to approximate a value function, in the case of the value-based methods, or a policy function, in the case of policy gradient methods. Moreover, actor-critic RL algorithms <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b24">25]</ref> combine these two NN approximations, in order to gain even better performance. Consequently, finding a suitable NN architecture is also a vital part of designing RL experiments.</p><p>In this work, we are going to explore deep RL as a new application of NAS, i.e. deriving well-performing NN architectures for RL tasks. The motivation for the research in this direction is the following:</p><p>1. Only a single NN architecture is often chosen for many common benchmarks such as ATARI <ref type="bibr" target="#b26">[27]</ref> and MuJoCo <ref type="bibr" target="#b35">[36]</ref>, despite of them consisting of a big number of different environments. Hence, automatically finding a suitable network for each of the environments may lead to better results; 2. NAS can be useful in the cases of more complicated environments with bigger state and action spaces, where a more complicated deeper network might be required.</p><p>Early NAS methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> required training of numerous neural architectures. However, even training of one RL agent takes a significant amount of time. In this paper, we limit ourselves to fast one-shot methods, which perform architecture search in the time not significantly larger than the training time of a single neural network. Most of the NAS methods were developed for computer vision applications, the major part -for the object classification problem. At the same time, reinforcement learning is quite a different problem. The performance of an RL agent, that is, average reward, is not differentiable like the cross-entropy of object classification. Thus, only few popular NAS methods are suited for RL. In this work, we evaluate ENAS <ref type="bibr" target="#b43">[44]</ref> and SPOS <ref type="bibr" target="#b14">[15]</ref>.</p><p>The contribution of our paper is the following: we experimentally prove that modern one-shot NAS methods can be successfully applied for optimizing the neural architecture of RL agents. The source code is publicly available from https:// github.com/ NinaMaz/ NAS RL torch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Early neural architecture search (NAS) approaches treated this problem as a black-box optimization, that is, search over a discrete domain of architectures. Such methods are quite general but require training of numerous architectures and vast computational resources. One of the first proposed methods of this kind <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> used reinforcement learning for the optimization process itself. Architecture creation, layer by layer, was done by an RL agent. Thus, the reward was the performance of the constructed network. Other works proposed evolutionary optimization <ref type="bibr" target="#b28">[29]</ref>, bayesian optimization based on Gaussian processes <ref type="bibr" target="#b15">[16]</ref>, bayesian performance predictors based on architecture features <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b31">32]</ref>. Black-box optimization enjoy speedup from multi-fidelity methods <ref type="bibr" target="#b36">[37]</ref>. Several benchmarks for NAS were developed <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>The later family of methods -one-shot NAS -gone beyond black-box optimization and utilized the structure a neural network. These methods involve the supernetwork, which contain all the architectures from the search space as its subnetworks. Thus, all the architectures share weights of some of the blocks. The architecture search in the supernetwork is performed simultaneously with the training of networks themselves. The one-shot methods are: ENAS <ref type="bibr" target="#b27">[28]</ref>, numerous modifications of DARTS <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b5">6]</ref>, single path one-shot <ref type="bibr" target="#b14">[15]</ref>, random search with weight sharing <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Most of the existing research focuses on problems from computer vision and linguistics. There are no papers about applications of modern NAS methods to RL to the best of our knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reinforcement Learning Methods</head><p>In our experiments, we have used reinforcement learning for training both ENAS controller and sampled child networks. On the other hand, SPOS does not use a trainable controller for architecture sampling and, hence, the RL methods, mentioned in this section, do not concern it. An LSTM controller, used in the ENAS framework, is trained with REIN-FORCE <ref type="bibr" target="#b38">[39]</ref> algorithm. REINFORCE belongs to a group of policy-based methods, which focuses on the straightforward approximation of the optimal policy, via calculating the direct gradient of the parameterized objective function J:</p><formula xml:id="formula_0">? ? J ? = E t ? ? log ? ? (a t |s t )R t .</formula><p>In our case, ? are the parameters of the neural network, outputting the logits, from which the actions a t can be derived; ? is the policy, s t are the states, R t is the sum of the discounted rewards collected so far. Specifically, in the case of REINFORCE algorithm, the update to the parameters ? takes the form:</p><formula xml:id="formula_1">? ? ? + ? t R t ? ? log ? ? (a t |s t ),</formula><p>where ? is the discount factor. In order to reduce the variance of the gradient estimation from the formula above, we subtract the moving average baseline from the discounted reward function.</p><p>In terms of the training process of the child networks, we use another policybased method -Proximal Policy Optimization (PPO) <ref type="bibr" target="#b30">[31]</ref> to update their parameters. The objective function for PPO has the following form:</p><formula xml:id="formula_2">J P P O ? = E t min ratio t (?)A t , clip(ratio t (?), 1 + , 1 ? )A t ,</formula><p>where A t is the advantage function, ratio is the probability ratio under the new and old policies, is the clipping parameter.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Architecture Search Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adaptation to Reinforcement Learning</head><p>Most of the existing research on NAS is dedicated to computer vision (particularly, object classification) or computational linguistics applications. We adapt the existing one-shot NAS methods to reinforcement learning. One-shot methods assume the supernetwork which contain all the architectures from the search space as its subgraphs ( <ref type="figure" target="#fig_1">fig. 1</ref>). All the architectures share weights of some of the blocks. That is, each layer in the supernetwork i has a list of Block i options, only one option can be selected for a particular subnetwork. Such simplification was proposed to reduce the co-adaptation between blocks. The whole search space is A = Block 0 ? . . . ? Block n . Some initial or final layers of the supernetwork can be fixed and not contain choice blocks. One-shot methods typically contain a fitting stage. During the fitting stage, the subnetwork ? ? A is sampled by some rule and its weights ?(?) are updated by a SGD-like step for a batch of data B</p><formula xml:id="formula_3">?(?) ? ?(?) ? ?? ?(?) i?B (y i , N (?, ?(?), x i )),<label>(1)</label></formula><p>where (?) is the loss function, N (?, ?, x i ) is the network of the architecture ? having weights ?(?). The evaluation of the architecture ? typically involves calculation of performance (accuracy) on the validation dataset D val</p><formula xml:id="formula_4">1 |D val | i?D val [y i = N (?, ?(?), x i )].<label>(2)</label></formula><p>We adapt one-shot methods to RL in the following way. In our experiments, the neural network N (?, ?(?), x i ) corresponds to a policy ? ?(?) . Instead of SGD-like step (1) we do the step of PPO</p><formula xml:id="formula_5">?(?) ? ?(?) ? ?? ?(?) J P P O ?(?) .<label>(3)</label></formula><p>The performance of the network is estimated by E t [R t [? ?(?) ] instead of (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Efficient Neural Architecture Search (ENAS)</head><p>Child network Controller Action -sampling a child architecture Reward -the accuracy of the trained child network Efficient Neural Architecture Search (ENAS) <ref type="bibr" target="#b27">[28]</ref> is a NAS method, used in our experiments to find a well-performing neural network in the ATARI games environment. ENAS consists of a controller, which samples child models, a search strategy and an evaluation strategy.</p><p>In the case of ENAS, the controller is an LSTM network that outputs a one-hot-encoded architecture of a child model. The controller's inputs are the previous step's architecture and the reward it has received. The possible choices for a set of child architectures are determined by the search space. The particular variants of the search spaces that we have used are covered in section 5.2.</p><p>The authors of <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b42">43]</ref> have proposed to train an ENAS controller by using RL. In our experiments, we have employed a REINFORCE algorithm, which has also been used in the original paper to update the controller's network parameters.</p><p>In the original paper, due to the different nature of the problems ENAS has been tested on, the child networks, sampled by the controller, are trained until convergence. However, this becomes difficult when the RL environments are considered -the agents usually require longer training times, and the stochasticity of the environments makes the training process much more unpredictable. In our work, we will demonstrate that despite the aforementioned problems, the number of child training timesteps, chosen for our experiments, is still sufficient to determine which networks are better-performing than others.</p><p>Finally, the overall sequence of "sampling a child architecture -training a child model -feeding the resulting reward to LSTM controller" (see <ref type="figure" target="#fig_2">fig. 2</ref>) defines a single epoch of ENAS training. In the previous works on iterative RL NAS methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, the fact that such an epoch will take a long time to compute, has limited these methods' practicality in regard to the real-life problems. The authors of ENAS, however, have come up with the solution to this problem by sharing the weights of all of the child models. This way, the ENAS becomes much more efficient than its predecessors, and, therefore, much more suitable for RL problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Single-Path One-Shot with Uniform Sampling (SPOS)</head><p>The method "Single-path one-shot with uniform sampling" was proposed in <ref type="bibr" target="#b14">[15]</ref>. The SPOS method assumes two steps: 1) supernetwork fitting 2) best architecture selection. The distinctive feature of SPOS is that subnetworks are sampled from the supernetwork uniformly at random.</p><p>Thus, weights ? * of the supernetwork are the solution of the following problem</p><formula xml:id="formula_6">? * = argmin ? E ??P [L train (N (?, ?(?)))],</formula><p>where L train (?) is the train loss, P -the uniform distribution. During the architecture selection phase, the best subnetwork ? * is selected by the validation accuracy Acc val ? * = argmax ??A Acc val (N (?, ? * (?))).</p><p>This step requires only inference for the validation data. In the original paper <ref type="bibr" target="#b14">[15]</ref>, an evolutionary optimization was used to solve (4) since the search space was huge. Instead of evolutionary optimization, we do the full search since our search spaces are small. The adaptation of SPOS to RL is done as described in the Section 4.1. The subnetwork N (?, ?(?)) corresponds to a policy ? ?(?) . Thus, SPOS for optimizing the neural architecture of the RL agent solves the following problem</p><formula xml:id="formula_8">? * = argmax ? E ??P E t [R t [? ?(?) ]],<label>(5)</label></formula><formula xml:id="formula_9">? * = argmax ??A E t [R t [? ? * (?) ].<label>(6)</label></formula><p>5 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Atari Environment</head><p>In our experiments, we used the Open AI Gym framework <ref type="bibr" target="#b4">[5]</ref>, particularly -Breakout and Freeway Atari environments. We chose the Breakout because it's a popular benchmark, having moderate standard deviation of RL agent's reward (401.2 ? 26.9, <ref type="bibr" target="#b25">[26]</ref>). In opposite to the Breakout, the Freeway environment has very low relative standard deviation of reward (30.3 ? 0.7, <ref type="bibr" target="#b25">[26]</ref>). The reward of RL agent with random behavior for Breakout and Freeway is nearly zero so we can make sure that our policy network makes non-stochastic behavior. We have trained the child networks in the manner described in <ref type="bibr" target="#b11">[12]</ref>, i.e., we have used 8 agents, sharing a policy, trained simultaneously in 8 environments with PPO, in order to collect the trajectories for the policy update. Each of these agents is trained for 128 steps. After that, the controller collects the architecture's rewards. The controller's policy is updated every ten steps using the REINFORCE algorithm. Overall, the number of training steps for one experiment equals 10 million. More implementation details are in Appendix C.</p><p>It is important to note that the 'scratch' experimental results demonstrate lower reward values than the ones reported in the original PPO paper <ref type="bibr" target="#b30">[31]</ref>. This is due to the fact that in our experiments we have used a smaller number of training timesteps than the classical version of PPO (10M vs. 40M). The reason for this has been that the main aim of our research focuses on investigating whether NAS has a positive effect on RL training process overall, and not on beating the existing ATARI baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Search Spaces</head><p>The search spaces that we designed are the extension of the Nature-CNN architecture <ref type="bibr" target="#b25">[26]</ref>, where convolutional layers are followed by a linear layer which outputs the number of values equal to the size of our action space <ref type="bibr" target="#b25">[26]</ref>.</p><p>In order to facilitate the varying sizes of the layers' parameters and, hence, enable the weight sharing, we use the following techniques in the overall design of the network:</p><p>1. Convolution and max-pooling to the same size: Map the input to one size every time after each convolutional and max-pooling layer. 2. Padding to exact size: Map the output of the last convolutional layer to the target size to be able to fix it during the flattening of the convolutions.</p><p>In our experiments we have covered two architectural search spaces: Search space 1: 25 architectures. The architecture starts with a fixed convolutional layer with input channels equal to 4, and the output channels -to 32, kernel size -8 and moving stride -4. This layer is followed by two convolutional layers with output channels equal to 64 per each and with strides 1 and 2. The choices for kernel sizes are 1, 2, 3, 4, 5. The size of this search space is 5 2 .</p><p>Search space 2: 32 architectures. Instead of two convolutional layers (we do not take into account the first fixed convolutional layer, as we do not vary its kernel size) used in the search space 1, we have used 5 convolutional layers with output channels equal to 64, moving strides equal to 1. The choices for kernels are 2 and 5, which makes the size of this search space equal to 2 5 .  The experimental architecture spaces that we have used can be seen in <ref type="figure" target="#fig_4">Figure  3a</ref> and <ref type="figure" target="#fig_4">Figure 3b</ref>, and also in Appendix A. The search spaces do not contain very deep architectures, having depth 7 at most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Methodology</head><p>Firstly, we trained all the architectures from scratch (implementation details are in Appendix C). For all of those architectures we saved mean reward and total reward averaged over last 100 episodes. These metrics were used as a tabular benchmark, eliminating the need to train the same architecture multiple times. We used these metrics later to compare the performances of the found architectures by various NAS methods.</p><p>Both NAS methods under evaluation (ENAS, SPOS) share the same highlevel structure:</p><p>1. Run neural architecture search method for the given search space A; 2. Select top-K architectures from the search space A by a proxy performance; 3. Train these top-K architectures from scratch; 4. Return the best one by the true performance.</p><p>In our experiments, we selected top-3 architectures for the methods under evaluation. We repeated the search 4 times with different seeds and averaged results.</p><p>The proxy performance is a part of the NAS method, and it is fast to calculate. The calculation time of the proxy performance is negligible to the time of RL agent training from scratch. For ENAS, the proxy performance of an architecture is the probability of sampling this architecture by the controller. For SPOS, the proxy performance of an architecture is calculated from weights of this architecture in the supernetwork. Namely, the proxy performance is the mean reward of an agent with corresponding weights.</p><p>The true performance is the total/mean reward of an RL agent trained from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Random search baseline</head><p>As a simple baseline, we used the following random search algorithm:</p><p>1. Select K architectures from the search space A at random; 2. Train these K architectures from scratch; 3. Return the best one by the true performance.</p><p>As for ENAS and SPOS, we selected top-3 architectures. The only difference with ENAS/SPOS methods is that architectures are selected at random instead of by proxy performance. We estimated the variance of the random search by repeating it for 1000 times. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Experiments with larger search spaces</head><p>We have also tried to expand the search space by increasing the number of consecutive convolutional layers up to 9 and choosing a suitable kernel size and a number of output channels for each of them. However, the results were close to the ones received by random search, which can be caused by the fact that NAS can experience problems with increasingly complex search spaces. For that reason, we do not present the results of these experiments in the paper. <ref type="table" target="#tab_0">Table 1</ref> shows the results of our experiments. For the Breakout environment, ENAS performs better than random search on the search space 1, while SPOS performs better on the search space 2. For the Freeway environment, there is no clear benefit of NAS methods. Variance of the both of the methods are quite high. At the same time, SPOS is simpler since it does not contain an auxiliary controller network. It is interesting to compare the performances of found architectures with the performance of manually selected Nature-CNN architecture <ref type="bibr" target="#b25">[26]</ref> (see description in the Appendix B). The Nature-CNN architecture belongs to the search space 1. For the fair comparison, in <ref type="table" target="#tab_0">Table 1</ref>, we report the rewards after training of the Nature-CNN architecture with our pipeline (Section 5.1). The architectures found by NAS methods outperform the manually selected Nature-CNN by a considerable margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>The existing research on RL methods typically uses the same architectures for different environments. However, we found that this is not optimal. Different architectures are optimal for different environments, see Appendix D. <ref type="figure" target="#fig_6">Figure 4</ref> shows histograms of RL agents' mean rewards from different search spaces. The red vertical lines depict the best architecture found by NAS methods.  We conclude that NAS methods can find top architectures in both of the search spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Traditionally, the progress in RL field came mostly from the development of new methods. Neural architectures of RL agents remained relatively simple when compared to computer vision applications. In this paper, we have applied modern neural architecture search methods for optimizing the architecture of RL agents. We have evaluated ENAS <ref type="bibr" target="#b43">[44]</ref> and SPOS <ref type="bibr" target="#b14">[15]</ref> methods. Both of them found better architectures than manually picked by experts. We suppose that many RL application can benefit from using better neural architectures. Testing NAS methods for larger search spaces is an interesting topic for further research.  B Nature CNN architecture <ref type="table" target="#tab_4">Table 3</ref> describes the architecture "Nature CNN" <ref type="bibr" target="#b25">[26]</ref> which is a member of the search space 1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Search Spaces</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer input output kernel stride</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation details</head><p>In this section, we present the hyperparameters used for training the RL agents in the ATARI games environment <ref type="table" target="#tab_6">(Table 4)</ref>, as well as the hyperparameters used for ENAS and SPOS (       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Supernetwork architecture for a generic design. Each block can be a part of neural network, and each connection between sequence of blocks can be a complete architecture. The blue path represent a complete architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>A general framework of Neural Architecture Search (NAS)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Convolution + ReLU (b) Convolution + ReLU Choices: kernel {1, 2, 3, 4}, Stride {2} (c) Convolution + ReLU Choices: kernel {1, 2, 3, 4}, Stride {1} Search space 1: Block(a): fixed kernel size:8, fixed stride:4; Block(b) and Block(c): kernel size is chosen between 1, 2, 3, 4, and 5, and fixed stride: 2, 1 respectively. (a) Convolution + ReLU (b) Convolution + ReLU Choices: kernel:{2, 5} (c) Convolution + ReLU Choices: kernel:{2, 5} (d) Convolution + ReLU Choices: kernel:{2, 5} (e) Convolution + ReLU Choices: kernel:{2, 5} (f) Convolution + ReLU Choices: kernel:{2, 5} Search space 2: Block(a): fixed kernel size:8, fixed stride:4; Block(b)-Block(f): kernel size is chosen between 2 and 5, with fixed stride: 1 for all of them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Search spaces used in experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :</head><label>4</label><figDesc>Histograms for the reward mean of RL agents having different architectures from search spaces. The red vertical line depicts the best architecture found by NAS methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Reward mean and total reward of RL agents with various architectures. Each score is the mean of last 100 episodes.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Search space 1</cell><cell cols="2">Search space 2</cell></row><row><cell></cell><cell></cell><cell>Breakout</cell><cell>Freeway</cell><cell>Breakout</cell><cell>Freeway</cell></row><row><cell>Random Search</cell><cell cols="5">reward mean 54.7 ? 8.3 28.4 ? 1.0 33.1 ? 29.5 21.6 ? 0.2 total reward 147.2 ? 25.5 31.7 ? 0.8 105.7 ? 94.2 22.0 ? 0.2</cell></row><row><cell cols="2">Nature-CNN [26], reward mean</cell><cell>57.1</cell><cell>13.1</cell><cell>-</cell><cell>-</cell></row><row><cell>reproduced</cell><cell>total reward</cell><cell>157.9</cell><cell>19.0</cell><cell>-</cell><cell>-</cell></row><row><cell>ENAS</cell><cell cols="5">reward mean 61.4 ? 1.8 26.4 ? 1.1 30.7 ? 21.7 21.5 ? 0.1 total reward 161.1 ? 9.8 30.7 ? 1.3 91.4 ? 64.4 22.0 ? 0.2</cell></row><row><cell>SPOS</cell><cell cols="5">reward mean 39.7 ? 18.6 29.6 ? 0.8 39.9 ? 41.0 21.7 ? 0.1 total reward 144.4 ? 55.0 29.4 ? 5.0 180.6 ? 72.5 22.0 ? 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 describes</head><label>2</label><figDesc>two search spaces that we used in our experiments. An abbreviation {1, ..., n} means that a parameter can vary from 1 to n in a search space.</figDesc><table><row><cell></cell><cell cols="3">Search Space 1</cell><cell></cell><cell></cell><cell cols="3">Search Space 2</cell><cell></cell></row><row><cell>Layer</cell><cell cols="4">input output kernel stride</cell><cell>Layer</cell><cell cols="4">input output kernel stride</cell></row><row><cell>Conv-1</cell><cell>4</cell><cell>32</cell><cell>8</cell><cell>4</cell><cell>Conv-1</cell><cell>4</cell><cell>32</cell><cell>8</cell><cell>4</cell></row><row><cell>Conv-2</cell><cell>32</cell><cell>64</cell><cell>1, .., 5</cell><cell>2</cell><cell>Conv-2</cell><cell>32</cell><cell>64</cell><cell>2, 5</cell><cell>1</cell></row><row><cell>Conv-3</cell><cell>64</cell><cell>64</cell><cell>1, .., 5</cell><cell>1</cell><cell>Conv-3</cell><cell>64</cell><cell>64</cell><cell>2, 5</cell><cell>1</cell></row><row><cell>Padding</cell><cell>?</cell><cell cols="2">121  *  64 ?</cell><cell>?</cell><cell>Conv-4</cell><cell>64</cell><cell>64</cell><cell>2, 5</cell><cell>1</cell></row><row><cell cols="3">Linear 121  *  64 512</cell><cell>?</cell><cell>?</cell><cell>Conv-5</cell><cell>64</cell><cell>64</cell><cell>2, 5</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv-6</cell><cell>64</cell><cell>64</cell><cell>2, 5</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Padding</cell><cell>?</cell><cell cols="2">121  *  64 ?</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Linear 121  *  64 512</cell><cell>?</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Detailed description of the search spaces.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Convolution network architecture that used in<ref type="bibr" target="#b25">[26]</ref> that's contain 3 convolution layer followed by flatten and linear layers.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>). We use the same set of parameters for both training from scratch experiments, and training the child networks sampled by the NAS controllers.</figDesc><table><row><cell>Hyperparameter's name</cell><cell>Value</cell></row><row><cell># timesteps</cell><cell>10M</cell></row><row><cell># runner timesteps</cell><cell>128</cell></row><row><cell>(PPO)</cell><cell>0.1</cell></row><row><cell>value loss coef. (PPO)</cell><cell>0.25</cell></row><row><cell>? (GAE)</cell><cell>0.95</cell></row><row><cell>entropy coef.</cell><cell>0.01</cell></row><row><cell>learning rate</cell><cell>CosineAnnealing(0.00025)</cell></row><row><cell># parallel env.</cell><cell>8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The hyperparameters for training PPO agents on ATARI games.</figDesc><table><row><cell>Hyperparameter's name</cell><cell>Value</cell></row><row><cell># child network epochs</cell><cell>10</cell></row><row><cell># NAS runner timesteps</cell><cell>3</cell></row><row><cell>NAS entropy coef.</cell><cell>0.0001</cell></row><row><cell>NAS learning rate</cell><cell>CosineAnnealing(0.001)</cell></row><row><cell>baseline momentum.</cell><cell>0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The hyperparameters for ENAS training.D The best architecturesTables 6,<ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9</ref> show the best architectures for each game. The architectures tend to be similar for both of the search spaces except the kernel size for the last layers.</figDesc><table><row><cell>Layer</cell><cell cols="4">input output kernel stride</cell></row><row><cell>Conv-1</cell><cell>4</cell><cell>32</cell><cell>8</cell><cell>4</cell></row><row><cell>Conv-2</cell><cell>32</cell><cell>64</cell><cell>4</cell><cell>2</cell></row><row><cell>Conv-3</cell><cell>64</cell><cell>64</cell><cell>5</cell><cell>1</cell></row><row><cell>Padding</cell><cell>?</cell><cell cols="2">121  *  64 ?</cell><cell>?</cell></row><row><cell cols="3">Linear 121  *  64 512</cell><cell>?</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>The best architecture for Breakout extracted from search space 1 by SPOS using reward mean criteria.</figDesc><table><row><cell>Layer</cell><cell cols="4">input output kernel stride</cell></row><row><cell>Conv-1</cell><cell>4</cell><cell>32</cell><cell>8</cell><cell>4</cell></row><row><cell>Conv-2</cell><cell>32</cell><cell>64</cell><cell>3</cell><cell>2</cell></row><row><cell>Conv-3</cell><cell>64</cell><cell>64</cell><cell>2</cell><cell>1</cell></row><row><cell>Padding</cell><cell>?</cell><cell cols="2">121  *  64 ?</cell><cell>?</cell></row><row><cell cols="3">Linear 121  *  64 512</cell><cell>?</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>The best architecture for Freeway extracted from search space 1 by SPOS using reward mean criteria.</figDesc><table><row><cell>Layer</cell><cell cols="4">input output kernel stride</cell></row><row><cell>Conv-1</cell><cell>4</cell><cell>32</cell><cell>8</cell><cell>4</cell></row><row><cell>Conv-2</cell><cell>32</cell><cell>64</cell><cell>5</cell><cell>1</cell></row><row><cell>Conv-3</cell><cell>64</cell><cell>64</cell><cell>5</cell><cell>1</cell></row><row><cell>Conv-4</cell><cell>64</cell><cell>64</cell><cell>5</cell><cell>1</cell></row><row><cell>Conv-5</cell><cell>64</cell><cell>64</cell><cell>2</cell><cell>1</cell></row><row><cell>Conv-6</cell><cell>64</cell><cell>64</cell><cell>2</cell><cell>1</cell></row><row><cell>Padding</cell><cell>?</cell><cell cols="2">121  *  64 ?</cell><cell>?</cell></row><row><cell cols="3">Linear 121  *  64 512</cell><cell>?</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>The best architecture for Breakout extracted from search space 2 by SPOS using reward mean criteria.</figDesc><table><row><cell>Layer</cell><cell cols="4">input output kernel stride</cell></row><row><cell>Conv-1</cell><cell>4</cell><cell>32</cell><cell>8</cell><cell>4</cell></row><row><cell>Conv-2</cell><cell>32</cell><cell>64</cell><cell>5</cell><cell>1</cell></row><row><cell>Conv-3</cell><cell>64</cell><cell>64</cell><cell>5</cell><cell>1</cell></row><row><cell>Conv-4</cell><cell>64</cell><cell>64</cell><cell>5</cell><cell>1</cell></row><row><cell>Conv-5</cell><cell>64</cell><cell>64</cell><cell>5</cell><cell>1</cell></row><row><cell>Conv-6</cell><cell>64</cell><cell>64</cell><cell>5</cell><cell>1</cell></row><row><cell>Padding</cell><cell>?</cell><cell cols="2">121  *  64 ?</cell><cell>?</cell></row><row><cell cols="3">Linear 121  *  64 512</cell><cell>?</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>The best architecture for Freeway extracted from search space 2 by SPOS using reward mean criteria.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Authors are thankful to Mikhail Konobeeb.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Understanding neural architecture search techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Lorraine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00438</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the theory of dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="1952" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">716</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using simulation and domain adaptation to improve efficiency of deep robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4243" to="4250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">OpenAI Gym</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<idno>ArXiv abs/1606.01540</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<ptr target="https://github.com/openai/baselines.2017" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Nas-bench-102: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00326</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Efficient multiobjective neural architecture search via lamarckian evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09081</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01561</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Regularization Theory and Neural Networks Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Girosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1995.7.2.219</idno>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3389" to="3396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00420</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural architecture search with bayesian optimisation and optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2016" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Recurrent Experience Replay in Distributed Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kapturowski</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1lyTjAqYX" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">NAS-Bench-NLP: Neural Architecture Search Benchmark for Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Klyuchnikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07116</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Offline reinforcement learning: Tutorial, review, and perspectives on open problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.01643</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07638</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Darts+: Improved differentiable architecture search with early stopping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06035</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Best practices for scientific research on neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Lindauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02453</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An empirical model of large-batch training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06162</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14236</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
		<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<idno>ArXiv abs/1911.08265</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Proximal Policy Optimization Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-objective Neural Architecture Search via Predictive Network Performance Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09336</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE. 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multi-fidelity Neural Architecture Search with Knowledge Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Trofimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08341</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Savani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11858</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pc-darts: Partial channel connections for memory-efficient differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05737</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">NAS evaluation is frustratingly hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Esperan?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12522</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Nas-bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7105" to="7114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
