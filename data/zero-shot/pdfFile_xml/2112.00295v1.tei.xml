<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiple Fusion Adaptation: A Strong Framework for Unsupervised Semantic Segmentation Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
							<email>zhangkai193@mails.ucas.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Research</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
							<email>wangrui@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichang</forename><surname>Li</surname></persName>
							<email>haichang@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hxh@iscas</forename><forename type="middle">Ac</forename><surname>Cn</surname></persName>
						</author>
						<title level="a" type="main">Multiple Fusion Adaptation: A Strong Framework for Unsupervised Semantic Segmentation Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>ZHANG, SUN, WANG, LI, HU: MULTIPLE FUSION ADAPTATION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper challenges the cross-domain semantic segmentation task, aiming to improve the segmentation accuracy on the unlabeled target domain without incurring additional annotation. Using the pseudo-label-based unsupervised domain adaptation (UDA) pipeline, we propose a novel and effective Multiple Fusion Adaptation (MFA) method. MFA basically considers three parallel information fusion strategies, i.e., the cross-model fusion, temporal fusion and a novel online-offline pseudo label fusion. Specifically, the online-offline pseudo label fusion encourages the adaptive training to pay additional attention to difficult regions that are easily ignored by offline pseudo labels, therefore retaining more informative details. While the other two fusion strategies may look standard, MFA pays significant efforts to raise the efficiency and effectiveness for integration, and succeeds in injecting all the three strategies into a unified framework. Experiments on two widely used benchmarks, i.e., GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes, show that our method significantly improves the semantic segmentation adaptation, and sets up new state of the art (58.2% and 62.5% mIoU, respectively). The code will be available at https://github.com/KaiiZhang/MFA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper considers the unsupervised domain adaptation (UDA) for semantic segmentation. In real-world segmentation tasks, there usually exists a domain gap between the training (source domain) and testing data (target domain), which substantially compromises the segmentation accuracy. Instead of using additional annotated data on the target domain for adaptation, which is notoriously expensive, an alternative way is to adapt the already-learned  <ref type="figure">Figure 1</ref>: The proposed Multiple Fusion Adaptation (MFA) employs a co-learning framework to integrate three information fusions i.e., cross-model fusion, temporal fusion and online-offline pseudo label fusion. The learner (Net A) is co-supervised by the offline pseudo labels, as well as the online labels generated by it co-learner (Net B). To make the online label predictions more stable, MFA smooths the co-learner by temporal average (Mean Net B). Importantly, we design the online pseudo labels to be complementary to the offline pseudo labels, which promotes better fusion effect. In the co-learning framework, Net A and Net B will exchange their role of learner and co-learner. We only present Net A as the learner here for easier understanding.</p><p>model through UDA <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref>. In another word, we aim to improve the segmentation accuracy on an unlabeled target domain without incurring additional annotation. A popular pipeline adopted by many state-of-the-art methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> consists of two training stages, i.e., a warm-up supervised training on the source domain and a sequential self-training on the target domain. Specifically, the first stage trains a warm-up model on the source domain data. For better generalization ability, the warm-up training process is typically assisted with some domain alignment constraints <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>. Then, the second training stage further adapts the warm-up model to the target domain through self-training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. The self-training usually uses the warm-up model to assign pseudo labels on the target domain, which are used to re-train (fine-tune) the model. This paper proposes a novel and effective Multiple Fusion Adaptation (MFA) method, based on the above-described two-stage UDA pipeline. We employ three basic information fusion to improve the domain adaptation, namely a novel online-offline pseudo label fusion, cross-model fusion and temporal fusion. In MFA, all the three fusion strategies are integrated in a co-learning framework, as illustrated in <ref type="figure">Figure 1</ref>. Each learner is co-supervised by two types of pseudo labels, i.e., the online and the offline pseudo labels. The offline pseudo labels are generated with a popular method <ref type="bibr" target="#b23">[24]</ref>, while the online pseudo labels are generated by the temporal average model of the co-learner. This MFA pipeline has two advantages:</p><p>? The novel online-offline pseudo label fusion. So far as we know, prior two-stage UDA methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref> usually employ the offline pseudo labels. Among the iterations of "assigning pseudo label" and "re-training", the pseudo labels are updated after several training epochs, yielding the "offline" manner. While the offline manner allows additional postprocessing and has the advantage of balanced pseudo labels <ref type="bibr" target="#b23">[24]</ref>, it is prone to the ignorance of hard and informative samples <ref type="bibr" target="#b9">[10]</ref>. It is because the offline manner only preserves the most confident predictions among all the target domain data, which are relatively easy. As a remedy, we supplement the offline pseudo labels with online ones, which focus on the relatively hard details (i.e., the informative samples) within each training iteration. Moreover, since the online pseudo labels are generated by the up-to-date model (which has better accuracy than the historical ones), they reduce the exposure to noisy supervision and thus benefit the self-training process.</p><p>? A highly efficient integration of three fusion strategies. While the cross-model fusion and temporal fusion are quite popular, MFA pays significant efforts to raise the efficiency and effectiveness for integration, and succeeds in injecting all the three strategies into a unified framework. Specifically, MFA employs a co-learning framework consisted of two learners, as illustrated in <ref type="figure">Figure 1</ref>. Each learner (model) in MFA is co-supervised by the offline pseudo labels generated by itself, as well as the online labels generated by its co-learner. To make the online predictions more stable, MFA smooths the co-learner by temporal average. Consequentially, MFA simultaneously enforces information fusion between 1) a model and its co-learner (and vice versa), 2) the up-to-date status and the temporal-averaged status and 3) online and offline pseudo labels. Combining these parallel fusions, MFA suppresses the pseudo label noises in the self-training stage and thus improves the segmentation adaptation.</p><p>Equipped with these two advantages, MFA is capable to improve UDA for semantic segmentation. First, the novel online-offline pseudo label fusion enables MFA to retain more informative details for adaptive training. Second, MFA manages a highly efficient integration of the online-offline pseudo label fusion and two commonly-adopted fusions, which further increases the accuracy of the pseudo labels. We evaluate the proposed MFA through extensive experiments. Experimental results show that MFA significantly improves the baseline and achieve performance on par with the state of the art. For example, on GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes, MFA achieves 58.2% and 62.5% mIoU, respectively. Moreover, ablation study validates the effectiveness of each component in MFA. The main contributions of this paper are summarized as follows:</p><p>? We propose MFA, an unsupervised semantic segmentation adaptation method based on self-training. MFA efficiently integrates three different information fusion strategies to improve the pseudo-label-based UDA. ? Among the three fusion strategies of MFA, the online-offline pseudo label fusion is a novel one specifically designed for adaptive segmentation. The online pseudo labels supplement the self-training with relatively hard and informative samples, which may be easily ignored by the offline pseudo labels. ? We conduct extensive experiments to evaluate the proposed MFA. Experimental results show that MFA achieves superior adaptive semantic segmentation and the training cost is relatively low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Semantic segmentation adaptation. We divide the existing UDA semantic segmentation methods into two categories: domain alignment <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> and self-training <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, and the existing state-of-art approaches are usually a combination of two methods. The main motivation of domain alignment is to reduce the discrepancy between two domains. CyCADA <ref type="bibr" target="#b5">[6]</ref> uses CycleGan <ref type="bibr" target="#b22">[23]</ref> to transfer image style. FDA <ref type="bibr" target="#b18">[19]</ref> proposes exchanging the low-frequency component of fourier transform without learning to achieve the same purpose. In addition, SIM <ref type="bibr" target="#b17">[18]</ref> introduces the feature alignment of things and stuff respectively. In self-training, pseudo label learning <ref type="bibr" target="#b23">[24]</ref> is a widely used approach. CBST <ref type="bibr" target="#b23">[24]</ref> proposes an iterative pseudo label learning strategy and solve the class imbalance issue by class-independent confidence ranking. The improvement of pseudo label learning is an important research direction. In CRST <ref type="bibr" target="#b24">[25]</ref>, a confidence regularized self-training method is proposed to address the problem of overconfident wrong label. <ref type="bibr" target="#b14">[15]</ref> presents a two-phase pseudo label densification framework through voting-based and easy-hard classification based method. In <ref type="bibr" target="#b11">[12]</ref>, weak labels are explored to enhance pseudo label learning. Our work considers suppressing the pseudo label noise through multiple fusion strategies.</p><p>Temporal average. Temporal ensembling <ref type="bibr" target="#b7">[8]</ref> averages the outputs of the network-in-training to increase the prediction accuracy for the unlabelled samples. The mean teacher <ref type="bibr" target="#b15">[16]</ref> averages model weights at different training steps to get a teacher model. The teacher model offers supervision signal through consistency constraint on the unlabeled samples. These works show that the temporal average of the deep model is more stable and accurate than the deep model at a single training step.</p><p>Learning with noisy labels. The information fusion between different models is an effective approach to suppress noises in labels. Co-teaching <ref type="bibr" target="#b3">[4]</ref> cross-trains two networks and let them teach each other given the possibly clean labels by small-loss trick. Co-teaching+ <ref type="bibr" target="#b19">[20]</ref> bridges the "Disagreement" strategy with the Co-teaching to enhance robustness under extremely noisy supervision. In these works, the labels are all available, which is different from the unsupervised domain adaptation problem. We note that <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> also consider the noise issue of pseudo labels on semantic segmentation adaptation. <ref type="bibr" target="#b21">[22]</ref> estimates the uncertainty of predictions and reduces the impact of low-confidence samples during pseudo label learning. <ref type="bibr" target="#b20">[21]</ref> take this issue by exploiting the feature distances from prototypes. Our method tackles the noisy pseudo label problem from a different viewpoint. We use co-learning and integrate multiple fusion strategies to resist the noisy pseudo labels, as well as to retain informative samples. Experimental results show that the proposed MFA marginally surpasses <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>The proposed MFA adopts the popular UDA pipeline of two-stage training, i.e., a warmup training on the source domain and a following self-training on the target domain. We first give a formal description of the two-stage UDA pipeline in Section 3.1. Based on this pipeline, MFA improves the adaptive segmentation through multiple fusions in the selftraining stage, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Basically, MFA uses two independent models (Net A and Net B) to set up a co-learning framework. Both models have the dual role of learner and co-learner. Before we collaboratively fine-tune them through self-training, we combine the two warm-up models to generate offline pseudo labels on the target domain (Section 3.2). During self-training, MFA smooths each model through temporal moving average and gets a corresponding "mean net" (i.e., Mean Net A and Mean Net B in <ref type="figure" target="#fig_1">Figure 2</ref>). The function of Mean Net (or the temporal moving average operation) is two-fold. First, temporal moving average stabilizes the update of each mean net , therefore making the online predictions more stable. Second, according to the discovery in semi-supervised learning <ref type="bibr" target="#b15">[16]</ref>, temporal moving average benefits from the ensemble of multiple models and thus maintains higher prediction accuracy. Given the current training mini-batch , each mean net predicts a respective set of online pseudo labels (Section 3.3). Finally, MFA enforces co-supervision on each model. Specifically, each learner is co-supervised by the offline pseudo labels, as well as the online ones generated by its co-learner (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries on Two-stage UDA</head><p>In domain adaptive segmentation, we have two datasets belonging to different domains, i.e., the source domain and the unlabeled target domain. The source domain dataset is denoted as</p><formula xml:id="formula_0">D S = x i S , y i S N S i=1</formula><p>, where x S ? R H?W ?3 is a color image in source domain, N S is the number of source data and y S ? R H?W is the corresponding semantic map. The definition of the target</p><formula xml:id="formula_1">domain dataset D T = x j T , y j T N T j=1</formula><p>is similar, except that y T is unknown. Let F represents a semantic segmentation network, and ? stands the parameters of F. The goal of the UDA problem is to estimate ? to minimize the prediction error on the unlabeled target domain.</p><p>In two-stage UDA, the parameter ? of the warm up model are first obtained through training on the source domain (i.e., stage 1). Then in the self-training (i.e., stage 2), given the input sample from target domain, the prediction? T = F (x T | ? ) is the predicted class probability map, where? T ? R H?W ?C and C is the number of classes. And max (? c T ) ? R H?W is the prediction confidence map. The one-hot map of pseudo labels is obtained by:</p><formula xml:id="formula_2">P (x T | ? ) = one-hot arg max c F (c | x T , ? )<label>(1)</label></formula><p>In the standard self-training strategy, an optional way is to retrain the initialized model multiple times by merging the pseudo label data with the source data, which is applied in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. However, this strategy needs to reinitialize F to start training, which is very time-consuming. Therefore, we choose another way used in <ref type="bibr" target="#b21">[22]</ref> as our baseline, i.e., fine-tune the warm-up model on the pseudo labels. The loss function for self-supervision is formulated as follows:</p><formula xml:id="formula_3">L sel f (x T , ? ) = ? ? batch m ?p T ? log (F (x T | ? ))<label>(2)</label></formula><p>Wherep T is obtained by x T and ? in Equation 1. And m ? R H?W is a binary mask for filtering out the unreliable pseudo labels. Specifically, if m h,w = 1, then we haveP T,h,w selected for training. In contrast, if m h,w = 0, the corresponding pseudo label is regarded as unreliable and thus ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Offline Pseudo Label</head><p>In Equation 2, the pseudo labels are typically selected in an offline manner, i.e., they will not be instantly updated during model optimization. MFA combines two different warm-up models for offline pseudo label prediction? T . Consequentially, the offline pseudo labels benefit from model ensemble. Given the raw predictions, we use the CBST <ref type="bibr" target="#b23">[24]</ref> method to select the training samples (pixels) by: We highlight some regions with red bounding boxes to draw attention of the complementarity between these labels. For example, in the first column, the offline pseudo labels omit some important details about the pedestrians, and the online pseudo labels make up for this problem. In the second column, the person riding a motorcycle is recalled by online pseudo labels.</p><formula xml:id="formula_4">m = 1 if c = arg max (? c T ) &amp; max (? c T ) &gt; ? c 0 otherwise (3)</formula><p>where c ? [0,C) is the class index, and ? c is the thresholds for the corresponding class. Following <ref type="bibr" target="#b23">[24]</ref>, we set each threshold ? c to ensure class balance, i.e., all the classes has an identical proportion of the selected pixels.</p><p>The offline manner of CBST has the advantage of stabilizing supervision signals and avoiding the gradual dominance of large classes <ref type="bibr" target="#b23">[24]</ref>. In MFA, the ensemble of two warmup models further benefits the accuracy of the pseudo labels. However, the offline manner is prone to the problems of ignorance of hard samples <ref type="bibr" target="#b9">[10]</ref> and longer exposure to the potential noisy labels. We thus introduce the online pseudo labels as a supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Online Pseudo Label</head><p>Temporal average. MFA instantly assigns online pseudo labels to images in the current mini-batch. In another word, the online pseudo labels do NOT require a post-processing over the whole target domain samples. To alleviate the potential instability issue, MFA first smooths each model through temporal average to generate mean net, which is formulated as:</p><formula xml:id="formula_5">? mean t = ?? mean t?1 + (1 ? ?)? t<label>(4)</label></formula><p>Where ? is a smoothing coefficient hyper-parameter, ? denotes the model parameters and t is the training step. Given the mean net with parameters ? mean and an input image x T , MFA generates the corresponding online pseudo labels by Equation 1.</p><p>Online CBST. The online pseudo labels require to be filtered as well, so as to remove the labels with relatively low confidence. To this end, we propose a novel Online-CBST. </p><formula xml:id="formula_6">P 1 b = F (x b | ? A ) P 2 b = F (x b | ? B ) LP 1 b = arg max P 1 b , axis = 1 LP 2 b = arg max P 2 b , axis = 1 for i=1 to 2 do MP i b = max P i b , axis = 1 for c=1 to C do MP i c,b = MP i b LP i b == c M i c = sort MP i c,b , order = descending len i c = length(M i c ) ? ? ? i c = M i c len i c LP i b LP i b == c &amp; LP i b &lt; ? i c = 255 end for end for return LP 1 b , LP 2 b</formula><p>It is similar to the original CBST, except that it uses the data in current mini-batch (rather than the whole training data) as the reference for label selection. In other words, we set a respective filtering threshold for each class, so that all the classes have equal proportion of pseudo-labeled samples in current mini-batch. In analogy to the original CBST, we design the Online-CBST to have linearly-increasing proportion ?(t) ? [? min , ? max ], which is the desired proportion in the t?th training step. ? min and ? max are the pre-defined minimum and maximum proportions. Initially, the accuracy of each warm-up model is relatively low. So we use a small ?(0) = ? min to retain the most confident pseudo labels and abandon the others.</p><p>Since the online pseudo labels are generated by the up-to-date model, they reduce the exposure to noisy supervision. Moreover, Online CBST is beneficial for recalling the relatively hard details (i.e., the informative samples) within each training iteration. In Section 4.5, we analyzed this in more detail. Given the online pseudo labels P(x T | ? mean A ) and P(x T | ? mean B ), the Online-CBST correspondingly generates two masks m A and m B for selecting the pixels. The pseudo code for generating the online labels is to be accessed in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Co-supervision in MFA</head><p>Given both the offline and online pseudo labels, MFA enforces a co-supervision on each learner in <ref type="figure" target="#fig_1">Figure 2 (a)</ref>. We illustrate the loss functions for such co-supervision as follows.</p><p>To cooperate with the offline pseudo labels, MFA uses the loss function L sel f defined by Equation 2 for self-supervision on both Net A and Net B. As for the online pseudo labels, MFA uses P(x T | ? mean A ) (predictions from Net A) for supervising Net B and uses P(x T | ? mean B ) for supervising Net A, yielding the so-called cross-model supervision. The detailed loss functions are formulated as:</p><formula xml:id="formula_7">L cross (? A , ? mean B ) = ? ? batch m B ? P (x T | ? mean B ) ? log (F (x T | ? A )) L cross (? B , ? mean A ) = ? ? batch m A ? P (x T | ? mean A ) ? log (F (x T | ? B ))<label>(5)</label></formula><p>in which m B (m A ) is the selection-mask for P(x T | ? mean B ) (P(x T | ? mean A )) generated by the proposed Online-CBST.</p><p>Besides the co-supervision with online and offline pseudo labels, MFA enforces a respective consistency constraint between each learner and its temporal average. Similar to mean teacher <ref type="bibr" target="#b15">[16]</ref>, the consistency loss is the expected distance between the prediction of the model and the prediction of the temporal average model, which is formulated as:</p><formula xml:id="formula_8">L cst (? , ? mean ) = E x [ F (x T | ? mean ) ? F (x T | ? ) ]<label>(6)</label></formula><p>In summary, MFA sums up all the losses to collaboratively train Net A and Net B by:</p><formula xml:id="formula_9">L all =L sel f (x T , ? A ) + L sel f (x T , ? B ) + ? cst (L cst (? A , ? mean A ) + L cst (? B , ? mean B )) + ? cross (L cross (? A , ? mean B ) + L cross (? B , ? mean A )) ,<label>(7)</label></formula><p>in which ? cst and ? cross are the weighting factors for L cst and L cross , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the proposed MFA under two widely adopted cross-domain segmentation settings, i.e., GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes. GTA5 and SYNTHIA are both synthetic datasets. The GTA5 <ref type="bibr" target="#b12">[13]</ref> dataset consists of 24,966 synthesized images of resolution 1914 ? 1052. Same as existing works, we evaluate our method on 19 common categories shared by GTA5 and Cityscapes. The SYNTHIA <ref type="bibr" target="#b13">[14]</ref> dataset has 9,400 synthesized images of resolution 1280 ? 720 with fine annotations. Following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, we report the per-class IoU and mIoU on the 13 common categories shared by SYNTHIA and Cityscapes. Cityscapes is a real-world semantic segmentation dataset <ref type="bibr" target="#b2">[3]</ref>, which consists of 5, 000 images of resolution 2048 ? 1024 with pixel-level annotations. It is split into a training set, validation set and test set with 2, 975, 500 and 1, 525 images, respectively. In line with the standard evaluation setting, we use the 2, 975 training images (without the ground-truth labels) as target domain images, and then evaluate the domain adaptive segmentation accuracy on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, we use DeepLabV2 [1] based on ResNet101 <ref type="bibr" target="#b4">[5]</ref> as the backbone model. We recall that MFA is based on the two-stage UDA pipeline and requires warm-up training. To promote the divergence between the initial status of two learners, we adopt two state-ofthe-art domain alignment methods proposed by FDA <ref type="bibr" target="#b18">[19]</ref> and SIM <ref type="bibr" target="#b17">[18]</ref>. These two methods serves as the strong baseline for MFA. That being said, we will show that MFA achieves significant improvement over these (e.g., +10.1% mIoU on GTA5-to-Cityscapes), which results in the state-of-the-art performance. Moreover, MFA is compatible to any warm-up models and is potential to benefit from the future progress in domain alignment.</p><p>We use SGD optimization strategy with momentum 0.9. We initialize the learning rate to 2e ?4 , and adjust it according to the poly learning rate scheduler with a power of 0.9. As for the hyper-parameters, Equation 4 has ? = 0.99 for temporal average, and Online-CBST has ? min = 0.2, ? max = 0.7. Moreover, we set ? cst = 1.0, ? cross = 0.5 for Equation <ref type="bibr" target="#b6">7</ref>.  <ref type="table">Table 1</ref>: Results on GTA5-to-Cityscapes. MFA surpasses all the competing methods. For a fair comparison, "*" indicates additional distillation stage is used, which is proposed in <ref type="bibr" target="#b20">[21]</ref>. <ref type="table">Table 1</ref> compares MFA against several state-of-the-art UDA methods on the GTA5-to-Cityscapes benchmark, from which we draw two observations. First, comparing MFA against all the competing two stage methods, we find that MFA surpasses all the competing methods by a large margin. For example, it achieves 55.7% mIoU, which is higher than the strongest competitor ProDA by +2.0%. We achieve the best scores on most categories <ref type="bibr">(11 in 19)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Effectiveness of MFA</head><p>Second, under the fair comparison, MFA* presents 58.2% mIoU, with additional model distillation stage <ref type="bibr" target="#b20">[21]</ref>. In line with <ref type="bibr" target="#b20">[21]</ref>, we use the SimCLRv2 <ref type="bibr" target="#b1">[2]</ref> pretrained weights and same distillation strategy.</p><p>We also compare MFA with competing methods on the SYNTHIA-to-Cityscapes benchmark in the <ref type="table">Table 2</ref> and draw consistent observations as on GTA5-to-Cityscapes. MFA achieves higher mIoU than prior state-of-the-art methods. We report 58.7% mIoU (after self-training stage) and 62.5% mIoU (after distillation stage) on this benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The Efficiency of MFA</head><p>The basic self-training strategy needs iteration of "assigning pseudo label" and "re-training", which is adopted by <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24]</ref>. Benefit from the online-offline fusion, MFA converges faster without multiple iterations and reinitialization. The training in MFA lasts 65 epochs, and is more efficient than other methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. We note that the warm-up models in FDA <ref type="bibr" target="#b18">[19]</ref> and the proposed MFA achieve close performance, but MFA achieves 55.7 mIoU after self-training stage (compared to 50.5 in FDA after two round self-training stage). We thus infer that the superiority of MFA is due to the well-engineered self-training with multiple information fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">The Benefit of Online Pseudo Labels</head><p>We visualize some examples of online and offline pseudo labels in <ref type="figure" target="#fig_1">Figure 2 (b)</ref>. It is clearly observed that online pseudo labels are complementary to the offline ones and focus on the relatively hard details. In the first column, the offline pseudo labels omit the pedestrians, which look small in the image. In contrast, the online pseudo labels succeed in pointing the existence of these pedestrians. In the second column, the person riding a motorcycle is omitted by the offline pseudo labels and is recalled by online pseudo labels. We owe this to two reasons. First, the mean net is updated in time, which is conducive to producing  <ref type="table">Table 2</ref>: Results on SYNTHIA-to-Cityscapes. MFA achieves better performance than the other state-of-the-art methods. The symbol "*" indicates additional distillation stage is used.</p><p>more high-quality pseudo labels along with self-training. Second, since the proposed online CBST focuses on current batch of samples, the relatively difficult samples are more likely to be recalled in the confidence ranking.  <ref type="table">Table 3</ref>: Ablation study on the GTA5-to-Cityscapes adaptation. ST: the basic selftraining method without any fusions. TF: temporal fusion by consistency loss. CMF: cross-model fusion by jointly generating offline pseudo labels. OOF: online-offline fusion through online pseudo label supervision. <ref type="table">Table 3</ref> investigates the contribution of each component of MFA on GTA5-to-Cityscapes. In the first row, "Warm" is the best warm-up model based on domain alignment methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. "ST" is the basic self-training ( without any information fusion). We divide MFA to three key components, i.e., the temporal fusion (TF), the cross-model fusion (CMF) and the onlineoffline pseudo label fusion (OOF). Accordingly, we draw three observations. First, self-training brings +3.8% mIoU improvement over the warm-up training. Such improvement is consistent with many other two-stage UDA methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>. Second, all the three components are important for MFA. By incrementally adding the key components, the performance reached 50.6%, 51.7% and 55.7%, respectively. Third, these three fusion strategies adds up to +6.3% mIoU improvement compared to basic ST. They jointly enable MFA achieve significant superiority against other two-stage UDA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a self-training method named Multi-Fusion Adaptation (MFA) for domain adaptive semantic segmentation. Through a co-learning framework, MFA integrates three information fusion strategies, i.e., cross-model fusion, temporal fusion, and online-offline pseudo label fusion. These fusions jointly suppress the pseudo label noises and explore informative samples during the self-training procedure. Consequentially, MFA significantly improves adaptive semantic segmentation and sets new state of the art on two popular benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The proposed Multiple Fusion Adaptation (MFA) framework (b) Examples of the online and offline pseudo labels (a) The overall framework of the proposed Multiple Fusion Adaptation (MFA). MFA collaboratively trains two models (Net A and Net B), with two supervision signals (Section 3.4), i.e., the offline and online pseudo labels. The offline pseudo labels are generated by a popular baseline CBST [24], as introduced in Section 3.2. The online pseudo labels for Net A are generated by the temporal average of Net B (mean Net B), and vice versa (Section 3.3). (b) Some examples of the online and offline pseudo labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Online CBST Input: Mean Net F(? A ) and F(? B ), minibatch target data x b . Parameter: Ratio ? of selected pseudo labels. Output: LP 1 b and LP 2 b from ? A and ? B , respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>35.6 80.1 19.8 17.5 38.0 39.9 41.5 82.7 27.9 73.6 64.9 19.0 65.0 12.0 28.6 4.5 31.1 42.0 42.7 WSDA [12] 91.6 47.4 84.0 30.4 28.3 31.4 37.4 35.4 83.9 38.3 83.9 61.2 28.2 83.7 28.8 41.3 8.8 24.7 46.4 48.2 BDA [9] 91.0 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5 SIM [18] 90.6 44.7 84.8 34.3 28.7 31.6 35.0 37.6 84.7 43.3 85.3 57.0 31.5 83.8 42.6 48.5 1.9 30.4 39.0 49.2 Seg-U [22] 90.4 31.2 85.1 36.9 25.6 37.5 48.8 48.5 85.3 34.8 81.1 64.4 36.8 86.3 34.9 52.2 1.7 29.0 44.6 50.3 FDA [19] 92.5 53.3 82.4 26.5 27.6 36.4 40.6 38.9 82.3 39.8 78.0 62.6 34.4 84.9 34.1 53.1 16.9 27.7 46.</figDesc><table><row><cell>Method</cell><cell>road</cell><cell>sdwk</cell><cell>bldng</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>veg</cell><cell>trrn</cell><cell>sky</cell><cell>psn</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>moto</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell cols="21">AdaStruct [17] 86.5 25.9 79.8 22.1 20.0 23.6 33.1 21.8 81.8 25.9 75.9 57.3 26.2 76.3 29.8 32.1 7.2 29.5 32.5 41.4</cell></row><row><cell>Cycada [6]</cell><cell cols="20">86.7 4 50.5</cell></row><row><cell>TPLD [15]</cell><cell cols="20">94.2 60.5 82.8 36.6 16.6 39.3 29.0 25.5 85.6 44.9 84.4 60.6 27.4 84.1 37.0 47.0 31.2 36.1 50.3 51.2</cell></row><row><cell>ProDA [21]</cell><cell cols="20">91.5 52.4 82.9 42.0 35.7 40.0 44.4 43.3 87.0 43.8 79.5 66.5 31.4 86.7 41.1 52.5 0.0 45.4 53.8 53.7</cell></row><row><cell>MFA(ours)</cell><cell cols="20">94.5 61.1 87.6 41.4 35.4 41.2 47.1 45.7 86.6 36.6 87.0 70.1 38.3 87.2 39.5 54.7 0.3 45.4 57.7 55.7</cell></row><row><cell>ProDA* [21]</cell><cell cols="20">87.8 56.0 79.7 46.3 44.8 45.6 53.5 53.5 88.6 45.2 82.1 70.7 39.2 88.8 45.5 59.4 1.0 48.9 56.4 57.5</cell></row><row><cell>MFA*(ours)</cell><cell cols="20">93.5 61.6 87.0 49.1 41.3 46.1 53.5 53.9 88.2 42.1 85.8 71.5 37.9 88.8 40.1 54.7 0.0 48.2 62.8 58.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>46.7 80.3 14.1 11.6 79.2 81.3 54.1 27.9 73.7 42.2 25.7 45.3 51.4 WSDA [12] 92.0 53.5 80.9 3.8 6.0 81.6 84.4 60.8 24.4 80.5 39.0 26.0 41.7 51.9 SIM [18] 83.0 44.0 80.3 17.1 28.7 15.8 81.8 59.9 33.1 70.2 37.3 28.5 45.8 52.1 TPLD [15] 80.9 44.3 82.2 20.5 30.1 77.2 80.9 60.6 25.5 84.8 41.1 24.7 43.7 53.5 Seg-U [22] 87.6 41.9 83.1 31.3 19.9 81.6 80.6 63.0 21.8 86.2 40.7 23.6 53.1 54.9 FDA [19] 79.3 35.0 73.2 19.9 24.0 61.7 82.6 61.4 31.1 83.9 40.8 38.4 51.1 52.5 ProDA [21] 87.1 44.0 83.2 45.8 34.2 86.7 81.3 68.4 22.1 87.7 50.0 31.4 38.6 58.5 MFA(ours) 85.4 41.9 84.1 22.2 23.9 83.6 80.7 71.5 35.8 86.6 47.6 37.2 62.5 58.7 ProDA* [21] 87.8 45.7 84.6 54.6 37.0 88.1 84.4 74.2 24.3 88.2 51.1 40.5 45.6 62.0 MFA*(ours) 81.8 40.2 85.3 38.0 33.9 82.3 82.0 73.7 41.1 87.8 56.6 46.3 63.8 62.5</figDesc><table><row><cell>Method</cell><cell>road</cell><cell>sdwk</cell><cell>bldng</cell><cell>light</cell><cell>sign</cell><cell>veg</cell><cell>sky</cell><cell>psn</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>moto</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell cols="4">AdaStruct [17] 84.3 42.7 77.5</cell><cell>4.7</cell><cell>7.0</cell><cell cols="9">77.9 82.5 54.3 21.0 72.3 32.2 18.9 32.3 46.7</cell></row><row><cell>BDA [9]</cell><cell>86.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2021. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Contextualrelation consistent domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02424</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Instance adaptive self-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12197</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Seokju Lee, and In So Kweon. Unsupervised intra-domain adaptation for semantic segmentation through self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkyu</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Rameau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3764" to="3773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain adaptive semantic segmentation using weak labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujoy</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="571" to="587" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IX 16</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fei Pan, and In So Kweon. Two-phase pseudo label densification for self-training based domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkyu</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="532" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weightaveraged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Differential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12635" to="12644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">How does disagreement help generalization against label corruption?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04215</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10979</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03773</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unpaired image-toimage translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
