<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Paraphrase Detection with the Adversarial Paraphrasing Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Nighojkar</surname></persName>
							<email>anighojkar@usf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Advancing Machine and Human Reasoning Lab</orgName>
								<orgName type="institution">University of South Florida Tampa</orgName>
								<address>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Licato</surname></persName>
							<email>licato@usf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Advancing Machine and Human Reasoning Lab</orgName>
								<orgName type="institution">University of South Florida Tampa</orgName>
								<address>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Paraphrase Detection with the Adversarial Paraphrasing Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>If two sentences have the same meaning, it should follow that they are equivalent in their inferential properties, i.e., each sentence should textually entail the other. However, many paraphrase datasets currently in widespread use rely on a sense of paraphrase based on word overlap and syntax. Can we teach them instead to identify paraphrases in a way that draws on the inferential properties of the sentences, and is not over-reliant on lexical and syntactic similarities of a sentence pair? We apply the adversarial paradigm to this question, and introduce a new adversarial method of dataset creation for paraphrase identification: the Adversarial Paraphrasing Task (APT), which asks participants to generate semantically equivalent (in the sense of mutually implicative) but lexically and syntactically disparate paraphrases. These sentence pairs can then be used both to test paraphrase identification models (which get barely random accuracy) and then improve their performance. To accelerate dataset generation, we explore automation of APT using T5, and show that the resulting dataset also improves accuracy. We discuss implications for paraphrase detection and release our dataset in the hope of making paraphrase detection models better able to detect sentence-level meaning equivalence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Although there are many definitions of 'paraphrase' in the NLP literature, most maintain that two sentences that are paraphrases have the same meaning or contain the same information. <ref type="bibr" target="#b35">Pang et al. (2003)</ref> define paraphrasing as "expressing the same information in multiple ways" and <ref type="bibr" target="#b1">Bannard and Callison-Burch (2005)</ref> call paraphrases "alternative ways of conveying the same information." <ref type="bibr" target="#b16">Ganitkevitch et al. (2013)</ref> write that "paraphrases are differing textual realizations of the same meaning." A definition that seems to sufficiently encompass the others is given by <ref type="bibr" target="#b2">Bhagat and Hovy (2013)</ref>: "paraphrases are sentences or phrases that use different wording to convey the same meaning." However, even that definition is somewhat imprecise, as it lacks clarity on what it assumes 'meaning' means.</p><p>If paraphrasing is a property that can hold between sentence pairs, 1 then it is reasonable to assume that sentences that are paraphrases must have equivalent meanings at the sentence level (rather than exclusively at the levels of individual word meanings or syntactic structures). Here a useful test is that recommended by inferential role semantics or inferentialism <ref type="bibr" target="#b4">(Boghossian, 1994;</ref><ref type="bibr" target="#b38">Peregrin, 2006)</ref>, which suggests that the meaning of a statement s is grounded in its inferential properties: what one can infer from s and from what s can be inferred.</p><p>Building on this concept from inferentialism, we assert that if two sentences have the same inferential properties, then they should also be mutually implicative. Mutual Implication (MI) is a binary relationship between two sentences that holds when each sentence textually entails the other (i.e., bidirectional entailment). MI is an attractive way of operationalizing the notion of two sentences having "the same meaning," as it focuses on inferential relationships between sentences (properties of the sentences as wholes) instead of just syntactic or lexical similarities (properties of parts of the sentences). As such, we will assume in this paper that two sentences are paraphrases if and only if they are M I. <ref type="bibr">2</ref> In NLP, modeling inferential relationships between sentences is the goal of the textual entailment, or natural language inference (NLI) tasks <ref type="bibr" target="#b7">(Bowman et al., 2015)</ref>. We test MI using the version of RoBERTa large released by <ref type="bibr" target="#b32">Nie et al. (2020)</ref> trained on a combination of SNLI <ref type="bibr" target="#b7">(Bowman et al., 2015)</ref>, multiNLI <ref type="bibr" target="#b47">(Williams et al., 2018)</ref>, FEVER-NLI <ref type="bibr" target="#b31">(Nie et al., 2019)</ref>, and ANLI <ref type="bibr" target="#b32">(Nie et al., 2020)</ref>.</p><p>Owing to expeditious progress in NLP research, performance of models on benchmark datasets is 'plateauing' -with near-human performance often achieved within a year or two of their releaseand newer versions, using a different approach, are constantly having to be created, for instance, GLUE <ref type="bibr" target="#b45">(Wang et al., 2019)</ref> and SuperGLUE <ref type="bibr" target="#b44">(Wang et al., 2020)</ref>. The adversarial paradigm of dataset creation <ref type="bibr">(Jia and Liang, 2017a,b;</ref><ref type="bibr" target="#b8">Bras et al., 2020;</ref><ref type="bibr" target="#b32">Nie et al., 2020)</ref> has been widely used to address this 'plateauing,' and the ideas presented in this paper draw inspiration from it. In the remainder of this paper, we apply the adversarial paradigm to the problem of paraphrase detection, and demonstrate the following novel contributions:</p><p>? We use the adversarial paradigm to create a new benchmark examining whether paraphrase detection models are assessing the meaning equivalence of sentences rather than being over-reliant on word-level measures. We do this by collecting paraphrases that are MI but are as lexically and syntactically disparate as possible (as measured by low BLEURT scores). We call this the Adversarial Paraphrasing Task (APT).</p><p>? We show that a SOTA language model trained on paraphrase datasets perform poorly on our benchmark. However, when further trained on our adversarially-generated datasets, their MCC scores improve by up to 0.307.</p><p>? We create an additional dataset by training a paraphrase generation model to perform our adversarial task, creating another large dataset that further improves the paraphrase detection models' performance.</p><p>? We propose a way to create a machinegenerated adversarial dataset and discuss ways to ensure it does not suffer from the plateauing that other datasets suffer from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Paraphrase detection (given two sentences, predict whether they are paraphrases) (Zhang and Patrick, </p><formula xml:id="formula_0">AP T 5 T5 base -generated APT dataset (Note that AP T 5 = AP M T 5 ? AP T w T 5 ) AP M T 5 MSRP subset of AP T 5 AP T w T 5</formula><p>TwitterPPDB subset of AP T 5 2005; <ref type="bibr" target="#b15">Fernando and Stevenson, 2008;</ref><ref type="bibr" target="#b41">Socher et al., 2011;</ref><ref type="bibr" target="#b24">Jia et al., 2020)</ref> is an important task in the field of NLP, finding downstream applications in machine translation <ref type="bibr" target="#b9">(Callison-Burch et al., 2006;</ref><ref type="bibr" target="#b0">Apidianaki et al., 2018;</ref><ref type="bibr" target="#b30">Mayhew et al., 2020)</ref>, text summarization, plagiarism detection <ref type="bibr" target="#b21">(Hunt et al., 2019)</ref>, question answering, and sentence simplification <ref type="bibr" target="#b17">(Guo et al., 2018)</ref>. Paraphrases have proven to be a crucial part of NLP and language education, with research showing that paraphrasing helps improve reading comprehension skills <ref type="bibr" target="#b27">(Lee and Colln, 2003;</ref><ref type="bibr" target="#b19">Hagaman and Reid, 2008)</ref>. Question paraphrasing is an important step in knowledgebased question answering systems for matching questions asked by users with knowledge-based assertions <ref type="bibr" target="#b13">(Fader et al., 2014;</ref><ref type="bibr" target="#b48">Yin et al., 2015)</ref>.</p><p>Paraphrase generation (given a sentence, generate its paraphrase) <ref type="bibr" target="#b18">(Gupta et al., 2018)</ref> is an area of research benefiting paraphrase detection as well. Lately, many paraphrasing datasets have been introduced to be used for training and testing ML models for both paraphrase detection and generation. MSRP <ref type="bibr" target="#b12">(Dolan and Brockett, 2005)</ref> contains 5801 sentence pairs, each labeled with a binary human judgment of paraphrase, created using heuristic extraction techniques along with an SVM-based classifier. These pairs were annotated by humans, who found 67% of them to be semantically equivalent. The English portion of PPDB <ref type="bibr" target="#b16">(Ganitkevitch et al., 2013)</ref> contains over 220M paraphrase pairs generated by meaning-preserving syntactic transformations. Paraphrase pairs in PPDB 2.0 <ref type="bibr" target="#b37">(Pavlick et al., 2015)</ref> include fine-grained entailment relations, word embedding similarities, and style annotations. TwitterPPDB <ref type="bibr" target="#b26">(Lan et al., 2017)</ref> consists of 51,524 sentence pairs captured from Twitter by linking tweets through shared URLs. This ap-proach's merit is its simplicity as it involves neither a classifier nor a human-in-the-loop to generate paraphrases. Humans annotate the pairs, giving them a similarity score ranging from 1 to 6.</p><p>ParaNMT <ref type="bibr" target="#b46">(Wieting and Gimpel, 2018)</ref> was created by using neural machine translation to translate the English side of a Czech-English parallel corpus (CzEng 1.6 <ref type="bibr" target="#b5">(Bojar et al., 2016)</ref>), generating more than 50M English-English paraphrases. However, ParaNMT's use of machine translation models that are a few years old harms its utility <ref type="bibr" target="#b33">(Nighojkar and Licato, 2021)</ref>, considering the rapid improvement in machine translation in the past few years. To rectify this, we use the google-translate library to translate the Czech side of roughly 300k CzEng2.0 <ref type="bibr" target="#b25">(Kocmi et al., 2020)</ref> sentence pairs ourselves. We call this dataset ParaParaNMT (PP-NMT for short, where the extra paraprefix reflects its similarity to, and conceptual derivation from, ParaNMT).</p><p>Some work has been done in improving the quality of paraphrase detectors by training them on a dataset with more lexical and syntactic diversity. <ref type="bibr" target="#b43">Thompson and Post (2020)</ref> propose a paraphrase generation algorithm that penalizes the production of n-grams present in the source sentence. Our approach to doing this is with the APT, but this is something worth exploring. <ref type="bibr" target="#b42">Sokolov and Filimonov (2020)</ref> use a machine translation model to generate paraphrases much like ParaNMT. An interesting application of paraphrasing has been discussed by <ref type="bibr" target="#b30">Mayhew et al. (2020)</ref> who, given a sentence in one language, generate a diverse set of correct translations (paraphrases) that humans are likely to produce. In comparison, our work is focused on generating adversarial paraphrases that are likely to deceive a paraphrase detector, and models trained on the adversarial datasets we produce can be applied to Mayhew et al.'s work too.</p><p>ANLI <ref type="bibr" target="#b32">(Nie et al., 2020)</ref>, a dataset designed for Natural Language Inference (NLI) <ref type="bibr" target="#b7">(Bowman et al., 2015)</ref>, was collected via an adversarial human-andmodel-in-the-loop procedure where humans are given the task of duping the model into making a wrong prediction. The model then tries to learn how not to make the same mistakes. AFLite (Bras et al., 2020) adversarially filters dataset biases making sure that the models are not learning those biases. They show that model performance on SNLI <ref type="bibr" target="#b7">(Bowman et al., 2015)</ref> drops from 92% to 62% when biases were filtered out. However, their approach is to filter the dataset, which reduces its size, making model training more difficult. Our present work tries instead to generate adversarial examples to increase dataset size. Other examples of adversarial datasets in NLP include work done by <ref type="bibr" target="#b22">Jia and Liang (2017a)</ref>; <ref type="bibr" target="#b49">Zellers et al. (2018</ref><ref type="bibr" target="#b50">Zellers et al. ( , 2019</ref>. Perhaps the closest to our work is PAWS <ref type="bibr" target="#b53">(Zhang et al., 2019)</ref>, short for Paraphrase Adversaries from Word Scrambling. The idea behind PAWS is to create a dataset that has a high lexical overlap between sentence pairs without them being 'paraphrases.' It has 108k paraphrase and non-paraphrase pairs with high lexical overlap pairs generated by controlled word swapping and back-translation, and human raters have judged whether or not they are paraphrases. Including PAWS in the training data has shown the state-of-the-art models' performance to jump from 40% to 85% on PAWS's test split. In comparison to the present work, PAWS does not explicitly incorporate inferential properties, and we seek paraphrases minimizing lexical overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adversarial Paraphrasing Task (APT)</head><p>Semantic Textual Similarity (STS) measures the degree of semantic similarity between two sentences. Popular approaches to calculating STS include BLEU <ref type="bibr" target="#b36">(Papineni et al., 2002)</ref>, BertScore <ref type="bibr" target="#b51">(Zhang et al., 2020)</ref>, and BLEURT <ref type="bibr" target="#b40">(Sellam et al., 2020)</ref>. BLEURT is a text generation metric building on BERT's <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> contextual word representations. BLEURT is warmed-up using synthetic sentence pairs and then fine-tuned on human ratings to generalize better than BERTScore <ref type="bibr" target="#b51">(Zhang et al., 2020)</ref>. Given any two sentences, BLEURT assigns them a similarity score (usually between -2.2 to 1.1). However, high STS scores do not necessarily predict whether two sentences have equivalent meanings. Consider the sentence pairs in <ref type="table" target="#tab_4">Table 3</ref>, highlighting cases where STS and paraphrase appear to misalign. The existence of such cases suggests a way to advance automated paraphrase detection: through an adversarial benchmark consisting of sentence pairs that have the same MI-based meaning, but have BLEURT scores that are as low as possible. This is the motivation behind what we call the Adversarial Paraphrasing Task (APT), which has two components:</p><p>1. Similarity of meaning: Checked through MI (Section 1). We assume if two sentences are M I (Mutually Implicative), they are semantically equivalent and thus paraphrases. Note <ref type="figure">Figure 1</ref>: The mTurk study and the reward calculation. We automatically end the study when a subject earns a total of $20 to ensure variation amongst subjects.</p><p>that MI is a binary relationship, so this APT component does not bring any quantitative variation but is more like a qualifier test for APT. All AP T sentence pairs are M I.</p><p>2. Dissimilarity of structure: Measured through BLEURT, which assigns each sentence pair a score quantifying how lexically and syntactically similar the two sentences are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Manually Solving APT</head><p>To test the effectiveness of APT in guiding the generation of mutually implicative but lexically and syntactically disparate paraphrases for a given sentence, we designed an Amazon Mechanical Turk (mTurk) study <ref type="figure">(Figure 1)</ref>. Given a starting sentence, we instructed participants to "[w]rite a sentence that is the same in meaning as the given sentence but as structurally different as possible. Your sentence should be such that you can infer the given sentence from it AND vice-versa. It should be sufficiently different from the given sentence to get any reward for the submission. For example, a simple synonym substitution will most likely not work."</p><p>The sentences given to the participants came from MSRP and PPNMT (Section 1). Both of these datasets have pairs of sentences in each row, and we took only the first one to present to the par-ticipants. Neither of these datasets has duplicate sentences by design. Every time a sentence was selected, a random choice was made between MSRP and PPNMT, thus ensuring an even distribution of sentences from both datasets. Each attempt was evaluated separately using Equation 1, where mi is 1 when the sentences are M I and 0 otherwise:</p><formula xml:id="formula_1">reward = mi (1 + e 5 * bleurt ) 2<label>(1)</label></formula><p>This formula was designed to ensure (1) the maximum reward per submission was $1, and (2) no reward was granted for sentence pairs that are non-MI or have BLEURT &gt; 0.5. Participants were encouraged to frequently revise their sentences and click on a 'Check' button which showed them the reward amount they would earn if they submitted this sentence. Once the 'Check' button was clicked, the participant's reward was evaluated (see <ref type="figure">Figure  1</ref>) and the sentence pair added to AP H (regardless of whether it was AP T ). If 'Submit' was clicked, their attempt was rewarded based on Equation 1. The resulting dataset of sentence pairs, which we call AP H (Adversarial Paraphrase by Humans), consists of 5007 human-generated sentence pairs, both M I and non-M I (see <ref type="table">Table 2</ref>). Humans were able to generate AP T paraphrases for 75.48% of  <ref type="table">Table 2</ref>: Proportion of sentences generated by humans (AP H ) and T5 base (AP T 5 ). "Attempts" shows the number of attempts the participant made and "Uniques" shows the number of source sentences from the dataset that the performer's attempts fall in that category on. For instance, 1631 unique sentences were presented to humans, who made a total of 5007 attempts to pass AP T and were able to do so for 2659 attempts which amounted to 1231 unique source sentences that could be paraphrased to pass AP T . the sentences presented to them and only 53.1% of attempts were AP T , showing that the task is difficult even for humans. Note that 'M I attempts' and 'M I uniques' are supersets of 'AP T attempts' and 'AP T uniques,' respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Automatically Solving APT</head><p>Since human studies can be time-consuming and costly, we trained a paraphrase generator to perform APT. We used T5 base <ref type="bibr" target="#b39">(Raffel et al., 2020)</ref>, as it achieves SOTA on paraphrase generation <ref type="bibr">(Niu et al., 2020;</ref><ref type="bibr" target="#b3">Bird et al., 2020;</ref> and trained it on TwitterPPDB (Section 2). Our hypothesis was that if T5 base is trained to maximize the APT reward (Equation 1), its generated sentences will be more likely to be AP T . We generated paraphrases for sentences in MSRP and those in TwitterPPDB itself, hoping that since T5 base is trained on TwitterPPDB, it would generate better paraphrases (M I with lower BLEURT) for sentences coming from there. The proportion of sentences generated by T5 base is shown in <ref type="table">Table 2</ref>. We call this dataset AP T 5 , the generation of which involved two phases: Training: To adapt T5 base for APT, we implemented a custom loss function obtained from dividing the cross-entropy loss per batch by the total reward (again from Equation 1) earned from the model's paraphrase generations for that batch, provided the model was able to reach a reward of at least 1. If not, the loss was equal to just the crossentropy loss. We trained T5 base on TwitterPPDB for three epochs; each epoch took about 30 hours on one NVIDIA Tesla V100 GPU due to the CPU bound BLEURT component. More epochs may help get better results, but our experiments showed that loss plateaus after three epochs. Generation: Sampling, or randomly picking a next word according to its conditional probability distribution, introduces non-determinism in language generation. <ref type="bibr" target="#b14">Fan et al. (2018)</ref> introduce top-k sampling, which filters k most likely next words, and the probability mass is redistributed among only those k words. Nucleus sampling (or top-p sampling) <ref type="bibr" target="#b20">(Holtzman et al., 2020)</ref> reduces the options to the smallest possible set of words whose cumulative probability exceeds p, and the probability mass is redistributed among this set of words. Thus, the set of words changes dynamically according to the next word's probability distribution. We use a combination of top-k and top-p sampling with k = 120 and p = 0.95 in the interest of lexical and syntactic diversity in the paraphrases. For each sentence in the source dataset (MSRP 3 and TwitterPPDB for AP M T 5 and AP T w T 5 respectively), we perform five iterations, in each of which, we generate ten sentences. If at least one of these ten sentences passes AP T , we continue to the next source sentence after recording all attempts and classifying them as M I or non-M I. If no sentence in a maximum of 50 attempts passes AP T , we record all attempts nonetheless, and move on to the next source sentence. For each increasing iteration for a particular source sentence, we increase k by 20, but we also reduce p by 0.05 to avoid vague guesses. Note the distribution of M I and non-M I in the source datasets does not matter because we use only the first sentence from the sentence pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset Properties</head><p>T5 base trained with our custom loss function generated AP T -passing paraphrases for (56.19%) of starting sentences. This is higher than we initially expected, considering how difficult APT proved to be for humans <ref type="table">(Table 2)</ref>. Noteworthy is that only 6.09% of T5 base 's attempts were AP T . This does not mean that the remaining 94% of attempts can be discarded, since they amounted to the negative examples in the dataset. Since we trained it on TwitterPPDB itself, we expected that T5 base would generate better paraphrases, as measured by a higher chance of passing AP T on TwitterPPDB, than any other dataset we tested. This is supported by the data in <ref type="table">Table 2</ref>, which shows that T5 base was able to generate an AP T passing paraphrase for 84.8% of the sentences in TwitterPPDB.</p><p>The composition of the three adversarial datasets can be found in <ref type="table">Table 2</ref>. These metrics are useful to understand the capabilities of T5 base as a paraphrase generator and the "paraphrasability" of sentences in MSRP and TwitterPPDB. For instance, T5 base 's attempts on TwitterPPDB tend to be M I much less frequently than those on MSRP and human's attempts on MSRP + PPNMT. This might be because in an attempt to generate syntactically dissimilar sentences, the T5 base paraphraser also ended up generating many semantically dissimilar ones as well.</p><p>To visualize the syntactic and lexical disparity of paraphrases in the three adversarial datasets, we present their BLEURT distributions in <ref type="figure" target="#fig_0">Figure 2</ref>. As might be expected, the likelihood of a sentence pair being M I increases as BLEURT score increases (recall that AP T -passing sentence pairs are simply M I pairs with BLEURT scores &lt;= 0.5), but <ref type="figure" target="#fig_0">Figure 2</ref> shows that the shape of this increase is not straightforward, and differs among the three datasets.</p><p>As might be expected, humans are much more skilled at APT than T5 base , as shown by the fact that the paraphrases they generated have much lower mean BLEURT scores <ref type="figure" target="#fig_0">(Figure 2)</ref>, and the ratio of AP T vs non-AP T sentences is much higher ( <ref type="table">Table 2)</ref>. As we saw earlier, when T5 base wrote paraphrases that were low on BLEURT, they tended to become non-M I (e.g., line 12 in <ref type="table" target="#tab_4">Table 3</ref>). However, T5 base did generate more AP T -passing sentences with a lower BLEURT on Twitter-PPDB than on MSRP, which may be a result of overfitting T5 base on TwitterPPDB. Furthermore, all three adversarial datasets have a distribution of M I and non-M I sentence pairs balanced enough to train a model to identify paraphrases. <ref type="table" target="#tab_4">Table 3</ref> has examples from AP H and AP T 5 showing the merits and shortcomings of T5, BLEURT, and RoBERTa large (the MI detector used). Some observations from <ref type="table" target="#tab_4">Table 3</ref> include:</p><p>? Lines 1 and 3: BLEURT did not recognize the paraphrases, possibly due to the differences in words used. RoBERTa large however, gave the correct MI prediction (though it is worth noting that the sentences in line 1 are questions, rather than truth-apt propositions).</p><p>? Line 4: RoBERTa large and BLEURT (to a large extent since it gave it a score of 0.4) did not recognize that the idiomatic phrase 'break a leg' means 'good luck' and not 'fracture.'</p><p>? Lines 6 and 12: There is a loss of information going from the first sentence to the second and BLEURT and MI both seem to have understood the difference between summarization and paraphrasing.</p><p>? Line 7: T5 not only understood the scores but also managed to paraphrase it in such a way that was not syntactically and lexically similar, just as we wanted T5 to do when we fine-tuned it.</p><p>? Line 9: T5 base knows that Fort Lauderdale is in Florida but RoBERTa large does not.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To quantify our datasets' contributions, we designed experiment setups wherein we trained RoBERTa base <ref type="bibr" target="#b29">(Liu et al., 2019)</ref> for paraphrase detection on a combination of TwitterPPDB and our datasets as training data. RoBERTa was chosen for its generality, as it is a commonly used model in current NLP work and benchmarking, and currently achieves SOTA or near-SOTA results on a majority of NLP benchmark tasks <ref type="bibr" target="#b45">(Wang et al., 2019</ref><ref type="bibr" target="#b44">(Wang et al., , 2020</ref>;   <ref type="bibr" target="#b10">Chen et al., 2021)</ref>.</p><formula xml:id="formula_2">Training Dataset TwitterPPDB + Size AP H AP H -test MCC F1 MCC F1 AP H -train<label>46k</label></formula><p>For each source sentence, multiple paraphrases may have been generated. Hence, to avoid data leakage, we created a train-test split on AP H such that all paraphrases generated using a given source sentence will be either in AP H -train or in AP Htest, but never in both. Note that AP H is not balanced as seen in <ref type="table">Table 2</ref>. <ref type="table" target="#tab_5">Table 4</ref> shows the distribution of M I and non-M I pairs in AP H -train and AP H -test and 'M I attempts' and 'non-M I attempts' columns of <ref type="table">Table 2</ref> show the same for other adversarial datasets. The test sets used were AP H wherever AP H -train was not a part of the training data and AP H -test in every case.</p><p>Does RoBERTa base do well on AP H ? RoBERTa base was trained on each training dataset (90% training data, 10% validation data) for five epochs with a batch size of 32 with the training and validation data shuffled, and the trained model was tested on AP H and AP H -test. The results of this are shown in <ref type="table" target="#tab_7">Table 6</ref>. Note that since the number of M I and non-M I sentences in all the datasets is imbalanced, Matthew's Correlation Coefficient (MCC) is a more appropriate performance measure than accuracy <ref type="bibr" target="#b6">(Boughorbel et al., 2017)</ref>.</p><p>Our motivation behind creating an adversarial dataset was to improve the performance of paraphrase detectors by ensuring they recognize paraphrases with low lexical overlap. To demonstrate the extent of their inability to do so, we first compare the performance of RoBERTa base trained only on TwitterPPDB on specific datasets as shown Table 5. Although the model performs slightly well on MSRP, it does barely better than a random prediction on AP H , thus showing that identifying adversarial paraphrases created using APT is nontrivial for paraphrase identifiers.</p><p>Do human-generated adversarial paraphrases improve paraphrase detection? We introduce AP H -train to the training dataset along with Twit-terPPDB. This improves the MCC by 0.222 even though AP H -train constituted just 8.15% of the entire training dataset, the rest of which was Twit-terPPDB <ref type="table" target="#tab_7">(Table 6</ref>). This shows the effectiveness of human-generated paraphrases, as is especially impressive given the size of AP H -train compared to TwitterPPDB.</p><p>Do machine-generated adversarial paraphrases improve paraphrase detection? We set out to test the improvement brought by AP T 5 , of which we have two versions. Adding AP M T 5 to the training set was not as effective as adding AP H -train, increasing MCC by 0.188 on AP H and 0.151 on AP H -test, thus showing us that T5 base , although was able to clear AP T , lacked the quality which human paraphrases possessed. This might be explained by <ref type="figure" target="#fig_0">Figure 2</ref> -since AP M T 5 does not have many sentences with low BLEURT, we cannot expect a vast improvement in RoBERTa base 's performance on sentences with BLEURT as low as in AP H .</p><p>Since we were not necessarily testing T5 base 's performance -and we had trained T5 base on Twit-terPPDB -we used the trained model to perform APT on TwitterPPDB itself. Adhering to expectations, training RoBERTa base (the paraphrase detector) with AP T w T 5 yielded higher MCCs. Note that none of the sentences are common between AP T w T 5 and AP H since AP H is built on MSRP and PPNMT and the fact that the model got this performance when trained on AP T w T 5 is a testimony to the quality and contribution of APT.</p><p>Combining these results, we can conclude that although machine-generated datasets like AP T 5 can help paraphrase detectors improve themselves, a smaller dataset of human-generated adversarial paraphrases improved performance more. Overall, however, the highest MCC (0.525 in <ref type="table" target="#tab_7">Table 6</ref>) is obtained when TwitterPPDB is combined with all three adversarial datasets, suggesting that the two approaches nicely complement each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions and Conclusions</head><p>This paper introduced APT (Adversarial Paraphrasing Task), a task that uses the adversarial paradigm to generate paraphrases consisting of sentences with equivalent (sentence-level) meanings, but differing lexical (word-level) and syntactical similarity. We used APT to create a human-generated dataset / benchmark (AP H ) and two machinegenerated datasets (AP M T 5 and AP T w T 5 ). Our goal was to effectively augment how paraphrase detectors are trained, in order to make them less reliant on word-level similarity. In this respect, the present work succeeded: we showed that RoBERTa base trained on TwitterPPDB performed poorly on APT benchmarks, but this performance was increased significantly when further trained on either our human-or machine-generated datasets. The code used in this paper along with the dataset has been released in a publicly-available repository. <ref type="bibr">4</ref> Paraphrase detection and generation have broad applicability, but most of their potential lies in areas in which they still have not been substantially applied. These areas range from healthcare (improving accessibility to medical communications or concepts by automatically generating simpler language), writing (changing the writing style of an article to match phrasing a reader is better able to understand), and education (simplifying the language of a scientific paper or educational lesson to make it easier for students to understand). Thus, future research into improving their performance can be very valuable. But approaches to paraphrase that treat it as no more than a matter of detecting word similarity overlap will not suffice for these applications. Rather, the meanings of sentences are properties of the sentences as a whole, and are inseparably tied to their inferential properties. Thus, our approaches to paraphrase detection and generation must follow suit.</p><p>The adversarial paradigm can be used to dive deeper into comparing how humans and SOTA language models understand sentence meaning, as we did with APT. Furthermore, automatic generation of adversarial datasets has much unrealized potential; e.g., different datasets, paraphrase generators, and training approaches can be used to generate future versions of AP T 5 in order to produce AP T passing sentence pairs with lower lexical and syntactic similarities (as measured not only by BLEURT, but also by future state-of-the-art STS metrics). The idea of more efficient automated adversarial task performance is particularly exciting, as it points to a way language models can improve themselves while avoiding prohibitively expensive human participant fees.</p><p>Finally, the most significant contribution of this paper, APT, presents a dataset creation method for paraphrases that will not saturate because as the models get better at identifying paraphrases, we will improve paraphrase generation. As models get better at generating paraphrases, we can make APT harder (e.g., by reducing the BLEURT threshold of &lt; 0.5). One might think of this as students in a class who come up with new ways of copying their assignments from sources as plagiarism detectors improved. That brings us to one of the many applications of paraphrases: plagiarism generation and detection, which inherently is an adversarial activity. Until plagiarism detectors are trained on adversarial datasets themselves, we cannot expect them to capture human levels of adversarial paraphrasing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>BLEURT distributions on adversarial datasets. All figures divide the range of observed scores into 100 bins. Note that AP T sentence pairs are also M I, whereas those labeled 'MI' are not AP T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Notations used in the paper.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Examples from adversarial datasets. The source dataset (TP short for TwitterPPDB) tells which dataset the sentence pair comes from (and whether it is in AP M T 5 or AP T w T 5 for AP T 5 ). All datasets have AP T passing and failing M I and non-M I sentence pairs.</figDesc><table><row><cell>Dataset</cell><cell>Total</cell><cell>M I</cell><cell>non-M I</cell></row><row><cell>AP H -train</cell><cell cols="3">3746 2433 64.95% 1313 35.05%</cell></row><row><cell>AP H -test</cell><cell cols="3">1261 799 63.36% 462 36.64%</cell></row><row><cell cols="4">MSRP-train 4076 2753 67.54% 1323 32.46%</cell></row><row><cell>MSRP-test</cell><cell cols="3">1725 1147 66.50% 578 33.50%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Distribution of M I and non-M I pairs.</figDesc><table><row><cell>Test Set</cell><cell>RoBERTa base MCC F1</cell><cell cols="2">Random MCC F1</cell></row><row><cell cols="2">MSRP-train 0.349 0.833</cell><cell>0</cell><cell>0.806</cell></row><row><cell>MSRP-test</cell><cell>0.358 0.829</cell><cell>0</cell><cell>0.799</cell></row><row><cell>AP H</cell><cell>0.222 0.746</cell><cell>0</cell><cell>0.784</cell></row><row><cell>AP H -test</cell><cell>0.218 0.743</cell><cell>0</cell><cell>0.777</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance of RoBERTa base trained on just TwitterPPDB (no adversarial datasets) vs. random prediction.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Performance of RoBERTa base trained on adversarial datasets. Size is the number of training examples in the dataset rounded to nearest 1000.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this paper we study paraphrase between sentences, and do not address the larger scope of how our work might extend to paraphrasing between arbitrarily large text sequences.2  The notations used in this paper are listed inTable 1.arXiv:2106.07691v1 [cs.CL] 14 Jun 2021</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use the official train split released by<ref type="bibr" target="#b12">Dolan and Brockett (2005)</ref> containing 4076 sentence pairs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/ Advancing-Machine-Human-Reasoning-Lab/ apt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This material is based upon work supported by the Air Force Office of Scientific Research under award numbers FA9550-17-1-0191 and FA9550-18-1-0052. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Air Force. We would also like to thank Antonio Laverghetta Jr. and Jamshidbek Mirzakhalov for their helpful suggestions while writing this paper, and Gokul Shanth Raveendran and Manvi Nagdev for helping with the website used for the mTurk study.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated paraphrase lattice creation for hyter machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Cocos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="480" to="485" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Paraphrasing with bilingual parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Bannard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="597" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Squibs: What is a paraphrase?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.1162/COLI_a_00166</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="463" to="472" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Chatbot interaction with artificial intelligence: Human data augmentation with t5 and language transformer ensemble for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">J</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anik?</forename><surname>Ek?rt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><forename type="middle">R</forename><surname>Faria</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inferential role semantics and the analytic/synthetic distinction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boghossian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Studies: An International Journal for Philosophy in the Analytic Tradition</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Czeng 1.6: enlarged czech-english parallel corpus with processing tools dockered</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Du?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jind?ich</forename><surname>Libovick?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Nov?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Sudarikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du?an</forename><surname>Vari?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Text, Speech, and Dialogue</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimal classifier for imbalanced data using matthews correlation coefficient metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabri</forename><surname>Boughorbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Jarray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>El-Anbari</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0177678</idno>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="177678" to="0177678" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<title level="m">Adversarial filters of dataset biases</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved statistical machine translation using paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting>the Human Language Technology Conference of the NAACL, Main Conference<address><addrLine>Philipp Koehn, and Miles Osborne; New York City, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Transformer-based language model fine-tuning methods for covid-19 fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengfu</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Workshop on Paraphrasing (IWP2005). Asia Federation of Natural Language Processing</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open question answering over curated and extracted knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<idno type="DOI">10.1145/2623330.2623677</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1156" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Hierarchical neural story generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A semantic similarity approach to paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th annual research colloquium of the UK special interest group for computational linguistics</title>
		<meeting>the 11th annual research colloquium of the UK special interest group for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ppdb: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dynamic multi-level multi-task learning for sentence simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A deep generative framework for paraphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prawaan</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The effects of the paraphrasing strategy on the reading comprehension of middle school students at risk for failure in reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">L</forename><surname>Hagaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Reid</surname></persName>
		</author>
		<idno type="DOI">10.1177/0741932507311638</idno>
	</analytic>
	<monogr>
		<title level="j">Remedial and Special Education</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="222" to="234" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Machine learning models for paraphrase identification and its applications on plagiarism detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Janamsetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kinares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ozdemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yolcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dahal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gewali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICBK.2019.00021</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Big Knowledge (ICBK)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1215</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2021" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How to ask good questions? try to leverage paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfang</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.545</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6130" to="6140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Announcing czeng 2.0 parallel corpus with over 2 gigawords</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03006</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A continuously growing dataset of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuwei</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1126</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1224" to="1234" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Colln</surname></persName>
		</author>
		<title level="m">The effect of instruction in the paraphrasing strategy on reading fluency and comprehension</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Agent zero: Zero-shot automatic multiplechoice question generation for skill assessments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Wai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Roberta: A robustly optimized bert pretraining approach</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simultaneous translation and paraphrase for language education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klinton</forename><surname>Bicknell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Neural Generation and Translation (WNGT). ACL</title>
		<meeting>the ACL Workshop on Neural Generation and Translation (WNGT). ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Combining fact extraction and verification with neural semantic matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adversarial nli: A new benchmark for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mutual implication as a measure of textual equivalence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Nighojkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Licato</surname></persName>
		</author>
		<idno type="DOI">10.32473/flairs.v34i1.128519</idno>
	</analytic>
	<monogr>
		<title level="m">The International FLAIRS Conference Proceedings</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Nitish Shirish Keskar, and Caiming Xiong. 2020. Unsupervised paraphrase generation via dynamic blocking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="181" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ppdb 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Meaning as an inferential role</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaroslav</forename><surname>Peregrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Erkenntnis</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>text transformer</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Bleurt: Learning robust metrics for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Neural machine translation for paraphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Filimonov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Paraphrase generation as zero-shot multilingual translation: Disentangling semantic similarity from lexical and syntactic diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1042</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Bowman</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Answering questions with complex semantic constraints on open knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1145/2806416.2806542</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1301" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Swag: A large-scale adversarial dataset for grounded commonsense inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Hellaswag: Can a machine really finish your sentence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<title level="m">Bertscore: Evaluating text generation with bert</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Paraphrase identification by text canonicalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Patrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Language Technology Workshop 2005</title>
		<meeting>the Australasian Language Technology Workshop 2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="160" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Paws: Paraphrase adversaries from word scrambling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
