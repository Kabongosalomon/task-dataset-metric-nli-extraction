<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disentangled Motif-aware Graph Learning for Phrase Grounding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongshen</forename><surname>Mu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">DCD Lab</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siliang</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">DCD Lab</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">DCD Lab</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">City Cloud Technology (China) Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
							<email>yzhuang@zju.edu.cnyq@citycloud.com.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">DCD Lab</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Disentangled Motif-aware Graph Learning for Phrase Grounding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel graph learning framework for phrase grounding in the image. Developing from the sequential to the dense graph model, existing works capture coarse-grained context but fail to distinguish the diversity of context among phrases and image regions. In contrast, we pay special attention to different motifs implied in the context of the scene graph and devise the disentangled graph network to integrate the motif-aware contextual information into representations. Besides, we adopt interventional strategies at the feature and the structure levels to consolidate and generalize representations. Finally, the cross-modal attention network is utilized to fuse intra-modal features, where each phrase can be computed similarity with regions to select the bestgrounded one. We validate the efficiency of disentangled and interventional graph network (DIGN) through a series of ablation studies, and our model achieves state-of-the-art performance on Flickr30K Entities and ReferIt Game benchmarks. 1  We post-hoc motifs referring to the method in the topic model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>On giving an image-sentence pair, the Phrase Grounding (or more generally, Language Grounding) task aims to ground multiple noun phrases from the given sentence to the corresponding image regions, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. By aligning textual modality to visual modality, phrase grounding is able to link knowledge from both modalities and improves multimodal tasks that span visual question answering, visual commonsense reasoning, and robotic navigation.</p><p>Since the information that the image usually contains is more than that of the sentence, the general solution is to understand the surrounding visual context of a visual object, especially pay attention to the context considered by the sentence as well, and then map it to its referent words. Nevertheless, previous studies fail to consider such fine-grained context. Some methods neglect the importance of visual or textual context, nor do they ground individual phrases independently <ref type="bibr" target="#b20">(Plummer et al. 2018)</ref> or sequentially <ref type="bibr" target="#b8">(Dogan et al. 2019;</ref><ref type="bibr" target="#b14">Li et al. 2020)</ref>. This leads to poor performance when objects with completely different functions or actions have the similar visual effect in the same image. More recently, <ref type="bibr" target="#b1">Bajaj et al. (2019)</ref> and <ref type="bibr" target="#b16">Liu et al. (2020)</ref> utilize fully connected dense graphs to capture coarse-grained context among visual objects, and improve grounding results. However, their graphs have three fatal problems. First, the dense graph holds noisy false connections and makes errors in complex scenes. Second, they treat all the relations equally. Under such a framework, the node (i.e., phrase or visual object) representation is a mixture of all the possible relations. In reality, it is insufficient to reveal various highorder relation categories between nodes, or motifs 1 (e.g., decorative, part-of, spatial, action), in the contextual information. For instance, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, "a child" needs to be grounded in the image that includes multiple similar object regions. As mentioned above, the language graph (the lower left of <ref type="figure" target="#fig_0">Fig. 1)</ref> is only the subgraph of visual contents.</p><p>When we learn the representation of "a child" in the dense graph (the upper left of <ref type="figure" target="#fig_0">Fig. 1</ref>), it is difficult to ground the right region in noisy context. After the spurious neighbors are filtered in the scene graph (the upper right of <ref type="figure" target="#fig_0">Fig. 1)</ref>, it is also difficult to make a grounding decision because too many redundant motifs around the child prevent the model from grasping the vital motif. Third, there are some cooccurring visual objects in the datasets, while some of irrelevant co-occurrences are of high probability <ref type="bibr" target="#b34">(Wang et al. 2020)</ref>, which influences the graph structure and finally damages the model's robustness and generalization capability.</p><p>In this paper, we propose the Disentangled Interventional Graph Network (DIGN), a more explainable framework for phrase grounding. First, instead of using a fully connected dense graph, we use two scene graphs to model the context among phrases and regions. Then, we use two disentangled graph networks to integrate these various context into disentangled dimensions of node (i.e., phrase or visual object) representation. Each chunk, referred to as a motif-aware embedding, can represent the strength of a certain motif (the lower right of <ref type="figure" target="#fig_0">Fig. 1</ref>), which greatly reduces the noise from spurious connections and creates fine-grained intra-modal representations for later inter-modal fusion. Furthermore, we propose the interventional visual sample synthesizing training scheme to alleviate the co-occurring biases problem and improve the model's robustness.</p><p>Our key contributions are threefold:</p><p>? We devise motif-aware graph learning based on disentangled graph network, which distinguishes diverse context and considers fine-grained contextual fusing.</p><p>? We adopt the feature and the structure interventional strategies to learn robust representations in the graph.</p><p>? We conduct a series of experiments that demonstrate our DIGN model achieves more competitive performance than the prior state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>There are two lines of literature that are closely related to our work: phrase grounding and disentangling representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phrase Grounidng</head><p>To ground the spatial region described by the phrase in an image, early methods <ref type="bibr" target="#b30">(Wang et al. 2016a</ref><ref type="bibr" target="#b31">(Wang et al. , 2018</ref><ref type="bibr" target="#b20">Plummer et al. 2018;</ref><ref type="bibr" target="#b0">Akbari et al. 2019;</ref><ref type="bibr" target="#b22">Plummer et al. 2020</ref>) learn a shared embedding subspace and compute the distance of each phrase-region pair. The aforementioned methods ground each phrase independently and neglect textual and visual context information. One step forward, the RNN structure <ref type="bibr" target="#b12">(Hu, Xu et al. 2016;</ref><ref type="bibr" target="#b8">Dogan et al. 2019</ref>) and the Transformer model ) are used to capture context among phrases and regions. To formulate more complex and non-sequential dependencies, graph architecture <ref type="bibr" target="#b1">(Bajaj et al. 2019;</ref><ref type="bibr" target="#b16">Liu et al. 2020</ref>) becomes increasingly prevalent and leverage relationships between nodes to enrich representations and achieve alignment. They treat the context uniformly at a holistic aspect, which results in suboptimal representation and limited interpretability. An underlying fact is omitted that: grounding the phrase to the region is affected by different motifs in the graph's context. Our model disentangles context at a fine-grained level and incorporates motif-aware context into the representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disentangling Representation</head><p>Learning to identify and disentangle explanatory factors hidden in the observed milieu of sensory data is the foundation of artificial intelligence. Early attempts <ref type="bibr" target="#b27">(Tishby et al. 2015)</ref> apply the information bottleneck method only to capture and extract a relevant or meaningful data summary. Based on information-theoretic concepts, ?-VAE <ref type="bibr" target="#b11">(Higgins et al. 2017)</ref> can learn disentangled representations of independent visual data generative factors in a completely unsupervised manner. Later studies <ref type="bibr" target="#b2">(Burgess et al. 2018;</ref><ref type="bibr" target="#b6">Chen et al. 2018)</ref> achieve further improvements for disentangled factor learning at a more granular level. These existing efforts are made to discover different latent factors on non-relational data. More recently, disentangled representation learning is explored in the filed of graph-structured data <ref type="bibr" target="#b18">(Ma et al. 2019;</ref><ref type="bibr" target="#b40">Zheng et al. 2020)</ref>, most of which mainly disentangle latent factors behind user behavior in bipartite graphs commonly seen in recommender systems. This work focuses on not only disentangling motifs in the intra-modal scene graph but also keeping disentangled in cross-modal common subspace. The disentangled graph network for phrase grounding is related to previous modular method, MattNet <ref type="bibr" target="#b38">(Yu et al. 2018</ref>), but it is implicit to assign the meaning of the chunked representation and convenient to transfer to other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Given multiple phrases and the corresponding image, we denote a set of noun phrases as P = {p i } n i=1 and their spatial regions (or bounding boxes) as</p><formula xml:id="formula_0">B + = {b + i } n i=1 where b + i = (x 1 , y 1 , x 2 , y 2 )</formula><p>is the left-top and right-bottom coordinates. The primary goal is to select N bounding boxes from region proposals B = {b i } m i=1 (m &gt;&gt; n) as much overlap with ground truth annotations B + as possible and get grounding results. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates our disentangled interventional graph network (DIGN). The framework are composed of four stages to implement phrase grounding at a fine-grained level: 1) phrase and visual feature encoding; 2) two intra-modal disentangled graph networks; 3) interventional operators in the visual graph; 4) a cross-modal Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phrase and Visual Encoders</head><p>Each phrase consists of a word or a sequence of words. For phrase features, we encode each word of the phrase to a realvalued vector by a pre-trained BERT <ref type="bibr" target="#b7">(Devlin et al. 2019)</ref>, then compute the phrase representation by taking the sum of word embeddings in each p i . In respect of visual features, we use a region proposal network <ref type="bibr" target="#b23">(Ren et al. 2015)</ref> to extract the top-M proposals with diversity and discrimination. Each proposal b i is represented after the last fully-connected layer of RoI-Align pooling <ref type="bibr" target="#b10">(He et al. 2017)</ref>. To transform phrase and visual features to the identical dimensions, t i ? R d t in and v i ? R d v in are obtained severally through two fully-connected RPN A kid wearing a Gap hat looks into the camera with a funny face and ice cream cone in his hand while a woman looks on from behind him .</p><p>&lt;a gap hat&gt; &lt;a woman&gt; &lt;a kid&gt; </p><formula xml:id="formula_1">? p 1 p 2 p n ? ? b m b 2 b 1 ? t 1 t 5 t 2 t 3 t 6<label>t</label></formula><formula xml:id="formula_2">v 2 v 3 v 6 v 4 v 5 Visual Scene Graph Phrase Scene Graph G V G T for each phrase node embedding t i v 1 v 2 v 3 v 6 v 4 v 5 v 1 v 2 v 3 v 6 v 4 v 5 v 1 v 2 v 3 v 6 v 4 v 5 v 1 v 2 v 3 v 6 v 4 v 5</formula><p>Visual Motif-aware Disentangling Layer</p><formula xml:id="formula_3">t 1 t 5 t 2 t 3 t 6 t 4 t 1 t 5 t 2 t 3 t 6 t 4 t 1 t 5 t 2 t 3 t 6 t 4 t 1 t 5 t 2 t 3 t 6 t 4 Phrase Motif-aware Disentangling Layer ? ? Visual Disentangled Features Phrase Disentangled Features h t1 h t2 h t6 h v6 h v2 h v1</formula><p>Similarity Prediction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-modal Graph Network</head><p>Cross-modal Transformer </p><formula xml:id="formula_4">G V ,1 G V ,2 G V ,3 G V ,4 G T ,4 G T ,3 G T ,2 G T ,1 t i,1 t i,2 t i,3 t i,4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Region Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phrase Extraction</head><formula xml:id="formula_5">for each visual node embedding v i v i,4 v i,3 v i,2 v i,1 Map into K Subspaces Map into K Subspaces for each visual node embedding v i,4 v i,2 v i,1 Visual Feature Intervention G V = (V V ,E V ) Visual Scene Graph Map into K Subspaces v ineg Visual Motif-aware Disentangling Layer Visual Relation Intervention Map into K Subspaces G ' V = (V V ,E neg V ) Visual Scene Graph v 1 v 2 v 3 v 6 v 4 v 5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interventional Visual Representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-Modal Graph Network</head><p>To separate various context among noun phrases and region proposals and provide explicit channels to guide motifaware information flow, we introduce the following two intra-modal disentangled graph networks below.</p><p>Phrase Disentangled Graph Instead of a noisy dense graph to capture linguistic context, we construct a phrase scene graph G T = (V T , E T ) from the image caption including phrases by an off-the-shelf scene graph parser <ref type="bibr" target="#b25">(Schuster et al. 2015)</ref>. Formally, V T = {t i } n i=1 represents nodes equipped with phrase embeddings, (i, j) ? E T indicates that there is an edge connecting t i and t j , and N T i = {t j : (i, j) ? E T } denotes the neighbors of t i .</p><p>When grounding the phrase to the image region, the situation that similar object regions are required to be differentiated is universal and intractable. Intuitively, one phrase interacts with neighbors and makes use of its context to distinguish the regions. However, different situations need to consider the different context of the neighborhood. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, t 1 links with t 2 , t 3 , t 4 , and t 6 , whose context includes "wearing", "with", "in", and "look on from behind" high-order relation patterns. The context can be categorized into decorative, part-of, spatial, and action motifs respectively. Clearly, both the kid and the woman have funny faces, but a funny face is grounded to the kid because of his unique decorative motif (e.g., he has a gap hat) and spatial motif (e.g., the ice cream cone is on his hand), which do not belong to the context of the woman.</p><p>Therefore, unlike standard graph convolutional network <ref type="bibr" target="#b29">(Veli?kovi? et al. 2017;</ref><ref type="bibr" target="#b35">Xu et al. 2019</ref>) viewing context of the ego node as a holistic one to message and update, we aim to learn a disentangled representation over various motifs, as follows:</p><formula xml:id="formula_6">h T i = [h T i,1 , h T i,2 , ..., h T i,K ]<label>(1)</label></formula><p>where h T i is the final output of t i , K is the hyperparameter controlling the number of latent motifs. It is expected that</p><formula xml:id="formula_7">h T i,k ? R d T out</formula><p>K incorporates the contextual information of the k-th motif and is independent of other components. For the k-th chunked embeddings of all nodes, we define a motifaware graph as</p><formula xml:id="formula_8">G T,k = {V T k , E T }.</formula><p>As such, we build a set of subgraphs G T = {G T,1 , G T,2 , ..., G T,K } to refine motifaware linguistic features through graph disentangling layers.</p><p>To separate the original mixed embedding, we project t i into K subspaces, as shown below:</p><formula xml:id="formula_9">t 0 i,k = ?(W T k 0 t i + b T k 0 ) ?(W T k 0 t i + b T k 0 ) 2 (2) where W T k 0 ? R d T out K ?d T in and b T k 0 ? R d T out</formula><p>K are the parameters of channel k, the superscript 0 shows the original disentangled state; and ?(?) is a nonlinear activation function. We use l 2 -normalization to ensure numerical stability and prevent the neighbors with overly rich features from distorting our prediction. Other components of t i is computed as t 0 i,k , which are fed to the corresponding subgraph G T,k .</p><p>After initializing nodes of motif-aware subgraphs, we distill useful information of neighbors to comprehensively capture K latent factors' context. For each motif-aware subgraph, we design a phrase graph disentangling layer:</p><formula xml:id="formula_10">t 1 i,k = ?(W T ek 1 t 0 i,k + j?N T i a T j,k 1 W T k 1 t 0 j,k ) (3) a T j,k 1 = exp(t 0 j,k t 0 i,k ) K k =1 exp(t 0 j,k t 0 i,k ) (4) where a T j,k 1 (a T j,k 1 ? 0, K k =1 a T j,k 1 = 1) provide the k-th motif's importance why t j connects with t i ; N i is the first- order neighbors of t i ; W T ek 1 and W T k 1 ? R d T out K ? d T out K</formula><p>represent the learnable parameters of the ego node and neighbor nodes at the k-th aspect respectively; the superscript 1 represents the output of the graph disentangling layer from onehop neighbors; and ? is ReLU activation function followed by dropout with probability 0.5.</p><p>Equation <ref type="formula">4</ref> is the key step to the first-order neighbor routing. When calculating it with all neighbors, we can get the contextual information for motif k, which is similar to the attention mechanism, but not exactly the same. The attention mechanism like Graph Attentive networks ) coarsely aggregates one-hop neighbors and updates the ego node as an entirety, causing difficult to distinguish different context and entangling latent motifs. Conversely, the neighbor rooting mechanism slices the node into k components, for which each part can attend similar context and provide an interpretable result.</p><p>Having used one-hop neighbors, we can stack more graph disentangling layers to gather influential signals from higher-order neighbors. After L layers, we sum up the motifaware representations at different layer as the k-th chunk representation:</p><formula xml:id="formula_11">h T i,k = t 0 i,k + t 1 i,k + ... + t L i,k<label>(5)</label></formula><p>We argue that h T i,k describes the semantic context related with the k-th motif. Finally, we joint K chunks to update t i after L disentangling layers, as presented in equation 1. Consequently, we not only disentangle phrase representations but also explain each part of them.</p><formula xml:id="formula_12">Visual Disentangled Graph We establish a visual scene graph G V = (V V , E V )</formula><p>by the scene graph generation model <ref type="bibr" target="#b39">(Zellers et al. 2018</ref>) that exploits visual relationships among region proposals.</p><p>We hypothesize that the visual graph context implies the same categories of motifs as those in the context of the phrase graph. So we also slice the visual node embedding into K components, and every chunked node embeddings constructs independent subgraph</p><formula xml:id="formula_13">G V,k = (V V k , E V ). G V is composed of K motif-aware graphs {G V,1 , G V,2 , ..., G V,K }.</formula><p>Similar to the phrase graph network, we use another disentangled graph network to integrate visual context containing varying motifs into representations. More details are presented in Algorithm 1 of Appendix A.</p><p>We firstly map each v i into K subspaces to initialize K chunked embeddings by linear transformations:</p><formula xml:id="formula_14">v 0 i,k = ?(W V k 0 v i + b V k 0 ) ?(W V k 0 v i + b V k 0 ) 2 (6) where W V k 0 ? R d V out K ?d V in and b V k 0 ? R d V out K , v 0 i,k is the k-th part of v 0</formula><p>i . The aggregation and updating steps of a visual graph disentangling layer about the k-th motif can be described as:</p><formula xml:id="formula_15">v 1 i,k = ?(W V ek 1 v 0 i,k + j?N V i a V j,k 1 W V k 1 v 0 j,k ) (7) where W V ek 1 , W V k 1 ? R d V out K ? d V out K , a V j,k</formula><p>1 is the result of the neighbor rooting mechanism in visual graph network. After iterating more layers, the final output of each chunk h V i,k is obtained by summing up every layers' hidden states of v i :</p><formula xml:id="formula_16">h V i,k = v 0 i,k + v 1 i,k + ... + v L i,k<label>(8)</label></formula><p>The disentangled result h V i is depicted to:</p><formula xml:id="formula_17">h V i = [h V i,1 , h V i,2 , ..., h V i,K ]<label>(9)</label></formula><p>Interventional Visual Representation</p><p>The dataset co-occurring bias, the irrelevant relations, may lead to in-correct attention. When modeling the context of regions in the image by disentangled graph network, it is possible that the biased data plagues chunked representation with irrelevant information through context aggregation. Notably, visual features from the encoder are vulnerable to learn the bias <ref type="bibr" target="#b3">(Cadene et al. 2019</ref>). Thus, we intervene in representation learning in disentangled graph network to reduce the effect of bias and robust representations. The core idea of intervention is to change the environment, helping the model train with more unseen data and discover more possible reasons. We utilize the interventional strategies to improve motif-aware learning during the training process. It is efficient and effective to construct interventional samples at the feature and the structure levels in the graph-structure data. Compared to the masked attentive region strategy <ref type="bibr" target="#b15">(Liu et al. 2019)</ref>, our strategies do not depend on language attention and are more flexible to learn context-aware features in the graph. The pseudocode of the interventional process is listed in Appendix B.</p><p>We first introduce the strategy of structure intervention. We aim to change the original edges in the visual scene graph. When modifying the nodes of edges, we just change the target node of the directional edge. In this way, it can eliminate the impact of changing other connected nodes' features as much as possible. We interrupt and randomly interchange all the target nodes' edges in the visual graph. Integrated unrelated motif context into the representation by the neighbor routing mechanism, these contrastive samples help the network to grasp natural motifs between nodes.</p><p>The feature interventional samples are synthesized with the implementation of masking right chunked features with incorrect data. There are two types of masked feature interventional methods. One type is to replace features of neighbor nodes in corresponding dims. Neighbors are usually analogous to the ego node on the semantic aspect, making the model distinguish similar cases. The other type is to fill each dim of chunked embedding with noisy distribution. The generated features are treated as noisy training data. It is beneficial and general for the model to learn more unseen samples and have the resilience to noisy data. Therefore, the interventional samples can promote the model to find and understand the meaning of the replaced chunk automatically for phrase grounding.</p><p>We define the particular loss function in Section to make full use of interventional samples. Moreover, the intervention module is vital to motif-aware learning, which is demonstrated in the following ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Modal Transformer</head><p>To align two modal representations in a shared space, we introduce a cross-modal Transformer, as shown in the last column of <ref type="figure" target="#fig_1">Figure 2</ref>. Unlike simple concatenation of two modal features, the multi-head attention mechanism can benefit each head block to capture the corresponding motif's mutual information from both modal disentangled representations. When the visual information is injected to the linguistic encoding and the linguistic information is incorporated to the visual encoding, the Transformer can dynamically select judicious cues for target representation.</p><p>We denote the inputs for the Transformer as h</p><formula xml:id="formula_18">T = {h T 1 , h T 2 , ..., h T n } and h V = {h V 1 , h V 2 , ..., h V m },</formula><p>which are processed by phrase and visual graph networks respectively. Specifically, multi-head attention is computed as Multihead(Q, K, V ), where Q is the query, K is the key, and V is the value. The details can refer to <ref type="bibr" target="#b28">Vaswani et al. (2017)</ref>. Two modal features are interacted by:</p><formula xml:id="formula_19">c T = Multihead(W q h T , W k h V , W v h V ) (10) c V = Multihead(W q h V , W k h T , W v h T )<label>(11)</label></formula><p>where W q , W k and W v are learnable weights, c T is the blended feature from phrases, while c V is the guided representation from regions. Through this form Transformer, linguistic and visual features are associated so that the similarity prediction is defined as:</p><formula xml:id="formula_20">sim(c T i , c V j ) = c T i ? c V j</formula><p>(12) where we just use an inner (dot) product to achieve state-ofthe-art performance. The similarity score is thought as the the probability whether the phrase p i is grounded to the image region b i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Objectives</head><p>Our loss function includes three parts. The first part is the phrase independent loss. For encouraging K components of the node embedding to be independent, we apply distance correlation <ref type="bibr" target="#b26">(Sz?kely et al. 2007</ref>) to characterizing independence of any two paired vectors. Each pair's coefficient is zero if and only if these vectors are independent. After phrase disentangled graph network, we formulate the independent loss of phrase chunked embeddings as:</p><formula xml:id="formula_21">T ind = n i=1 K k=1 K k =k+1 ?(h T i,k , h T i,k ) (13) ?(h T i,k , h T i,k ) = Cov(h T i,k , h T i,k ) D(h T i,k ) ? D(h T i,k )<label>(14)</label></formula><p>where Cov(?) and D(?) represent the covariance and the variance between vectors respectively. Identically, the visual independent loss as the second part is computed as:</p><formula xml:id="formula_22">V ind = m i=1 K k=1 K k =k+1 ?(h V i,k , h V i,k )<label>(15)</label></formula><p>The third part is cross-modal grounding loss. For a phrase p i , the area of best-matched result b i in B is maximum, overlapping with the ground truth b + i . We denote {B ? b i } as M ?1 failed results. For outputs of the interventional model, we change their representations and treat them unmatched with ground truth. In total, there are 2M ? 1 negative samples. Instead of minimizing the negative log-likelihood of correct correspondence scores, we consider a contrastive loss function, called InfoNCE <ref type="bibr" target="#b19">(Oord, Li, and Vinyals 2018)</ref>, in this paper:</p><formula xml:id="formula_23">ground = n i=1 ?log exp(sim(p i , b i )/? ) 2M ?1 j=1 exp(sim(p i , b j )/? )<label>(16)</label></formula><p>where sim(?) is the output of similarity prediction, the sum is over one positive and 2M ? 1 negative regions, ? denotes the temperature parameter. During training process, for any phrase p i , the model is tuned to maximize the numerator of the log argument and minimize its denominator as well.</p><p>As a result, the model can thoroughly learn the difference between the true one and interventional samples.</p><p>Overall, all the losses are optimized to update learnable parameters:</p><formula xml:id="formula_24">L = T ind + V ind + ground (17)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Evaluation</head><p>We validate our model on two common datasets for phrase grounding. Flickr30K Entities <ref type="bibr" target="#b21">(Plummer et al. 2015)</ref> contains 31,783 images where each image corresponds to five captions with annotated noun phrases. Consistent with the work <ref type="bibr" target="#b8">(Dogan et al. 2019)</ref>, when a single phrase is annotated with multiple ground-truth bounding-box, we merge the boxes and use the union region as their ground-truth. We divide the dataset into 30k images for training, 1k for validation, and 1k for testing.</p><p>ReferIt Game <ref type="bibr" target="#b13">(Kazemzadeh et al. 2014</ref>) contains 20,000 images along with 99,535 segmented image regions. Each image is equipped with multiple referring phrases and corresponding bounding-boxes. We use the same split as <ref type="bibr" target="#b0">Akbari et al. (2019)</ref>, which contains 10k training and 10k test images. And we add the supplementary experiment on the Ref-COCO+ in Appendix D.</p><p>A noun phrase in the dataset is grounded correctly if and only if the predicted box and its ground-truth have at least 0.5 IoU (intersection over union). Based on the criteria, our measure of performance is grounding accuracy, which is the ratio of correctly grounded noun phrases to the total number of phrases in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>We use the uncased Bert 1 to get 768-dim phrase embeddings and a java toolkit 2 to parse phrase scene graph for each caption. For visual scene graph generation 3 , we use Faster R-CNN with VGG-16 backbone as a mechanism to extract top-100 proposal regions with 2048-dimension features and top-500 relations. All the dimensions of d T in , d V in ,  <ref type="bibr" target="#b9">(Fukui, Park et al. 2016)</ref> 48.69 -RtP <ref type="bibr" target="#b21">(Plummer et al. 2015)</ref> 50.89 -SimNet <ref type="bibr" target="#b31">(Wang et al. 2018)</ref> 51.05 31.26 CGRE <ref type="bibr" target="#b17">(Luo et al. 2017)</ref> -31.85 MSRC <ref type="bibr" target="#b4">(Chen et al. 2017a)</ref> 57.53 32.21 IGOP <ref type="bibr" target="#b37">(Yeh et al. 2017)</ref> 53.97 34.70 SPC+PPC <ref type="bibr" target="#b21">(Plummer et al. 2015)</ref>  For phrase and visual disentangled graph networks, the number of neighbor rooting layer is 2. We do a series of experiments to study the impact of the chunked number K in {1, 2, 4, 8, 16} and evaluate on two datasets. The results on Flickr30K is 77.44%, 78.26%, 78.73%. 78.56% and 78.08% respectively. We can see our model reaches the best performance at 4. The accuracy on ReferIt is 63.94%, 64.34%, 65.15%, 64.14% and 64.83% respectively. It is observed that the accuracy fluctuates after the k set to 4. Considering effectiveness and efficiency, we set K to 4. We only use one layer of Transformer with 4 multiheads as cross-modal mapping. For the InfoNCE loss, the hyperparameter ? is set to 0.2. We train the end-to-end network by the SGD optimizer with learning rate 1e-3, weight decay 1e-4, and momentum 0.9. The model iterates 6 epochs with batch size 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Comparison</head><p>The phrase grounding results of two benchmarks are reported in <ref type="table">Table .</ref> We classify prior works and ours into three classes (i.e. independent, sequential and graph models) discussed in Section . Our model surpasses all state-of-the-art techniques and achieves the best performances of 78.73% and 65.15% on Flickr30K and ReferIt respectively. It can be apparently seen that our method achieves an absolute increase of ?2% compared to LCMCG on Flickr30K. Although both methods consider the context, our model learns the disentangled motif-aware context in scene graphs, which </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>Extensive experiments are conducted to investigate the benefits of each block, with results enclosed in <ref type="table">Table .</ref> To be more specific, the effectiveness of the disentangled graph network, two kinds of interventional strategies, and the cross-modal Transformer are clarified as follows.</p><p>The first line is used to validate the performance of our model is from our model design rather than better pre-trained extractor. The baseline (the second line) utilizes the graph attention mechanism (GAT) in scene graphs and MLPs in inter-modal fusion for grounding. Disentangled graph network (DGN) is substituted for GAT to learn disentangled embeddings, which boosts the performance from 72.44% to 74.34% and from 60.31% to 61.71% on Flickr30K and ReferIt respectively. The third line replaces MLPs with a cross-modal Transformer (CMT) to transform K independent components of intra-modal embedding to common subspaces and achieve alignment, improving accuracy within 0.8%?1%. The fourth and fifth lines are added to the third one by intervening in structure and feature levels respectively. Against the third line, the fourth one enhances the performance from 75.15% to 77.44% on Flickr30K and from 62.74% to 63.87% on ReferIt. In contrast to the third one, the fifth line has 1.7% and 1.6% impact on two benchmarks. Particularly, the performance of the structure intervention on Flickr30K is more influential than that of in the performance than the feature intervention. Because the number of images on Flickr30K is more than that on ReferIt, it is reasonable that the model on Flickr30K grasps more rich and complex context. The last line is our whole model and obtains the highest score across datasets. The main improvement can be shown from the second line to the second line (1.4%?1.9%) and from the third line to the last line (2.4%?3.6%). It is strongly proved that DGN and interventional methods are the most critical modules. In summary, The results show consistent patterns of effects in two datasets, showcasing that our model can generalize for the phrase grounding tasks.</p><p>(a) Man jumps in the ocean to prove his love to the girl in the picture.</p><p>(b) People are actively grilling food on grills on a sunny day while other people stand nearby.</p><p>(d) Girl with brown hair in a blue shirt and blue and white skirt holding on to a shopping cart in a store.</p><p>(c) Two people are feeding sheep in a field with a dog nearby and three more people looking at them . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAT+MLPs Fusion</head><note type="other">DGN+MLPs</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative results</head><p>From <ref type="figure" target="#fig_2">Figure 3</ref>, we can see the contributions of the disentangled graph network module and interventional module. The upper part of the figure compares the graph attention network with the disentangled graph network, which proves the disentangled module can discriminate similar objects by chunked representations rather than messed representations. The lower part provides some interesting data, in terms of the fact that the interventional module can reduce the bias and break spurious relations at the structure level (e.g. the dog may usually co-occur with people and streets instead sheep) and recognize slight differences between objects at the feature level (e.g. the features of "blue shirt" and "blue and white skirt" are replaced with others to learn intrinsic color character).</p><p>We empirically inspect what kind of motifs are picked up by visualizing the weights of neighborhood routing. <ref type="figure" target="#fig_3">Figure  4</ref> shows the post-hoc explanations of learned spatial motif and part-of motif. There are more details in Appendix D. The left column shows two latent motifs of context in the phrase scene graph, and the right column exhibits motifs corresponding to phrase ones in the visual scene graph. For the phrase subgraph, the colored nodes represent that they are the most related in the current motif against other motifs. There are too many relationships in the visual scene graph, which are difficult to show a complete graph in the figure. So we just visualize the most prominent motif context between chunked nodes by the colored edges. From the results in <ref type="figure" target="#fig_3">Figure 4</ref>, the motif-aware context incorporated by the visual graph is more than that in the phrase graph even though disentangled. Hence, it is reasonable to compute the similarity of corresponding chunked embeddings to achieve grounding. Additional grounding results of our model are provided in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a disentangled interventional motif-aware learning framework for the phrase grounding task. Disentangled graph network distinguishes the importance of different motifs, integrates motif-aware context into node representations in intra-modal scene graphs, and provides interpretable results. Then interventional schemes at the aspects of feature and structure improve the resilience to the biased data and the ability of generalization. Finally, fine-grained intra-modal representations are fused in the cross-modal Transformer to finish phrase grounding. Our method (DGIN) is demonstrated on two public datasets, outperforming state-of-the-art by a considerable increase. In our future work, these tasks, constructing a more accurate scene graph and grounding phrase, should be considered to enhance each other. Additionally, we would like to make indepth analyses of cross-modal fusing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Category-wise accuracy comparesion</head><p>For a more in-depth understanding, <ref type="table" target="#tab_4">Table 3</ref> shows the models per category's performance on Flickr30K. Learning discriminative motif-ware context contributes to the success of some categories, such as "people", "clothing", "body parts", "scene" and "other". It is reasonable because "people" does share some motifs like decorative, part-of, action with the other categories mentioned above. For the sparse data of "animals", "vehicles" and "instruments" categories, most of them increase steadily owing to interventional schemes.</p><p>When intervening in the model, we provide more unseen environments and make representations robust and general. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Motif-aware Visualization Results</head><p>Intact learned motif examples are depicted in <ref type="figure" target="#fig_4">Figure 5</ref> in phrase and visual scene graphs respectively. We set the hyper-parameter of latent motifs as 4, and get four kinds of post-hoc interpretable motifs in the context of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Grounding Results</head><p>In <ref type="figure">Figure 6</ref>, the results of phrase grounding demonstrate the following abilities. Firstly, our model is able to ground multiple phrase to corresponding regions such as (a), (b), (f), (g). Second, the situation of two ambiguous instances in the sentence is solved by our disentangled model (e.g., the white shirt in (c) and the sunglasses in (h) exist some matching bounding boxes). The blue dotted ground truth in (d) and</p><p>A person in a red shift and camouflage pants is bent at the waist and has their hands against a large tree . (f) A girl in a yellow tennis suit , green visor and white tennis shoes holding a tennis racket in a position where she is going to hit the tennis ball .</p><p>(g) A woman in a cowboy hat on a horse chases a calf around a dirt ring .</p><p>(h) A man with a orange hat holding a stick is talking to a man wearing sunglasses and wearing a striped shirt .</p><p>(i) A boy in a wet red shirt gives water to a turtle while a boy dressed like Spider man and others look on .</p><p>(a) A man poses in mid jump beside a left turn road sign and a recumbent bicycle in the snow , behind him there is a large body of water.</p><p>(b) A dog with its two front paws on a rock in a field.</p><p>(c) A group of people in formal attire are in the background and a man in a white shirt with khaki pants jumps from the dirt road to the green grass .</p><p>(d) Male with brown hair and mustache , wearing a gray shawl over a printed shirt , while a small orange and blue boat cruises along the water .</p><p>(e) white on bottom, water, sky, mountain (j) the white fence on left, sky. bridge under man <ref type="figure">Figure 6</ref>: Qualitative results on Flickr30K (the first four columns) and ReferIt (the last column). The colored phrases of captions are grounded to regions in same color.</p><p>the dotted yellow box in (i) are failed to be grounded. On the one hand, it is not easy to recognize even humans for such a mustache's tinny size. On the other hand, common sense like the spiderman wearing red striped cloth is not involved in our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An example of phrase grounding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>DIGN architecture. Node colors interpret different motifs' context, and shades of color reveal the motif's importance. layers combined with the ReLU activation function and a batch normalization layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The visualization examples of models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of motif-aware graph learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>long brown hair is wearing a tan sweater and is sitting on a couch with a man. Illustration of motif-ware graph disentangling layer in both scene graphs. Edges' colors represent different motifs in the visual graph. Different nodes' color show chunked embeddings learned different motifs in the phrase graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Category-wise accuracy on Flickr30K test dataset</figDesc><table><row><cell>Method</cell><cell cols="7">people clothing body parts animals vehicles instruments scene other</cell></row><row><cell>SMPL</cell><cell>57.89</cell><cell>34.61</cell><cell>15.87</cell><cell>55.98</cell><cell>52.25</cell><cell>23.46</cell><cell>34.22 26.23</cell></row><row><cell>GroundeR</cell><cell>61.00</cell><cell>38.12</cell><cell>10.33</cell><cell>62.55</cell><cell>68.75</cell><cell>36.42</cell><cell>58.18 29.08</cell></row><row><cell>RtP</cell><cell>64.73</cell><cell>46.88</cell><cell>17.21</cell><cell>65.83</cell><cell>68.72</cell><cell>37.65</cell><cell>51.39 31.77</cell></row><row><cell>IGOP</cell><cell>68.71</cell><cell>56.83</cell><cell>19.50</cell><cell>70.07</cell><cell>73.75</cell><cell>39.50</cell><cell>60.38 32.45</cell></row><row><cell>SPC+PPC</cell><cell>71.69</cell><cell>50.95</cell><cell>25.24</cell><cell>76.23</cell><cell>66.50</cell><cell>35.80</cell><cell>51.51 35.98</cell></row><row><cell>CITE</cell><cell>73.20</cell><cell>52.34</cell><cell>30.59</cell><cell>76.25</cell><cell>75.75</cell><cell>48.15</cell><cell>55.64 42.83</cell></row><row><cell>SeqGROUND</cell><cell>76.02</cell><cell>56.94</cell><cell>26.18</cell><cell>75.56</cell><cell>66.00</cell><cell>39.36</cell><cell>68.69 40.60</cell></row><row><cell>QRC Net</cell><cell>76.32</cell><cell>59.58</cell><cell>25.24</cell><cell>80.50</cell><cell>78.25</cell><cell>50.62</cell><cell>67.12 43.60</cell></row><row><cell>GG++</cell><cell>78.86</cell><cell>68.34</cell><cell>39.80</cell><cell>81.38</cell><cell>76.58</cell><cell>42.35</cell><cell>68.82 45.08</cell></row><row><cell cols="2">MultiGrounding 75.60</cell><cell>58.30</cell><cell>44.90</cell><cell>87.60</cell><cell>83.80</cell><cell>57.50</cell><cell>68.20 59.80</cell></row><row><cell>LCMCG</cell><cell>86.82</cell><cell>79.92</cell><cell>53.54</cell><cell>90.73</cell><cell>84.75</cell><cell>63.58</cell><cell>77.12 58.65</cell></row><row><cell>DIGN</cell><cell>86.95</cell><cell>78.09</cell><cell>61.46</cell><cell>86.54</cell><cell>86.63</cell><cell>65.73</cell><cell>81.94 66.71</cell></row><row><cell></cell><cell cols="6">D The Ref-COCO+ Result</cell><cell></cell></row><row><cell cols="8">Our model is tested on Ref-COCO+ and achieves 70.21%</cell></row><row><cell cols="8">accuracy, which surpasses MattNet (64.93%) and CM-Att-</cell></row><row><cell cols="8">Erase 1 (68.09%). However, there is a slight disparity in</cell></row><row><cell cols="8">comparison with VLBert 2 (71.84%), ViLBert 3 (72.34%),</cell></row><row><cell cols="8">UNITER 4 (74.94%). The cross-modal Transformer series</cell></row><row><cell cols="8">pretrain on large scale V+L datasets like Concept Caption</cell></row><row><cell cols="8">to learn more prior knowledge and get better results.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://pypi.org/project/pytorch-pretrained-bert/ 2 https://nlp.stanford.edu/software/scenegraph-parser.shtml 3 https://github.com/rowanz/neural-motifs</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Xihui Liu, et al. Improving referring expression grounding with cross-modal attention-guided erasing. In CVPR, 2019. 2 Su, Weijie, et al. Vl-bert: Pre-training of generic visuallinguistic representations. In ICLR, 2019. 3 Lu, J., et al. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NeurIPS, 2019. 4 Chen, Y. C., et al. Uniter: Learning universal image-text representations. In arXiv:1909.11740.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Visual Disentangled Graph Network</head><p>The visual disentangled graph network is clarified in the Algorithm 1. The visual embeddings firstly transform into K subspaces (the 2nd-4th lines) to initialize the states of K motif-aware subgraphs. We then build L graph disentangling layers with the neighbor routing and updating mechanisms, which integrate each kind of motif-aware context into related chunked embeddings (the 5th-17th lines).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Visual Disentangled Graph Network</head><p>for graph disentangling layer l = 1, ..., L do 6:</p><p>for v j that satisfies j ? N i do 7:</p><p>end for 10:</p><p>for component k = 1, ..., K do 11:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Interventional Visual representation</head><p>Algorithm 2 shows the overall pseudocode of the interventional algorithm at the feature and the structure levels in the visual disentangled graph network (VDGN). For each training sample, we randomly use one specific synthesizing mechanism, and ? is the trade-off weight.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-level multimodal common semantic space for image-phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">G3raphGround: Graph-Based Language Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bajaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03599</idno>
		<title level="m">Understanding disentangling in ?-VAE</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rubi: Reducing unimodal biases for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MSRC: Multimodal spatial regression with semantic context for phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kovvuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Query-guided regression network with context policy for phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural sequential phrase grounding (seqground)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">?-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What Does BERT with Vision Look At</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving referring expression grounding with cross-modal attention-guided erasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning Cross-Modal Context Graph for Visual Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Comprehension-guided referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Disentangled graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conditional imagetext embedding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kordas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadi Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10598</idno>
		<title level="m">Shapeshifter Networks: Crosslayer Parameter Sharing for Scalable and Effective Deep Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Grounding of textual phrases in images by reconstruction</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Zero-shot grounding of objects from natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generating semantically precise scene graphs from textual descriptions for improved image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth workshop on vision and language</title>
		<meeting>the fourth workshop on vision and language</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="70" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Measuring and testing dependence by correlation of distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Sz?kely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Bakirov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2769" to="2794" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITW</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning two-branch neural networks for image-text matching tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="394" to="407" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Structured matching for phrase localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neighbourhood watch: Referring expression comprehension via language-guided graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual commonsense r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A fast and accurate one-stage approach to visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Interpretable and globally optimal prediction for textual grounding using image concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Disentangling User Interest and Popularity Bias for Recommendation with Causal Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11011</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
