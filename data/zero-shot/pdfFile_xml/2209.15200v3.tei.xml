<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An efficient encoder-decoder architecture with top-down attention for speech separation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Tsinghua Laboratory of Brain and Intelligence (THBI)</orgName>
								<orgName type="institution">IDG/McGovern Institute of Brain Research Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Tsinghua Laboratory of Brain and Intelligence (THBI)</orgName>
								<orgName type="institution">IDG/McGovern Institute of Brain Research Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
							<email>xlhu@tsinghua.edu.cn.</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Tsinghua Laboratory of Brain and Intelligence (THBI)</orgName>
								<orgName type="institution">IDG/McGovern Institute of Brain Research Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An efficient encoder-decoder architecture with top-down attention for speech separation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks have shown excellent prospects in speech separation tasks. However, obtaining good results while keeping a low model complexity remains challenging in real-world applications. In this paper, we provide a bio-inspired efficient encoder-decoder architecture by mimicking the brain's top-down attention, called TDANet, with decreased model complexity without sacrificing performance. The top-down attention in TDANet is extracted by the global attention (GA) module and the cascaded local attention (LA) layers. The GA module takes multi-scale acoustic features as input to extract global attention signal, which then modulates features of different scales by direct top-down connections. The LA layers use features of adjacent layers as input to extract the local attention signal, which is used to modulate the lateral input in a top-down manner. On three benchmark datasets, TDANet consistently achieved competitive separation performance to previous state-of-the-art (SOTA) methods with higher efficiency. Specifically, TDANet's multiply-accumulate operations (MACs) are only 5% of Sepformer, one of the previous SOTA models, and CPU inference time is only 10% of Sepformer. In addition, a large-size version of TDANet obtained SOTA results on three datasets, with MACs still only 10% of Sepformer and the CPU inference time only 24% of Sepformer. Our study suggests that top-down attention can be a more efficient strategy for speech separation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In cocktail parties, people's communications are inevitably disturbed by various sounds <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b0">1]</ref>, such as environmental noise and extraneous audio signals, potentially affecting the quality of communication. Humans can effortlessly perceive the speech signal of a target speaker in a cocktail party to improve the accuracy of speech recognition <ref type="bibr" target="#b6">[7]</ref>. In speech processing field, the corresponding challenge is to separate different speakers' audios from the mixture audio, known as speech separation.</p><p>Due to rapid development of deep neural networks (DNNs), DNN-based speech separation methods have significantly improved <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref>. As in natural language processing, the SOTA speech separation methods are now embracing increasingly complex models to achieve better separation performance, such as DPTNet <ref type="bibr" target="#b32">[33]</ref> and Sepformer <ref type="bibr" target="#b39">[40]</ref>. These models typically use multiple transformer layers <ref type="bibr" target="#b20">[21]</ref> to capture longer contextual information, leading to a large number of parameters and high computational cost and having a hard time deploying to edge devices. We question whether such complexity is always needed in order to improve the separation performance. Human brain has the ability to process large amounts of sensory information with extremely low energy consumption <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. We therefore resort to our brain for inspiration. Numerous neuroscience studies have suggested that in solving the cocktail part problem, the brain relies on a cognitive process called top-down attention <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15]</ref>. It enables human to focus on task-relevant stimuli and ignore irrelevant distractions. Specifically, top-down attention modulates (enhance or inhibit) cortical sensory responses to different sensory information <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. With neural modulation, the brain is able to focus on speech of interest and ignore others in a multi-speaker scenario <ref type="bibr" target="#b11">[12]</ref>.</p><p>We note that encoder-decoder speech separation networks (e.g., SuDoRM-RF <ref type="bibr" target="#b36">[37]</ref> and A-FRCNN <ref type="bibr" target="#b37">[38]</ref>) contain top-down, bottom-up, and lateral connections, similar to the brain's hierarchical structure for processing sensory information <ref type="bibr" target="#b12">[13]</ref>. These models mainly simulate the interaction between lower (e.g., A1) and higher (e.g., A2) sensory areas in primates, neglecting the role of higher cortical areas such as frontal cortex and occipital cortex in accomplishing challenging auditory tasks like the cocktail party problem <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b4">5]</ref>. But they provide good frameworks for applying top-down attention mechanisms.  We propose an encoder-decoder architecture equipped with top-down attention for speech separation, which is called TDANet. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, TDANet adds a global attention (GA) module to the encoder-decoder architecture, which modulates features of different scales in the encoder top-down through the attention signal obtained from multi-scale features. The modulated features are gradually restored to high-resolution auditory features through local attention (LA) layers in the top-down decoder. The experimental results demonstrated that TDANet achieved competitive separation performance on three datasets (LRS2-2Mix, Libri2Mix <ref type="bibr" target="#b33">[34]</ref>, and WHAM! <ref type="bibr" target="#b31">[32]</ref>) with far less computational cost. Taking LRS2-2Mix dataset as an example -when comparing with Sepformer, TDANet's MACs are only 5% of it and CPU inference time is only 10% of it.</p><formula xml:id="formula_0">(N,<label>T? 2 )</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In recent years, DNN-based speech separation methods have received widespread attention, and speech separation performance has been substantially improved. Existing DNN-based methods are categorized into two major groups: time-frequency domain <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref> and time domain <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b37">38]</ref>. Time domain methods obtain better results compared to time-frequency domain methods as they avoid explicit phase estimation. Recently, time domain methods Sepformer <ref type="bibr" target="#b39">[40]</ref> and DPTNet <ref type="bibr" target="#b32">[33]</ref> replace LSTM layers with transformer layers to avoid performance degradation due to long-term dependencies and to process data in parallel for efficiency. These methods achieve better separation performance, but the number of model parameters and computational cost become larger due to additional transformer layers. Although large-scale neural networks can achieve better performance in various tasks such as BERT <ref type="bibr" target="#b27">[28]</ref> and DALL?E 2 <ref type="bibr" target="#b42">[43]</ref>, it is also an important topic to design lightweight models that can be deployed to low-resource platforms.</p><p>The encoder-decoder speech separation model SuDoRM-RF <ref type="bibr" target="#b36">[37]</ref> achieves a certain extent of trade-off between model peformance and complexity, but its performance is still far from SOTA methods.</p><p>Other researchers have designed a fully recurrent convolutional neural network A-FRCNN <ref type="bibr" target="#b37">[38]</ref> with an asynchronous update scheme and obtained competitive separation results with a relatively small number of parameters, but the computational complexity still has room to improve.</p><p>Some earlier works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31</ref>] make use of top-down attention in designing speech separation models, but they only consider the projection of attentional signals to the top layer of the multilayer LSTM and without applying attention to lower layers. This approach may not fully take advantage of top-down attention. In addition, these approaches have a large gap in model performance and complexity with recent models including SuDoRM-RF and A-FRCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Audio encoder</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Separation network</head><p>Audio decoder</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio decoder</head><p>Mixture audio SPK A separated audio SPK B separated audio <ref type="figure">Figure 2</ref>: The overall pipeline for the speech separation task. We assume that the mixture audio contains two speakers here. The parameters are shared between the two audio decoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall pipeline</head><p>Given an audio containing multiple speakers, the speech separation methods aim to extract different speakers' utterances and route them to different output channels. Let y ? R 1?T be a multi-speaker time-domain audio signal with the length T :</p><formula xml:id="formula_1">y = C i x i + n,<label>(1)</label></formula><p>consisting of C speaker signals x i ? R 1?T plus the noise signal n ? R 1?T . We expect the separated speechesx i ? R 1?T from all speakers to be closer to target speeches x i .</p><p>Our proposed method uses the same three-part pipeline as in Conv-TasNet <ref type="bibr" target="#b29">[30]</ref>: an audio encoder, a separation network and an audio decoder. The audio encoder transforms y into a frame-based N -dimensional embedding sequence E ? R N ?T with length T , called mixture audio embedding, which can be implemented using a 1-D convolutional layer with kernel size L and stride size L/4 . Similar to existing separation methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41]</ref>, instead of directly estimating the target speech embeddings, we use a DNN-based separation network f (E; ?) to generate a set of masks M = {M i ? R N ?T |i = 1, ..., C} associated with the target speakers. Each target speech embedding? i ? R N ?T is generated by applying the corresponding mask M i to the mixture audio embedding E:</p><formula xml:id="formula_2">? i = E M i ,<label>(2)</label></formula><p>where denotes element-wise product. Finally, the target waveformx i is reconstructed using the target speech embedding? i through an audio decoder, which can be implemented using a 1-D transposed convolutional layer with the same kernel and stride sizes as the audio encoder.</p><p>Overall there are three main components: (a) encoder (see Section 3.2), (b) global attention (GA) module (see Section 3.3) and (c) decoder (see <ref type="bibr">Section 3.4)</ref>. The process detail is as follows. First, the encoder obtains multi-scale features using the bottom-up connections. Second, the GA module extracts global features at the top layer using multi-scale features, and then the global features are used as attention to modulate features from different scales using top-down connections. Third, the adjacent layer features are used as the input into the decoder to extract the acoustic features through LA layers. This completes one cycle of information processing inside TDANet. Similar to A-FRCNN <ref type="bibr" target="#b37">[38]</ref>, we cycle TDANet several times and use the output features from the last cycle as the separation network output (see Section 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder of TDANet</head><p>The encoder of TDANet is designed for extracting features at different temporal resolutions. Lower layers have higher temporal resolutions, while higher layers have lower temporal resolutions. The encoder works as follows (see <ref type="figure" target="#fig_0">Figure 1a</ref>). For a mixture audio embedding E, the encoder processes E in a bottom-up manner step by step. The bottom-up connections in the encoder are implemented by down-sampling layers, which consist of a 1-D convolutional layer followed by a global layer normalization (GLN) <ref type="bibr" target="#b29">[30]</ref> and PReLU. To efficiently expand the perceptual field, these convolutional layers use dilation convolution with N kernels with size 5, stride size 2 and dilation size 2 instead of standard convolution to aggregate longer contexts. In this way, we obtain features with different</p><formula xml:id="formula_3">resolutions {F i ? R N ? T 2 i?1 |i = 1, ..., S + 1},</formula><p>where S denotes the number of down-sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Global attention (GA) module of TDANet</head><p>The GA module works in two steps:</p><p>(1) It receives multi-scale features {F i |i = 1, ..., S + 1} as input and computes global feature</p><formula xml:id="formula_4">G m ? R N ? T 2 S ; (2) It uses the G m as top-down attention to modulate {F i |i = 1, ..., S + 1}.</formula><p>Details of the first step in the GA module: Taking multi-scale features {F i |i = 1, ..., S + 1} as input, we use the average pooling layers to compress these features in temporal dimension from T 2 i?1 to T 2 S for reducing computational cost. The features with different temporal resolutions are fused into the global feature G ? R N ? T 2 S by a summation operation. The calculation of global feature can be written as:</p><formula xml:id="formula_5">G = S i p(F i ),<label>(3)</label></formula><p>where p(?) denotes the average pooling layer. G will be treated as input to the transformer layer to obtain the speaker's acoustic patterns from global contexts.</p><p>The transformer layer contains two components: multi-head self-attention (MHSA) and feed-forward network (FFN) layers, as shown in <ref type="figure">Figure 3</ref>. The MHSA layer is the same as the encoder structure defined in Transformer <ref type="bibr" target="#b20">[21]</ref>. This layer has been widely used in recent speech processing tasks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39]</ref>. First, the position information d of different frames is added to global feature G to get</p><formula xml:id="formula_6">G = G + d,? ? R N ? T 2 S .</formula><p>Then, each attention head calculates the dot product attention between all elements of? in different feature spaces to direct model focus to different aspects of information.</p><p>Finally, the output? ? R N ? T 2 S of MHSA is fed into GLN and then connected G through the residual connection to obtain? ? R N ? T 2 S .</p><formula xml:id="formula_7">(N,<label>T? 8 )</label></formula><p>MHSA FFN (N, T? 8 ) <ref type="figure">Figure 3</ref>: The structure of transformer layer.</p><p>The FFN layer followed by MHSA layer consists of three convolutional layers. First, G is processed in a 1 ? 1 convolutional layer with GLN mapped to the 2Ndimensional representation space. Then, the second convolutional layer composed of a 5 ? 5 depthwise convolutional layer followed by GLN extracts relationship among 2N sequences. Finally, a 1 ? 1 convolutional layer followed by GLN compresses the 2N -dimensional features into N -dimensions to obtain G m ? R N ? T 2 S and also use a residual connection to alleviate the vanishing gradient problem.</p><p>Details of the second step in the GA module: We use G m as top-down attention (see <ref type="figure" target="#fig_0">Figure 1b</ref>) to modulate F i before adding it to the decoder. Specifically, we up-sample G m along the time dimension using nearest neighbor interpolation to obtain the same time dimension as F i , and then use the Sigmoid function to obtain the attention signal. In this way, the multi-scale semantic information of G m can guide the local features F i to focus more on feature details that may be lost in the bottom-up path in order to improve quality of separated audios. The modulated features are calculated as:</p><formula xml:id="formula_8">F i = ?(?(G m )) F,<label>(4)</label></formula><p>where ? denotes nearest neighbor interpolation, ? stands for the Sigmoid function and denotes element-wise product. The modulated features F i ? R N ? T 2 i?1 are used as input to the decoder to extract different speakers' features.</p><p>The GA module functions as higher cortical areas such as frontal cortex and occipital cortex in the brain which play an important role in accomplishing complicated sensory tasks. Here, it is only designed for top-down modulation in performing speech separation tasks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b4">5]</ref>.  <ref type="figure">Figure 4</ref>: The structure of LA layer in the decoder, where "Conv1D" denotes the 5 ? 5 depthwise convolutional layer followed by GLN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Decoder of TDANet</head><p>The decoder progressively extracts acoustic features through top-down connections whose core component is the LA layer, as shown in <ref type="figure" target="#fig_0">Figure 1c</ref>. The LA layer uses a small amount of parameters (? 0.01M) to generate adaptive parameters ? and ? from higher-level features as a guide for extracting finer-grained speech features. <ref type="figure">Figure 4</ref> shows detailed architecture of the LA layer. First, LA layer uses N 1-D kernels with length 5 to perform convolution in temporal dimension using features F i+1 , result-</p><formula xml:id="formula_9">ing in ? ? R N ? T 2 i?1 .</formula><p>Meanwhile, another identical 1-D convolutional layer followed by GLN and Sigmoid uses F i+1 as input,</p><formula xml:id="formula_10">resulting in attention signal ? ? R N ? T 2 i?1 . Consequently, ? = h 2 (?(F i+1 )),<label>(5)</label></formula><formula xml:id="formula_11">? = ?(h 1 (?(F i+1 ))),<label>(6)</label></formula><p>where h 1 and h 2 denote two different parameters of 1-D convolutional layers followed by GLN. F i , ? and ? have the same dimensions. The learnable parameters are used to adaptively modulate F i via a top-down local attention ?. This process is formulated by</p><formula xml:id="formula_12">F i = ? F i + ?.<label>(7)</label></formula><p>After the feature extraction from the highest layer to the lowest layer, the output of the decoder use C convolutional layers with N kernels with length of 1 and stride of 1 followed by an activation function (ReLU) to obtain C masks M i , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Unfolding method of TDANet</head><p>Clearly, the bottom-up, lateral and top-down connections <ref type="figure" target="#fig_0">(Figure 1)</ref> make TDANet a recurrent model. We need to unfold it through time for training and testing. We adopt the unfolding scheme summation connection (SC) as in A-FRCNN <ref type="bibr" target="#b37">[38]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> shows a single block in the whole unfolding scheme. We repeat the block B times (weight sharing), such that the model's input adds up with each block's output as the next block's input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment configurations 4.1 Dataset simulations</head><p>We evaluated TDANet and other existing methods on three datasets: LibriMix, WHAM!, LRS2-2Mix). The speaker identities of training/validation set and test set are non-intersecting, meaning unseen speaker data from the training phase were used during testing.</p><p>Libri2Mix <ref type="bibr" target="#b33">[34]</ref>. In this dataset, the target speech in each mixture audio was randomly selected from a subset of LibriSpeech's train-100 <ref type="bibr" target="#b16">[17]</ref> and mixed with uniformly sampled Loudness Units relative to Full Scale (LUFS) <ref type="bibr" target="#b9">[10]</ref> between -25 and -33 dB. To simulate real-world scenarios, random noise samples were added to the mixture audio with loudness uniformly sampled between -38 and -30 LUFS. Each mixture audio contains two different speakers and have a duration of 3 seconds.</p><p>WHAM! <ref type="bibr" target="#b31">[32]</ref>. This dataset acts as a noisy version of WSJ0-2Mix <ref type="bibr" target="#b18">[19]</ref>. In the WSJ0-2Mix dataset, the overlap between two speaker audios in the mixture is 100%, which is too idealistic, and thus model trained on this dataset does not achieve generalization to speeches from a broader range of speakers <ref type="bibr" target="#b33">[34]</ref>. In the WHAM! dataset, speeches were mixed with noise recorded in scenes such as cafes, restaurants and bars. The signal-to-noise ratio of the noise was uniformly sampled between -6db and 3db, making it more challenging mixture audios than WSJ0-2Mix.</p><p>LRS2-2Mix. The LRS2 dataset <ref type="bibr" target="#b22">[23]</ref> contains thousands of video clips acquired through BBC. LRS2 contains a large amount of noise and reverberation interference, which is more challenging and closer to the actual environment than the WSJ0 <ref type="bibr" target="#b1">[2]</ref> and LibriSpeech <ref type="bibr" target="#b16">[17]</ref> corpora. We created a new speech separation dataset, LRS2-2Mix, using the LRS2 corpus, where the training set, validation set and test set contain 20000, 5000 and 3000 utterances, respectively. The two different speaker audios with 16 kHz sample rate were randomly selected from the LRS2 corpus and were mixed with signal-to-noise ratios sampled between -5 dB and 5 dB. The data were simulated using a script consistent with WSJ0-2Mix 2 . The length of mixture audios is 2 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model and training configurations</head><p>We set the kernel size L of the audio encoder and audio decoder in the overall pipeline to 4 ms and stride size L/4 to 1 ms. The number of down-sampling S was set to 4. The number of channels N of all convolutional layers in each layer was set to 512, and the SC method proposed by A-FRCNN <ref type="bibr" target="#b37">[38]</ref> was used to unfold B = 16 times at the macro level. For the MHSA layer, we set the number of attention heads to 8, the dimension to 512, and used sine and cosine functions with different frequencies for the position encoding. In addition, we set the number of channels for the three convolutional layers in the FFN layer to (512, 1024, 512), the kernel sizes to (1, 5, 1), the stride sizes to <ref type="figure" target="#fig_0">(1, 1, 1)</ref>, and the bias settings to (False, True, False). To avoid overfitting, we set the probability of all dropouts to 0.1.</p><p>We trained all models for 500 epochs. The batch size was set to 1 at the utterance level. Our proposed model used the Adam <ref type="bibr" target="#b15">[16]</ref> optimizer with an initial learning rate of 0.001. We used maximization of scale-invariant signal-to-noise ratio (SI-SNR) <ref type="bibr" target="#b18">[19]</ref> as the training goal (See Appendix A for details). Once the best model was not found for 15 successive epochs, we adjusted the learning rate to half of the previous one. Moreover, we stopped training early when the best model was not found for 30 successive epochs. During the training process, we used gradient clipping with a maximum L2 norm of 5 to avoid gradient explosion. For all experiments, we used 8? GeForce RTX 3080 for training and testing. The PyTorch implementation of our method will be released once this paper has passed final review. This project is under the MIT license.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation metrics</head><p>In all experiments, we report the results of scale-invariant signal-to-noise ratio improvement (SI-SNRi) <ref type="bibr" target="#b28">[29]</ref> and signal-to-distortion ratio improvement (SDRi) <ref type="bibr" target="#b8">[9]</ref> used to measure clarity of separated audios. For measuring model efficiency, we report the processing time consumption per second (real-time factor, RTF) for all models, indicated by "RTF" in the tables throughout this paper. It was calculated by processing ten audio tracks of 1 second in length and 16 kHz in sample rate on CPU and GPU (total processing time / 10), represented as "CPU RTF" and "GPU RTF", respectively. The numbers were then averaged after running 1000 times. Also, we used the parameter size and the number of MACs to measure the model size, where the MACs were calculated using the open-source tool PyTorch-OpCounter 3 . This toolkit is used under the MIT license.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation study</head><p>To better understand the effectiveness of top-down attention, we investigated the impacts of two types of attention components in the model for separation performance, including GA module and LA layer. All experimental results were obtained by training and testing on the LRS2-2Mix dataset. We first constructed a control model that does not contain the GA module and LA layer. It takes the top-layer features of the encoder as input to the decoder. Note that excluding the LA layer means replacing the gray box in <ref type="figure">Figure 4</ref> with summation operation. This architecture then becomes a typical U-net <ref type="bibr" target="#b17">[18]</ref>, similar to a basic block used as in SuDoRM-RF <ref type="bibr" target="#b36">[37]</ref>. From <ref type="table" target="#tab_1">Table 1</ref>, it is seen that SI-SNRi of this model is only 10.1 dB.</p><p>When only adding the GA module into TDANet, the SI-SNRi increased by 2.3 dB compared to when it was absent ( <ref type="table" target="#tab_1">Table 1</ref>). The GA module contains a transformer layer and top-down projections. We are interested in whether the performance improvement came from the transformer layer or the top-down projections. <ref type="table" target="#tab_5">Table 4</ref> from Appendix B.1 demonstrates that the performance improvement is independent of the transformer layer. When only adding the LA layer, we observed that the SI-SNRi increased by 1.8 dB without increasing computational cost. We noticed that the GA module had larger impact on performance improvement. One possible explanation is that the GA module modulates on a wide range of features, while the LA layer only focuses on adjacent ones. When both GA module and LA layer were used, the performance were the best.</p><p>More ablation studies are reported in Appendix B. We observed that multi-scale features using summation fusion strategy as GA module input were efficient for performance improvement without significantly increasing computational cost (Appendix B.2). In addition, TDANet obtained the best separation performance when both MHSA and FFN layers were present in the GA module (Appendix B.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparisons with state-of-the-art methods</head><p>We conducted extensive experiments to quantitatively compare speech separation performance of our proposed TDANet with some existing speech separation models on three datasets. The results are shown in <ref type="table">Table 2</ref>. We selected some typical models that have shown good separation performance, including BLSTM-TasNet <ref type="bibr" target="#b24">[25]</ref>, Conv-TasNet <ref type="bibr" target="#b29">[30]</ref>, SuDoRM-RF <ref type="bibr" target="#b36">[37]</ref>, DualPathRNN <ref type="bibr" target="#b34">[35]</ref>, DPTNet <ref type="bibr" target="#b32">[33]</ref>, Sepformer <ref type="bibr" target="#b39">[40]</ref>, and A-FRCNN <ref type="bibr" target="#b37">[38]</ref>. SuDoRM-RF contains two variants with good performance, suffixed with 1.0x and 2.5x, indicating that these variants consist of 16 and 40 blocks, respectively. A-FRCNN has a suffix "16" denoting that the model is expanded 16 times at the macro level. LRS2-2Mix is the newly proposed speech separation dataset in this paper, so none of these models reported results on this dataset. We trained and tested these models using the Asteroid Toolkit <ref type="bibr" target="#b35">[36]</ref> to obtain the missing results. This toolkit is under MIT license.</p><p>Separation performance. TDANet obtained competitive separation performance with much lower complexity than previous SOTA models on the three datasets (see <ref type="table">Table 2</ref>), clearly demonstrating the importance of top-down attention for the encoder-decoder network. On the LRS2-2Mix dataset, TDANet lost only 0.3 dB SI-SNRi in separation performance but with only 8% of number of parameters compared to Sepformer, one of previous SOTA models. On the other two datasets, TDANet also obtained competitive results compared to previous SOTA models.</p><p>Some audio examples from LRS2-2Mix separated by different models are provided in the Supplementary Material (to be released after final review).</p><p>In most examples, we found that separation results with TDANet sound better than those from other models. <ref type="table">Table 2</ref>: Quantitative comparison between TDANet and other existing models on three datasets (test set). "-" indicates that their results have not been reported in other papers. "*" denotes that they were not reported in the original paper but were trained and tested using the Asteroid Toolkit <ref type="bibr" target="#b35">[36]</ref>. The results of DPTNet on Libri2Mix and WHAM! datasets were reported in <ref type="bibr" target="#b43">[44]</ref>, but not in the original paper. Separation effciency. In addition to number of model parameters, MACs as well as training and inference time are also important indicators of model complexity, as models with a small amount of parameters may also have a large number of MACs and could be slow to train and infer. By using RTF and MACs as metrics, we evaluated complexity of pervious models ( Large-size version. For the proposed method, we also constructed a large-size version of TDANet, namely TDANet Large, by modifying the kernel length of the audio encoder and audio decoder from 4ms to 2ms and the stride size from 2ms to 0.5ms. Note that this does not change the model size. TDANet Large obtained SOTA separation performance without a significant increase in computational cost on all three datasets (see <ref type="table">Table 2</ref> and <ref type="table" target="#tab_3">Table 3</ref>). TDANet Large can still maintain a small computational complexity compared to other existing lightweight models. For example, the inference time on the GPU is about 3 times faster, and MACs are about 14 times smaller compared to A-FRCNN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>To fill the gap between the performance of SOTA and lightweight models, we designed an encoderdecoder speech separation network with top-down global attention and local attention, inspired by our brain's top-down attention for solving the cocktail party problem. Our extensive experimental results showed that TDANet could significantly reduce model complexity and inference speed while ensuring separation performance.</p><p>Limitations: The TDANet only reflects certain working principles of the auditory system and is not a faithful model of the auditory system. For example, it requires a bottom-up encoder and a top-down decoder, and it is unclear how such components are implemented in the brain.</p><p>compared with the configuration with top-down projections (SI-SNRi 12.4 dB, <ref type="table" target="#tab_1">Table 1</ref>). These results further demonstrate the importance of top-down global attention. To examine the influence on separation performance between two different features as input to the GA module: multi-scale fused features G and top-layer features F S+1 , we experimented with each of them separately. The experimental results are shown in <ref type="table" target="#tab_6">Table 5</ref>. We chose G instead of F S+1 because the former achieved better separation performance (0.4 dB ?) without a remarkable increase in computational cost. In addition, we investigated two different strategies for multi-scale feature fusion: summation and concatenation. <ref type="table" target="#tab_7">Table 6</ref> shows the results. Summation outperformed concatenation in terms of both separation performance and computational efficiency. Therefore, we employed summation operation as the fusion strategy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Effect of transformer layer components</head><p>The transformer layer contains two components: MHSA and FFN layers. We have investigated the effectiveness of MHSA and FFN layers on separation performance and inference efficiency in <ref type="table" target="#tab_8">Table 7</ref>. We used the TDANet with both GA module and LA layers as the control model. When we remove MHSA and FFN layers from GA, the top-layer features of the encoder are used as top-down global attention. We observed that the these layers played a critical role in improving separation performance. When adding either MHSA or FFN layer, we observed that the impact of FFN (? 1.3 dB) on performance was more significant than that of MHSA (? 0.7 dB). One possible reason may be that the perceptual field of the top-layer features is already large enough so that the transformer structure, known to be good at modeling long sequences, does not improve separation performance as much. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Main architecture of TDANet. N and T denote the number of channels and length of features, respectively. By down-sampling S times, TDANet contains S + 1 features with different temporal resolutions. Here, we set S to 3. The red, blue, and orange arrows indicate bottom-up, top-down, and lateral connections, respectively. (a) The structure of the encoder, where "DWConv" denotes a depthwise convolutional layer with a kernel size of 5 and stride size of 2 followed by GLN. (b) The "Up-sample" layer denotes nearest neighbor interpolation. (c) The structure of decoder, where the LA layer adaptively modulates features of different scales by a set of learnable parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison over separation performance and efficiency among different types of top-down attention. " ? " indicates the case where a component is used in the TDANet. "?" indicates the opposite.</figDesc><table><row><cell cols="7">GA module LA layer SI-SNRi (dB) SDRi (dB) Params (M) MACs (G/s) CPU RTF (s)</cell></row><row><cell>? ? ? ?</cell><cell>? ? ? ?</cell><cell>10.1 12.4 11.9 13.2</cell><cell>10.5 12.7 12.2 13.5</cell><cell>0.7 2.3 0.7 2.3</cell><cell>2.5 4.6 2.6 4.7</cell><cell>0.65 0.79 0.65 0.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 )</head><label>3</label><figDesc>. We observed that TDANet outperformed the previous SOTA models in model complexity. For example, compared to DPTNet and Sepformer, TDANet's MACs are 5% of them, and backward time is 13% and 47% of them, respectively, significantly reducing the time consumption during training. In addition, compared with A-FRCNN and SuDoRM-RF 2.5x, TDANet's GPU inference time is 19% and 18% of them, GPU training time is 47% and 37% of them, and CPU inference time is 15% and 46% of them, respectively. These results suggest that TDANet can be more easily deployed to low-resource devices.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of inference time and MACs on the LRS2-2Mix test set. The test environment for feedforward RTF (F GPU RTF) and backward RTF (B GPU RTF) is Nvidia GeForce RTX 2080 Ti, while the test environment for CPU RTF is Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz, single-threaded.</figDesc><table><row><cell>Model</cell><cell cols="4">F GPU RTF (ms) B GPU RTF (ms) CPU RTF (s) MACs (G/s)</cell></row><row><cell>BLSTM-TasNet</cell><cell>233.85</cell><cell>654.14</cell><cell>5.9</cell><cell>43.0</cell></row><row><cell>Conv-TasNet</cell><cell>15.28</cell><cell>56.91</cell><cell>0.82</cell><cell>10.2</cell></row><row><cell>SuDoRM-RF 1.0x</cell><cell>27.86</cell><cell>95.37</cell><cell>0.75</cell><cell>4.6</cell></row><row><cell>SuDoRM-RF 2.5x</cell><cell>64.70</cell><cell>228.57</cell><cell>1.73</cell><cell>10.1</cell></row><row><cell>DualPathRNN</cell><cell>88.79</cell><cell>241.54</cell><cell>8.13</cell><cell>85.3</cell></row><row><cell>DPTNet</cell><cell>103.26</cell><cell>689.06</cell><cell>10.49</cell><cell>102.5</cell></row><row><cell>A-FRCNN-16</cell><cell>61.16</cell><cell>183.65</cell><cell>5.32</cell><cell>125.3</cell></row><row><cell>Sepformer</cell><cell>65.61</cell><cell>184.91</cell><cell>7.55</cell><cell>86.9</cell></row><row><cell>TDANet (ours)</cell><cell>11.76</cell><cell>86.56</cell><cell>0.79</cell><cell>4.7</cell></row><row><cell>TDANet Large (ours)</cell><cell>23.77</cell><cell>97.92</cell><cell>1.78</cell><cell>9.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Effect of transformer layer. "TL" denotes transformer layer in the GA module.</figDesc><table><row><cell cols="6">TL SI-SNRi (dB) SDRi (dB) Params (M) MACs (G/s) CPU RTF (s)</cell></row><row><cell>?</cell><cell>10.3</cell><cell>10.7</cell><cell>2.3</cell><cell>4.6</cell><cell>0.77</cell></row><row><cell>?</cell><cell>10.1</cell><cell>10.5</cell><cell>0.7</cell><cell>2.5</cell><cell>0.65</cell></row><row><cell cols="3">B.2 Effect of multi-scale input to GA module</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison over separation performance and model complexity between different multiscale fused top-layer features used for input into the GA module.</figDesc><table><row><cell cols="6">Methods SI-SNRi (dB) SDRi (dB) Params (M) MACs (G/s) CPU RTF (s)</cell></row><row><cell>G</cell><cell>13.2</cell><cell>13.5</cell><cell>2.3</cell><cell>4.7</cell><cell>0.79</cell></row><row><cell>F S+1</cell><cell>12.8</cell><cell>13.1</cell><cell>2.3</cell><cell>4.7</cell><cell>0.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison over separation performance and model complexity between different fusion strategies.</figDesc><table><row><cell cols="6">Fusion strategy SI-SNRi (dB) SDRi (dB) Params (M) MACs (G/s) CPU RTF (s)</cell></row><row><cell>Summation</cell><cell>13.2</cell><cell>13.5</cell><cell>2.3</cell><cell>4.7</cell><cell>0.79</cell></row><row><cell>Concatenation</cell><cell>13.0</cell><cell>13.3</cell><cell>3.6</cell><cell>6.1</cell><cell>0.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison on the importance of MHSA and FFN layers.</figDesc><table><row><cell cols="7">MHSA FFN SI-SNRi (dB) SDRi (dB) Params (M) MACs (G/s) CPU RTF (s)</cell></row><row><cell>? ? ? ?</cell><cell>? ? ? ?</cell><cell>11.2 11.9 12.5 13.2</cell><cell>11.5 12.2 12.8 13.5</cell><cell>0.2 1.3 1.3 2.3</cell><cell>2.6 3.6 3.7 4.7</cell><cell>0.68 0.73 0.73 0.79</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.merl.com/demos/deep-clustering/create-speaker-mixtures.zip</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/Lyken17/pytorch-OpCounter</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Method</head><p>We apply the permutation-invariant training (PIT) method <ref type="bibr" target="#b21">[22]</ref> to solve the permutation problem <ref type="bibr" target="#b18">[19]</ref> in order to maximize the scale-invariant signal-to-noise ratio (SI-SNR) <ref type="bibr" target="#b28">[29]</ref>. The SI-SNR for each speaker is defined as:</p><p>where</p><p>In the above equation, ?, ? denotes the inner product and || ? || 2 2 denotes the L2-norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional experiments B.1 Effect of transformer layer in the GA module</head><p>The GA module contains a transformer layer and top-down projections. We are interested in whether the performance gain is from the transformer layer or the top-down projections. For TDANet with only the GA module, we removed top-down projections from the GA module, keeping only the transformer layer. When the transformer layer is present, we take the output of it as the decoder input. When the transformer layer is absent, we use the top-layer features of the encoder as input of the decoder. <ref type="table">Table 4</ref> shows the experimental results. We found that the presence of the transformer layer does not significantly affect separation performance; both configurations yielded poor results</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Some experiments on the recognition of speech, with one and with two ears</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cherry</forename><surname>E Colin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the acoustical society of America</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="975" to="979" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Csr-i (wsj0) complete ldc93s6a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Garofolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Download. Philadelphia: Linguistic Data Consortium</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cocktail party phenomenon revisited: attention and memory in the classic selective listening procedure of Cherry (1953)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Noelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page">243</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An energy budget for signaling in the grey matter of the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Attwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simon B Laughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cerebral Blood Flow &amp; Metabolism</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1133" to="1145" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Auditory processing in the posterior parietal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><forename type="middle">E</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">E</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon W Gifford</forename><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Cognitive Neuroscience Reviews</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="218" to="231" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Top-down enhancement and suppression of the magnitude and speed of neural activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Gazzaley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cognitive neuroscience</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="507" to="517" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cocktail party problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Haykin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1875" to="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention to simultaneous unrelated auditory and visual events: behavioral and neural correlates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jennifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zatorre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral cortex</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1609" to="1620" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>F?votte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Algorithms to measure audio programme loudness and true-peak audio level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bs Series</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Updated energy budgets for neural computation in the neocortex and cerebellum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><surname>Howarth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padraig</forename><surname>Gleeson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Attwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cerebral Blood Flow &amp; Metabolism</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1222" to="1232" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Selective cortical representation of attended speaker in multi-talker speech perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">485</biblScope>
			<biblScope unit="page" from="233" to="236" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structural and functional brain networks: from connections to cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Jeong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">342</biblScope>
			<biblScope unit="page">1238411</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The cocktail-party problem revisited: early processing and selection of multi-talker speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Adelbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bronkhorst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="1465" to="1487" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Top-down attention regulates the neural expression of audiovisual integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fern?ndez</forename><surname>Luis Mor??s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="272" to="285" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention (MICCAI)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep clustering: discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John R Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep attractor network for single-microphone speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="246" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multitalker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Role of the right inferior parietal cortex in auditory selective attention: An rTMS study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Corinne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bareham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cortex</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="30" to="38" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tasnet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Listen, Think and Listen Again: Capturing Top-down Auditory Attention for Speaker-independent Speech Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: IJCAI</title>
		<imprint>
			<biblScope unit="page" from="4353" to="4360" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modeling attention and memory for auditory selection in a cocktail party environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SDR-half-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Le Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM transactions on audio, speech, and language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Which Ones Are Speaking? Speaker-Inferred Model for Multi-Talker Speech Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association (Interspeech)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4609" to="4613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">WHAM!: Extending Speech Separation to Noisy Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wichern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association (Interspeech)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1368" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dual-Path Transformer Network: Direct Context-Aware Modeling for End-to-End Monaural Speech Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association (Interspeech). 2020</title>
		<imprint>
			<biblScope unit="page" from="2642" to="2646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Librimix: an open-source dataset for generalizable speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joris</forename><surname>Cosentino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11262</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2020</title>
		<imprint>
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Asteroid: the PyTorch-based audio source separation toolkit for researchers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Pariente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association (Interspeech)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sudo rm-rf: efficient networks for universal audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP). IEEE. 2020</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Speech separation using an asynchronous fully recurrent convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22509" to="22522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sandglasset: a light multi-granularity self-attentive network for timedomain speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2021</title>
		<imprint>
			<biblScope unit="page" from="5759" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need in speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Subakan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2021</title>
		<imprint>
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the Use of Deep Mask Estimation Module for Neural Source Separation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="5328" to="5332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">On the Design and Training Strategies for RNN-based Online Neural Speech Separation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07340</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stepwise-refining speech separation network via fine-grained encoding in high-order latent domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengwei</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="378" to="393" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
