<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Imposing Relation Structure in Language-Model Embeddings Using Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Theodoropoulos</surname></persName>
							<email>christos.theodoropoulos@kuleuven.be</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">U</forename><surname>Leuven</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
							<email>james.henderson@idiap.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">C</forename><surname>Coman</surname></persName>
							<email>andrei.coman@idiap.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">U</forename><surname>Leuven</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">EPFL, Idiap Research Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Imposing Relation Structure in Language-Model Embeddings Using Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Though language model text embeddings have revolutionized NLP research, their ability to capture high-level semantic information, such as relations between entities in text, is limited. In this paper, we propose a novel contrastive learning framework that trains sentence embeddings to encode the relations in a graph structure. Given a sentence (unstructured text) and its graph, we use contrastive learning to impose relation-related structure on the tokenlevel representations of the sentence obtained with a CharacterBERT (El Boukkouri et al., 2020) model. The resulting relation-aware sentence embeddings achieve state-of-the-art results on the relation extraction task using only a simple KNN classifier, thereby demonstrating the success of the proposed method. Additional visualization by a tSNE analysis shows the effectiveness of the learned representation space compared to baselines. Furthermore, we show that we can learn a different space for named entity recognition, again using a contrastive learning objective, and demonstrate how to successfully combine both representation spaces in an entity-relation task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained language models (LMs), such as BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>, RoBERTa <ref type="bibr" target="#b20">(Liu et al., 2019)</ref> and <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, capture contextualized information effectively and are used in a wide variety of natural language processing (NLP) tasks. They have revolutionized NLP research. The main mechanism of these models is multi-head self-attention <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref>, which enables capturing patterns of semantic and syntactic interest in text. However, their ability to encapsulate high level semantic information, such as relations in the text, and domain-specific knowledge, is limited because they are trained on very large corpora using the main objectives of language modeling. In many NLP tasks, pretrained LM embeddings are used as model input. A common strategy is to concatenate the embeddings that are extracted from different LMs and let the model decide which part of the information is useful for the task. This empirical approach does not provide strong intuition and results in poor explainability capabilities because most of the task-specific models are black boxes.</p><p>In this study, we present a novel contrastive learning (CL) framework to leverage the embedding space of CharacterBERT and impose a relation structure on the embeddings. The proposed framework receives a sentence and a graph that represents the text relations in a structured way, and the CL paradigm is applied to impose this structure on the token embeddings of the CharacterBERT text encoder. Different graph formulations that represent the text relations are explored. The main goal is to create a common embedding space where relations can be easily detected. To evaluate progress towards this goal, we use the ADE dataset <ref type="bibr" target="#b11">(Gurulingappa et al., 2012)</ref>, which is widely used for relation extraction (RE) <ref type="bibr" target="#b36">(Zhao and Grishman, 2005)</ref>; <ref type="bibr" target="#b14">(Jiang and Zhai, 2007)</ref>; <ref type="bibr" target="#b28">(Sun et al., 2011)</ref>; <ref type="bibr" target="#b24">(Plank and Moschitti, 2013)</ref> and named entity recognition (NER) tasks <ref type="bibr" target="#b5">(Curran and Clark, 2003)</ref>; <ref type="bibr" target="#b9">(Florian et al., 2006)</ref>; <ref type="bibr" target="#b21">(Nadeau and Sekine, 2007)</ref>; <ref type="bibr" target="#b10">(Florian et al., 2010)</ref> in the challenging field of information extraction (IE) from biomedical text.</p><p>To evaluate the efficacy of our approach, a simple baseline neural network classifier for RE, using the pretrained CharacterBERT medical version representations, is trained. The representations of the CharacterBERT tuned version after applying CL are used to train the same classifier, which vastly outperforms the baseline classifier. A tSNE <ref type="bibr" target="#b31">(Van der Maaten and Hinton, 2008)</ref> analysis illustrates that meaningful relation-related clusters can be identified in the learned embedding space. This provides a second strong indication that structure can be effectively imposed on LM embeddings using our proposed framework.</p><p>Even if the main focus of this work is not solving the IE problem directly, to further explore the capabilities of the relation-aware representation space, we train a simple KNN classifier for RE that is competitive with state-of-the-art performance. Strict evaluation <ref type="bibr" target="#b2">(Bekoulis et al., 2018b)</ref>; <ref type="bibr" target="#b29">(Taill? et al., 2020)</ref> of the RE task presupposes correct detection of the boundaries and the entity type of each argument in the relation. Hence, we apply the CL paradigm to learn a distinct embedding space for the entities and use a KNN classifier to solve the NER task. Finally, we perform a strict evaluation of the complete entity-relation extraction task. This transparent, computationally inexpensive and intuitively simple approach has comparable results to the state-of-the-art models. This achievement illustrates how informative and meaningful the learned embedding spaces are.</p><p>In summary, our key contributions are:</p><p>? We propose a novel CL framework for imposing a relation-related structure on LM embeddings.</p><p>? We investigate different ways to model texts and graphs and show the effectiveness of embedding relations in pairs of token embeddings.</p><p>? We exploit the capabilities of the learned representation spaces by using them in the IE task and achieve competitive results to stateof-the-art models, even if we use transparent and intuitively simple KNN classifiers.</p><p>The paper is structured as follows. Section 2 presents the ADE dataset and the data preprocessing steps, and section 3 explains the framework in detail. In section 4, we evaluate the quality of the framework in baseline setups. The tSNE analysis is presented in section 5. In section 6, we use the framework to solve the IE task and compare the results to state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>This study focuses on biomedical text, and ADE dataset is used. The sentences are annotated with labels for drugs and adverse effects, as well as the relations among these entities. Adverse effects (AEs) cover a range of signs, symptoms, diseases, disorders, abnormalities, organ damage and even death caused by that drug. The corpus is annotated at the sentence-level, so non-local relations (between entities of different sentences) do not exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Preprocessing</head><p>The input of the main CL framework consists of the encoded padded sentence and the relation graph, which is extracted from the sentence. The graphs are used only in the training setup. To prepare the input for CharacterBERT, tokenization is applied to each sentence using the character-CNN module <ref type="bibr" target="#b22">(Peters et al., 2018)</ref>. The BERT tokenizer handles out-of-vocabulary (OOV) words by splitting these words into word pieces. However, the existence of word pieces can be an obstacle in creating and testing the CL experiments of this study from the implementation point of view. Additionally, word pieces may add biases to the model <ref type="bibr" target="#b8">(El Boukkouri et al., 2020)</ref>, especially in biomedical text where most of the drugs and many adverse effects are OOV words. Hence, CharacterBERT is chosen instead of BERT.</p><p>For each sentence, a knowledge graph is obtained to model the relations between the drugs and the adverse effects. The graph nodes are initialized with embeddings that are extracted by the final layer of the pretrained medical version of CharacterBERT. The graph convolutional network (GCN) <ref type="bibr" target="#b17">(Kipf and Welling, 2016)</ref>, which is a key layer of the main proposed CL framework ( <ref type="figure" target="#fig_0">Fig. 1,  Fig. 2</ref>), receives two inputs: an N xF matrix (N : number of nodes, F : number of features) with the embeddings (features) of each node and an adjacency matrix N xN , which models the connections (edges) of the undirected graph. Generally, the adjacency matrices are very sparse if we consider all the tokens and create the whole graph because the relations are rare and there are many singleton nodes. Alternatively, the tokens that are part of a relation can only be used, and the essential subgraph is extracted. For example, in the sentence "Methods: we report two cases of pseudoporphyria caused by naproxen and oxaprozin." There are two AE relations between AE pseudoporphyria and the drugs naproxen and oxaprozin. Hence, by creating the subgraph, only these AE and drug tokens are included, and the singleton nodes (rest of the sentence tokens) are removed.</p><p>The drug and the AE entities may consist of more than one word. There are two methods to model this case. On the one hand, the whole phrase can be represented as one node in the graph by averaging the embeddings of each distinct word of the phrase. On the other hand, each node refers to the last word of the entity. For example, if the initial relation is between the drug "gabapentin" and the adverse effect "renal impairment", then in the graph, the relation [gabapentin, impairment] is modeled. The latter approach is mainly adopted in nonspan-based relation extraction models <ref type="bibr" target="#b2">(Bekoulis et al., 2018b)</ref>; <ref type="bibr" target="#b35">(Zhao et al., 2020)</ref>. In this study, the second approach is adopted because it gives the flexibility in applying contrastive learning at the token and relation levels.</p><p>The normalization of the adjacency matrix is essential for aggregating and propagating the information in the graph effectively (Kipf and Welling, 2016) and is described by the following equations:</p><formula xml:id="formula_0">A hat = A + I,<label>(1)</label></formula><formula xml:id="formula_1">A norm = D ?0.5 * A hat * D ?0.5 ,<label>(2)</label></formula><p>where A is the initial adjacency matrix, I is the identity matrix and D is the degree matrix. Initially, the whole corpus is stored in one text file. Hence, the data should be transformed and stored using a different more flexible format. For each sentence of the dataset, a distinct JSON file is created and contains a list with the tokens 1 , a list with NE tags adopting the BIO encoding scheme <ref type="bibr" target="#b26">(Sang and Veenstra, 1999)</ref>; <ref type="bibr" target="#b25">(Ratinov and Roth, 2009</ref>), a list with token index pairs that are members of an existing relation, the padded encoded version of the sentence, the attention mask vector of the sentence, a list with the embeddings of each node of the graph and the normalized adjacency matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataset Statistics</head><p>The ADE dataset is not officially split into training, validation, and test sets. Hence, we evaluate our models using 10-fold cross-validation similar to <ref type="bibr" target="#b18">(Li et al., 2017)</ref>. We use the same splits as <ref type="bibr" target="#b7">(Eberts and Ulges, 2020)</ref>. As <ref type="bibr" target="#b29">(Taill? et al., 2020)</ref> stresses, many works on the IE task do not report the data preprocessing and detailed statistics of the datasets. This is an obstacle for a sanity check and reproducibility. The ADE dataset consists of 4,272 sentences, with 5,063 drug entities (1,048 unique drugs), 5,776 AE entities (2,983 unique AEs) and 6,821 relations. We report the statistics of each split <ref type="table" target="#tab_1">(Table 1)</ref> and propose using this particular split for a fair comparison 2 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Framework</head><p>In essence, contrastive learning is a paradigm for learning representations which capture some auxiliary information by training them to distinguish positive from negative instances of this auxiliary information. Our framework is inspired by the recent publications on image view-based CL of visual representation <ref type="bibr" target="#b15">(Khosla et al., 2020)</ref>; <ref type="bibr" target="#b34">(Zhang et al., 2020)</ref>; <ref type="bibr" target="#b13">(Henaff, 2020)</ref>; ; <ref type="bibr" target="#b12">(He et al., 2020)</ref>, but differs from the existing work by the application of CL to the graph and text modalities. Our work is also inspired by the semantic bootstrapping hypothesis <ref type="bibr" target="#b23">(Pinker, 1996)</ref>, which proposes that children acquire their native language through exposure to sentences of the language (i.e., a language model) paired with structured representations of their meaning <ref type="bibr" target="#b0">(Abend et al., 2017)</ref>. The main CL framework for imposing relationaware structure on the token embeddings is tested under two different settings. The difference in each setting is related to the modeling of the graph and the level of applying the CL paradigm. To solve the end-to-end IE task, a second model is proposed for learning a distinct embedding space where the named entities are projected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architectures</head><p>In the first setting ( <ref type="figure" target="#fig_0">Fig. 1)</ref>, we apply the CL method to the embeddings of graph nodes in their graph context and the embeddings of sentence tokens in their sentence context. We call this variation in the main CL framework CLGS. The positive and sampled negative graph representations are computed by a graph convolutional network (GCN) (Kipf and Welling, 2016); <ref type="bibr" target="#b27">(Schlichtkrull et al., 2018)</ref> layer followed by a pooling layer. We model the graph considering only the tokens that are part of a relation (subgraphs). To obtain one representation for the graph, average and maximum pooling strategies are tried. Tanh (range: [-1, 1]) is chosen as the activation function of the GCN layer because the text encoder also extracts negative embeddings. Hence, a similar range of embedding values should be extracted from the graph. The sentence is passed to the text encoder (CharacterBERT), which has the first six layers frozen. CharacterBERT is initialized with the pretrained weights (medical version). A pooling layer follows, to create a representation for the whole sentence. Taking the average, maximum embedding vector and the <ref type="bibr">[CLS]</ref> token representation are tested as pooling strategies. The addition of a projection layers before applying CL is a common approach ; <ref type="bibr" target="#b34">(Zhang et al., 2020)</ref>. ReLU is used as the activation function of the projection layers to introduce nonlinearity. By adding the projection layers, there is the danger that the task will be solved mainly in the projection layers, while the final goal is pushing structured relation-aware information in the text encoder. Finally, CL is applied to the resulting pair of graph and sentence representations, so that the pooled sentence token embeddings are trained to carry the information in the pooled graph node embeddings.  In the second setting, we apply the CL method to the embeddings of graph relations and the embeddings of pairs of sentence tokens. This variation in the CL framework is called CLDR. The graph is simplified to the extreme level. Each relation is modeled completely independently in the graph, and the relation representations are extracted by concatenation of the nodes that are connected in the disjoint graphs <ref type="figure" target="#fig_1">(Fig. 2)</ref>. This graph modeling makes the CL at the relation level a more tractable task. In addition, sampling negative graphs can be implemented more easily in a more controlled way. In this setting, because the graphs only have two nodes, the adjacency matrix should not be normalized in a balanced way. If the adjacency matrix is 0.5 0.5 0.5 0.5 , then the final node embeddings will be the same for the two nodes that form the graph. Hence, we suggest focusing more on the self-loop of each node to keep its predefined contextualized information up to a certain level 3 . The final adjacency matrix has the following format ? 1?? 1?? ? , where ? is a hyperparameter of the model. The ? parameter defines the balance of focusing on the self-loop of each node and its neighbor (connected node). Intuitively, a ? value equal to 0.8 is a good choice for focusing attention on the self-loop and having distinct embeddings for the connected nodes. ReLU is used as the activation function of the GCN layer.</p><p>On the text side, the pair of tokens that form a relation in the disjoint graphs are chosen, and the concatenation of their representations is used as the final relation representation. Finally, CL is applied on the relation level, so that the pairs of sentence token embeddings are trained to carry the information in the pairs of related graph node embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCN CharacterBERT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concatenated Relation Representations</head><p>Nodes  A distinct model (called CLNER) for learning meaningful representations for named entities is designed <ref type="figure" target="#fig_2">(Fig. 3)</ref>. CharacterBERT captures contextualised information very well. Hence, only one dense layer is added after CharacterBERT. Then a random sampling for the named entities is performed in a balanced way. A pool of sampled entities of the batch is selected and CL is applied on the token level.</p><formula xml:id="formula_2">Embeddings R x Z x (2 x 768) Normalized Adjacency Matrix R x Z x (2 x 2) Padded Encoded Sentence (1 x W)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CharacterBERT Attention</head><p>Masks  </p><formula xml:id="formula_3">B x (1 x W) Padded Encoded Sentence B x (1 x W)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sampling Strategy</head><p>Hard negative sampling is important to effectively apply the CL paradigm. The negative graphs are created by randomly selecting tokens that are not part of an adverse effect entity, keeping the correct drug tokens, and vice versa. Hence, hard incorrect drug and adverse effects relation pairs are introduced to the graph. The positive and negative graphs of each sentence have the same number of relations but not necessarily the same number of nodes. The sampling strategy is similar for the CLGS <ref type="figure">(Fig. 4</ref>) and the CLDR model <ref type="figure">(Fig. 5)</ref>. For the CLDR model, the positive graph is simplified to a disjoint graph, and then hard negative sampling is performed. In the CLNER, random sampling 4 is executed at the batch level. Analysis of the number of different entity tags (drug, AE or outside token) in the batch is performed a priori to choose an appropriate number of positive and negative samples (balanced sampling). <ref type="bibr">4</ref> Hard negative sampling based on the Euclidean distance and cosine similarity is also tested, but the performance is not increased. Hence, the complexity-performance trade-off leads us to finally select random sampling. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Design Choices</head><p>In this subsection, the justification for the model design choices is discussed. For the CLGS and CLDR models, the GCN layer is the key element because it can produce useful node representations considering the graph links. The propagation rule of the GCN layer is described by the following equation:</p><formula xml:id="formula_4">X l+1 = ?(A norm * X l * W l ),<label>(3)</label></formula><p>where ?(?) is the activation function (e.g., ReLU, Tanh), A norm is the normalized adjacency matrix (Eq. 2), X l the node embeddings and W l the weights of the l layer. In the first setting (CLGS model), the graph is propagated through the GCN layer, and a final pooled graph representation is extracted. We hy-pothesize that using the CL paradigm, the model can learn which part of the information is essential for the relation representations by keeping the structure-related information in the graph representation. In the second setting (CLDR model), the level of abstraction is reduced because instead of applying CL in the graph sentence, we use the CL paradigm at the relation level. The strategy of creating disjoint graphs results in learning similar representations for the drug and AE nodes. To address this, the relations are represented asymmetrically as a concatenation of the nodes. We hypothesize that relation-related information can be imposed in the pair-of-tokens embeddings of the LM by applying CL to them and these pair-of-nodes embeddings of the graph relations <ref type="figure" target="#fig_1">(Fig. 2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Details</head><p>The models are trained using a CL loss function that is similar to the SimCLR loss function . In the first setting (CLGS model), the main concept is to leverage the two graph and sentence representations so the true representation pair is close and similar in the learned embedding space. At each training time, a set of Z graphs (the positive and some negative graphs) and the corresponding sentence are passed on the model, and the corresponding representations are calculated. Therefore, the contrastive loss receives the graph and sentence representations and for the i-th pair is as follows:</p><formula xml:id="formula_5">l (S?G) i = ?log( exp(&lt;S i ,G i &gt;/? ) Z z=1 exp(&lt;S i ,G z &gt;/? ) ), (4)</formula><p>where &lt; S i , G i &gt; represents the cosine similarity and ? is a temperature parameter.</p><p>In the second setting (CLDR model), the pair of node embeddings that are extracted from the disjoint graphs encode their relation, because this is the main functionality of the GCN layer. Hence, the main idea is to increase the similarity between the representations of the correct relation in the graph and the relation representations that are extracted from the text encoder. The contrastive loss for each sentence is as follows:</p><formula xml:id="formula_6">l (RS?RG) = R r=1 ?log( exp(&lt;RSr,RGr&gt;/? ) Z z=1 exp(&lt;RSr,RGz&gt;/? ) ),<label>(5)</label></formula><p>where R is the total number of relations in the sentence, RS is the relation representation of the text encoder and RG is the relation representation of the graph.</p><p>For the CLNER model, the contrastive loss is as follows:</p><formula xml:id="formula_7">l N E = N n=1 ?log( P p=1 exp(&lt;RNn,RNp&gt;/? ) K k=1 exp(&lt;RNn,RN k &gt;/? ) ), (6)</formula><p>where N is the total number of tokens in the batch, P is the number of the positive samples (same NE tag), K is the total number of samples and RN is the extracted token representation. We use a batch-size of 8 for training the CLGS and CLDR models, and 16 for the CLNER model. ADAM optimizer (Kingma and Ba, 2014) is selected with a learning rate of 1e-5 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation -Baseline</head><p>For the CLGS model, the first evaluation step is a simple similarity check. We use the trained CLGS model to extract the sentence representation and the positive and negative graph representations for all the sentences in the test set. Then, a similarity check is applied using the extracted sentence and graph representations. The most similar graph is predicted as the positive sentence graph. Given the positive and all the negative hard graphs extracted from each sentence, the model should be able to detect the correct graph. The different model variations perform well, but the mean pooling selection in the graph and sentence side results in better performance, as the accuracy is over 91%. The addition of the projection layers is not advantageous.  The second evaluation step is applied to both models (CLGS and CLDR). Following previous research on representation learning <ref type="bibr" target="#b13">(Henaff, 2020)</ref>; ; <ref type="bibr" target="#b12">(He et al., 2020)</ref>; <ref type="bibr" target="#b34">(Zhang et al., 2020)</ref>, we evaluate the tuned CharacterBERT text encoder, taken from the trained CLGS and CLDR models, in a linear classification setting, where all the candidate relations (concatenation of the token embeddings) are created, and a linear classification layer is trained for the RE task. As a baseline model, we use the pretrained medical Character-BERT to create the representation for the relations 6 . This linear setting directly provides insight into how successfully the relation-related structure is imposed at the token level of the text encoder, by evaluating the quality of the learned representations for RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Precision  Using the tuned CharacterBERT representation from the CLGS model (mean graph and text pooling) results in poor performance. The pooling layer smooths the information. Hence, structure-related information cannot be passed at the token level of the text encoder. A smarter pooling strategy that preserves most of the relation-aware information would be ideal, but designing such pooling is difficult. The main obstacle is the varied number of relations. In contrast, when we use the tuned Char-acterBERT of the CLDR model, the basic classifier vastly outperforms the baseline model. This is a strong indication that the relation-related structure is successfully imposed on the pairs of token embeddings of the text encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">tSNE Analysis</head><p>A tSNE analysis is performed to further explore the quality of the learned embedding spaces. Using the tuned CharacterBERT of the CLDR model, the relation representation space is created. We project the positive (orange dots) and hard negative relations (blue dots), where one of the two relation tokens is correct. In the tSNE plot ( <ref type="figure" target="#fig_4">Fig. 6)</ref>, meaningful relation clusters can be easily identified, which demonstrates the efficiency of our framework (CLDR model). The relation representations are asymmetric, as the drug and AE tokens have similar representations <ref type="figure">(Fig. 7)</ref>. This means that we cannot solve RE and NER tasks using the same representation space. Hence, we learn a different space for the named entities (CLNER model).</p><p>In the tSNE plot in the entity representation space <ref type="figure">(Fig. 8)</ref>, we can detect insightful entity clusters. In particular, the clusters related to the drug tags (B-DRUG, I-DRUG) are very dense and well shaped. This is a strong finding that illustrates that The insights of the tSNE analysis, with the welldefined clusters in the embedding spaces, lead us to approach the entity-relation task using intuitively simple and transparent KNN classifiers. For the RE task, we utilize the tuned CharacterBERT of the CLDR model to create the candidate relation representations. At the inference step, for each candidate relation, we decide whether it is positive based on the labels of the k-nearest neighbors in the learned embedding space. The value of k is chosen based on the performance in the randomly selected validation set (10% of training set) for each fold. We adopt the same strategy for the NER task using the CLNER model and project each token to the named entity representation space.</p><p>To solve both NER and RE tasks, we combine the two semantic spaces. First, we determine whether a candidate relation (concatenation of the tokens) is predicted as positive in the relation representation space, which is obtained by the tuned CharacterBERT of the CLDR model. Then, we determine whether the boundaries and the types of the two entities in the candidate relation are predicted correctly in the entity representation space obtained by the CLNER model. All possible candidate relations and the named entities of the test set are classified.</p><p>We strictly evaluate the performance of the IE task. As <ref type="bibr" target="#b2">(Bekoulis et al., 2018b)</ref> state, an entity is considered correct if its boundaries are detected correctly and the predicted type (drug or AE) matches the ground truth. In the same setup, a relation is considered correct if its type and the two entities (boundaries and type) involved in the relation are correctly predicted. We measure precision, recall and F1 score. Following previous work on IE, we report the macro-averaged F1 score, and as 10-fold cross-validation is adopted, we average the scores over the folds.  In the RE task, we achieve very competitive results using a simple and transparent KNN classifier. In contrast, the state-of-the-art models <ref type="bibr" target="#b33">(Wang and Lu, 2020)</ref>; <ref type="bibr" target="#b35">(Zhao et al., 2020)</ref> are very complex and computationally expensive. This fact highlights the high quality of the learned relation representation space (CLDR model). In principle, the NER task is a sequence-tagging problem. However, we obtain good performance with a KNN classifier that performs the inference in the learned entity representation space (CLNER model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Notably, the last column of Table 4 (RE-) presents the performance of the RE KNN classifier in predicting whether there is a relation between two tokens, without considering the NER task (type and boundaries of the entities). In this case, the F1 score is 86.5, and this value is the upper bound performance of our approach. Hence, incorporating a state-of-the-art model for the NER task (e.g., <ref type="bibr">Lu, 2020, Eberts and</ref><ref type="bibr" target="#b7">Ulges, 2020)</ref> could further improve the scores of the RE task under strict evaluation. However, we use the SpERT model <ref type="bibr" target="#b7">(Eberts and Ulges, 2020)</ref> for NER (F1 score: 89.25), but the results in the RE task are not improved. This illustrates that our NER results are already very competitive.</p><p>The above results reveal the quality of the representations for both NER and RE tasks. Hence, the proposed CL framework can be used as a preprocessing and representation learning step in the pipeline for IE models. The CL framework can be trained to leverage the embedding space and create meaningful, disentangled representations for the IE task. We successfully evaluated the representations with a simple KNN classifier, but the learned representations can be used as input in complex models for entity and relation classification to achieve better results and faster convergence. We will explore this research direction in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We present a novel CL framework, which, in principle, is text encoder-agnostic, for effectively imposing relation-related structure to LMs and leveraging the embedding space. We evaluate the quality of the learned representations using relative baselines and competitively solve an entity-relation task. The overall results indicate that the learned representations are very powerful. The performed tSNE analysis illustrates that meaningful clusters can be easily identified in the learned embedding spaces. We note that the proposed framework can be used as a representation learning step for complex IE systems. In future work, we intend to explore the capabilities of our approach in continual learning settings and exploit external graph structured knowledge in representation learning of language data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>CL framework CLGS -1 st Setting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>CL framework CLDR -2 nd Setting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Model CLNER for learning named entity representations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Example of sampling negative graphs -CLGS model Methods: we report two cases of pseudoporphyria caused by naproxen and oxaprozin. Example of sampling negative graphs -CLDR model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>tSNE plot -Relation representation space obtained with CharacterBERT of CLDR model (1: relation, 0: no relation) Figure 7: tSNE plot -Relation representation space obtained with CharacterBERT of CLDR model -Named Entities Figure 8: tSNE plot -Entity representation space obtained with CLNER model 6 Entity-Relation task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Statistics of 10-fold splits -ADE dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Results -CLGS model: Finding the correct graph with similarity check</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>RE -linear classification setting</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Test set results: macro-averaged F1 score</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The sentence tokenization is performed using the SpaCy library.2 To facilitate further research, the preprocessed data and the code will be publicly available in the official repository of the paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We remind that the nodes are initialized with embeddings extracted from the pretrained CharacterBERT medical version.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">More information about hyperparameter tuning-selection is given in the Appendix section.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We also try fine-tuning both the text encoder and the linear head, but the performance is not improved. the CLNER model can extract very good representations for the NER task.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bootstrapping language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2017.02.009</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="116" to="143" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adversarial training for multi-context joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1307</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2830" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint entity recognition and relation extraction as a multi-head selection problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2018.07.032</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language independent ner using a maximum entropy tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
		<idno type="DOI">10.3115/1119176.1119200</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="164" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Span-based joint entity and relation extraction with transformer pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
		<idno type="DOI">10.3233/FAIA200321</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2006" to="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Characterbert: Reconciling elmo and bert for word-level open-vocabulary representations from characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hicham</forename><forename type="middle">El</forename><surname>Boukkouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lavergne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Zweigenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.609</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6903" to="6915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Factorizing complex models: a case study in mention detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imed</forename><surname>Zitouni</surname></persName>
		</author>
		<idno type="DOI">10.3115/1220175.1220235</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving mention detection robustness to noisy input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Pitrelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imed</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zitouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="335" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Development of a benchmark corpus to support the automatic extraction of drugrelated adverse effects from medical case reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdul</forename><forename type="middle">Mateen</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliane</forename><surname>Fluck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hofmann-Apitius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2012.04.008</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="885" to="892" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Henaff</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A systematic exploration of the feature space for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A neural joint model for entity and relation extraction from biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-017-1609-9</idno>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint models for extracting adverse drug events from biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI&apos;16</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI&apos;16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2838" to="2844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
		<idno type="DOI">10.1075/li.30.1.03nad</idno>
	</analytic>
	<monogr>
		<title level="m">Lingvisticae Investigationes</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Language Learnability and Language Development: With New Commentary by the Author</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Pinker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Harvard University Press</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Embedding semantic similarity in tree kernels for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Representing text chunks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik F Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorn</forename><surname>Veenstra</surname></persName>
		</author>
		<idno type="DOI">10.3115/977035.977059</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Ninth Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="173" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-93417-4_38</idno>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised relation extraction with large-scale word clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="521" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Let&apos;s stop error propagation in the end-to-end relation extraction literature!</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Taill?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Guigue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Scoutheeten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3689" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Neural metric learning for fast end-to-end relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Kavuluru</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07458</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Two are better than one: Joint entity and relation extraction with tablesequence encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.133</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1706" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langlotz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00747</idno>
		<title level="m">Contrastive learning of medical visual representations from paired images and text</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Modeling dense cross-modal interactions for joint entity-relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/558</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4032" to="4038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Extracting relations with integrated information using kernel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<idno type="DOI">10.3115/1219840.1219892</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
