<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rich Semantics Improve Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Afham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Telecommunication Engineering</orgName>
								<orgName type="institution">University of Moratuwa</orgName>
								<address>
									<country key="LK">Sri Lanka</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mohamed Bin Zayed University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
							<email>salman.khan@mbzuai.ac.ae</email>
							<affiliation key="aff1">
								<orgName type="institution">Mohamed Bin Zayed University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<settlement>AU</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris Khan</surname></persName>
							<email>muhammad.haris@mbzuai.ac.ae</email>
							<affiliation key="aff1">
								<orgName type="institution">Mohamed Bin Zayed University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
							<email>muzammal.naseer@mbzuai.ac.ae</email>
							<affiliation key="aff1">
								<orgName type="institution">Mohamed Bin Zayed University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<settlement>AU</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Fahad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mohamed Bin Zayed University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rich Semantics Improve Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>AFHAM ET AL.: RICH SEMANTICS IMPROVE FEW-SHOT LEARNING 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human learning benefits from multi-modal inputs that often appear as rich semantics (e.g., description of an object's attributes while learning about it). This enables us to learn generalizable concepts from very limited visual examples. However, current fewshot learning (FSL) methods use numerical class labels to denote object classes which do not provide rich semantic meanings about the learned concepts. In this work, we show that by using 'class-level' language descriptions, that can be acquired with minimal annotation cost, we can improve the FSL performance. Given a support set and queries, our main idea is to create a bottleneck visual feature (hybrid prototype) which is then used to generate language descriptions of the classes as an auxiliary task during training. We develop a Transformer based forward and backward encoding mechanism to relate visual and semantic tokens that can encode intricate relationships between the two modalities. Forcing the prototypes to retain semantic information about class description acts as a regularizer on the visual features, improving their generalization to novel classes at inference. Further, this strategy imposes a human prior on the learned representations, ensuring that the model is faithfully relating visual and semantic concepts, thereby improving model interpretability. Our experiments on four datasets and ablations show the benefit of effectively modeling rich semantics for FSL. Code is available at: https://github.com/MohamedAfham/RS_FSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AFHAM ET AL.: RICH SEMANTICS IMPROVE FEW-SHOT LEARNING</head><p>This bird has a white belly, black spots near the breast and secondaries, and a black eyebrow Classification Boundaries without rich semantics Classification Boundaries with rich semantics This bird has a white throat with a light brown belly and abdomen Small bird, beige with scattered black spots, goldenrod eyebrows, pale gray pointed bill, black eyes</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Traditional classification models use class labels for supervision, expressed in a numerical form or as one-hot encoded vectors <ref type="bibr" target="#b12">[13]</ref>. However, humans do not solely rely on such numerical class-labels to acquire learning. Instead, humans learn by communicating through natural language, which is grounded in a complex structure consisting of semantic attributes, relationships and abstract representations. Psychologists and cognitive scientists have argued natural language descriptions to be a central element of human learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. The <ref type="figure">Figure 1</ref>: In FSL setting, where we require generalizability to novel classes with limited samples, modeling semantic attributes of classes can help disambiguate confusing classes. We suggest that the numerical class labels traditionally used in FSL are inadequate to represent diverse semantic attributes of an object class, which can be modeled via low-cost class-level language descriptions (colored boxes). Our approach effectively utilizes language information to learn both highly discriminative and transferable visual representations that help to avoid errors in ambiguous cases (e.g., visually similar fine-grained classes). depiction of semantic class labels with numerical IDs leads to a semantic gap between the class semantic representation and the learned visual features.</p><p>We consider a few-shot learning setting where language descriptions for the seen (base) classes are available during training but not for the novel (few-shot) class that appear during inference. Remarkably, studying the potential of language descriptions has particular relevance to FSL, where a model must learn to generalize from few-samples and several categories can only be discriminated with subtle attribute-based differences <ref type="figure">(Fig. 1)</ref>. We hypothesize that by predicting natural language descriptions as an auxiliary task during training, the model can learn useful representations that help transfer better to novel tasks during the inference stage. This helps the representations to explicitly model the shared semantics between the few-shot samples so that the class descriptions can be successfully generated.</p><p>The language description task while learning to classify images forces the model to attain following desirable attributes: (a) model high-level compositional patterns occurring in the visual data e.g., attributes in fine-grained bird classes; (b) avoid over-fitting on a given FSL task by imposing a regularizer demanding natural description from the class prototype; and (c) provide intuitive explanation for the learned class concepts e.g., the description of an object type, attributes, function and affordance in a human interpretable form. Importantly, the error feedback obtained from such a supervised task (natural language description) can help align a model with the 'human prior'. Our RS-FSL approach is generic in nature and can be plugged into existing baselines models or with other multi-task objectives (e.g., equivariant self-supervised learning losses). Although we discard the language description module at inference, it is useful as a debugging tool to understand the model's behaviour in case of wrong predictions (e.g., highlighting which attributes were mistaken or ambiguous).</p><p>Contributions. Our objective induces a generative task of natural language description for few-shot classes that forces the model to learn correlations between same class samples such that consistent class descriptions can be generated. The limited set of class-specific samples acts as a bottleneck that encourages extraction of shared semantics. We then introduce a novel transformer decoding approach for few-shot class description that relates the hybrid prototypes (obtained using the collective support and query image features) with the corresponding descriptions in both forward and backward directions. Our design allows modeling long-range dependencies between the sequence elements compared to previous recurrent architectures. Finally, our experiments on four datasets show consistent improvements across the FSL tasks. We extensively ablate our approach and analyze its different variants. The proposed transformer decoder acts as a plug-and-play module and shows gains across popular FSL baselines namely ProtoNet <ref type="bibr" target="#b29">[30]</ref>, RFS <ref type="bibr" target="#b32">[33]</ref> and Meta-baseline <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Only a few-methods explore the potential of rich semantic descriptions in the context of FSL. A predominant approach has been the incorporation of unsupervised word embeddings or a set of manual attributes to represent class semantics. For example, <ref type="bibr" target="#b4">[5]</ref> uses semantic embeddings of the labels or attributes to guide the latent representation of an auto-encoder as a regularizer mechanism. Xing et al. <ref type="bibr" target="#b37">[38]</ref> dynamically learn the class prototypes as a convex combination of visual and semantic label embeddings (based on GloVe <ref type="bibr" target="#b23">[24]</ref>). However, these embeddings require manual labeling (in case of attributes) or remain noisy if acquired via unsupervised learning. Additionally, representing rich semantics in a single vector remains less flexible to encode the complex semantics. In contrast, our approach flexibly learns the semantic representations with class-level language descriptions to improve upon the noisy unsupervised word embeddings. Schwartz et al. <ref type="bibr" target="#b27">[28]</ref> extended <ref type="bibr" target="#b37">[38]</ref> to exploit various semantics (category labels, text descriptions and manual attributes) in a joint framework. However, they use language descriptions as inputs rather than an extra supervision signal to train the visual backbone. Thus, these methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b37">38]</ref> require attribute information or descriptions for novel classes during inference which can be hard to acquire for few-shot classes.</p><p>Image-level captions have been used in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39]</ref> to align visual and semantic spaces with a multi-modal transformer model <ref type="bibr" target="#b13">[14]</ref>. This can help learning from a limited set of base classes and scales to unseen classes <ref type="bibr" target="#b38">[39]</ref>. <ref type="bibr" target="#b8">[9]</ref> models annotator rationale as a spatial attention and the relevant attributes for a given input image. However, unlike our work, these methods do not study the FSL problem where image-level captions can cause overfitting. Furthermore, they require image-specific captions and rationales (not just "what" but also "why") which can be costly, even for a small number of base classes. Since acquiring high-quality explanations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref> from experts can be expensive, efforts have been made to reduce the manual cost needed to acquire such annotations. To this end, ALICE model <ref type="bibr" target="#b18">[19]</ref> acquires contrastive natural language descriptions from human annotators about the most informative pairs of object classes identified via active learning. In contrast, our approach only requires classlevel descriptions that are easy to acquire compared to image level semantic annotations.</p><p>Andreas et al. <ref type="bibr" target="#b0">[1]</ref> use the language descriptions during the pertaining stage in FSL to learn natural task structure. Once the model is pretrained to match images to natural descriptions, it can be used to learn new concepts by aligning natural descriptions with the images at inference. In contrast to inference stage alignment, LSL <ref type="bibr" target="#b21">[22]</ref> introduced a GRU branch with language supervision to enrich the backbone features and discards the branch during inference. However, decoding mechanism in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref> does not explicitly encode both forward and backward relations in the language, suffers in encoding long-term relationships and cannot relate multiple class-level descriptions with the visual class prototypes.  <ref type="figure">Figure 2</ref>: Overall architecture of the RS-FSL. Fundamentally, it consists of a visual backbone followed by a prototypical network to compute the classification loss (denoted by classification branch). We aim at encoding visual-semantic relationships that are in turn harnessed for enriching the visual features. To this end, we propose generating class-level language descriptions constrained on a hybrid prototype via developing a transformer based forward and backward decoding mechanism (denoted as language branch). Our method jointly trains the classification and language losses. aim is to learn concepts in novel classes C novel with few examples, given C base ? C novel = / 0. Pretraining. Following the recent works in FSL <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref>, we pretrain the visual backbone, parameterized by ? , on C base in a supervised manner. We assume access to a dataset of image (x) and label (y ? C base ) pairs:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Branch Language Branch (Discarded during Inference)</head><formula xml:id="formula_0">Feature Pooling ? ? ? ? ? ? ? ? Embedded Description Tokens Class Description " ( #*+ " ( +#"1 (" ( ) ( ( ) Hybrid prototype</formula><formula xml:id="formula_1">D = {x i , y i } M i=1 .</formula><p>The visual backbone maps the input image x to a feature embedding space R d by an embedding function f ? : x ? v. In turn, the linear classifier f ? : v ? p, parameterized by ?, maps the features generated by f ? to the label space R L where L denotes the number of classes in C base . We optimize both ? and ? by minimizing the standard cross-entropy loss:</p><formula xml:id="formula_2">L pre (p, y) = ? log exp(p y ) ? j exp(p j )<label>(1)</label></formula><p>Episodic Training. After pretraining the visual backbone, we adopt the episodic training paradigm which has shown effectiveness for FSL. It simulates few-shot scenario faced at test time via constructing episodes by sampling small number of few-shot classes from a large labelled collection of classes C base . Specifically, each episode is created by sampling N classes from the C base forming a support class set C supp ? C base . Then two different example sets are sampled from these classes. The first is a support-set S e = {(s i , y i )} N?K i=1 comprising K examples from each of N classes, and the second is a query-set Q e = {(q j , y j )} Q j=1 containing Q examples from the same N classes. The episodic training for few-shot classification boils down to minimizing, for each episode e, the loss of prediction on the examples in the query-set (q j , y j ) ? Q e , given the support set S e :</p><formula xml:id="formula_3">L epi = E (S e ,Q e ) Q ? j=1 log P ? (y j |q j , S e ).<label>(2)</label></formula><p>Prototypical Networks. We develop proposed method on a popular metric-based metalearning method named Prototypical network <ref type="bibr" target="#b29">[30]</ref>, owing to its simplicity and effectiveness.</p><p>However, ours is a plug-and-play training module which can work seamlessly with other FSL methods (as demonstrated in Sec. 4.2). Prototypical networks leverage the support set to compute a centroid (a.k.a prototype) for each class in a given episode, and query examples are classified based on distance to each prototype. The model is a convolutional neural network with parameters ? , that learns a d-dimensional space where examples from the same class are clustered together and those of different classes are far apart. Formally, for each episode e, a prototype p c corresponding to class c ? C base is computed by averaging the embeddings of all support samples belonging to class c:</p><formula xml:id="formula_4">p c = 1 |S c e | ? (s i ,y i )?S c e f ? (x),<label>(3)</label></formula><p>where f ? is the pretrained visual backbone, and S c e is the subset of support belonging to class c. The model generates a distribution over N classes in an episode after applying softmax over cosine similarities between the embedding of the query q j and the prototypes p c :</p><formula xml:id="formula_5">P ? (y = c|q j , S e ) = exp(?. f ? (q j ), p c ) ? k exp(?. f ? (q j ), p k ) ,<label>(4)</label></formula><p>where ., . is the cosine similarity, k ? C supp , and ? is the learnable parameter to scale the cosine similarity for computing logits <ref type="bibr" target="#b3">[4]</ref>. The model is trained by minimizing Eq. 2. In the following section, we propose capturing rich and shared class-level semantics for FSL tasks via predicting natural language descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Capturing Rich Semantics for FSL</head><p>We show that by leveraging class-level semantic descriptions, the performance of FSL tasks can be improved. To this end, we create a bottleneck visual feature (termed hybrid prototype) to generate the language descriptions of classes as an auxiliary task. We introduce a language description branch featuring a Transformer based forward and backward decoding mechanism to connect hybrid prototypes with the corresponding descriptions both in forward and backward directions. This enforces the model to capture correlations between the same class examples so as to produce consistent class level descriptions.The hybrid prototype that is obtained using the support and query visual features facilitates the extraction of shared semantics. Furthermore, our language branch allows modelling long-range dependencies between the sequence of token vectors compared to prior recurrent architectures. We elaborate these components below.</p><p>Mapping visual features to language description. For each class c ? C base , we assume to have d c class-level language descriptions </p><formula xml:id="formula_6">W c = {w 1 , w 2 , ..., w d c }. Each w i = (w i,1 , w i,2 , ....w i,T i ) : i ? {1, d c } is</formula><formula xml:id="formula_7">p c = 1 |S c e | + |Q c e | ? x?(S c e ? Q c e ) f ? (x).<label>(5)</label></formula><p>Notably, the hybrid prototypep c contrasts with p c that only averages the support features. Our language module, associates the hybrid prototypep c with the corresponding descriptions . We replicate the same architecture for both forward and backward decoding mechanisms. After the last Transformer layer, we apply a linear layer to get output un-normalized log probabilities over the token vocabulary.</p><p>W c in both forward and backward directions. Specifically, the proposed transformer decoding function g ? , parameterized by ? , takes the hybrid prototypep c and predicts class semantic descriptionsW c . It comprises a forward and a backward model which allows it to generate each descriptionw i token-by-token from left-to-right and right-to-left, respectively, and uses the following language loss for training:</p><formula xml:id="formula_8">L lang (? , ? ) = 1 2 N ? i=1 T i ? t=2 ? log g ? (w i,t = w i,t |p c ) + N ? i=1 1 ? t=T i ?1 ? log g ? (w i,t = w i,t |p c ) .</formula><p>We jointly train the (pretrained) visual backbone f ? and the language description branch g ? , in an episodic manner, after combining both the classification and language losses:</p><formula xml:id="formula_9">L train = L epi + ? L lang ,<label>(6)</label></formula><p>where ? balances the contribution of L lang towards the joint loss L train . Our overall objective provides a learning signal that facilitates aligning the model with the human semantic prior of which representations are more transferable than others. We discard the language description branch after the training loop during inference. However, it can be beneficial towards understanding the model behaviour for incorrect predictions e.g. finding which attributes were mistaken during inference. Transformer Decoder Architecture. Inspired by recent advances in language modelling, we propose to use Transformers <ref type="bibr" target="#b33">[34]</ref>, for decoding class-level descriptions in both forward and backward directions <ref type="figure" target="#fig_2">(Fig. 3)</ref>. Transformers feature multi-head self-attention and not only can propagate the contextual information over sequence of description tokens but have the expressive capability to relate the hybrid prototype to semantic tokens in class-level descriptions. In a training episode, each image is accompanied with a corresponding class description. The hybrid prototypep c of a given class is replicated to match the number of descriptions available in the episode and denoted byp rep c . To reduce the complexity of the resulting feature tensor, it is projected through a linear layer. The projected hybrid prototyp? p pro j c is then fed to the decoder module. We embed the class-level descriptions using the Embedding Module <ref type="figure">(Fig. 2)</ref> which is initialised with pretrained GloVe <ref type="bibr" target="#b23">[24]</ref>. The result is a set of embedded description tokens, which are fed to both forward and backward transformer decoders. The decoder first performs a multi-head self-attention over description token vectors and then applies multi-head attention between the projected hybrid prototype and descriptive token vectors. In each multi-head attention block, the inputs are transformed to query (Q), key (K) and value (V) triplets using a set of transformation matrices. Attention mechanism is similar to <ref type="bibr" target="#b33">[34]</ref> where the future elements of the description are masked to perform masked multi-head attention (see architecture in <ref type="figure" target="#fig_2">Fig. 3</ref>). It then applies a two-layer fully connected network to each vector. All these operations are followed by dropout, enclosed in a residual connection, and followed by layer normalization. After passing through transformer layers, we apply a linear layer which is common to both forward and backward decoders of each vector to produce un-normalized log probabilities over the token vectors. Following recent works <ref type="bibr" target="#b7">[8]</ref>, our transformer employs GELU activation <ref type="bibr" target="#b11">[12]</ref> instead of ReLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. CU-Birds <ref type="bibr" target="#b36">[37]</ref> is an image dataset with 200 different birds species each having 40-60 images. Following <ref type="bibr" target="#b30">[31]</ref>, we split the available classes into 100 for training, 50 for validation and 50 for testing. VGG-Flowers is a fine-grained classification dataset comprising 102 flowers categories. Following <ref type="bibr" target="#b30">[31]</ref>, we split the dataset into 51 for training, 26 for validation and 25 for test classes. For both CUB and VGG-Flowers datasets, we acquire natural language descriptions for the images from <ref type="bibr" target="#b25">[26]</ref> which provides 10 captions per image. Since our method leverages class-level descriptions, we sample the required number of descriptions from the (available) captions of the images belonging to each class, but those are consistently used for all the class images. miniImageNet <ref type="bibr" target="#b34">[35]</ref> is a popular dataset for few-shot classification tasks. It consists of 100 image classes extracted from the original ImageNet dataset <ref type="bibr" target="#b6">[7]</ref>. Each class contains 600 images of size 84 ? 84. We follow the splitting protocol proposed by <ref type="bibr" target="#b34">[35]</ref>, and use 64 classes for training, 16 for validation, and 20 for testing. Since, class level descriptions for this dataset are unavailable, we manually gathered them from the web. Some representative examples of class-level descriptions for miniIm-ageNet are shown in the supplementary material. ShapeWorld is a synthetic multi-modal dataset proposed by <ref type="bibr" target="#b15">[16]</ref>. It consists of 9000, 1000, and 4000 few-shot tasks for training, validation and testing, respectively. Each task has a single support set of K = 4 images that are representing a visual concept with an associated natural language description, which we consider as the class-level descriptions. Each concept describes a spatial relation between two objects, and each object is optionally qualified by color and/or shape, with 2-3 distractor shapes around. The task is to predict whether a query image belongs to the concept or not. Implementation Details. For fair comparisons with prior works, we deploy the following CNN architectures as visual backbones: 4-layer convolutional architecture proposed in <ref type="bibr" target="#b29">[30]</ref> for CUB, ResNet-12 for miniImageNet <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33]</ref>, and ResNet-18 <ref type="bibr" target="#b30">[31]</ref> for VGG-Flowers. For evaluation on all datasets, we use the challenging FSL setting of 5-way 1-shot and report accuracy averaged across few-shot tasks along with 95% confidence interval. During pretraining stage, we use SGD optimizer with an initial learning rate of 0.05, momentum of 0.9, and weight decay of 0.0005. We train the model for 100 epochs with a batch size of 64 and the learning rate decays twice by a factor of 0.1 at 60 and 80 epochs. Both forward and backward decoders are configured with a hidden layer size of 768, 12 attention heads, a feed-forward dimension of 3072 and with a 0.1 dropout probability. We use Adam optimizer with a constant learning rate of 0.0005 throughout and train the models for 600 epochs. Standard data augmentation e.g., random crop, color jittering and random horizontal flipping are applied during the meta-training stage. We fix ? = 5 in all experiments. ? is initialized as 1 for experiments with CUB and VGG-Flowers while for miniImageNet experiments it's initialized as 10. We use 2 layers of transformer decoders based on validation and study  the effect of different layers in <ref type="figure" target="#fig_3">Fig. 4</ref>. We use 20 class-level descriptions for both CUB and VGG-Flowers while for miniImageNet we use all 5 descriptions available per class. During inference, we discard the language description branch and rely on the visual backbone to perform few-shot classification. To be consistent with previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31]</ref>, we sample 600 few-shot tasks from the set of novel classes. For ShapeWorld dataset, following <ref type="bibr" target="#b21">[22]</ref> we train for 50 epochs with a constant learning rate of 0.00005 with Adam optimizer. During training, we use ? = 20 and similar transformer decoder architecture parameters as the experiments in other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with state-of-the-art</head><p>We compare the performance of our method with eleven existing top performing approaches on CUB dataset in Tab. 1 (right). Our method delivers a significant improvement of 6.36% over a strong baseline method <ref type="bibr" target="#b3">[4]</ref>. Tab. 1 (left) reports experimental results on miniIma-geNet. RS-FSL provides an improvement of 2.16% over the competitive baseline <ref type="bibr" target="#b3">[4]</ref>. Compared to prior works, our method attains a higher accuracy of 65.33% and demonstrates the best performance. Experimental results for ShapeWorld dataset are reported in Tab. 2(b). RS-FSL outperforms the existing best performing method LSL <ref type="bibr" target="#b21">[22]</ref> by a margin of 1.15%. Further, the results for VGG-Flowers dataset are shown in Tab. 2(a). RS-FSL performs favorably against all competing methods and achieves the best accuracy of 75.33%. Overall, RS-FSL consistently show gains on all four datasets. We note the improvement is more significant in CUB as compared to others since the class samples conform better with the language descriptions as compared to e.g., Flowers dataset. Thus our class-level descriptions show more effectiveness there. The increment on miniImageNet is relatively less pronounced due to the limited class descriptions obtained manually by us (only 5 per class). We also display some qualitative examples in supplementary material which show that RS-FSL encourages the model to focus on semantically relevant visual regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis and Ablation Study</head><p>We perform all ablation experiments on CUB dataset with a Conv-4 visual backbone architecture following ProtoNet <ref type="bibr" target="#b29">[30]</ref> baseline.</p><p>(a) Performance on VGG-Flowers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Accuracy</head><p>ProtoNet <ref type="bibr" target="#b29">[30]</ref> 72.38?0.98 Matching Net <ref type="bibr" target="#b34">[35]</ref> 73.51?0.94 MAML <ref type="bibr" target="#b9">[10]</ref> 65.46?1.05 Chen et al. <ref type="bibr" target="#b2">[3]</ref> 74.09?0.84 Relation Net <ref type="bibr" target="#b31">[32]</ref> 55.59?1.09 Meta-Baseline <ref type="bibr" target="#b3">[4]</ref>    <ref type="bibr" target="#b21">[22]</ref>.</p><p>The effect of different baselines and word embeddings. To demonstrate the generalizability of our approach, we show its improvements across three popular baseline FSL approaches both with and without using language prediction during training (see Tab. 3(a)). We note that the proposed language prediction mechanism constrained on a bottleneck visual feature (hybrid prototype) consistently improves the performance under all three baseline methods: ProtoNet <ref type="bibr" target="#b29">[30]</ref>, RFS <ref type="bibr" target="#b32">[33]</ref>, and Meta-Baseline <ref type="bibr" target="#b3">[4]</ref>. Tab. 3(b) reports the impact on performance when using three different word embeddings to represent the words: Word2Vec <ref type="bibr" target="#b19">[20]</ref> , GloVE <ref type="bibr" target="#b23">[24]</ref>, and fastText <ref type="bibr" target="#b1">[2]</ref>. Our method retains similar accuracies under both Word2Vec and GloVe, however, it performs slightly inferior when deploying fastText. This reveals that RS-FSL is robust to the choice of word embeddings and favorable gains are obtained over the baseline method regardless of the embedding type used.  <ref type="table">Table 3</ref>: (a) Performance of different baselines both with and without our rich semantic (RS) modeling and (b) performance when using three different word embeddings.</p><p>Number of class-level descriptions. <ref type="figure" target="#fig_3">Fig. 4 (left)</ref> shows that upon increasing the number of class-level descriptions from 1 to 20 the accuracy peaks to a maximum of 63.86%, however, it starts to saturate after increasing beyond 20. This could be because beyond a certain number of class-level descriptions, the semantic attributes collected from the available descriptions possibly become saturated, rendering the additional descriptions redundant.    <ref type="table">Table 4</ref>: Performance using only the forward decoding and the developed bidirectional decoding. <ref type="figure" target="#fig_3">Fig. 4 (right)</ref> shows that our method retains the highest accuracy (63.86%) when employing two Transformer layers. However, it starts to deteriorate after further increasing the number of layers i.e. 3 to 5. The inferior performance is most likely due to over-fitting caused by the over-parameterization of the language description model given a relatively small dataset. Language decoding mechanisms. We replace the bidirectional language decoding mechanism with just the forward decoding and observe that the former improves accuracy by 1.83% compared to latter (Tab. 4). Bidirectional decoding can better relate the visual cues with language semantics as it can model two-way interaction between the tokens, thereby facilitating the learning of generalizable visual representations vital for few-shot scenarios. Auxiliary self-supervision. We compare the performance of different auxiliary self-supervised approaches with our proposed method (Tab. 5). The first auxiliary self-supervised task is predicting the rotation angle of the visual input <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref>, and the others are predicting language using LSTM-GRU based recurrent architecture (ProtoNet +GRU) <ref type="bibr" target="#b21">[22]</ref> and predicting the semantic word embedding corresponding to the prototype (ProtoNet + Word Embeddings). We observe that the proposed transformer based (bidirectional) language decoding mechanism significantly improves the performance (5.97%) over the method that is not using any auxiliary self-supervision (ProtoNet (without semantics)). Further, our approach outperforms the other auxiliary self-supervision methods, Proto+Rotation, Proto+Word Embeddings and Proto+GRU, by a margin of 4.6%, 3.61% and 2.62% respectively. <ref type="figure" target="#fig_4">Fig. 5</ref> shows class-level descriptions generated by RS-FSL for novel query images (in CUB dataset) during inference. We note that generated descriptions allow us interpreting the model behaviour for incorrect prediction, e.g. finding the bird attributes that are confused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Accuracy</head><p>ProtoNet (without semantics) 57.97?0.96 ProtoNet + Rotation 59.20?0.97 ProtoNet + Word Embeddings 60.25?0.93 ProtoNet + GRU <ref type="bibr" target="#b21">[22]</ref> 61.24?0.96 RS-FSL + Class Descriptions 63.86?0.91 <ref type="table">Table 5</ref>: Comparison between different auxiliary training methods. Average few-shot 5-way 1-shot accuracy reported with 95% confidence interval Extra Cost vs Performance boost. We use Nvidia-RTX Quadro 6000 single-GPU for our training. We observed that training in miniImageNet dataset without auxiliary supervision took around 4 hours for training while RS-FSL took 5 hours with additional transformer decoder layers. Further our model obtains a performance boost of 2% over the baseline in miniImageNet (Tab. 1 left).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a new FSL approach that models rich semantics shared across few-shot examples. This is realized by leveraging class-level descriptions, available with less annotation effort. We create a hybrid prototype which is used to produce class-level language predictions as an auxiliary task while training. We develop a Transformer based bi-directional decoding mechanism to connect visual cues with semantic descriptions to enrich the visual features. Experiments on four datasets show the benefit of our approach. <ref type="figure" target="#fig_6">Fig. 7</ref> visualizes qualitative examples, where RS-FSL encourages the model to focus on semantically relevant visual regions in order to obtain a correct classification. Specifically, we show example predictions with corresponding attention maps from the meta-baseline <ref type="bibr" target="#b3">[4]</ref> (red) and our method (green) on five different few-shot tasks in VGG-Flowers dataset. Ground-truth labels are shown in blue. We note that, for all query images, RS-FSL produces better attention maps, focused on semantically relevant visual features, which allows it to produce correct classifications.  our method (green) on five different few-shot tasks in VGG-Flowers dataset. Ground-truth labels are shown in blue. We note improvements in our attention maps leading to correct predictions in visually similar classes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>In the standard few-shot image classification setting, we have access to a labelled dataset of base classes C base with enough number of examples in each class, and the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>a language description of variable length T i tokens where w i,1 = s represents the start of the sentence token and w i,T i = /s denotes the end of the sentence token. Letp c be the hybrid prototype formed after averaging the embeddings of support examples S c e and query examples Q c e of class c, as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Transformer decoder architecture (bottom box) for generating classlevel natural language descriptions based on multi-head attention (top box)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Performance upon varying the number of class-level descriptions (left) and the number of Transformer decoder layers (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>?Figure 5 :</head><label>5</label><figDesc>This bird has a fat bill and a short stubby tail with a blue streak on its wingThis large with red eyes a belly and burgundy colored throat ? This bird is brown and greyish with a very dark eye and black feathers on its wing ? ? This is a dark orange bird with a yellow wings flank and tail and white throat Class-level descriptions generated by RS-FSL for novel query images during inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Some representative examples of class-level descriptions for miniImageNet dataset. As these descriptions are not readily available, we manually collected them from the web.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Example predictions with corresponding attention maps from the meta-baseline (red) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with prior works on CUB and miniImageNet. Our method, RS-FSL, allows exploiting semantic information during training only.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison on VGG-Flowers and ShapeWorld.</figDesc><table /><note>? Results reported in</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2021. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:2104.12709v2 [cs.CV] 12 Nov 2021</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Rich Semantics Improve Few-shot Learning Qualitative Examples. <ref type="figure">Fig. 6</ref> displays some examples of class-level descriptions for mini-ImageNet dataset. Since these descriptions were not readily available, we manually collected them from the web. For a given class, all descriptions have been collected such that they mostly represent the shared semantics across that class. For instance, for a frying pan class, the second description has attributes like 'flat-bottomed pan', 'shallow sides', and 'slightly curved' that are shared by the three shown examples belonging to this class.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning with latent language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2166" to="2179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Metabaseline: Exploring simple meta-learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9062" to="9071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-level semantic feature augmentation for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4594" to="4605" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Contrastive constraints guide explanation-based category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Cantelon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1645" to="1655" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Virtex: Learning visual representations from textual annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11162" to="11173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Annotator rationales for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1395" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boosting few-shot visual learning with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8058" to="8067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>Gaussian error linear units (gelus</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Syed Afaq Ali Shah, and Mohammed Bennamoun. A guide to convolutional neural networks for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Rahmani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="207" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01169</idno>
		<title level="m">Transformers in vision: A survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hyperbolic image embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valentin Khrulkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mirvakhabova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6417" to="6427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Shapeworld -a new test methodology for multimodal language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kuhnle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><forename type="middle">A</forename><surname>Copestake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subhransu Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10649" to="10657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Revisiting local descriptor based image-to-class measure for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7253" to="7260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Alice: Active learning with contrastive natural language explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4380" to="4391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Explanation in artificial intelligence: Insights from the social sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shaping visual representations with language for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodr?guez L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Self-supervised knowledge distillation for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jathushan</forename><surname>Rajasegaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09785</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring complementary strengths of invariant and equivariant representations for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Mamshad Nayeem Rizve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10836" to="10846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01905</idno>
		<title level="m">Baby steps towards few-shot learning with multiple semantics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to recognize objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Linda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="244" to="250" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">When does self-supervision improve few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Chyi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="645" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: A good embedding is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="266" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearest-neighbor classification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04623</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive cross-modal few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro O O</forename><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Openvocabulary object detection using captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">Dela</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">Hao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="14393" to="14402" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
